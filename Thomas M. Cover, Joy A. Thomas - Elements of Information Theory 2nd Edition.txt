2.1
a.  H(X)    = -Σp(x)logp(x)
    where p(x) = (1/2)^x
            = -Σ{x ∈ [1, ∞)}(1/2)^xlog(1/2)^x
            = Σ{x ∈ [1, ∞)}(1/2)^x * x
            = E[X], X ~ Geometric(1/2)
            = 2
b.  S = {1}, {1, 2}, {1, 2, 3}, ...
    let P be the number of questions required to determine X
    if X = x, the first x - 1 questions have answer no, the last question has answer yes
    P = X, P ~ Geometric(1/2), E[P] = E[X] = 2 = H(X)

2.2
a.  let D(X) and D(Y) be the domain of X and Y
    2^a = 2^b iff a = b, f(x) = 2^x is a bijection between D(X) and D(Y)
    I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X), as X is fixed given Y and vice versa,
    H(X|Y) = H(Y|X) = 0, H(X) = H(Y)
b.  f(x) = cosx is not a bijection between D(X) and D(Y): some different values of X may be mapped to the same Y
    Y is still fixed by X, H(Y|X) = 0
    H(X) - H(X|Y) = H(Y), H(X|Y) >= 0, H(X) >= H(Y)

2.3
H(p)    = Σpilogpi
pilogpi = 0 iff pi = 0 (by definition) or pi = 1
as p is a probability vector, Σpi = 1
the probability vector p that minimizes H(p) is for some k
    pi = 1, i = k
    pi = 0, i != k
and there are n such vectors in total

2.4
a.  chain rule (equation 2.14)
b.  H(g(X) | X) = 0 as given X, g(X) is a constant
c.  chain rule (equation 2.14), another possible expansion
d.  H(X | g(X)) >= 0, non-negativity of entropy

2.5
H(Y|X)  = -Σ{x}Σ{y}p(x, y)logp(y|x) = 0
as p(x, y) >= 0, for all p(x, y) > 0, logp(y|x) = 0, p(y|X = x) = 1, p(y|X != x) = 0
therefore given X there's only one value of Y with probability 1 for all X

2.6
a.  I(X;Y|Z)    = H(X|Z) - H(X|Y, Z)
    I(X;Y)      = H(X) - H(X|Y)
    if H(X) > 0, X = Y = Z, 
    I(X;Y|Z) = H(X|Z) - H(X|Y, Z) = 0 < H(X) = H(X) - H(X|Y) = I(X;Y)
b.  let X and Z be independent, Y = X + Z, H(X) > 0
    I(X;Y|Z) = H(X|Z) - H(X|Y, Z) = H(X) > H(X) - H(X|Y) = I(X;Y)

2.7
a.  (follow same logic as to determine the lower bound of comparison sort)
    for n coins there are 2n + 1 possibilities: one of them are lighter, one of them are heavier, or all fair
    a weighting have 3 outcomes: left is heavier, right is heavier or equal
    k weightings form a ternary decision tree of height k with 3^k leaves
    2n + 1 <= 3^k
    n <= (3^k - 1) / 2
b.  // thanks https://en.wikipedia.org/wiki/Balance_puzzle#Twelve-coin_problem
    divide the coins into three piles of 4 coins A, B and C
    weight A and B, if A is heavier 
        C is a pile of fair coins 
        move 3 coins of A aside as A', move 3 coins from B to A, move 3 coins from C to B, weight A and B again
        if A is still heavier
            either the coin in A both times is heavier, or the coin in B both times is lighter
            weight one of them with a fair coin in C to determine the counterfeit coin
        if A is now lighter
            one of the three coins moved from B to A is lighter
            weight two of them, if equal the third one is lighter, otherwise the lighter is counterfeit
        if A and B are now equal
            one of the coins in A' is heavier
            weight two of them, if equal the third one is heavier, otherwise the heavier is counterfeit
    if A is lighter
        symmetric
    if A and B are equal
        one of C is counterfeit, (n, k) = (4, 2)
        weight 2 of C with 2 of A or B reduces the problem to (n, k) = (2, 1)
        and then any coin of (2, 1) problem can be compared to a fair coin 

2.8
let Xi be the result of ith drawing
with replacement H(Xi) = H(X1) 
    = - (r / (r + w + b))log(r / (r + w + b))
      - (w / (r + w + b))log(r / (r + w + b))
      - (b / (r + w + b))log(b / (r + w + b))
with replacement, for i > 1, let ri, wi, bi be the number of red, white and black balls upon ith drawing
    ri / (ri + wi + bi)
    = ri / (r + w + b - i + 1)
    >= (r - i + 1) / (r + w + b - i + 1)
    > r / (r + w + b)
    similar for wi / (ri + wi + bi) and bi / (ri + wi + bi)
    since f(x) = xlogx is strictly increasing, H(Xi) < H(X1), entropy is lower without replacement

2.9
a.  1.  H(X|Y) >= 0, H(Y|X) >= 0, H(X|Y) + H(Y|X) >= 0
    2.  H(X|Y) + H(Y|X) is symmetric
    3.  H(X|Y) + H(Y|X) = 0 iff H(X|Y) = H(Y|X) = 0
        H(X|Y) = 0 iff X is fixed given Y
        H(Y|X) = 0 iff Y is fixed given X, one to one function between X and Y
    4.  H(X|Y) + H(Y|Z) >= H(X|Y, Z) + H(Y|Z)   // conditioning cannot increase entropy
                        = H(X, Y|Z)             // chain rule
                        = H(X|Z) + H(Y|X, Z)    // chain rule
                        >= H(X|Z)
        similarly H(Z|Y) + H(Y|X) >= H(Z|X), ρ(x, y) + ρ(y, z) >= ρ(x, z)
b.  1.  I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
        H(X) + H(Y) - 2I(X;Y) = H(X|Y) + H(Y|X)
    2.  H(X) + H(Y) - 2I(X;Y)
        = H(X) + H(Y) - (H(X) + H(Y) - H(X, Y)) - I(X;Y)
        = H(X, Y) - I(X;Y)
    3.  apply I(X;Y) = H(X) + H(Y) - H(X, Y) again

2.10
a.  H(X)    = -Σp(x)logp(X)
            = -Σαp1(x)logαp1(x) - Σ(1-α)p2(x)log(1-α)p2(x)
            = -αΣp1(x)logα - αΣp1(x)logp1(x) - (1-α)Σp2(x)log(1-α) - (1-α)Σp2(x)logp2(x)
            = -αlogα - (1-α)log(1-α) + αH(X1) + (1-α)H(X2)
b.  αH(X1) + (1-α)H(X2) is linear wrt. α
    d^2(-αlogα - (1-α)log(1-α))/d^2α
    = -1/αln2 - 1/(1-α)ln2 < 0 is concave
    dH(X1)/dα = logα - 1/ln2 + log(1-α) + 1/ln2 + H(X1) - H(X2) = 0
    log((1-α)/α) = H(X2) - H(X1)
    α = 2^H(X1) / (2^H(X1) + 2^H(X2))
    maxH(X) = -2^H(X1) / (2^H(X1) + 2^H(X2)) * (H(X1) - log(2^H(X1) 
            + 2^H(X2))) -2^H(X2) / (2^H(X1) + 2^H(X2)) * (H(X2) - log(2^H(X1) + 2^H(X2))) 
            + 2^H(X1) / (2^H(X1) + 2^H(X2))H(X1) + 2^H(X2) / (2^H(X1) + 2^H(X2))H(X2)
            = 2^H(X1) / (2^H(X1) + 2^H(X2))log(2^H(X1) + 2^H(X2)) + 2^H(X2) / (2^H(X1) + 2^H(X2))log(2^H(X1) + 2^H(X2))
            = log(2^H(X1) + 2^H(X2))
    2^H(X)  <= 2^H(X1) + 2^H(X2)
    effective alphabet size of X is smaller than or equal to the combined alphabet size of X1 and X2

2.11
a.  I(X1;X2)/H(X1)  = (H(X1) - H(X1|X2)) / H(X1)
                    = 1 - H(X1|X2)/H(X1) = ρ
b.  0 <= I(X1;X2) = H(X1) - H(X1|X2) <= H(X1)
    0 <= ρ = I(X1;X2) / H(X1) <= 1
c.  ρ = 0 iff I(X1;X2) = 0 iff X1 and X2 are independent
d.  ρ = 1 iff I(X1;X2)/H(X) = 1 iff H(X1|X2) = 0
    X1 is a function of X2 or vice versa

2.12
a.  H(X) = -2/3log2/3 - 1/3log1/3 ≒ 0.918
    H(Y) = H(X) ≒ 0.918
b.  H(X|Y)  = p(Y = 0)H(X|Y = 0) + p(Y = 1)H(X|Y = 1)
            = 0 + 2/3 * 1 = 2/3
    H(Y|X)  = p(X = 0)H(Y|X = 0) + p(X = 1)H(Y|X = 1)
            = 2/3 * 1 + 1/3 * 0 = 2/3
c.  H(X, Y) = H(X) + H(Y|X) ≒ 1.585
d.  H(Y) - H(Y|X) ≒ 0.251
e.  I(X;Y)  = H(Y) - H(Y|X) = 0.251
f.  see figure 2.2

2.13
// thanks solutions manual
the Taylor expansion of ln(x) around 1 is 
    ln(x) = ln(1) + (x-1) - (x - 1)^2 / 2c^2 
for some c between 1 and x
the third term is always negative, ln(x) <= x - 1
substitute x by 1/y, 
    ln(1/y) <= 1/y - 1
    -lny <= 1/y - 1
    lny >= 1 - 1/y

2.14
a.  H(X, Y, Z)  = H(X) + H(Y|X) + H(Z|Y, X) = H(X) + H(Y|X)
                = H(X) + H(Z|X) + H(Y|Z, X) = H(X) + H(Z|X)
    H(Y|X) = H(Z|X)
    H(Z) >= H(Z|X) = H(Y|X) = H(Y)
    similarly H(Z) >= H(X)
b.  X is a random variable with H(X) > 0, Y = -X, Z = X + Y = 0
    H(X) = H(Y) > 0 = H(Z)
c.  either X or Y is constant, then there's a one-to-one function between X and Z, H(X) + H(Y) = H(X) = H(Z)

2.15
I(X1;X2, .., Xn)    = I(X2, .., Xn;X1)
                    = I(X2;X1) + I(X3;X1|X2) + .. + I(Xn;X1|Xn-1, .., X2)
for all 1 <= i <= n, 
    p(x1, xi|x2, .., xi-1)
    = p(x1, .., xi) / p(x2, .., xi-1)
    = p(x1, x2)p(x3|x2)..p(xi|xi-1) / p(x2)p(x3|x2)..p(xi-1|xi-2)
    = p(x1, x2)p(xi|xi-1) / p(x2)
    = p(x1|x2)p(xi|xi-1)
    = p(x1|x2, .., xi-1)p(xi|x2, .., xi-1)  // definition of Markov chain
hence X1 and Xi are independent given X2, .., Xi-1, I(Xi;X1|X2, .., Xi-1)
I(X1;X2, .., Xn)    = I(X2;X1)

2.16
a.  I(X1;X3)    <= I(X1;X2) // data processing inequality
                = H(X2) - H(X2|X1)
                <= H(X2) <= log|X2| = logk
b.  I(X1;X3) <= logk = 0, X1 and X3 are independent

2.17
a.  X1, .., Xn are iid, H(X1, .., Xn) = ΣH(Xi) = nH(p)
b.  (Z1, .., Zk, K) is a function of (X1, .., Xn), by problem 2.2 H(Z1, .., Zk, K) <= H(X1, .., Xn)
c.  chain rule of entropy
d.  H(Z1, .., Zk|K) = Σp(K = k)H(Z1, .., Zk|K = k)
    where (Z1, .., Zk) is a uniform distribution with 2^k values, H(Z1, .., Zk|K = k) = k
                    = Σp(K = k)k = E[K]
e.  H(K) >= 0, non-negativity of entropy
f(X) = 0 if there are more 1s in first two characters than the last 2 characters
f(X) = 1 if there are more 1s in last two characters than first 2 characters
f(X) = ε otherwise

2.18
// ((n, k)) is n multichoose k
2 sequences of 4 matches, each with probability (1/2)^4 = (1/2)^4
2 * ((4, 1)) = 2 * C(4, 1) = 8 sequences of 5 matches, p = (1/2)^5 each
2 * ((4, 2)) = 2 * C(5, 2) = 20 sequences of 6 matches, p = (1/2)^6 each
2 * ((4, 3)) = 2 * C(6, 3) = 40 sequences of 7 matches, p = (1/2)^7 each
P(Y = 4) = 2 * (1/2)^4 = 1/8
P(Y = 5) = 8 * (1/2)^5 = 1/4
P(Y = 6) = 20 * (1/2)^6 = 20/64 = 5/16
P(Y = 7) = 40 * (1/2)^7 = 40/128 = 5/16
H(X)    = 2 * (1/2)^4 * 4 + 8 * (1/2)^5 * 5 + 20 * (1/2)^6 * 6 + 40 * (1/2)^7 * 7
        = 5.8125
H(Y)    = -(1/8log(1/8) + 1/4log(1/4) + 5/16log(5/16) + 5/16log(5/16))
        = 1.924
H(Y|X)  = 0, length of sequence is fixed given the sequence
H(X|Y)  = H(X, Y) - H(Y)
        = H(X) + H(Y|X) - H(Y)
        = 3.8885

2.19
H(X)    = -Σp(x)logp(x)
        = -Σ(Anlog^2n)^-1log(Anlog^2n)^-1
        = Σ(logA / Anlog^2n) + Σ(1 / Anlogn) + Σ(2loglogn / Anlog^2n)
the first and third terms are nonnegative
the second term is infinite as
Σ(1 / nlogn)    >= int(2, +∞, 1/nlogn)
                = int(2, +∞, ln2/nlnn)
                = int(2, +∞, ln2 * lnlnn)
                = +∞

2.20
H(Xn, R) = H(X):
    obviously (Xn, R) is a function of X
    given any Xi and the run-length code, the sequence can be recovered, X is a function of any Xi and R
H(R) < H(X):
    R is a function of X, and two sequences of X may be mapped to the same run-length coding (e.g. 01 and 10)
    more precisely for any 1 <= i <= n,
    H(X)    = H(Xi, R)
            = H(Xi) + H(R|Xi)
            <= H(Xi) + H(R)
            <= log2 + H(R)
    H(X) - H(R) <= log2 = 1

2.21
P(p(X) <= d)log1/d
= -Σ{x: p(x) <= d}p(x)logd
<= -Σ{x: p(x) <= d}p(x)logp(x)  // p(x) <= d <= 1, -logp(x) >= logd
<= -Σ{x}p(x)logp(x) = H(X)

2.22
a.  chain rule of mutual information can be derived from chain rule of entropy (equation 2.62 - 2.64)
    I(X1, X2;Y) = D(p(x1, x2, y) || p(x1, x2)p(y))
                // q(x1, x2, y) = p(x1, x2)p(y), (x1, x2) and y are independent
                = D(p(x1, y) || p(x1)p(y)) + D(p(x2|x1, y) || p(x2|x1)) 
                = I(X1;Y) + E[log(p(X2|X1, Y) / p(X2|X1))]
                = I(X1;Y) + E[log(p(X2, Y|X1) / p(X2|X1)p(Y|X1))]
                = I(X1;Y) + D(p(x2, y|x1) || p(x2|x1)p(y|x1))
                = I(X1;Y) + I(X2;Y|X1)
    chain rule of mutual information can be derived from chain rule of relative entropy
    no order between chain rule of entropy and mutual entropy
b.  Jensen's inequality => D(p||q) >= 0 (equation 2.83 - 2.89) => I(X;Y) >= 0 (text proof of equation 2.90)

2.23
given values of n-1 out of n variables, the last is fixed
for any i < n, Xi and (X1, .., Xi-1) are independent since:
    when n is even, p(Xi = 1) = p(Xi = 0) = 1/2:
        flip each bit of the sequence won't change the number of 1s
        each even sequence with Xi = 0 has a corresponding even sequence with Xi = 1
    when n is odd and n >= 3, p(Xi = 1) = p(Xi = 0) = 1/2:
        n = 3, all even sequences are 000, 110, 101, 011, p(Xi = 0) = p(Xi = 1) = 1/2 for all i
        for n > 3, remove any two variables from the sequence that's not Xi
        if the removed variables have even number of 1s, p(Xi = 1) = p(Xi = 0) by induction
        if the removed variables have odd number of 1s, flip the remaining sequence won't change its oddity
    when (X1, .., Xi-1) has even number of 1s, (Xi, .., Xn) has even number of 1s and length >= 2, p(Xi = 1) = p(Xi = 0)
    when (X1, .., Xi-1) has odd number of 1s, similarly p(Xi = 1) = p(Xi = 0)
I(Xn-1;Xn|X1, .., Xn-2) = H(Xn-1|X1, .., Xn-2) - H(Xn|X1, .., Xn-1)
                        = H(Xn-1|X1, .., Xn-2)
                        = 1
I(Xi-1;Xi|X1, .., Xi-2) = H(Xi-1|X1, .., Xi-2) - H(Xi|X1, .., Xi-1)
                        = 0

2.24
a.  -1/4log1/4 - 3/4log3/4
    = 1/2 - 3/4(2 - log3)
    = 1/2 - 3/4(1.584 - 2)
    = 0.812
b.  f(x) = 1/(1-0) = 1 on [0, 1]
    int(0, 1, -plogp - (1-p)log(1-p)) = 0.721
c.  // skipped

2.25
let X and Y be independent random variables, Z = X + Y, H(X|Z) > 0 (e.g. X and Y are uniform binary iid)
I(X;Y;Z)    = I(X;Y) - I(X;Y|Z)
            = 0 - I(X;Y|Z)
            = 0 - H(X|Z) + H(X|Y, Z)
            = -H(X|Z) < 0
a.  H(X, Y, Z) - H(X) - H(Y) - H(Z) + I(X;Y) + I(Y;Z) + I(Z;X)
    = H(Z) + H(Y|Z) + H(X|Y, Z) - H(X) - H(Y) - H(Z) + I(X;Y) + I(Y;Z) + I(Z;X)
    = -I(Y;Z) + I(Y;Z) + H(X|Y, Z) - H(X) + I(Z;X) + I(X;Y)
    = H(X|Y, Z) - H(X) + H(X) - H(X|Z) + I(X;Y)
    = I(X;Y) - I(X;Y|Z)
b.  H(X, Y, Z) - H(X, Y) - H(Y, Z) - H(Z, X) + H(X) + H(Y) + H(Z)
    = H(X, Y, Z) + (H(X) + H(Y) - H(X, Y)) + (H(X) + H(Z) - H(X, Z)) + (H(Y) + H(Z) - H(Y, Z)) - H(X) - H(Y) - H(Z)
    = H(X, Y, Z) + I(X;Y) + I(X;Z) + I(Y;Z) - H(X) - H(Y) - H(Z)
    = I(X;Y;Z)

2.26
a.  from problem 2.13, lnx >= 1 - 1/x for x > 0
    let y = 1/x, ln(1/y) >= 1 - y for y > 0
    -lny >= 1 - y, lny <= y - 1
b.  -D(p||q)    = -Σp(x)log(p(x) / q(x))
                = Σp(x)log(q(x) / p(x))
                <= Σp(x)(q(x) / p(x) - 1)   // apply part a
                <= Σ(q(x) - p(x))
                = Σq(x) - Σp(x) = 0
c.  lny = y - 1 iff y = 1
    q(x) / p(x) = 1, p(x) = q(x) for all x

2.27
H(q) + (pm-1 + pm)H(pm-1 / (pm-1 + pm), pm / (pm-1 + pm))
= H(q) - (pm-1 + pm)(pm-1 / (pm-1 + pm)log(pm-1 / (pm-1 + pm)) + pm/(pm-1 + pm)log(pm/(pm-1 + pm)))
= H(q) - (pm-1logpm-1 - pm-1log(pm-1 + pm) + pmlogpm - pmlog(pm-1 + pm))
= H(q) - pm-1logpm-1 - pmlogpm + (pm-1 + pm)log(pm-1 + pm)
= H(q) - pm-1logpm-1 - pmlogpm + qm-1logqm-1
= H(p)

2.28
f(x) = xlogx is strictly convex on (0, 1), so
    -pilogpi / 2 - pjlogpj / 2 <= -(pi/2 + pj/2)log(pi/2 + pj/2)
    -pilogpi - pjlogpj <= (pi + pj)log((pi + pj) / 2)
equal only when pi = pj
by convexity a more uniform distribution has higher entropy 

2.29
a.  H(X, Y|Z)   = H(X|Z) + H(Y|X, Z)
                >= H(X|Z)
    equal when H(Y|X, Z) = 0, Y is a function of (X, Z)
b.  I(X, Y;Z)   = I(X;Z) + I(Y;Z|X)
                >= I(X;Z)
    equal when I(Y;Z|X) = 0, Y and Z are independent given X
c.  H(X, Y, Z) - H(X, Y)    = H(X, Y) + H(Z|X, Y) - H(X, Y)
                            = H(Z|X, Y)
                            <= H(Z|X)
                            = H(X, Z) - H(X)
    equal when H(Z|X, Y) = H(Z|X), Z and Y are independent given X
d.  I(Z;Y|X) - I(Z;Y) + I(X;Z)
    = H(Z|X) - H(Z|Y, X) - H(Z) + H(Z|Y) + H(Z) - H(Z|X)
    = H(Z|Y) - H(Z|Y, X)
    = I(X;Z|Y)
    always equal

2.30
// thanks solutions manual
by lagrange multiplier, measure entropy in nats
L(p, l1, l2)    = H(X) - l1(Σpi - 1) - l2(Σipi - A)
                = -Σpilogpi - l1(Σpi - 1) - l2(Σipi - A)
dL/dpi  = -logpi - 1 - l1 - l2i = 0
dL/dl1  = Σpi - 1 = 0
dL/dl2  = Σipi - A = 0
logpi = -1 - l1 - l2i
pi = e^(-1 - l1 - l2i) = ab^i for some constants a and b
as H(X) is concave to pi the range (0, 1), pi = ab^i is the global maximum
Σpi = Σab^i = a/(1-b) = 1
Σipi = Σiab^i = ab/(1-b)^2 = A
b/(1-b) = A, b = A/(A+1), a = 1-b = 1/(A+1)
pi = A^i / (A+1)^(i+1)
H(X)    <= -Σpilogpi 
        = -Σab^ilog(ab^i)
        = -alogaΣb^i - alogbΣib^i
        = -aloga/(1-b) - alogb * b/(1-b)^2
        = -loga - Alogb
        = log(A + 1) - AlogA + Alog(A + 1)
        = (A + 1)log(A + 1) - AlogA

2.31
as X -> Y -> g(Y), 
    I(X;Y) = H(X) - H(X|Y) >= I(X;g(Y)) = H(X) - H(X|g(Y))
    H(X|g(Y)) >= H(X|Y)
equality iff I(X;Y|g(Y)) = 0, or given g(Y) X and Y are independent
either X and Y are independent unconditionally
or g(Y) contains all the mutual information between X and Y

2.32
a.  f(a) = 1, f(b) = 2, f(c) = 3
    // can be solved algebraically
    // but in principle artificial randomness independent to X won't help
    Pe  = Σ{x, y}p(y)P(f(y) != x)
        = Σ{y}(p(y) * 1/2)
        = 1/2
b.  H(Pe) + Pelog|X| >= H(X|f(Y)) = H(X|Y)  // f(Y) is a bijection
    H(X|Y)  = Σp(y)H(X|Y = y)
            = 1/3 * (-2/4 * log(1/4) - 1/2 * log(1/2)) * 3
            = 3/2
    H(Pe) + Pelog3 >= 3/2
    -PelogPe - (1-Pe)log(1-Pe) + Pelog3 >= 3/2
    Pe >= 0.416 (Pe = 1/2)

2.33
X is equivalent to a mixture of two disjoint random variables X1 and X2 where
    p(X = X1) = p1
    p(X = X2) = 1 - p1
    D(X1) = {1}
    D(X2) = {2, .., m}
from problem 2.10 H(X) = H(p1) + p1H(X1) + (1-p1)H(X2) = H(p1) + (1-p1)H(X2)
to maximize H(X2), X2 must be uniformly distributed, p2 = p3 = .. = pm
H(X)    <= H(p1) + (1-p1)log|X2|
H(1 - Pe) + Pelog|X - 1| >= H(X)

2.34
I(X0;Xi)    = H(X0) - H(X0|Xi)
            >= I(X0;Xi+1)
            = H(X0) - H(X0|Xi+1)
H(X0|Xi+1) <= H(X0|Xi)

2.35
H(p)    = -1/2 * log1/2 - 2 * 1/4 * log1/4
        = 1/2 + 1 = 3/2
H(q)    = -3 * 1/3 * log1/3
        = 1.585
H(p||q) = Σp(x)log(p(x) / q(x))
        = 1/2log3/2 + 2 * 1/4log3/4
        = 0.0850
H(q||p) = Σq(x)log(q(x) / p(x))
        = 1/3 * log(2/3) + 2 * 1/3 * log(4/3)
        = 0.0817

2.36
p(0) = q(1) = 2/3
p(1) = q(0) = 1/3
H(p||q) = Σp(x)log(p(x) / q(x))
        = 2/3 * log2 + 1/3 * log(1/2)
H(q||p) = Σq(x)log(q(x) / p(x))
        = 1/3 * log(1/2) + 2/3 * log2
    
2.37
Σ{x, y, z}p(x, y, z)log(p(x, y, z) / p(x)p(y)p(z))
= Σ{x, y, z}p(x, y, z)log(p(x, y|z) / p(x)p(y))
= Σ{x, y, z}p(x, y, z)logp(x, y|z) - Σ{x, y, z}p(x, y, z)logp(x) - Σ{x, y, z}p(x, y, z)logp(y)
= H(X) + H(Y) + Σ{x, y, z}p(x, y|z)p(z)logp(x, y|z)
= H(X) + H(Y) + Σ{z}p(z)Σ{x, y}p(x, y|z)logp(x, y|z)
= H(X) + H(Y) - H(X, Y|Z)
when H(X) + H(Y) - H(X, Y|Z) = 0, 
    H(X) + H(Y) = H(X, Y|Z)
    H(X) + H(Y) = H(X|Z) + H(Y|X, Z)
where H(X) >= H(X|Z) and H(Y) >= H(Y|X, Z)
hence H(X) = H(X|Z) and H(Y) = H(Y|X, Z), X and Z are independent, Y and (X, Z) are independent

2.38
by problem 2.10, X is equivalent to
    p(X = X1) = α
    p(X = X2) = 1 - α
    p(X1 = x) = p(x) / α if x ∈ S
    p(X2 = x) = p(x) / (1 - α) if x ∉ S
H(X) = H(α) + αH(X1) + (1 - α)H(X2)
H(X|Y)  = Σp(y)H(X|Y = y)
        = αH(X1) + (1 - α)H(X2)
H(X) - H(X|Y) = H(α)

2.39
a.  H(X, Y, Z)  = H(X) + H(Y|X) + H(Z|X, Y)
                = H(X) + H(Y) + H(Z|X, Y)
                = 2 + H(Z|X, Y)
                >= 2
b.  H(Z|X, Y) = 0, Z = X * Y

2.40
a.  H(X) = log|X| = 3
b.  H(Y)    = -Σp(y)logp(y)
            = -Σ(1/2)^ylog(1/2)^y
            = Σy(1/2)^y
            = (1/2) / (1 - 1/2)^2
            = 2
c.  there is a bijection between (X + Y, X - Y) and (X, Y)
    hence H(X + Y, X - Y) = H(X, Y) = H(X) + H(Y|X) = H(X) + H(Y) = 5

2.41
a.  I(X;Q, A)   = H(Q, A) - H(Q, A|X)
                = H(Q) + H(A|Q) - H(Q|X) - H(A|Q, X)
                = H(Q) - H(A|Q) - H(Q) - 0  // A is a function of (Q, X), Q and X are independent
                = H(A|Q)
    the uncertainty removed is equal to the uncertainty of the answer conditional to the question
b.  I(X;Q1, A1, Q2, A2) 
    = H(Q1, A1, Q2, A2) - H(Q1, A1, Q2, A2|X)
    = H(Q1) + H(A1|Q1) + H(Q2|Q1, A1) + H(A2|Q1, A1, Q2) - H(Q1|X) - H(A1|X, Q1) - H(Q2|X, Q1, A1) - H(A2|X, Q1, A1, Q2)
    = H(Q1) + H(A1|Q1) + H(Q2|Q1, A1) + H(A2|Q1, A1, Q2) - H(Q1) - H(Q2|A1)
    = H(A1|Q1) + H(A2|A1, Q2)
    <= H(A1|Q1) + H(A2|Q2) 
    = 2H(A1|Q1) = 2I(X;Q1, A1)

2.42
a.  H(5X) = H(X), bijection
b.  I(g(X);Y) <= I(X;Y), data processing equality 
c.  H(X0|X-1) >= H(X0|X1, X-1), conditioning cannot increase entropy
d.  H(X, Y) = H(X) + H(Y|X) <= H(X) + H(Y), H(X, Y) / (H(X) + H(Y)) <= 1

2.43
a.  define two random variables B and T that
        B = 1 if top side is face
        B = 0 if top side is tail
        T = 1 if bottom side is face
        T = 0 if bottom side is tail
    hence there is a bijection between B and T
    I(B;T) = H(B) - H(B|T) = H(B) = 1
b.  again define two random variables T and F, D(T) = D(F) = {1, 2, 3, 4, 5, 6}
    I(B;F)  = H(T) - H(T|F)
            = -6 * 1/6 * log1/6 - H(T|F)
            = 2.585 - H(T|F)
            // given face top can be 1 out of 4 numbers with equal probability
            = 2.585 - 4 * 1/4 * log1/4
            = 2.585 - 2
            = 0.585

2.44
a.  it's impossible to divide all 9 outcomes to two sets of equal sum of probability
    define B as
        B = 0 when X1X2 ∈ {AB, AC, BC}
        B = 1 when X1X2 ∈ {BA, CA, CB}
    and when X1X2 ∈ {AA, BB, CC} the result is ignored
b.  H(X1, X2) = 2H(X) = -Σpilogpi
    theoretically at most 2H(X) / H(1/2) = -Σpilogpi fair bits can be generated

2.45
H(X) = -Σpilogpi
d^2H(X) / dpi   = d(-logpi - 1) / dpi
                = -1/pi < 0 for 0 < pi <= 1
H(X) is concave to all pi, if there's a local extreme value it's the global maximum
by Lagrange multiplier subject to
    ElogX = Σp(x)logx = c 
    Σp(x) = 1
L(p, l1, l2) = -Σpilogpi - l1(Σpilogi - c) - l2(Σpi - 1)
dL/dpi = -logpi - 1 - l1logi - l2 = 0
logpi = -1 - l1logi - l2
pi  = e^(-1 - l1logi - l2)
    = e^(-1 - l2)i^l1
    = ai^b for some constants a and b
where
    Σai^b = 1
    Σai^blogi = c
H(X)    = -Σpilogpi
        = -Σai^blog(ai^b)
        = -logaΣai^b - bΣai^blogi
        = -loga - bc < ∞

2.46
// thanks solutions manual
lemma 1 (extended axiom 3): Hm(p1, .., pm) = Hm-k+1(Sk, pk+1, .., pm) + SkHk(p1/Sk, .., pk/Sk) where Sk = p1 + .. + pk
    let H2(p, 1-p) = h(p)
    Hm(p1, .., pm)  = Hm-1(S2, p3, .., pm) + S2h(p2/S2) // grouping axiom
                    = Hm-2(S3, p4, .., pm) + S3h(p3/S3) + S2h(p2/S2)
                    = Hm-k+1(Sk, pk+1, .., pm) + Σ{2 <= i <= k}Sih(pi/Si)
    similarly,
    Hk(p1/Sk, .., pk/Sk)    = h(pk/Sk) + Σ{2 <= i <= k-1}Sih(pi/Si)/Sk
                            = 1/Sk * Σ{2 <= i <= k-1}Sih(pi/Si)
    Hm(p1, .., pm)  = Hm-k+1(Sk, pk+1, .., pm) + SkHk(p1/Sk, .., pk/Sk)
lemma 2: let f(m) = Hm(1/m, .., 1/m), f(mn) = f(m) + f(n)
    f(mn)   = Hmn(1/mn, .., 1/mn)
            = Hmn-n+1(1/m, 1/mn, .., 1/mn) + 1/mHn(1/n) // lemma 1
            = Hmn-2n+2(1/m, 1/m, 1/mn, .., 1/mn) + (2/m)Hn(1/n)
            = Hmn-mn+m(1/m, .., 1/m) + Hn(1/n)
            = f(m) + f(n)
lemma 3: H2(1, 0) = 0
    H3(p1, p2, 0)   = H2(p1, p2) + p2H2(1, 0)   // grouping axiom
                    = H2(1, 0) + H2(p1, p2)
    H2(1, 0) = p2H2(1, 0) for all p2, H2(1, 0) = 0
lemma 4: f(m + 1) - f(m) -> 0 as m -> ∞
    f(m + 1)    = Hm+1(1/(m + 1), .., 1/(m + 1))
                = h(1/(m + 1)) + m/(m + 1)Hm(1/m, .., 1/m)
                = h(1/(m + 1)) + m/(m + 1) * f(m)
    f(m + 1) - m/(m + 1) * f(m) = h(1 / (m + 1))
    lim{m -> ∞}(f(m + 1) - m/(m + 1) * f(m)) = lim{m -> ∞}h(1 / (m + 1)) = lim{p -> 0}h(p) = 0   // continuity axiom
    let 
        an+1 = f(n + 1) - f(n)
        bn = h(1/n)
    an+1    = h(1/(n + 1)) + n/(n + 1) * f(n) - f(n)
            = -1/(n + 1) * f(n) + bn+1
            = -1/(n + 1) * Σ{2 <= i <= n}ai + bn+1
    (n + 1)bn+1 = (n + 1)an+1 + Σ{2 <= i <= n}ai
    Σ{2 <= n <= N}nbn   = Σ{2 <= n <= N}(nan + Σ{2 <= i <= n - 1}ai)
                        = Σ{2 <= i <= N}(iai + (N - i)ai)
                        = NΣ{2 <= i <= N}ai
    Σ{2 <= n <= N}nbn / Σ{1 <= n <= N}n = 2/(N + 1) * Σ{2 <= i <= N}ai
    as lim{n -> ∞}bn = lim{n -> ∞}h(2/n) = 0, lim{n -> ∞}(Σ{2 <= n <= N}nbn / Σ{1 <= n <= N}n) = 0
    aN+1 = f(n + 1) - f(n) = bN+1 - 1/(N + 1) * Σ{2 <= n <= N}an -> 0 as N -> ∞
lemma 5: f(n) = logn
    let P be arbitrary prime number
    let g(n) = f(n) - f(P)logn / logP
    g(mn)   = f(mn) - f(P)logmn / logP
            = f(m) + f(n) - f(P)logm / logP - f(P)logn / logP   // lemma 2
            = g(m) + g(n)
    g(P)    = 0
    let an  = g(n + 1) - g(n)
            = f(n + 1) - f(n) - f(P)log(n + 1) / logP + f(P)logn / logP
            = f(n + 1) - f(n) + f(P)log(n / (n + 1)) / logP
    lim{n -> ∞}an = 0   // lemma 4
    for some integer n let n(1) = floor(n/P), n(1) <= n/P, n = n(1)P + l for some 0 <= l < P
    g(Pn(1)) = g(n(1))
    g(n)    = g(n(1)) + g(n) - g(Pn(1))
            = g(n(1)) + Σ{Pn(1) <= i <= n - 1}ai
    let n(i + 1) = floor(n(i) / P), Pn(i + 1) + li = n(i) for some 0 <= li < P
    g(n)    = g(n(k)) + Σ{i <= j <= k}Σ{Pn(j) <= i <= n(j - 1)}ai
    and n(k) <= n/P^k, when k = floor(logn / logP) + 1, n(k) = 0, g(n(k)) = g(0) = 0
    g(n)    = Σ{1 <= i <= bn}ai // why?
    // would like to verify but the original paper of this proof is in German
    lim(g(n) / logn) = 0, lim(f(n) / logn - f(P) / logP) = 0, lim(f(n) / logn) = f(P) / logP = c
    by normalization condition, f(2) / log2 = c = 1, f(P) = logP
    for composite number N = ΠPi, f(N) = f(ΠPi) = Σf(Pi) = ΣlogPi = logN
let p = r/s be arbitrary rational number
f(s)    = Hs(1/s, .., 1/s)
        = Hr+1(1/s, .., 1/s, (s - r)/s) + (s - r)/s * f(s - r)
        = h(r/s) + r/s * f(r) + (s - r)/s * f(s - r)
h(r/s)  = f(s) - r/s * f(r) - (s - r)/s * f(s - r)
        = logs - r/s * logr - (s - r)/s * log(s - r)
        = -r/s * log(r/s) - (s - r)/s * log((s - r)/s)
by continuity h(x) is also defined for irrational numbers
inductively,
Hn(p1, .., pn)  = Hn-1(S2, p3, .., pn) + S2H(p1/S2, p2/S2)
                = -S2logS2 - Σ{3 <= i <= n}pilogpi - p1/S2 * log(p1/S2) - p2/S2 * log(p2/S2)
                = -p1logpi - p2logp2 - Σ{3 <= i <= n}pilogpi
                = -Σ{1 <= i <= n}pilogpi

2.47
there are n^2 arrangements but fewer events
1.  the resulting sequence is still ordered, one out of n cards is replaced at the original place, p = n/n^2 = 1/n
2.  the chosen card is replaced one place from origin, 2 + 2(n-2) = 2n - 2 arrangements 
    (the first and the last card have only one possible arrangement)
    which are mapped to n - 1 events, p = 2/n^2 each
3.  the chosen card is replaced at at least 2 place from origin, 2(n - 2) + (n - 2)(n - 3) = (n - 2)(n - 1) arrangements
    all events in this category are disjoint, p = 1/n^2 each
H(X)    = -1/nlog(1/n) - (n - 1) * (2/n^2) * log(2/n^2) - (n - 2)(n - 1)/n^2 * log(1/n^2)
        = logn / n + 4(n - 1)/n^2 * logn - 2(n - 1)/n^2 + 2(n - 2)(n - 1)/n^2 * logn
        = logn / n + 2(n - 1)/n * logn - 2(n - 1)/n^2
        = (2n - 1)/n * logn - 2(n - 1)/n^2

2.48
a.  X^N is a function of N, 
    I(N; X^N) = H(N) - H(N|X^N)
    = H(N)
    = -Σpilogpi
    = -Σ(1/2)^ilog(1/2)^i
    = Σi(1/2)^i
    = (1 - 1/2)/(1/2)^2 = 2
b.  H(X^N|N) = 0
c.  H(X^N) = H(N) = 2
d.  I(N;X^N)    = H(N) - H(N|X^N)
                = H(N)
                = H(1/3) = 0.918
e.  H(X^N|N)    = Σp(n)H(X^N|N = n)
                = 1/3 * 6 + 2/3 * 12
                = 10
f.  H(X^N)      = H(1/3) + 1/3H(X^6) + 2/3H(X^12)   // problem 2.10
                = 10.918

3.1
a.  E[X]    = int(0, ∞, xf(x)dx)
            >= int(t, ∞, xf(x)dx)
            >= int(t, ∞, tf(x)dx)
            = tPr{X >= t}
    Pr{X >= t} <= E[X] / t
    let D(X) = {0, 1, 2, ..}
    Σ{x >= 1}p(x) = E[X] = Σ{x >= 1}xp(x)
    Σ(x >= 2)(x - 1)p(x) = 0
    as (x - 1) > 0 for x >= 2, p(x) >= 0, p(x) = 0 for all x >= 2
    any random variable where p(0) + p(1) = 1 achieves this inequality with equality
b.  Pr{|Y - μ| > ε} = Pr{(Y - μ)^2 > ε^2}
                    <= Pr{(Y - μ)^2 >= ε^2}
                    = Pr{X >= ε^2}
                    <= E[X] / ε^2   // Markov's inequality
                    = E[(Y - μ)^2] / ε^2
                    = E[Y^2 - 2μY + μ^2] / ε^2
                    = (E[Y^2] - E^2[Y]) / ε^2
                    = σ^2 / ε^2
c.  Pr{|Zn - μ| > ε}    <= Var(Z - μ) / ε^2
                        = Var(ΣZi / n) / ε^2
                        = 1/n^2 * ΣVar(Zi) / ε^2
                        = n/n^2 * Var(Zi) / ε^2
                        = σ^2/(nε^2)

3.2
1/n * log(p(X^n)p(Y^n) / p(X^n, Y^n))
= 1/n * Σlog(p(Xi)p(Yi) / p(Xi, Yi))    // i.i.d.
-> E[log(p(Xi)p(Yi) / p(Xi, Yi))]   // weak law of large numbers
= -E[log(p(Xi, Yi) / p(Xi)p(Yi))]
= -I(X;Y)


3.3
define X as
    p(2/3) = 3/4
    p(3/5) = 1/4
let Xi be iids ~ p(x), let piece of cake after n cuts = C
C = ΠXi
1/n * logC  = 1/n * logΠXi
            = 1/n * ΣlogXi
            -> E[logXi]
            = 3/4 * log(2/3) + 1/4 * log(3/5)
            = -0.623
logC -> -0.623 * n
C -> 2^(-0.623n) = 0.649^n

3.4
a.  |-1/n * logp(x^n) - H| <= ε
    H - ε <= -1/n * logp(x^n) <= H + ε
    -n(H - ε) >= logp(x^n) >= -n(H + ε)
    2^(-n(H - ε)) >= p(x^n) >= 2^(-n(H + ε))
    by property 2, Pr{X^n ∈ A^n} > 1 - ε
    hence Pr{X^n ∈ A^n} -> 1
b.  Pr{X^n ∈ B^n}   = Pr{|1/n * ΣXi - μ| <= ε}
                    = 1 - Pr{|1/n * ΣXi - μ| > ε}
                    >= 1 - Var(1/n * ΣXi) / ε^2 // Chebyshev's inequality
                    = 1 - Var(Xi) / nε^2
                    -> 1
    Pr{X^n ∈ A^n ∩ B^n} >= 1 - (Pr{X^n ∉ A^n} + Pr{X^n ∉ B^n})
                        -> 1
c.  |A^n ∩ B^n| <= |A^n| <= 2n(H + ε) // property 3
d.  Pr{X^n ∈ A^n ∩ B^n} = Σ{x^n ∈ A^n ∩ B^n}p(x)
                        <= Σ{x^n ∈ A^n ∩ B^n}2^(-n(H - ε))
                        = |A^n ∩ B^n|2^(-n(H - ε))
                        -> 1
    |H(A^n ∩ B^n)| -> 1/2^(-n(H - ε)) = 2^n(H - ε)
    for large enough n, |H(A^n ∩ B^n)| >= (1/2) * 2^n(H - ε)

3.5
a.  1 >= Pr{Cn(t)}  = Σ{x^n ∈ Cn(t)}p(x^n)
                    >= Σ{x^n ∈ Cn(t)}2^(-nt)
                    = |Cn(t)|2^(-nt)
    |Cn(t)| <= 2^nt
b.  when t = H(X) + ε, Cn(t) = {x^n: p(x^n) <= 2^-n(H - ε)} ⊇ Aε^n
    hence for t >= H + ε, Pr{Cn(t)} >= Pr{Aε^n} -> 1 for small ε

3.6
p(X1, .., Xn)^(1/n)
= 2^(1/n * logp(X1, .., Xn))
-> 2^(Elogp(X))
= 2^-H(X)

3.7
a.  there are 
        C(100, 0) + C(100, 1) + C(100, 2) + C(100, 3)
        = 166751
    codewords, log(166751) = 17.347 <= 18
    minimum length of codewords is 18 bits
b.  0.995^100 * C(100, 0) 
    + 0.005 * 0.995^99 * C(100, 1) 
    + 0.005^2 * 0.995^98 * C(100, 2) 
    + 0.005^3 * 0.995^97 * C(100, 3)
    = 0.998
c.  μ = E[ΣXi] = 100 * E[Xi] = 0.5
    σ^2 = Var(ΣXi) = 100 * Var(Xi) = 100 * 0.005(1 - 0.005) = 0.4795
    Pr{X > 3}   <= Pr{|X - 0.5| > 2.5}
                <= σ^2 / 2.5^2
                = 0.00368

3.8
let P = (X1X2..Xn)^(1/n)
logP    = 1/n * logX1X2..Xn
        = 1/n * ΣlogXi
        -> E[logXi]  // weak law of large numbers
        = 1/2 * 0 + 1/4 * 1 + 1/4 * log3
        = 0.646
P -> 2^0.646 = 1.565

3.9
a.  lim(-1/n * logq(X1, .., Xn))
    = lim(-1/n * Σlogq(Xi))
    = -Ep[logq(Xi)]
    = -Σp(x)logq(x)
b.  1/n * log(q(X1, .., Xn) / p(X1, .., Xn))
    -> Σp(x)logq(x) + H(X)
    = Σp(x)logq(x) - Σp(x)logp(x)
    = Σp(x)log(q(x) / p(x))
    = -Σp(x)log(p(x) / q(x))
    = -D(p||q)

3.10
ln(Vn^(1/n))    = 1/n * lnVn
                = 1/n * ΣlnXi
                -> E[lnXi]
                = int(0, 1, lnxdx)
                = int(0, 1, d(xlngx - x))
                = -1
Vn^(1/n) -> e^-1 = 1/e
while E[Vn] = E[ΠXi] = ΠE[Xi] = (1/2)^n, 
E[Vn]^(1/n) = ((1/2)^n)^(1/n) = 1/2 > 1/e

3.11
a.  Pr{X ∉ A ∩ B}   <= Pr{X ∉ A} + Pr{X ∉ B}
                    = ε1 + ε2
    Pr{A ∩ B}   >= 1 - ε1 - ε2
b.  3.34:   part a
    3.35:   definition of discrete probability
    3.36:   the definition of Aε^n
    3.37:   2^-n(H - ε) is independent to x
    3.38:   intersection cannot increase set size
c.  |Bδ^n|  >= (1 - ε - δ)2^n(H - ε)
    as δ < 1/2,
    |Bδ^n|  > (1 - δ - ε)2^n(H - ε)
    log|Bδ^n|   > log(1 - δ - ε) + n(H - ε)
    log|Bδ^n|/n > log(1 - δ - ε)/n + H - ε
                -> H - ε

3.12
a.  let p2n(x) = pn(x)/2 + qn(x)/2, where qn(x) = 1/n * Σ{n + 1 <= i <= 2n}I(Xi = x)
    E[D(p2n || p)]  = E[D(pn/2 + qn/2||p/2 + p/2)]
                    <= E[1/2D(pn||p) + 1/2D(qn/2||p)]   // convexity of relative entropy
                    = 1/2E[D(pn||p)] + 1/2E[D(qn/2||p)]
                    = E[D(pn||p)]   // Xi's are iid
b.  pn(x)   = 1/n * Σ{1 <= i <= n}I(Xi = x)
            = 1/n * (Σ{i != j}I(Xi = x) + I(Xj = x))
            = (n - 1) / n * 1/(n - 1) * Σ{i != j}I(Xi = x) + I(Xj = x)/n
            = (n - 1)/n * pn-1(j, x) + I(Xj = x)/n
    where pn-1(j, x) = 1/(n - 1) * Σ{i != j}I(Xi = x)
    npn(x)  = Σ(1/n * Σ{1 <= j <= n}Σ{i != j}(I(Xi = x) + I(Xj = x)))
            = Σ{1 <= j <= n}((n - 1)/n * pn-1(j, x)) + Σ{1 <= j <= n}I(Xj = x)
            = (n - 1)/n * Σ{1 <= j <= n}pn-1(j, x) + pn(x)
    pn(x)   = 1/n * Σpn-1(j, x)
    E[D(pn||p)  = E[D(Σpn-1(j, x)/n || p)]
                <= E[Σ1/n * D(pn-1(j, x)||p)]   // convexity
                = E[D(pn-1(x)||p)]  // iid

3.13
a.  H(X) = -0.6log0.6 - 0.4log0.4 = 0.971
b.  2^-n(H + ε) <= p(X1, .., Xn) = 0.6^k * 0.4^(n - k) <= 2^-n(H - ε)
    11 <= k <= 19
    Pr{Aε^n}    = Σ{11 <= k <= 19}C(n, k)0.6^k * 0.4^(n - k)
                = 0.936
    |Aε^n|  = Σ{11 <= k <= 19}C(n, k)
            = 26366510
c.  as Σ{12 <= k <= 25}C(25, k)0.6^k * 0.4^(25 - k) > 0.9
    Σ{13 <= k <= 25}C(25, k)0.6^k * 0.4^(25 - k) = 0.8462322310242371062726656
    0.6^12 * 0.4^(25 - 12) = 1.46081389744226304 × 10^-8
    (0.9 - 0.8462322310242371062726656) / 1.46081389744226304 × 10^-8 <= 3680673
    |B| = Σ{13 <= k <= 25}C(25, k) + 3680673 = 20457889
d.  |Aε^n ∩ B| = Σ{13 <= k <= 19}C(25, k) + 3680673 = 16708810
    Pr{Aε^n ∩ B} = Σ{13 <= k <= 19}C(25, k)0.6^k * 0.4^(25 - k) + (0.9 - 0.846) = 0.871

