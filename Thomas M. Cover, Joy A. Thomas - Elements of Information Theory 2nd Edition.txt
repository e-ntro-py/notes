2.1
a.  H(X)    = -Σp(x)logp(x)
    where p(x) = (1/2)^x
            = -Σ{x ∈ [1, ∞)}(1/2)^xlog(1/2)^x
            = Σ{x ∈ [1, ∞)}(1/2)^x * x
            = E[X], X ~ Geometric(1/2)
            = 2
b.  S = {1}, {1, 2}, {1, 2, 3}, ...
    let P be the number of questions required to determine X
    if X = x, the first x - 1 questions have answer no, the last question has answer yes
    P = X, P ~ Geometric(1/2), E[P] = E[X] = 2 = H(X)

2.2
a.  let D(X) and D(Y) be the domain of X and Y
    2^a = 2^b iff a = b, f(x) = 2^x is a bijection between D(X) and D(Y)
    I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X), as X is fixed given Y and vice versa,
    H(X|Y) = H(Y|X) = 0, H(X) = H(Y)
b.  f(x) = cosx is not a bijection between D(X) and D(Y): some different values of X may be mapped to the same Y
    Y is still fixed by X, H(Y|X) = 0
    H(X) - H(X|Y) = H(Y), H(X|Y) >= 0, H(X) >= H(Y)

2.3
H(p)    = Σpilogpi
pilogpi = 0 iff pi = 0 (by definition) or pi = 1
as p is a probability vector, Σpi = 1
the probability vector p that minimizes H(p) is for some k
    pi = 1, i = k
    pi = 0, i != k
and there are n such vectors in total

2.4
a.  chain rule (equation 2.14)
b.  H(g(X) | X) = 0 as given X, g(X) is a constant
c.  chain rule (equation 2.14), another possible expansion
d.  H(X | g(X)) >= 0, non-negativity of entropy

2.5
H(Y|X)  = -Σ{x}Σ{y}p(x, y)logp(y|x) = 0
as p(x, y) >= 0, for all p(x, y) > 0, logp(y|x) = 0, p(y|X = x) = 1, p(y|X != x) = 0
therefore given X there's only one value of Y with probability 1 for all X

2.6
a.  I(X;Y|Z)    = H(X|Z) - H(X|Y, Z)
    I(X;Y)      = H(X) - H(X|Y)
    if H(X) > 0, X = Y = Z, 
    I(X;Y|Z) = H(X|Z) - H(X|Y, Z) = 0 < H(X) = H(X) - H(X|Y) = I(X;Y)
b.  let X and Z be independent, Y = X + Z, H(X) > 0
    I(X;Y|Z) = H(X|Z) - H(X|Y, Z) = H(X) > H(X) - H(X|Y) = I(X;Y)

2.7
a.  (follow same logic as to determine the lower bound of comparison sort)
    for n coins there are 2n + 1 possibilities: one of them are lighter, one of them are heavier, or all fair
    a weighting have 3 outcomes: left is heavier, right is heavier or equal
    k weightings form a ternary decision tree of height k with 3^k leaves
    2n + 1 <= 3^k
    n <= (3^k - 1) / 2
b.  // thanks https://en.wikipedia.org/wiki/Balance_puzzle#Twelve-coin_problem
    divide the coins into three piles of 4 coins A, B and C
    weight A and B, if A is heavier 
        C is a pile of fair coins 
        move 3 coins of A aside as A', move 3 coins from B to A, move 3 coins from C to B, weight A and B again
        if A is still heavier
            either the coin in A both times is heavier, or the coin in B both times is lighter
            weight one of them with a fair coin in C to determine the counterfeit coin
        if A is now lighter
            one of the three coins moved from B to A is lighter
            weight two of them, if equal the third one is lighter, otherwise the lighter is counterfeit
        if A and B are now equal
            one of the coins in A' is heavier
            weight two of them, if equal the third one is heavier, otherwise the heavier is counterfeit
    if A is lighter
        symmetric
    if A and B are equal
        one of C is counterfeit, (n, k) = (4, 2)
        weight 2 of C with 2 of A or B reduces the problem to (n, k) = (2, 1)
        and then any coin of (2, 1) problem can be compared to a fair coin 

2.8
let Xi be the result of ith drawing
with replacement H(Xi) = H(X1) 
    = - (r / (r + w + b))log(r / (r + w + b))
      - (w / (r + w + b))log(r / (r + w + b))
      - (b / (r + w + b))log(b / (r + w + b))
with replacement, for i > 1, let ri, wi, bi be the number of red, white and black balls upon ith drawing
    ri / (ri + wi + bi)
    = ri / (r + w + b - i + 1)
    >= (r - i + 1) / (r + w + b - i + 1)
    > r / (r + w + b)
    similar for wi / (ri + wi + bi) and bi / (ri + wi + bi)
    since f(x) = xlogx is strictly increasing, H(Xi) < H(X1), entropy is lower without replacement

2.9
a.  1.  H(X|Y) >= 0, H(Y|X) >= 0, H(X|Y) + H(Y|X) >= 0
    2.  H(X|Y) + H(Y|X) is symmetric
    3.  H(X|Y) + H(Y|X) = 0 iff H(X|Y) = H(Y|X) = 0
        H(X|Y) = 0 iff X is fixed given Y
        H(Y|X) = 0 iff Y is fixed given X, one to one function between X and Y
    4.  H(X|Y) + H(Y|Z) >= H(X|Y, Z) + H(Y|Z)   // conditioning cannot increase entropy
                        = H(X, Y|Z)             // chain rule
                        = H(X|Z) + H(Y|X, Z)    // chain rule
                        >= H(X|Z)
        similarly H(Z|Y) + H(Y|X) >= H(Z|X), ρ(x, y) + ρ(y, z) >= ρ(x, z)
b.  1.  I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
        H(X) + H(Y) - 2I(X;Y) = H(X|Y) + H(Y|X)
    2.  H(X) + H(Y) - 2I(X;Y)
        = H(X) + H(Y) - (H(X) + H(Y) - H(X, Y)) - I(X;Y)
        = H(X, Y) - I(X;Y)
    3.  apply I(X;Y) = H(X) + H(Y) - H(X, Y) again

2.10
a.  H(X)    = -Σp(x)logp(X)
            = -Σαp1(x)logαp1(x) - Σ(1-α)p2(x)log(1-α)p2(x)
            = -αΣp1(x)logα - αΣp1(x)logp1(x) - (1-α)Σp2(x)log(1-α) - (1-α)Σp2(x)logp2(x)
            = -αlogα - (1-α)log(1-α) + αH(X1) + (1-α)H(X2)
b.  αH(X1) + (1-α)H(X2) is linear wrt. α
    d^2(-αlogα - (1-α)log(1-α))/d^2α
    = -1/αln2 - 1/(1-α)ln2 < 0 is concave
    dH(X1)/dα = logα - 1/ln2 + log(1-α) + 1/ln2 + H(X1) - H(X2) = 0
    log((1-α)/α) = H(X2) - H(X1)
    α = 2^H(X1) / (2^H(X1) + 2^H(X2))
    maxH(X) = -2^H(X1) / (2^H(X1) + 2^H(X2)) * (H(X1) - log(2^H(X1) 
            + 2^H(X2))) -2^H(X2) / (2^H(X1) + 2^H(X2)) * (H(X2) - log(2^H(X1) + 2^H(X2))) 
            + 2^H(X1) / (2^H(X1) + 2^H(X2))H(X1) + 2^H(X2) / (2^H(X1) + 2^H(X2))H(X2)
            = 2^H(X1) / (2^H(X1) + 2^H(X2))log(2^H(X1) + 2^H(X2)) + 2^H(X2) / (2^H(X1) + 2^H(X2))log(2^H(X1) + 2^H(X2))
            = log(2^H(X1) + 2^H(X2))
    2^H(X)  <= 2^H(X1) + 2^H(X2)
    effective alphabet size of X is smaller than or equal to the combined alphabet size of X1 and X2

2.11
a.  I(X1;X2)/H(X1)  = (H(X1) - H(X1|X2)) / H(X1)
                    = 1 - H(X1|X2)/H(X1) = ρ
b.  0 <= I(X1;X2) = H(X1) - H(X1|X2) <= H(X1)
    0 <= ρ = I(X1;X2) / H(X1) <= 1
c.  ρ = 0 iff I(X1;X2) = 0 iff X1 and X2 are independent
d.  ρ = 1 iff I(X1;X2)/H(X) = 1 iff H(X1|X2) = 0
    X1 is a function of X2 or vice versa

2.12
a.  H(X) = -2/3log2/3 - 1/3log1/3 ≒ 0.918
    H(Y) = H(X) ≒ 0.918
b.  H(X|Y)  = p(Y = 0)H(X|Y = 0) + p(Y = 1)H(X|Y = 1)
            = 0 + 2/3 * 1 = 2/3
    H(Y|X)  = p(X = 0)H(Y|X = 0) + p(X = 1)H(Y|X = 1)
            = 2/3 * 1 + 1/3 * 0 = 2/3
c.  H(X, Y) = H(X) + H(Y|X) ≒ 1.585
d.  H(Y) - H(Y|X) ≒ 0.251
e.  I(X;Y)  = H(Y) - H(Y|X) = 0.251
f.  see figure 2.2

2.13
// thanks solutions manual
the Taylor expansion of ln(x) around 1 is 
    ln(x) = ln(1) + (x-1) - (x - 1)^2 / 2c^2 
for some c between 1 and x
the third term is always negative, ln(x) <= x - 1
substitute x by 1/y, 
    ln(1/y) <= 1/y - 1
    -lny <= 1/y - 1
    lny >= 1 - 1/y

2.14
a.  H(X, Y, Z)  = H(X) + H(Y|X) + H(Z|Y, X) = H(X) + H(Y|X)
                = H(X) + H(Z|X) + H(Y|Z, X) = H(X) + H(Z|X)
    H(Y|X) = H(Z|X)
    H(Z) >= H(Z|X) = H(Y|X) = H(Y)
    similarly H(Z) >= H(X)
b.  X is a random variable with H(X) > 0, Y = -X, Z = X + Y = 0
    H(X) = H(Y) > 0 = H(Z)
c.  either X or Y is constant, then there's a one-to-one function between X and Z, H(X) + H(Y) = H(X) = H(Z)

2.15
I(X1;X2, .., Xn)    = I(X2, .., Xn;X1)
                    = I(X2;X1) + I(X3;X1|X2) + .. + I(Xn;X1|Xn-1, .., X2)
for all 1 <= i <= n, 
    p(x1, xi|x2, .., xi-1)
    = p(x1, .., xi) / p(x2, .., xi-1)
    = p(x1, x2)p(x3|x2)..p(xi|xi-1) / p(x2)p(x3|x2)..p(xi-1|xi-2)
    = p(x1, x2)p(xi|xi-1) / p(x2)
    = p(x1|x2)p(xi|xi-1)
    = p(x1|x2, .., xi-1)p(xi|x2, .., xi-1)  // definition of Markov chain
hence X1 and Xi are independent given X2, .., Xi-1, I(Xi;X1|X2, .., Xi-1)
I(X1;X2, .., Xn)    = I(X2;X1)

2.16
a.  I(X1;X3)    <= I(X1;X2) // data processing inequality
                = H(X2) - H(X2|X1)
                <= H(X2) <= log|X2| = logk
b.  I(X1;X3) <= logk = 0, X1 and X3 are independent

2.17
a.  X1, .., Xn are iid, H(X1, .., Xn) = ΣH(Xi) = nH(p)
b.  (Z1, .., Zk, K) is a function of (X1, .., Xn), by problem 2.2 H(Z1, .., Zk, K) <= H(X1, .., Xn)
c.  chain rule of entropy
d.  H(Z1, .., Zk|K) = Σp(K = k)H(Z1, .., Zk|K = k)
    where (Z1, .., Zk) is a uniform distribution with 2^k values, H(Z1, .., Zk|K = k) = k
                    = Σp(K = k)k = E[K]
e.  H(K) >= 0, non-negativity of entropy
f(X) = 0 if there are more 1s in first two characters than the last 2 characters
f(X) = 1 if there are more 1s in last two characters than first 2 characters
f(X) = ε otherwise

2.18
// ((n, k)) is n multichoose k
2 sequences of 4 matches, each with probability (1/2)^4 = (1/2)^4
2 * ((4, 1)) = 2 * C(4, 1) = 8 sequences of 5 matches, p = (1/2)^5 each
2 * ((4, 2)) = 2 * C(5, 2) = 20 sequences of 6 matches, p = (1/2)^6 each
2 * ((4, 3)) = 2 * C(6, 3) = 40 sequences of 7 matches, p = (1/2)^7 each
P(Y = 4) = 2 * (1/2)^4 = 1/8
P(Y = 5) = 8 * (1/2)^5 = 1/4
P(Y = 6) = 20 * (1/2)^6 = 20/64 = 5/16
P(Y = 7) = 40 * (1/2)^7 = 40/128 = 5/16
H(X)    = 2 * (1/2)^4 * 4 + 8 * (1/2)^5 * 5 + 20 * (1/2)^6 * 6 + 40 * (1/2)^7 * 7
        = 5.8125
H(Y)    = -(1/8log(1/8) + 1/4log(1/4) + 5/16log(5/16) + 5/16log(5/16))
        = 1.924
H(Y|X)  = 0, length of sequence is fixed given the sequence
H(X|Y)  = H(X, Y) - H(Y)
        = H(X) + H(Y|X) - H(Y)
        = 3.8885

2.19
H(X)    = -Σp(x)logp(x)
        = -Σ(Anlog^2n)^-1log(Anlog^2n)^-1
        = Σ(logA / Anlog^2n) + Σ(1 / Anlogn) + Σ(2loglogn / Anlog^2n)
the first and third terms are nonnegative
the second term is infinite as
Σ(1 / nlogn)    >= int(2, +∞, 1/nlogn)
                = int(2, +∞, ln2/nlnn)
                = int(2, +∞, ln2 * lnlnn)
                = +∞

2.20
H(Xn, R) = H(X):
    obviously (Xn, R) is a function of X
    given any Xi and the run-length code, the sequence can be recovered, X is a function of any Xi and R
H(R) < H(X):
    R is a function of X, and two sequences of X may be mapped to the same run-length coding (e.g. 01 and 10)
    more precisely for any 1 <= i <= n,
    H(X)    = H(Xi, R)
            = H(Xi) + H(R|Xi)
            <= H(Xi) + H(R)
            <= log2 + H(R)
    H(X) - H(R) <= log2 = 1

2.21
P(p(X) <= d)log1/d
= -Σ{x: p(x) <= d}p(x)logd
<= -Σ{x: p(x) <= d}p(x)logp(x)  // p(x) <= d <= 1, -logp(x) >= logd
<= -Σ{x}p(x)logp(x) = H(X)

2.22
a.  chain rule of mutual information can be derived from chain rule of entropy (equation 2.62 - 2.64)
    I(X1, X2;Y) = D(p(x1, x2, y) || p(x1, x2)p(y))
                // q(x1, x2, y) = p(x1, x2)p(y), (x1, x2) and y are independent
                = D(p(x1, y) || p(x1)p(y)) + D(p(x2|x1, y) || p(x2|x1)) 
                = I(X1;Y) + E[log(p(X2|X1, Y) / p(X2|X1))]
                = I(X1;Y) + E[log(p(X2, Y|X1) / p(X2|X1)p(Y|X1))]
                = I(X1;Y) + D(p(x2, y|x1) || p(x2|x1)p(y|x1))
                = I(X1;Y) + I(X2;Y|X1)
    chain rule of mutual information can be derived from chain rule of relative entropy
    no order between chain rule of entropy and mutual entropy
b.  Jensen's inequality => D(p||q) >= 0 (equation 2.83 - 2.89) => I(X;Y) >= 0 (text proof of equation 2.90)

2.23
given values of n-1 out of n variables, the last is fixed
for any i < n, Xi and (X1, .., Xi-1) are independent since:
    when n is even, p(Xi = 1) = p(Xi = 0) = 1/2:
        flip each bit of the sequence won't change the number of 1s
        each even sequence with Xi = 0 has a corresponding even sequence with Xi = 1
    when n is odd and n >= 3, p(Xi = 1) = p(Xi = 0) = 1/2:
        n = 3, all even sequences are 000, 110, 101, 011, p(Xi = 0) = p(Xi = 1) = 1/2 for all i
        for n > 3, remove any two variables from the sequence that's not Xi
        if the removed variables have even number of 1s, p(Xi = 1) = p(Xi = 0) by induction
        if the removed variables have odd number of 1s, flip the remaining sequence won't change its oddity
    when (X1, .., Xi-1) has even number of 1s, (Xi, .., Xn) has even number of 1s and length >= 2, p(Xi = 1) = p(Xi = 0)
    when (X1, .., Xi-1) has odd number of 1s, similarly p(Xi = 1) = p(Xi = 0)
I(Xn-1;Xn|X1, .., Xn-2) = H(Xn-1|X1, .., Xn-2) - H(Xn|X1, .., Xn-1)
                        = H(Xn-1|X1, .., Xn-2)
                        = 1
I(Xi-1;Xi|X1, .., Xi-2) = H(Xi-1|X1, .., Xi-2) - H(Xi|X1, .., Xi-1)
                        = 0

2.24
a.  -1/4log1/4 - 3/4log3/4
    = 1/2 - 3/4(2 - log3)
    = 1/2 - 3/4(1.584 - 2)
    = 0.812
b.  f(x) = 1/(1-0) = 1 on [0, 1]
    int(0, 1, -plogp - (1-p)log(1-p)) = 0.721
c.  // skipped

2.25
let X and Y be independent random variables, Z = X + Y, H(X|Z) > 0 (e.g. X and Y are uniform binary iid)
I(X;Y;Z)    = I(X;Y) - I(X;Y|Z)
            = 0 - I(X;Y|Z)
            = 0 - H(X|Z) + H(X|Y, Z)
            = -H(X|Z) < 0
a.  H(X, Y, Z) - H(X) - H(Y) - H(Z) + I(X;Y) + I(Y;Z) + I(Z;X)
    = H(Z) + H(Y|Z) + H(X|Y, Z) - H(X) - H(Y) - H(Z) + I(X;Y) + I(Y;Z) + I(Z;X)
    = -I(Y;Z) + I(Y;Z) + H(X|Y, Z) - H(X) + I(Z;X) + I(X;Y)
    = H(X|Y, Z) - H(X) + H(X) - H(X|Z) + I(X;Y)
    = I(X;Y) - I(X;Y|Z)
b.  H(X, Y, Z) - H(X, Y) - H(Y, Z) - H(Z, X) + H(X) + H(Y) + H(Z)
    = H(X, Y, Z) + (H(X) + H(Y) - H(X, Y)) + (H(X) + H(Z) - H(X, Z)) + (H(Y) + H(Z) - H(Y, Z)) - H(X) - H(Y) - H(Z)
    = H(X, Y, Z) + I(X;Y) + I(X;Z) + I(Y;Z) - H(X) - H(Y) - H(Z)
    = I(X;Y;Z)

2.26
a.  from problem 2.13, lnx >= 1 - 1/x for x > 0
    let y = 1/x, ln(1/y) >= 1 - y for y > 0
    -lny >= 1 - y, lny <= y - 1
b.  -D(p||q)    = -Σp(x)log(p(x) / q(x))
                = Σp(x)log(q(x) / p(x))
                <= Σp(x)(q(x) / p(x) - 1)   // apply part a
                <= Σ(q(x) - p(x))
                = Σq(x) - Σp(x) = 0
c.  lny = y - 1 iff y = 1
    q(x) / p(x) = 1, p(x) = q(x) for all x

2.27
H(q) + (pm-1 + pm)H(pm-1 / (pm-1 + pm), pm / (pm-1 + pm))
= H(q) - (pm-1 + pm)(pm-1 / (pm-1 + pm)log(pm-1 / (pm-1 + pm)) + pm/(pm-1 + pm)log(pm/(pm-1 + pm)))
= H(q) - (pm-1logpm-1 - pm-1log(pm-1 + pm) + pmlogpm - pmlog(pm-1 + pm))
= H(q) - pm-1logpm-1 - pmlogpm + (pm-1 + pm)log(pm-1 + pm)
= H(q) - pm-1logpm-1 - pmlogpm + qm-1logqm-1
= H(p)

2.28
f(x) = xlogx is strictly convex on (0, 1), so
    -pilogpi / 2 - pjlogpj / 2 <= -(pi/2 + pj/2)log(pi/2 + pj/2)
    -pilogpi - pjlogpj <= (pi + pj)log((pi + pj) / 2)
equal only when pi = pj
by convexity a more uniform distribution has higher entropy 

2.29
a.  H(X, Y|Z)   = H(X|Z) + H(Y|X, Z)
                >= H(X|Z)
    equal when H(Y|X, Z) = 0, Y is a function of (X, Z)
b.  I(X, Y;Z)   = I(X;Z) + I(Y;Z|X)
                >= I(X;Z)
    equal when I(Y;Z|X) = 0, Y and Z are independent given X
c.  H(X, Y, Z) - H(X, Y)    = H(X, Y) + H(Z|X, Y) - H(X, Y)
                            = H(Z|X, Y)
                            <= H(Z|X)
                            = H(X, Z) - H(X)
    equal when H(Z|X, Y) = H(Z|X), Z and Y are independent given X
d.  I(Z;Y|X) - I(Z;Y) + I(X;Z)
    = H(Z|X) - H(Z|Y, X) - H(Z) + H(Z|Y) + H(Z) - H(Z|X)
    = H(Z|Y) - H(Z|Y, X)
    = I(X;Z|Y)
    always equal

2.30
// thanks solutions manual
by lagrange multiplier, measure entropy in nats
L(p, l1, l2)    = H(X) - l1(Σpi - 1) - l2(Σipi - A)
                = -Σpilogpi - l1(Σpi - 1) - l2(Σipi - A)
dL/dpi  = -logpi - 1 - l1 - l2i = 0
dL/dl1  = Σpi - 1 = 0
dL/dl2  = Σipi - A = 0
logpi = -1 - l1 - l2i
pi = e^(-1 - l1 - l2i) = ab^i for some constants a and b
as H(X) is concave to pi the range (0, 1), pi = ab^i is the global maximum
Σpi = Σab^i = a/(1-b) = 1
Σipi = Σiab^i = ab/(1-b)^2 = A
b/(1-b) = A, b = A/(A+1), a = 1-b = 1/(A+1)
pi = A^i / (A+1)^(i+1)
H(X)    <= -Σpilogpi 
        = -Σab^ilog(ab^i)
        = -alogaΣb^i - alogbΣib^i
        = -aloga/(1-b) - alogb * b/(1-b)^2
        = -loga - Alogb
        = log(A + 1) - AlogA + Alog(A + 1)
        = (A + 1)log(A + 1) - AlogA

2.31
as X -> Y -> g(Y), 
    I(X;Y) = H(X) - H(X|Y) >= I(X;g(Y)) = H(X) - H(X|g(Y))
    H(X|g(Y)) >= H(X|Y)
equality iff I(X;Y|g(Y)) = 0, or given g(Y) X and Y are independent
either X and Y are independent unconditionally
or g(Y) contains all the mutual information between X and Y

2.32
a.  f(a) = 1, f(b) = 2, f(c) = 3
    // can be solved algebraically
    // but in principle artificial randomness independent to X won't help
    Pe  = Σ{x, y}p(y)P(f(y) != x)
        = Σ{y}(p(y) * 1/2)
        = 1/2
b.  H(Pe) + Pelog|X| >= H(X|f(Y)) = H(X|Y)  // f(Y) is a bijection
    H(X|Y)  = Σp(y)H(X|Y = y)
            = 1/3 * (-2/4 * log(1/4) - 1/2 * log(1/2)) * 3
            = 3/2
    H(Pe) + Pelog3 >= 3/2
    -PelogPe - (1-Pe)log(1-Pe) + Pelog3 >= 3/2
    Pe >= 0.416 (Pe = 1/2)

2.33
X is equivalent to a mixture of two disjoint random variables X1 and X2 where
    p(X = X1) = p1
    p(X = X2) = 1 - p1
    D(X1) = {1}
    D(X2) = {2, .., m}
from problem 2.10 H(X) = H(p1) + p1H(X1) + (1-p1)H(X2) = H(p1) + (1-p1)H(X2)
to maximize H(X2), X2 must be uniformly distributed, p2 = p3 = .. = pm
H(X)    <= H(p1) + (1-p1)log|X2|
H(1 - Pe) + Pelog|X - 1| >= H(X)

2.34
I(X0;Xi)    = H(X0) - H(X0|Xi)
            >= I(X0;Xi+1)
            = H(X0) - H(X0|Xi+1)
H(X0|Xi+1) <= H(X0|Xi)

2.35
H(p)    = -1/2 * log1/2 - 2 * 1/4 * log1/4
        = 1/2 + 1 = 3/2
H(q)    = -3 * 1/3 * log1/3
        = 1.585
H(p||q) = Σp(x)log(p(x) / q(x))
        = 1/2log3/2 + 2 * 1/4log3/4
        = 0.0850
H(q||p) = Σq(x)log(q(x) / p(x))
        = 1/3 * log(2/3) + 2 * 1/3 * log(4/3)
        = 0.0817

2.36
p(0) = q(1) = 2/3
p(1) = q(0) = 1/3
H(p||q) = Σp(x)log(p(x) / q(x))
        = 2/3 * log2 + 1/3 * log(1/2)
H(q||p) = Σq(x)log(q(x) / p(x))
        = 1/3 * log(1/2) + 2/3 * log2
    
2.37
Σ{x, y, z}p(x, y, z)log(p(x, y, z) / p(x)p(y)p(z))
= Σ{x, y, z}p(x, y, z)log(p(x, y|z) / p(x)p(y))
= Σ{x, y, z}p(x, y, z)logp(x, y|z) - Σ{x, y, z}p(x, y, z)logp(x) - Σ{x, y, z}p(x, y, z)logp(y)
= H(X) + H(Y) + Σ{x, y, z}p(x, y|z)p(z)logp(x, y|z)
= H(X) + H(Y) + Σ{z}p(z)Σ{x, y}p(x, y|z)logp(x, y|z)
= H(X) + H(Y) - H(X, Y|Z)
when H(X) + H(Y) - H(X, Y|Z) = 0, 
    H(X) + H(Y) = H(X, Y|Z)
    H(X) + H(Y) = H(X|Z) + H(Y|X, Z)
where H(X) >= H(X|Z) and H(Y) >= H(Y|X, Z)
hence H(X) = H(X|Z) and H(Y) = H(Y|X, Z), X and Z are independent, Y and (X, Z) are independent

2.38
by problem 2.10, X is equivalent to
    p(X = X1) = α
    p(X = X2) = 1 - α
    p(X1 = x) = p(x) / α if x ∈ S
    p(X2 = x) = p(x) / (1 - α) if x ∉ S
H(X) = H(α) + αH(X1) + (1 - α)H(X2)
H(X|Y)  = Σp(y)H(X|Y = y)
        = αH(X1) + (1 - α)H(X2)
H(X) - H(X|Y) = H(α)

2.39
a.  H(X, Y, Z)  = H(X) + H(Y|X) + H(Z|X, Y)
                = H(X) + H(Y) + H(Z|X, Y)
                = 2 + H(Z|X, Y)
                >= 2
b.  H(Z|X, Y) = 0, Z = X * Y

2.40
a.  H(X) = log|X| = 3
b.  H(Y)    = -Σp(y)logp(y)
            = -Σ(1/2)^ylog(1/2)^y
            = Σy(1/2)^y
            = (1/2) / (1 - 1/2)^2
            = 2
c.  there is a bijection between (X + Y, X - Y) and (X, Y)
    hence H(X + Y, X - Y) = H(X, Y) = H(X) + H(Y|X) = H(X) + H(Y) = 5

2.41
a.  I(X;Q, A)   = H(Q, A) - H(Q, A|X)
                = H(Q) + H(A|Q) - H(Q|X) - H(A|Q, X)
                = H(Q) - H(A|Q) - H(Q) - 0  // A is a function of (Q, X), Q and X are independent
                = H(A|Q)
    the uncertainty removed is equal to the uncertainty of the answer conditional to the question
b.  I(X;Q1, A1, Q2, A2) 
    = H(Q1, A1, Q2, A2) - H(Q1, A1, Q2, A2|X)
    = H(Q1) + H(A1|Q1) + H(Q2|Q1, A1) + H(A2|Q1, A1, Q2) - H(Q1|X) - H(A1|X, Q1) - H(Q2|X, Q1, A1) - H(A2|X, Q1, A1, Q2)
    = H(Q1) + H(A1|Q1) + H(Q2|Q1, A1) + H(A2|Q1, A1, Q2) - H(Q1) - H(Q2|A1)
    = H(A1|Q1) + H(A2|A1, Q2)
    <= H(A1|Q1) + H(A2|Q2) 
    = 2H(A1|Q1) = 2I(X;Q1, A1)

2.42
a.  H(5X) = H(X), bijection
b.  I(g(X);Y) <= I(X;Y), data processing equality 
c.  H(X0|X-1) >= H(X0|X1, X-1), conditioning cannot increase entropy
d.  H(X, Y) = H(X) + H(Y|X) <= H(X) + H(Y), H(X, Y) / (H(X) + H(Y)) <= 1

2.43
a.  define two random variables B and T that
        B = 1 if top side is face
        B = 0 if top side is tail
        T = 1 if bottom side is face
        T = 0 if bottom side is tail
    hence there is a bijection between B and T
    I(B;T) = H(B) - H(B|T) = H(B) = 1
b.  again define two random variables T and F, D(T) = D(F) = {1, 2, 3, 4, 5, 6}
    I(B;F)  = H(T) - H(T|F)
            = -6 * 1/6 * log1/6 - H(T|F)
            = 2.585 - H(T|F)
            // given face top can be 1 out of 4 numbers with equal probability
            = 2.585 - 4 * 1/4 * log1/4
            = 2.585 - 2
            = 0.585

2.44
a.  it's impossible to divide all 9 outcomes to two sets of equal sum of probability
    define B as
        B = 0 when X1X2 ∈ {AB, AC, BC}
        B = 1 when X1X2 ∈ {BA, CA, CB}
    and when X1X2 ∈ {AA, BB, CC} the result is ignored
b.  H(X1, X2) = 2H(X) = -Σpilogpi
    theoretically at most 2H(X) / H(1/2) = -Σpilogpi fair bits can be generated

2.45
H(X) = -Σpilogpi
d^2H(X) / dpi   = d(-logpi - 1) / dpi
                = -1/pi < 0 for 0 < pi <= 1
H(X) is concave to all pi, if there's a local extreme value it's the global maximum
by Lagrange multiplier subject to
    ElogX = Σp(x)logx = c 
    Σp(x) = 1
L(p, l1, l2) = -Σpilogpi - l1(Σpilogi - c) - l2(Σpi - 1)
dL/dpi = -logpi - 1 - l1logi - l2 = 0
logpi = -1 - l1logi - l2
pi  = e^(-1 - l1logi - l2)
    = e^(-1 - l2)i^l1
    = ai^b for some constants a and b
where
    Σai^b = 1
    Σai^blogi = c
H(X)    = -Σpilogpi
        = -Σai^blog(ai^b)
        = -logaΣai^b - bΣai^blogi
        = -loga - bc < ∞

2.46
// thanks solutions manual
lemma 1 (extended axiom 3): Hm(p1, .., pm) = Hm-k+1(Sk, pk+1, .., pm) + SkHk(p1/Sk, .., pk/Sk) where Sk = p1 + .. + pk
    let H2(p, 1-p) = h(p)
    Hm(p1, .., pm)  = Hm-1(S2, p3, .., pm) + S2h(p2/S2) // grouping axiom
                    = Hm-2(S3, p4, .., pm) + S3h(p3/S3) + S2h(p2/S2)
                    = Hm-k+1(Sk, pk+1, .., pm) + Σ{2 <= i <= k}Sih(pi/Si)
    similarly,
    Hk(p1/Sk, .., pk/Sk)    = h(pk/Sk) + Σ{2 <= i <= k-1}Sih(pi/Si)/Sk
                            = 1/Sk * Σ{2 <= i <= k-1}Sih(pi/Si)
    Hm(p1, .., pm)  = Hm-k+1(Sk, pk+1, .., pm) + SkHk(p1/Sk, .., pk/Sk)
lemma 2: let f(m) = Hm(1/m, .., 1/m), f(mn) = f(m) + f(n)
    f(mn)   = Hmn(1/mn, .., 1/mn)
            = Hmn-n+1(1/m, 1/mn, .., 1/mn) + 1/mHn(1/n) // lemma 1
            = Hmn-2n+2(1/m, 1/m, 1/mn, .., 1/mn) + (2/m)Hn(1/n)
            = Hmn-mn+m(1/m, .., 1/m) + Hn(1/n)
            = f(m) + f(n)
lemma 3: H2(1, 0) = 0
    H3(p1, p2, 0)   = H2(p1, p2) + p2H2(1, 0)   // grouping axiom
                    = H2(1, 0) + H2(p1, p2)
    H2(1, 0) = p2H2(1, 0) for all p2, H2(1, 0) = 0
lemma 4: f(m + 1) - f(m) -> 0 as m -> ∞
    f(m + 1)    = Hm+1(1/(m + 1), .., 1/(m + 1))
                = h(1/(m + 1)) + m/(m + 1)Hm(1/m, .., 1/m)
                = h(1/(m + 1)) + m/(m + 1) * f(m)
    f(m + 1) - m/(m + 1) * f(m) = h(1 / (m + 1))
    lim{m -> ∞}(f(m + 1) - m/(m + 1) * f(m)) = lim{m -> ∞}h(1 / (m + 1)) = lim{p -> 0}h(p) = 0   // continuity axiom
    let 
        an+1 = f(n + 1) - f(n)
        bn = h(1/n)
    an+1    = h(1/(n + 1)) + n/(n + 1) * f(n) - f(n)
            = -1/(n + 1) * f(n) + bn+1
            = -1/(n + 1) * Σ{2 <= i <= n}ai + bn+1
    (n + 1)bn+1 = (n + 1)an+1 + Σ{2 <= i <= n}ai
    Σ{2 <= n <= N}nbn   = Σ{2 <= n <= N}(nan + Σ{2 <= i <= n - 1}ai)
                        = Σ{2 <= i <= N}(iai + (N - i)ai)
                        = NΣ{2 <= i <= N}ai
    Σ{2 <= n <= N}nbn / Σ{1 <= n <= N}n = 2/(N + 1) * Σ{2 <= i <= N}ai
    as lim{n -> ∞}bn = lim{n -> ∞}h(2/n) = 0, lim{n -> ∞}(Σ{2 <= n <= N}nbn / Σ{1 <= n <= N}n) = 0
    aN+1 = f(n + 1) - f(n) = bN+1 - 1/(N + 1) * Σ{2 <= n <= N}an -> 0 as N -> ∞
lemma 5: f(n) = logn
    let P be arbitrary prime number
    let g(n) = f(n) - f(P)logn / logP
    g(mn)   = f(mn) - f(P)logmn / logP
            = f(m) + f(n) - f(P)logm / logP - f(P)logn / logP   // lemma 2
            = g(m) + g(n)
    g(P)    = 0
    let an  = g(n + 1) - g(n)
            = f(n + 1) - f(n) - f(P)log(n + 1) / logP + f(P)logn / logP
            = f(n + 1) - f(n) + f(P)log(n / (n + 1)) / logP
    lim{n -> ∞}an = 0   // lemma 4
    for some integer n let n(1) = floor(n/P), n(1) <= n/P, n = n(1)P + l for some 0 <= l < P
    g(Pn(1)) = g(n(1))
    g(n)    = g(n(1)) + g(n) - g(Pn(1))
            = g(n(1)) + Σ{Pn(1) <= i <= n - 1}ai
    let n(i + 1) = floor(n(i) / P), Pn(i + 1) + li = n(i) for some 0 <= li < P
    g(n)    = g(n(k)) + Σ{i <= j <= k}Σ{Pn(j) <= i <= n(j - 1)}ai
    and n(k) <= n/P^k, when k = floor(logn / logP) + 1, n(k) = 0, g(n(k)) = g(0) = 0
    g(n)    = Σ{1 <= i <= bn}ai // why?
    // would like to verify but the original paper of this proof is in German
    lim(g(n) / logn) = 0, lim(f(n) / logn - f(P) / logP) = 0, lim(f(n) / logn) = f(P) / logP = c
    by normalization condition, f(2) / log2 = c = 1, f(P) = logP
    for composite number N = ΠPi, f(N) = f(ΠPi) = Σf(Pi) = ΣlogPi = logN
let p = r/s be arbitrary rational number
f(s)    = Hs(1/s, .., 1/s)
        = Hr+1(1/s, .., 1/s, (s - r)/s) + (s - r)/s * f(s - r)
        = h(r/s) + r/s * f(r) + (s - r)/s * f(s - r)
h(r/s)  = f(s) - r/s * f(r) - (s - r)/s * f(s - r)
        = logs - r/s * logr - (s - r)/s * log(s - r)
        = -r/s * log(r/s) - (s - r)/s * log((s - r)/s)
by continuity h(x) is also defined for irrational numbers
inductively,
Hn(p1, .., pn)  = Hn-1(S2, p3, .., pn) + S2H(p1/S2, p2/S2)
                = -S2logS2 - Σ{3 <= i <= n}pilogpi - p1/S2 * log(p1/S2) - p2/S2 * log(p2/S2)
                = -p1logpi - p2logp2 - Σ{3 <= i <= n}pilogpi
                = -Σ{1 <= i <= n}pilogpi

2.47
there are n^2 arrangements but fewer events
1.  the resulting sequence is still ordered, one out of n cards is replaced at the original place, p = n/n^2 = 1/n
2.  the chosen card is replaced one place from origin, 2 + 2(n-2) = 2n - 2 arrangements 
    (the first and the last card have only one possible arrangement)
    which are mapped to n - 1 events, p = 2/n^2 each
3.  the chosen card is replaced at at least 2 place from origin, 2(n - 2) + (n - 2)(n - 3) = (n - 2)(n - 1) arrangements
    all events in this category are disjoint, p = 1/n^2 each
H(X)    = -1/nlog(1/n) - (n - 1) * (2/n^2) * log(2/n^2) - (n - 2)(n - 1)/n^2 * log(1/n^2)
        = logn / n + 4(n - 1)/n^2 * logn - 2(n - 1)/n^2 + 2(n - 2)(n - 1)/n^2 * logn
        = logn / n + 2(n - 1)/n * logn - 2(n - 1)/n^2
        = (2n - 1)/n * logn - 2(n - 1)/n^2

2.48
a.  X^N is a function of N, 
    I(N; X^N) = H(N) - H(N|X^N)
    = H(N)
    = -Σpilogpi
    = -Σ(1/2)^ilog(1/2)^i
    = Σi(1/2)^i
    = (1 - 1/2)/(1/2)^2 = 2
b.  H(X^N|N) = 0
c.  H(X^N) = H(N) = 2
d.  I(N;X^N)    = H(N) - H(N|X^N)
                = H(N)
                = H(1/3) = 0.918
e.  H(X^N|N)    = Σp(n)H(X^N|N = n)
                = 1/3 * 6 + 2/3 * 12
                = 10
f.  H(X^N)      = H(1/3) + 1/3H(X^6) + 2/3H(X^12)   // problem 2.10
                = 10.918

3.1
a.  E[X]    = int(0, ∞, xf(x)dx)
            >= int(t, ∞, xf(x)dx)
            >= int(t, ∞, tf(x)dx)
            = tPr{X >= t}
    Pr{X >= t} <= E[X] / t
    let D(X) = {0, 1, 2, ..}
    Σ{x >= 1}p(x) = E[X] = Σ{x >= 1}xp(x)
    Σ(x >= 2)(x - 1)p(x) = 0
    as (x - 1) > 0 for x >= 2, p(x) >= 0, p(x) = 0 for all x >= 2
    any random variable where p(0) + p(1) = 1 achieves this inequality with equality
b.  Pr{|Y - μ| > ε} = Pr{(Y - μ)^2 > ε^2}
                    <= Pr{(Y - μ)^2 >= ε^2}
                    = Pr{X >= ε^2}
                    <= E[X] / ε^2   // Markov's inequality
                    = E[(Y - μ)^2] / ε^2
                    = E[Y^2 - 2μY + μ^2] / ε^2
                    = (E[Y^2] - E^2[Y]) / ε^2
                    = σ^2 / ε^2
c.  Pr{|Zn - μ| > ε}    <= Var(Z - μ) / ε^2
                        = Var(ΣZi / n) / ε^2
                        = 1/n^2 * ΣVar(Zi) / ε^2
                        = n/n^2 * Var(Zi) / ε^2
                        = σ^2/(nε^2)

3.2
1/n * log(p(X^n)p(Y^n) / p(X^n, Y^n))
= 1/n * Σlog(p(Xi)p(Yi) / p(Xi, Yi))    // i.i.d.
-> E[log(p(Xi)p(Yi) / p(Xi, Yi))]   // weak law of large numbers
= -E[log(p(Xi, Yi) / p(Xi)p(Yi))]
= -I(X;Y)


3.3
define X as
    p(2/3) = 3/4
    p(3/5) = 1/4
let Xi be iids ~ p(x), let piece of cake after n cuts = C
C = ΠXi
1/n * logC  = 1/n * logΠXi
            = 1/n * ΣlogXi
            -> E[logXi]
            = 3/4 * log(2/3) + 1/4 * log(3/5)
            = -0.623
logC -> -0.623 * n
C -> 2^(-0.623n) = 0.649^n

3.4
a.  |-1/n * logp(x^n) - H| <= ε
    H - ε <= -1/n * logp(x^n) <= H + ε
    -n(H - ε) >= logp(x^n) >= -n(H + ε)
    2^(-n(H - ε)) >= p(x^n) >= 2^(-n(H + ε))
    by property 2, Pr{X^n ∈ A^n} > 1 - ε
    hence Pr{X^n ∈ A^n} -> 1
b.  Pr{X^n ∈ B^n}   = Pr{|1/n * ΣXi - μ| <= ε}
                    = 1 - Pr{|1/n * ΣXi - μ| > ε}
                    >= 1 - Var(1/n * ΣXi) / ε^2 // Chebyshev's inequality
                    = 1 - Var(Xi) / nε^2
                    -> 1
    Pr{X^n ∈ A^n ∩ B^n} >= 1 - (Pr{X^n ∉ A^n} + Pr{X^n ∉ B^n})
                        -> 1
c.  |A^n ∩ B^n| <= |A^n| <= 2n(H + ε) // property 3
d.  Pr{X^n ∈ A^n ∩ B^n} = Σ{x^n ∈ A^n ∩ B^n}p(x)
                        <= Σ{x^n ∈ A^n ∩ B^n}2^(-n(H - ε))
                        = |A^n ∩ B^n|2^(-n(H - ε))
                        -> 1
    |H(A^n ∩ B^n)| -> 1/2^(-n(H - ε)) = 2^n(H - ε)
    for large enough n, |H(A^n ∩ B^n)| >= (1/2) * 2^n(H - ε)

3.5
a.  1 >= Pr{Cn(t)}  = Σ{x^n ∈ Cn(t)}p(x^n)
                    >= Σ{x^n ∈ Cn(t)}2^(-nt)
                    = |Cn(t)|2^(-nt)
    |Cn(t)| <= 2^nt
b.  when t = H(X) + ε, Cn(t) = {x^n: p(x^n) <= 2^-n(H - ε)} ⊇ Aε^n
    hence for t >= H + ε, Pr{Cn(t)} >= Pr{Aε^n} -> 1 for small ε

3.6
p(X1, .., Xn)^(1/n)
= 2^(1/n * logp(X1, .., Xn))
-> 2^(Elogp(X))
= 2^-H(X)

3.7
a.  there are 
        C(100, 0) + C(100, 1) + C(100, 2) + C(100, 3)
        = 166751
    codewords, log(166751) = 17.347 <= 18
    minimum length of codewords is 18 bits
b.  0.995^100 * C(100, 0) 
    + 0.005 * 0.995^99 * C(100, 1) 
    + 0.005^2 * 0.995^98 * C(100, 2) 
    + 0.005^3 * 0.995^97 * C(100, 3)
    = 0.998
c.  μ = E[ΣXi] = 100 * E[Xi] = 0.5
    σ^2 = Var(ΣXi) = 100 * Var(Xi) = 100 * 0.005(1 - 0.005) = 0.4795
    Pr{X > 3}   <= Pr{|X - 0.5| > 2.5}
                <= σ^2 / 2.5^2
                = 0.00368

3.8
let P = (X1X2..Xn)^(1/n)
logP    = 1/n * logX1X2..Xn
        = 1/n * ΣlogXi
        -> E[logXi]  // weak law of large numbers
        = 1/2 * 0 + 1/4 * 1 + 1/4 * log3
        = 0.646
P -> 2^0.646 = 1.565

3.9
a.  lim(-1/n * logq(X1, .., Xn))
    = lim(-1/n * Σlogq(Xi))
    = -Ep[logq(Xi)]
    = -Σp(x)logq(x)
b.  1/n * log(q(X1, .., Xn) / p(X1, .., Xn))
    -> Σp(x)logq(x) + H(X)
    = Σp(x)logq(x) - Σp(x)logp(x)
    = Σp(x)log(q(x) / p(x))
    = -Σp(x)log(p(x) / q(x))
    = -D(p||q)

3.10
ln(Vn^(1/n))    = 1/n * lnVn
                = 1/n * ΣlnXi
                -> E[lnXi]
                = int(0, 1, lnxdx)
                = int(0, 1, d(xlngx - x))
                = -1
Vn^(1/n) -> e^-1 = 1/e
while E[Vn] = E[ΠXi] = ΠE[Xi] = (1/2)^n, 
E[Vn]^(1/n) = ((1/2)^n)^(1/n) = 1/2 > 1/e

3.11
a.  Pr{X ∉ A ∩ B}   <= Pr{X ∉ A} + Pr{X ∉ B}
                    = ε1 + ε2
    Pr{A ∩ B}   >= 1 - ε1 - ε2
b.  3.34:   part a
    3.35:   definition of discrete probability
    3.36:   the definition of Aε^n
    3.37:   2^-n(H - ε) is independent to x
    3.38:   intersection cannot increase set size
c.  |Bδ^n|  >= (1 - ε - δ)2^n(H - ε)
    as δ < 1/2,
    |Bδ^n|  > (1 - δ - ε)2^n(H - ε)
    log|Bδ^n|   > log(1 - δ - ε) + n(H - ε)
    log|Bδ^n|/n > log(1 - δ - ε)/n + H - ε
                -> H - ε

3.12
a.  let p2n(x) = pn(x)/2 + qn(x)/2, where qn(x) = 1/n * Σ{n + 1 <= i <= 2n}I(Xi = x)
    E[D(p2n || p)]  = E[D(pn/2 + qn/2||p/2 + p/2)]
                    <= E[1/2D(pn||p) + 1/2D(qn/2||p)]   // convexity of relative entropy
                    = 1/2E[D(pn||p)] + 1/2E[D(qn/2||p)]
                    = E[D(pn||p)]   // Xi's are iid
b.  pn(x)   = 1/n * Σ{1 <= i <= n}I(Xi = x)
            = 1/n * (Σ{i != j}I(Xi = x) + I(Xj = x))
            = (n - 1) / n * 1/(n - 1) * Σ{i != j}I(Xi = x) + I(Xj = x)/n
            = (n - 1)/n * pn-1(j, x) + I(Xj = x)/n
    where pn-1(j, x) = 1/(n - 1) * Σ{i != j}I(Xi = x)
    npn(x)  = Σ(1/n * Σ{1 <= j <= n}Σ{i != j}(I(Xi = x) + I(Xj = x)))
            = Σ{1 <= j <= n}((n - 1)/n * pn-1(j, x)) + Σ{1 <= j <= n}I(Xj = x)
            = (n - 1)/n * Σ{1 <= j <= n}pn-1(j, x) + pn(x)
    pn(x)   = 1/n * Σpn-1(j, x)
    E[D(pn||p)  = E[D(Σpn-1(j, x)/n || p)]
                <= E[Σ1/n * D(pn-1(j, x)||p)]   // convexity
                = E[D(pn-1(x)||p)]  // iid

3.13
a.  H(X) = -0.6log0.6 - 0.4log0.4 = 0.971
b.  2^-n(H + ε) <= p(X1, .., Xn) = 0.6^k * 0.4^(n - k) <= 2^-n(H - ε)
    11 <= k <= 19
    Pr{Aε^n}    = Σ{11 <= k <= 19}C(n, k)0.6^k * 0.4^(n - k)
                = 0.936
    |Aε^n|  = Σ{11 <= k <= 19}C(n, k)
            = 26366510
c.  as Σ{12 <= k <= 25}C(25, k)0.6^k * 0.4^(25 - k) > 0.9
    Σ{13 <= k <= 25}C(25, k)0.6^k * 0.4^(25 - k) = 0.8462322310242371062726656
    0.6^12 * 0.4^(25 - 12) = 1.46081389744226304 × 10^-8
    (0.9 - 0.8462322310242371062726656) / 1.46081389744226304 × 10^-8 <= 3680673
    |B| = Σ{13 <= k <= 25}C(25, k) + 3680673 = 20457889
d.  |Aε^n ∩ B| = Σ{13 <= k <= 19}C(25, k) + 3680673 = 16708810
    Pr{Aε^n ∩ B} = Σ{13 <= k <= 19}C(25, k)0.6^k * 0.4^(25 - k) + (0.9 - 0.846) = 0.871

4.1
a.  Σ{i}bi  = Σ{i}Σ{j}ajPij
            = Σ{j}ajΣ{i}Pij
            = Σ{j}aj
            = 1
    bi  = Σ{j}ajPij >= 0
    H(b) - H(a) = -Σ{j}Σ{i}aiPjilogbj + Σ{i}ailogai
                = -Σ{j}Σ{i}aiPjilogbj + Σ{j}Σ{i}aiPjilogai
                = Σ{j}Σ{i}aiPjilog(ai / bj)
                = Σ{j}Σ{i}aiPjilog(aiPji / bjPji)
                >= (Σ{j, i}aiPji) * log(Σ{j, i}aiPji / Σ{j, i}bjPji)    // log sum inequality
                = log1/1 = 0
    H(b) >= H(a)
b.  μi  = Σ{i}μiPij
        = Σ{i}(1/n)Pij
        = 1/n * Σ{i}Pij = 1/n
c.  1/n = Σ{i}(1/n)Pij
    1/n = 1/n * Σ{i}Pij
    Σ{i}Pij = 1
    and Σ{j}Pij = 1 by definition of transition matrix

4.2
H(X0|X-1, .., X-n)  = H(X0, X-1, .., X-n) - H(X-1, .., X-n) 
                    = H(Xn, .., X0) - H(Xn, .., X1) // stationarity
                    = H(X0|Xn, .., X1)

4.3
4.82:   conditioning cannot increase entropy
4.83:   T is a invertible matrix, there's a bijection between T^-1TX and TX
4.84:   matrix operations
4.85:   T and X are independent

4.4
let P13 = P23 = 1, P31 = P32 = 1/2, all other entries of transition matrix is 0
H(Xn|X1 = 3) oscillates between 1 and 0

4.5
a.  N1 is a function of Tn, H(N1, Tn) = H(Tn) + H(N1|Tn) = H(Tn)
b.  chain rule of entropy
c.  N1 is uniformly distributed in {1, .., n - 1}, H(N1) = log(n - 1)
d.  H(Tn|N1)    = ΣPr{N1 = k}H(Tn|N1 = k)
                = Σ1/(n - 1)H(Tn|N1 = k)
    where given N1 = k, entropy of Tn is combined entropy of two subtrees Tk and Tn-k
                = Σ1/(n - 1)(H(Tk) + H(Tn-k))
e.  each Ti in Σ(H(Tk) + H(Tn-k)) is added twice
f.  (n - 1)Hn   = (n - 1)log(n - 1) + 2Σ{1 <= k <= n - 1}Hk
                = (n - 1)log(n - 1) + 2Σ{1 <= k <= n - 2}Hk　+ 2Hn-1
                = (n - 1)log(n - 1) - (n - 2)log(n - 2) + nHn-1

4.6
a.  1/n * H(X1, .., Xn) = 1/n * ΣH(Xi|X1, .., Xi-1)
    where H(Xi|X1, .., Xi-1) = H(Xi+1|X2, .., Xi) >= H(Xi+1|X1, .., Xi), H(Xi|X1, .., Xi-1) is non-increasing
    the average of ΣH(Xi|X1, .., Xi-1) over (1, n) <= over (1, n - 1)
b.  1/n * H(X1, .., Xn) = 1/n * ΣH(Xi|X1, .., Xi-1)
                        >= 1/n * nH(Xn|X1, .., Xn-1)
                        = H(Xn, .., Xn-1)

4.7
a.  by equation 4.8, μ0 = p10 / (p10 + p01), μ1 = p01 / (p10 + p01)
    H(X)    = p10 / (p10 + p01) * H(p01) + p10 / (p10 + p01) * H(p10)
b.  p01 = p10 = 1/2
    p10 / (p10 + p01) * H(p01) + p10 / (p10 + p01) * H(p10)
    <= (p10 + p01) / (p10 + p01) * H(1/2)
    = H(1/2)
    = 1/2 / (1/2 + 1/2) * H(1/2) + 1/2 / (1/2 + 1/2) * H(1/2)
    maximum entropy rate is H(1/2) = 1
c.  μ0  = ΣμiPi0
        = μ0(1 - p) + μ1
    μ0p = μ1
    μ0 + μ1 = (1 + p)μ0 = 1, μ0 = 1/(1 + p), μ1 = p/(1 + p)
    H(X)    = H(X2|X1)
            = 1/(1 + p) * H(p) + p/(1 + p)H(0)
            = 1/(1 + p) * H(p)
d.  H(X) <= 0.694 where x = 0.382
e.  assume p > 0
    N(1) = 2, N(2) = 3  // 11 is impossible, P11 = 0
    for a sequence of length t, if the first state is 0, the next state may be 0 or 1
    if the first state is 1, the next state must be 0
    N(t) = N(t - 1) + N(t - 2) is the Fibonacci sequence
    N(t) = Fib(t + 2)
    lim(1/t * logN(t))  = lim(1/t * log((((1 + sqrt(5)) / 2)^t - ((1 - sqrt(5)) / 2)^t) / sqrt(5)))
                        <= lim(1/t * (log(((1 + sqrt(5))/2)^t) - log(sqrt(5))))
                        = log((1 + sqrt(5))/2)
                        = 0.694 

4.8
H(X) / E[T] = (-p1logp1 - (1 - p1)log(1 - p1)) / (p1 + 2(1 - p1))
maximum is 0.694 at x = (sqrt(5) - 1) / 2

4.9
H(X0|Xn)    = H(X0, Xn) - H(Xn)
            >= H(X0, Xn|Xn-1) - H(Xn)
            = H(X0|Xn-1) + H(Xn|Xn-1) - H(Xn)
            >= H(X0|Xn-1)

4.10
a.  let Sn = ΣXi
    Pr{S1 = 0} = Pr{X1 = 0} = Pr{X1 = 1} = Pr{S1 = 1} = 1/2
    Pr{Sn+1 is even}    = Pr{Xn+1 = 0}Pr{Sn is even} + Pr{Xn+1 = 1}Pr{Sn is odd}
                        = Pr{Xn+1 = 0}Pr{Sn is odd} + Pr{Xn+1 = 1}Pr{Sn is even}
                        = 1/2
    for i, j <= n - 1, Xi and Xj are independent as stated
    for i <= n - 1 and j = n,
    P(Xn = 0|Xi = 0)    = Pr{Σ{j!=i}Xi is even} = 1/2
    P(Xn = 1|Xi = 0)    = Pr{Σ{j!=i}Xi is odd} = 1/2
    similarly P(Xn = 1|Xi = 1) = P(Xn = 0|Xi = 1) = 1/2
    P(Xn, Xi) = P(Xn)P(Xi)
b.  H(Xi, Xj) = H(Xi) + H(Xj) = 2H(1/2) = 2
c.  Xn is a function of (X1, .., Xn)
    H(X1, .., Xn) = H(X1, .., Xn-1) = (n - 1)H(1/2) = n - 1 < nH(X1)

4.11
a.  H(Xn|X0)    = H(Xn, X0) - H(X0)
                = H(X0, X-n) - H(X0)    // stationarity
                = H(X-n|X0)
b.  // thanks solutions manual
    let X0, .., Xn-1 be iid random variables ~ Bernoulli(1/2)
    and Xn+k = Xk for k >= 0
    H(Xn|X0) = 0 < H(Xn-1|X0) = 1
c.  H(Xn|X1, .., Xn-1, Xn+1)
    = H(Xn+1|X2, .., Xn, Xn+2)  // stationarity
    >= H(Xn+1|X1, .., Xn, Xn+2)
d.  similar to part c

4.12
a.  H(X1, .., Xn)   = ΣH(Xi|X1, .., Xi-1)
                    = H(X1) + (n - 1)H(X2|X1)
                    = 1 + (n - 1)H(0.1)
b.  H(X)    = lim(H(X1, .., Xn) / n)
            = lim(1/n + (n - 1)/n * H(0.1))
            = H(0.1)
c.  let S be the number of steps before reversing direction
    S = 1 + P - 1 = P, P ~ Geometric(0.1)
    E[S] = E[P] = 1/0.1 = 10

4.13
lim(1/2n * I(X1, .., Xn;Xn+1, .., X2n))
= lim(1/2n * H(Xn, .., X2n) - 1/2n * H(Xn, .., X2n|X1, .., Xn))
= lim(1/2n * H(X1, .., Xn) - 1/2n * Σ{i <= n}H(Xn+i|X1, .., Xn+i-1))    // stationarity and chain rule of entropy
= H(X)/2 - H(X)/2   // Cesaro mean
= 0

4.14
a.  (Y1, .., Yn) is a function of (X1, .., Xn)
    H(Y1, .., Yn) <= H(X1, .., Xn)
    lim(H(Y1, .., Yn)/n) <= lim(H(X1, .., Xn)/n)
    H(Y) <= H(X)
b.  (Z1, .., Zn) is a function of (X1, .., Xn+1)
    H(Z1, .., Zn) <= H(X1, .., Xn+1)
    lim(H(Z1, .., Zn)/n)    <= lim(H(X1, .., Xn+1)/n)
                            = lim(H(X1, .., Xn)/n + H(Xn+1|X1, .., Xn)/n)
                            = H(X)
    H(Z) <= H(X)

4.15
1/n * H(X1, .., Xn|X0, .., X-k)
= 1/n * H(X1+k, .., Xn+k|X0, .., Xk)  // stationarity
= 1/n * ΣH{1 <= i <= n}(Xk+i|X0, .., Xk+i-1)    // chain rule of entropy
-> H(X) // Cesaro mean as lim{n -> ∞}(H(Xk+n)|X0, .., Xk+n-1) = lim{n -> ∞}(H(Xn|X0, .., Xn-1)) = H(X)

4.16
a.  denote vertices of the graph from left to right as s1, s2 and s3 
    s1 is the state of bit 1, s2 is the state of bit 0 after bit 1, s3 is the state of bit 0 after bit 0
    obviously a valid sequence of bits can be mapped to a valid path on the graph
    but some path on the graph cannot be mapped to a valid sequence of bits
    there are 4 paths of length 1 but only 3 valid sequences of 2 bits
b.  two edges incoming at s1, X1(n) = X2(n - 1) + X3(n - 1)
    one edge incoming at s2, X2(n) = X1(n - 1)
    one edge incoming at s3, X3(n) = X2(n - 1)
    a valid sequence of length 1 is 0 or 1, mapped to s1 or s2, X1(1) = 1, X2(1) = 1, X3(1) = 0
c.  X(n)    = A^n-1 * X(1)
            = U^-1A^n-1U * X(1)
    // where A^n-1 = [λ1^n-1, 0, 0; 0, λ2^n-1, 0; 0, 0, λ3^n-1]
            = λ1^n-1U^-1[1, 0, 0;0, 0, 0;0, 0, 0]UX(1) + ..
            = λ1^n-1Y1 + λ2^n-1Y2 + λ3^n-1Y3
    assume the largest term dominates the sum as n -> ∞,
    1/n * logX(n)   = (n - 1)/n * logλ + logY/n
                    -> logλ
    // thanks wolframalpha
    eigenvalues of A is λ1 = 1.325, λ2 = -0.662 + 0.562i, λ3 = -0.662 - 0.562i
    1/n * logX(n) -> logλ1 = 0.406
d.  αμ2 + μ3 = α/(3 - α) + (1 - α)/(3 - α) = 1/(3 - α) = μ1
    μ1 = 1/(3 - α) = μ2
    (1 - α)μ2 = (1 - α)/(3 - α) = μ3
e.  H(X)    = -ΣμiPijlogPij
            = -1/(3 - α) * H(α)
    H(X) is maximized at x = 0.570, max{H(X)} = 0.406
f.  // thanks solutions manual
    the approach in part a - c directly counts the number of possible sequences which is X(n) -> λ^n
    the approach in part d - e computes the entropy rate and the number of sequences in the typical set is 2^nH
    where H = logλ, 2^nH = λ^n, and Pr{Aε^n} -> 1 as n -> ∞

4.17
a.  given X0 = x0, Pr{Xi = X0|X0 = x0} = Pr{Xi = x0} = 1/m
    Pr{Xi = X0} = Σ{x}Pr{Xi = x}Pr{X0 = x} = 1/m
    N ~ Geometric(1/m), E[N] = m
b.  f(x) = logx is concave, by Jensen's inequality
    E[logN] <= logE[N] = logm = H(X)
c.  // skipped

4.18
a.  I(Y1;Y2|X)  = H(Y1|X) - H(Y1|Y2, X)
                = 2H(Y1|X)  // given X, Y1 and Y2 are independent
                = 1/2 * H(p) + 1/2 * H(1 - p)
                = 2H(p)
b.  let tail = 0, head = 1, the distribution of (Y1, Y2) is
        p(0, 0) = 1/2 * p^2 + 1/2 * (1 - p)^2
        p(1, 1) = p(0, 0)
        p(0, 1) = 1/2 * p(1 - p) + 1/2 * p(1 - p) = p(1 - p)
        p(1, 0) = p(0, 1)
    I(X;Y1, Y2) = H(Y1, Y2) - H(Y1, Y2|X)
                = -Σ{i, j}p(i, j)logp(i, j) - 2H(p)
                = -(p^2 + (1 - p)^2)(log(p^2 + (1 - p)^2) - 1) - 2p(1 - p)(logp + log(1 - p)) - 2H(p)
c.  H(X, Y1, .., Yn)    = H(X) + H(Y1, .., Yn|X)
                        = 1 + nH(Y1|X) = 1 + nH(p)
    lim(1/n * H(X, Y1, .., Yn)) = lim(H(Y1, .., Yn)/n + H(X|Y1, .., Yn)/n)
                                = lim(H(Y1, .., Yn)/n)
                                = lim((1 + nH(p))/n)
                                = H(p)

4.19
a.  // assume uniform edge weight
    μ = [3/16, 3/16, 3/16, 3/16, 1/4]
b.  H(X)    = -ΣμiPijlogPij
            = -4 * 3 * 3/16 * 1/3 * log1/3 - 4 * 1/4 * 1/4 * log1/4
            = 3/4 * log3 + 1/4 * log4
            = 3log3 / 4 + 1/2
c.  I(Xn+1;Xn)  = I(X2;X1)  // stationarity
                = H(X2) - H(X2|X1)
                = H(X1) - H(X2|X1)
                = H(μ) - 3log3 / 4 - 1/2
                = -4 * 3/16 * log3/16 - 1/4 * log1/4 - 3log3 / 4 - 1/2
                = -3/4 * (log3 - 4) + 1/2 - 3log3 / 4 - 1/2
                = -3log3 / 2 + 3

4.20
H(X)    = log(2E) - H(E1/2E, .., En/2E)
        = log(2E) + ΣEi/2E * log(Ei/2E)
        = log(2E) + ΣEi/2E * (logEi - log2E)
        = 1/2E * ΣEi * logEi
King:
    at {1, 3, 7, 9}: 3 edges
    at {2, 4, 6, 8}: 5 edges
    at {5}: 8 edges
    2E = 3 * 4 + 5 * 4 + 8 = 40
    H(X)    = log40 + 4 * 3/40 * log3/40 + 4 * 5/40 * log5/40 + 8/40 * log8/40
            = 2.236
Rook:
    4 edges everywhere
    2E = 4 * 9 = 36
    H(X)    = log36 + 9 * 4/36 * log(4/36)
            = log4 = 2
Bishop (type 1):
    at {1, 3, 7, 9}: 2 edges
    at {5}: 4 edges
    {2, 4, 6, 8} unreachable
    2E = 2 * 4 + 4 = 12
    H(X)    = log12 + 4 * 2/12 * log(2/12) + 4/12 * log(4/12)
            = 2/3 + 2/3 = 4/3
Bishop (type 2):
    at {2, 4, 6, 8}: 2 edges
    {1, 3, 5, 7, 9} unreachable
    2E = 8
    H(X)    = 1/8 * (4 * 2log2)
            = 1
Queen:
    at {1, 3, 7, 9}: 6 edges
    at {2, 4, 6, 8}: 6 edges
    at {5}: 8 edges
    2E = 8 * 6 + 8 = 56
    H(X)    = 1/56 * (8 * 6log6 + 8log8)
            = 1/56 * (48 + 48log3 + 24)
            = 1/56 * (72 + 48log3)
            = 9/7 + 6log3/7

4.21
1.  {(1, 2), (2, 3), (3, 4), (4, 5)}
    H(X)    = 1/8 * (2 * 1log1 + 3 * 2log2)
            = 1/8 * 3 * 2 = 6/8 = 0.75
2.  {(1, 2), (1, 3), (1, 4), (1, 5)}
    H(X)    = 1/8 * (4 * 1log1 + 4log4)
            = 1
3.  {(1, 2), (2, 3), (3, 4), (4, 1)}
    H(X)    = 1/8 * (4 * 2log2)
            = 1
4.  {(1, 2), (1, 3), (3, 4), (4, 5)}
    H(X)    = 1/8 * (3 * 1log1 + 2log2 + 3log3)
            = 1/8 * (2 + 3log3)
            = 1/4 + 3/8 * log3 = 0.844
5.  {(1, 2), (2, 3), (3, 1), (1, 4)}
    H(X)    = 1/8 * (1log1 + 2 * 2log2 + 3log3)
            = 1/8 * (4 + 3log3)
            = 1/2 + 3/8 * log3 = 1.094
a.  maximum is 1/2 + 3/8 * log3
b.  minimum is 0.75

4.22
a.  4 corners, 3 edges each
    12 edges, 4 edges each
    6 sides, 5 edges each
    1 core, 6 edges
    2E = 4 * 3 + 12 * 4 + 6 * 5 + 6 = 96
    p(corner)   = 3/96 = 1/32
    p(edge)     = 4/96 = 1/24
    p(side)     = 5/96
    p(core)     = 6/96 = 1/16
b.  H(X)    = 1/96 * (4 * 3log3 + 12 * 4log4 + 6 * 5log5 + 6log6)
            = 2.085

4.23
a.  H(X)    = limH(Xn|X1, .., Xn-1)
            <= limH(Xn)
            = H(X1)
b.  lim(Xn|X1, .., Xn-1) = H(X1) = H(Xn)
    as n -> ∞, Xn is independent to the history

4.24
a.  H(Y)    = lim(1/n * H(Y1, .., Yn))
            = lim(1/n * H(X1, .., Xn+1))
            = lim(1/n * H(X1, .., Xn) + 1/n * H(Xn+1|X1, .., Xn))
            = H(X)
b.  H(Z)    = lim(1/n * H(Z1, .., Zn))
            = lim(1/n * H(X2, .., X2n+1))
            = lim(1/n * H(X1, .., X2n))
            = lim(2 * 1/2n * H(X1, .., X2n))
            = 2H(X)
    H(Z) >= H(X)
c.  H(V)    = lim(H(Vn|V1, .., Vn-1))
            = lim(H(X2n|X2, X4, .., X2n-2))
            >= lim(H(X2n|X1, X2, .., X2n-1))
            = H(X)
d.  same to part b

4.25
a.  I(X;Y1, .., Yn) = H(X) - H(X|Y1, .., Yn)
                    <= H(X) - H(X|Y1, .., Yn+1)
                    = I(X;Y1, .., Yn+1)
b.  H(X|Y1) = H(X|Y1, .., Yn) for all n
    X are independent to all Yi, i >= 2

4.26
see part a fo 4.24

4.27
p(Xk+1 = 0|Xk = 0, Xk-1 = 0) = p(Zk+1 = 0) = 1 - p
p(Xk+1 = 0|Xk = 0, Xk-1 = 1) = p(Xk+1 = 0|Xk = 1, Xk-1 = 0) = p(Zk+1 = 1) = p
p(Xk+1 = 0|Xk = 1, Xk-1 = 1) = p(Zk+1 = 0) = 1 - p
Xk+1 is independent to Xk-1 and Xk
H(X)    = lim(H(Xn|X1, .., Xn-1))
        = lim(H(Xn|Xn-2, Xn-1))
        = ΣPr{Xk = i, Xk-1 = j}H(Xk+1|Xk = i, Xk-1 = j)
        = H(p)

4.28
a.  Pr{Y1+k, .., Yn+k}  = ΣθPr{Y1+k, .., Yn+k, θ}
                        = ΣθPr{Y1+k, .., Yn+k|θ}Pr{θ}
                        = ΣθPr{Y1, .., Yn|θ}Pr{θ}   // given θ, Y1, .., Yn = X11, .., X1n or X21, .., X2n is stationary
                        = Pr{Y1, .., Yn}
b.  Pr{Y1 = 1, Y2 = 1}
    = ΣθPr{Y1 = 1, Y2 = 1|θ}Pr{θ}
    = 1/2 * p1^2 + 1/2 * p2^2 
    Pr{Y1 = 1}
    = ΣθPr{Y1 = 1|θ}Pr{θ}
    = (p1 + p2)/2
    Pr{Y1 = 1}Pr{Y2 = 1} = (p1 + p2)^2 / 4 != Pr{Y1 = 1, Y2 = 1}
    Y1 and Y2 are not independent
c.  lim(1/n * H(Y1, .., Yn))
    = lim(1/n * H(Y1, .., Yn, θ) - 1/n * H(θ|Y1, .., Yn))
    = lim(1/n * H(Y1, .., Yn, θ))
    = lim(1/n * H(X) + 1/n * H(Y1, .., Yn|θ))
    = lim(1/n * ΣPr{θ}H(Y1, .. Yn|θ))
    = H(p1) / 2 + H(p2) / 2
d.  by Theorem 3.1.1 (AEP), the probability converges to H(X) only if X1, .., Xn are iids
e.  AEP does not apply to non-iid sequences, but since as n -> ∞, H(θ|Y1, .., Yn) -> 0
    (Y1, .., Yn) can be encoded by the coding of {X1i} and {X2i}, with an additional bit to distinguish these two
a'. Pr{Zi = 0}  = ΣθPr{Zi = 0|θ}Pr{θ}
                = (p1 + p2)/2, independent to i
    {Zi} are iids and stationary
b'. see part a'
c'. H(Z)    = lim(H(Zn|Z1, .., Zn-1))
            = lim(H(Zn))
            = H((p1 + p2)/2)
d'. AEP
e'. AEP

4.29
a.  {Sn} is not stationary
    Pr{S2 = 2} = Pr{X1 = 1, X2 = 1} = 1/4
    Pr{S1 = 1} = Pr{X1 = 1} = 1/2
b.  there's a bijection between (S1, .., Sn) and (X1, .., Xn)
    H(S1, .., Sn)   = H(X1, .., Xn)
                    = nH(X1)
                    = -nΣp(x)logp(x)
                    = -nΣ(1/2)^xlog(1/2)^x
                    = nΣx(1/2)^x
                    = 2n
c.  H(S)    = lim(1/n * H(S1, .., Sn))
            = lim(1/n * 2n)
            = 2
d.  Sn ~ NB(n, 1/2)
    H(Sn)   = -Σp(x)logp(x)
            = -ΣC(n + k - 1, k)(1/2)^(n+k)logC(n + k - 1, k)(1/2)^(n+k)
    and H(Sn) is the average description length of Sn, which is also the number of coin flips required to generate Sn

4.30
a.  1/3 * 1/2 + 1/3 * 1/4 + 1/3 * 1/4 = 1/3
    [1/3, 1/3, 1/3] is a stationary distribution of P
b.  H(X)    = -ΣμiPijlogPij
            = -2 * 1/4 * log1/4 - 1/2 * log1/2
            = 1 + 1/2 = 3/2
c.  Zi is a function of Xi and Xi-1
    Xi is a function of Zi and Xi-1 and recursively a function of (Zi, Zi-1, .., Z1)
    there's a bijection between (Z1, .., Zn) and (X1, .., Xn)
    H(Z1, .., Zn)   = H(X1, .., Xn)
                    -> nH(X) = 3n/2
d.  H(Xn) = H(μ) = -3 * 1/3log1/3 = log3
    Pr{Zi = 0}  = Pr{Xi = Xi-1}
                = 3 * 1/3 * 1/2 = 1/2
    Pr{Zi = 1}  = μ1P12 + μ2P23 + μ3P31
                = 1/3(1/4 + 1/4 + 1/4) = 1/4
    Pr{Zi = 2}  = μ1P13 + μ2P21 + μ3P32
                = 1/3(1/4 + 1/4 + 1/4) = 1/4
    H(Zn)   = -Σp(z)logp(z)
            = -2 * 1/4 * log1/4 - 1/2 * log1/2
            = 1 + 1/2 = 3/2
e.  by part d, Zn and Zn-1 are independent, H(Zn|Zn-1)  = H(Zn) = 3/2
f.  by part d, Zn and Zn-1 are independent

4.31
a.  H(X)    = lim(1/n * H(X1, .., Xn))
            = lim(1/n * nH(p))
            = H(p)
    there's a bijection between {Xn} and {Yn}
    H(X1, .., Xn) = H(Y1, .., Yn)
    H(Y)    = lim(1/n * H(Y1, .., Yn))
            = lim(1/n * H(X1, .., Xn))
            = H(p)

4.32
H(X-n|X0, X1)   = H(X-n, X0, X1) - H(X0, X1)
                = H(X-n) + H(X0|X-n) + H(X1|X0, X-n) - H(X0) - H(X1|X0)
                = H(X0) + H(X0|X-n) + H(X1|X0) - H(X0) - H(X1|X0)   // stationarity and Markovity
                = H(X0|X-n)
                = H(Xn+1|X1)    // stationarity
                = H(Xn+1|X1, X0)    // Markovity

4.33
I(X1;X4) + I(X2;X3) - I(X1;X3) - I(X2;X4)
= H(X1) + H(X4) - H(X1, X4) + H(X2) + H(X3) - H(X2, X3) - H(X1) - H(X3) + H(X1, X3) - H(X2) - H(X4) + H(X2, X4)
= H(X1, X3) + H(X2, X4) - H(X1, X4) - H(X2, X3)
= H(X3) + H(X1|X3) + H(X4) + H(X2|X4) - H(X4) - H(X1|X4) - H(X3) - H(X2|X3)
= H(X1|X3) - H(X1|X4) + H(X2|X4) - H(X2|X3)
since 
    I(X1;X3) >= I(X1;X4), H(X1) - H(X1|X3) >= H(X1) - H(X1|X4), H(X1|X4) >= H(X1|X3)
    similarly H(X2|X4) >= H(X2|X3)
>= 0

4.34
// thanks solutions manual
I(X;Y) + I(Z;W) - I(X;Z) - I(X;W)
>= I(Z,W;X) + I(Z;W) - I(X;Z) - I(X;W)  // data processing inequality
= H(Z, W) + H(X) - H(Z, W, X) + H(Z) + H(W) - H(Z, W) - H(X) - H(Z) + H(X, Z) - H(X) - H(W) + H(X, W)
= - H(Z, W, X) + H(X, Z) - H(X) + H(X, W)
= H(Z|X) - H(Z|X, W)
= I(Z, W|X) >= 0

4.35
H(Xn|X0) - H(Xn-1|X0) - H(Xn-1|X0) + H(Xn-2|X0)
= H(Xn|X0) - H(Xn-1|X0, X-1) - H(Xn-1|X0) + H(Xn-2|X0, X-1) // Markovity
= H(Xn|X0) - H(Xn|X1, X0) - H(Xn-1|X0) + H(Xn-1|X1, X0) // stationarity
= I(Xn;X1|X0) - I(Xn-1;X1|X0)
= H(X1|X0) - H(X1|X0, Xn) - H(X1|X0) + H(X1|X0, Xn-1)
= H(X1|X0, Xn-1) - H(X1|X0, Xn)
= H(X1|X0, Xn-1, Xn) - H(X1|X0, Xn) // Markovity
= -I(X1;Xn-1|X0, Xn) <= 0

5.1
instantaneous code is a subset of uniquely decodable code hence L2 <= L1
L2 is uniquely decodable so L2 satisfies Kraft inequality, by 5.2.2 there exists a prefix code with exactly the same set
of lengths, therefore L1 <= L2, L1 = L2

5.2
by Kraft's inequality, ΣD^-li <= 1, 1/D + 1/D^2 + 1/D^3 <= 1/2 for integer D, D >= 3
let S be the set and probability of numbers, assume the codewords of the numbers are written in unknown language (e.g.
Martian language), a lower bound of the size of the alphabet can be computed from the corresponding codeword lengths, 
since human uses decimal digits mainly because human has 10 fingers, D maybe the number of fingers of Martians

5.3
consider the D-ary tree in the proof of theorem 5.2.1, when ΣD^-li < 1, there must be a leaf at level lmax that is not
a codeword, any string starting with this leaf also cannot be decoded (case l >= lmax)
by definition of instantaneous code, prefix of a codeword of length lmax cannot be codeword (case 1 <= l < lmax)

5.4
a.  x1: 0
    x2: 10
    x3: 110
    x4: 11100
    x5: 11101
    x6: 11110
    x7: 11111
b.  E[L]    = 0.49 + 0.26 * 2 + 0.12 * 3 + (0.04 + 0.04 + 0.03 + 0.02) * 5
            = 2.02
c.  x1: 0
    x2: 1
    x3: 20
    x4: 21
    x5: 220
    x6: 221
    x7: 222

5.5
(00, 10, 01, 010, 011)
a Huffman code for (1/5, .., 1/5) is 
    (000, 001, 10, 11, 01)
which has the same expected codeword length to (00, 10, 01, 010, 011)

5.6
a.  possible optimal code for (1/3, 1/3, 1/3)
b.  (00, 01, 10, 11) is a better code for any |X| = 4
c.  (0, 1) is a better code for any |X| = 2 

5.7
a.  the question scheme is equivalent to a code scheme, hence the minimal average number of questions = the minimal
    length of codewords >= H(X1, .., Xn) = ΣH(Xi) = ΣH(pi)
b.  consider the possible defective sets as a set of 2^n symbols, the last question of the longest sequence of questions
    = the last bit of longest codewords distinguishes the two least possible sets, i.e. the set {Xi = 0} and the set
    {Xi = 1, Xj = 0 for all j != i} where pi = min{pj}
c.  by theorem 5.4.1, the optimal code has E[L] <= H(X1, .., Xn) + 1 = ΣH(Xi) + 1

5.8
C1 = (0, 10, 11), E[C(Xi+1)|Xi = 1] = 1.5
C2 = (10, 0, 11), E[C(Xi+1)|Xi = 2] = 1.5
C3 = (-, 0, 1), E[C(Xi+1)|Xi = 3] = 1
u1 = 1/2 * u1 + 1/4 * u2
u2 = 1/4 * u1 + 1/2 * u2 + 1/2 * u3
u3 = 1/4 * u1 + 1/4 * u2 + 1/2 * u3
(u1, u2, u3) = (2/9, 4/9, 1/3)
E[C(Xi+1)] = 3/2 * 2/9 + 3/2 * 4/9 + 1 * 1/3 = 1/3 + 2/3 + 1/3 = 4/3
H(U)    = -ΣuiPijlogPij
        = (2/9 + 4/9) * (1/2log2 + 2 * 1/4log4) + 1/3 * H(1/2)
        = 2/3 * 2/3 + 1/3 = 4/3

5.9
given ε > 0, take p so that H(p) < ε (always possible as H(0) = 0, H(1/2) = 1, H(p) is continuous)
define a symbol set p(X = a) = p, p(X = b) = 1 - p
Huffman code of X is (0, 1), E[L] = 1, E[L] - H(X) > 1 - ε

5.10
a.  by theorem 5.3.1, L = HD(X) iff D^-li = pi, since li is a positive integer pi = 3^-k for some k
b.  pi = 3^-ki, let k >= ki for all i
    Σpi = Σ3^-ki = 1
    3^k * Σ3^(k-ki) = 3^k
    since 3^k is odd, both terms on the left side must be odd
    each 3^(k-ki) is odd, only the sum of odd number of odd terms is odd, hence m is odd

5.11
let C be the suffix code, let C' be the reverse of C (i.e. C'(x) is the reversed codeword C(x))
C'(x) is a prefix code hence uniquely decodable
given a sequence of codewords, reverse it and the sequence can be uniquely decoded by C'
as C and C' has the same codeword lengths, E[C(X)] = E[C'(X)]
let L be the expected codeword length of Huffman code, since Huffman is an optimal prefix code,
    min{E[C(X)]} = min{E[C'(X)]} = L

5.12
a.  C1 = (00, 1, 010, 011)
b.  C2 = (00, 01, 10, 11) is a prefix code
    E[C1(X)] = 2/3 + 1/3 + 3/4 + 3/12 = 2
    E[C2(X)] = 2
c.  in C2, 2 > log3

5.13
let X be the set of objects in the universe, H(X) + 1 > 38.5, H(X) > 37.5
as uniform distribution maximizes entropy,
    log|X| >= H(X) > 37.5,
    |X| >= 2^37.5

5.14
a.  C1 = (0000, 0001, 001, 10, 11, 01)
b.  C2 = (000, 001, 002, 01, 02, 1)
c.  L1 = (4 + 4 * 2 + 3 * 3 + 2 * (4 + 5 + 6))/21 = 51/21
    L2 = (3 * (1 + 2 + 3) + 2 * (4 + 5) + 6)/21 = 42/21 = 2

5.15
a.  (00, 1, 010, 0110, 0111), L = 2.3
b.  pi = D^-li, (1/4, 1/2, 1/8, 1/16, 1/16)

5.16
a.  (0, 10, 1100, 1110, 1111, 1101), L = 2
b.  (a, b, ca, cb, cc, cd), L = 1.25
c.  1.25 * 2 = 2.5
d.  L4 >= H4(X) = H(X)/log4 = H(X)/2
    LQB = L4 * 2 >= H(X)
    L4 <= H4(X) + 1 = H(X)/2 + 1
    LQB = L4 * 2 <= H(X) + 2

5.17
a.  (00, 10, 11, 010, 011), l = (2, 2, 2, 3, 3)
b.  pi = 0.9 * 0.1^i, i >= 0
    Σ{k > i}pk = Σ{k > i}0.9 * 0.1^k = 0.9 * 0.1^i, Huffman code repeatedly combines last two symbols
    l = (1, 2, 3, ..)

5.18
a.  no, 0 is a prefix of 01
b.  yes, it's a suffix code
c.  yes, all uniquely decodable code is nonsingular

5.19
// thanks solutions manual
a.  let Qn be the amount of numbers can be determined by n questions, Q1 = 1, Qn+1 = 1 + 2Qn, Q6 = 63
    it's possible to determine 63 out of 100 numbers in 6 questions by e.g. binary search, let the set of the 63 numbers
    be S, the expected return is R = Σ{x ∈ S}p(x)v(x), so the player should choose 63 numbers to maximize R
b.  Huffman code is the optimal code = optimal question scheme to determine a number, the player should ask questions
    corresponding to the ith bit of Huffman code
    R = E[v(X)] - L = Σv(x)p(x) - L, Σv(x)p(x) - H(X) - 1 <= R <= Σv(x)p(x) - H(X)
c.  minimize Σvipi - H(X) = Σvipi + Σpilogpi, subject to Σpi = 1
    as Σvipi is linear in pi, -H(X) is convex, local extreme value is global minimum
    L(p, l) = Σvipi + Σpilogpi - l(Σpi - 1)
    dL/dpi  = vi + logpi + 1/ln2 - l = 0
    logpi = l - vi - 1/ln2
    pi = 2^(l - vi - 1/ln2)
    Σpi = Σ2^(l - vi - 1/ln2)
        = 2^(l - 1/ln2)Σ2^-vi = 1
    2^(1/ln2 - l) = Σ2^-vi
    pi = 2^-vi / Σ2^-vj

5.20
a.  minimize Σpicili is equivalent to minimize Σ(pici / Σpici)li as Σpici is a constant
    which in turn is equivalent to minimize a symbol set of distribution q where qi = pici / Σpici
    hence the optimal code lengths are 
        li* = -logqi = -log(pici/Σpici) = logΣpici - logpici
b.  run Huffman procedure with respect to qi = pici / Σpici
c.  let li = ceil(-logqi)
    -logqi <= li < -logqi + 1
    -Σpicilogqi <= Σpicili < Σpici(-logqi + 1)
    C* <= C_Huffman < C* + Σpici

5.21
assume two different symbol sequences x = (x1, .., xn) and y = (y1, .., ym) are mapped to the same codeword sequence
the two concatenations xy and yx must be different:
    if xy = yx, assume without loss of generality |x| <= |y|, x is a prefix of y
    by definition of C, either y = x or |C(y)| > |C(x)|
hence the two different symbol sequences xy and yx have the same length and map to the same code
which contradict the assumption Ck(x1, .., xk) is one-to-one

5.22
for any code C, E[|C(X)|] = Σpili is a linear function of (p1, .., pn), which is continuous
by Huffman procedure, there are optimal code with codewords C(X) <= m, so only finite number of code (< D^(m^2)) matters
therefore the optimal expected codeword length is the minimum of finite continuous functions which is also continuous

5.23
a.  assume finite X
    a uniformly generated sequence starts with a codeword of length k is D^-k, define Ei be the event that a uniformly
    generated sequence starts with codeword xi
    when the code is not instantaneous, C(xi) = C(xj) for some i < j, Ei contains Ej
    Pr{E1 ∪ E2 .. Em}   = Pr{E1 ∪ .. ∪ Ei ∪ .. ∪ Ej-1 ∪ Ej+1 ∪ .. ∪ Em}
                        <= Σ{i != j}Pr{Ei}
                        = Σ{i != j}D^-li
                        = 1 - D^-lj < 1
    hence there must be a sequence that does not start with a codeword
b. // skipped

5.24
// thanks solutions manual
a.  for uniform distribution there is an optimal prefix code in which all codeword lengths differ by at most 1:
        assume |C(xi)| - |C(xj)| >= 2 for some different symbols xi and xj
        define a new code C' where
            C'(xi) = C(xi)||0
            C'(xj) = C(xi)||1
        since C is instantaneous, C(xi) is not a prefix of any other codewords, similarly C(xi)||0 and C(xi)||1
        C' is instantaneous and L' = (mL + 2|C(xi)| + 2 - |C(xi)| - |C(xj)|)/m <= L
    hence there exists Huffman code where ||C(xi)| - |C(xj)|| <= 1 for all i, j
    let Cn be a code for n uniformly distributed symbols, Ln be the expected codeword length of Cn
    for n = 2^b, Huffman procedure gives an optimal Cn where |Cn(xi)| = b for all xi
    for m < 2^(b + 1) = k, Ck is a prefix code for the domain of Cm, optimal Lm <= log|2^(b+1)| = b + 1, there exists an
    optimal Cm in which all codewords have length b or b + 1
    let tm be the number of codewords of length b + 1 in optimal Cm, it must have tm+1 >= tm + 2, otherwise
        tm+1 <= tm + 1, define Cm' by first choose (m - tm) codewords of length b from Cm+1
        tm' = m - (m - tm) = tm <= tm+1 - 1, at least one codeword of length b + 1 for Cm+1 is not in the new codomain
        if its sibling is defined for Cm', remove the last bit from it
        if its sibling is not defined for Cm', switch one codeword of length b + 1 with the first b bits of it
        the resulting tm'' = tm+1 - 2 <= tm - 1
        Lm''    = (tm''(b + 1) + (m - tm'')b) / m
                <= ((tm - 1)(b + 1) + (m - tm + 1)b) / m
                < Lm
        contradicting the assumption that Cm is optimal
    tm+1 >= tm + 2, from tm = 0 for m = 2^b to tm = 2^(b+1) for m = 2^(b+1), tm = 2(m - 2^b)
    Lm  = 1/m * (mb + 2(m - 2^b))
        = b + 2(m - 2^b)/m
        = b + 2 - 2^(b+1)/m
    for 2^b <= m < 2^(b+1)
b.  for tm = 0, m = 2^b for some b
c.  H(X) = log|X| = logm
    ρ   = L - H = b + 2 - 2^(b+1)/m - logm
    dρ/dm   = 2^(b+1)/m^2 - 1/mln2 = 0
    2^(b+1) = m/ln2, m = 2^(b+1)ln2
    ρ   = b + 2 - 2^(b+1)/m - logm
        = b + 2 - 1/ln2 - log(2^(b+1)ln2)
        = b + 2 - 1/ln2 - b - 1 + log(ln2)
        = 1 - 1/ln2 - log(ln2)
        = 0.086

5.25
a.  for x1 to be assigned a codeword length >= 2, it must be combined twice
    first time x1 is combined with another group x2 of probability x2 >= 2/5, which means the second time x1 is combined 
    with a group x3 of probability <= 1 - 2/5 * 2 = 1/5
    but then x2 and x3 or any subgroup of x3 should be combined first by the definition of Huffman procedure
b.  for x1 to be assigned a codeword length = 1, x1 must be combined with x2 as the last step of Huffman procedure
    p(x2) = 2/3, if x2 is a single symbol the probabilities are not in decreasing order
    otherwise x2 is a combination of two groups x3 and x4 where p(x3) + p(x4) = 2/3, it must have p(x3) >= 1/3 or
    p(x4) >= 1/3, x1 should be chosen instead of x3 or x4

5.26
a.  define pi = Wi / ΣWj, Σpi = 1, pi >= 0 is a probability vector
    as ΣWj is constant, a sequence of pairwise merges that minimizes V wrt. Wi also minimizes V wrt. pi
    assume a step of Huffman procedure merges two group of probabilities pi and pj, by equation 5.66
        L(p) = L*(p') + pi + pj
    Huffman procedure stops when |X| = 1, L(p) = 0, V for pi equals to the average codeword length, hence Huffman
    procedure minimizes V
b.  by part a, 
        H(X) <= L < H(X) + 1
    where V = LW, multiply both sides by W then
        WH(X) <= V < WH(X) + W

5.27
a.  define
        Suf(ci, cj) is the set of all danging suffixes by removing a suffix of cj which is also a prefix of ci from ci
        ci = C(xi)
        Si = ∪{j}Suf(ci, cj)
    then S = ∪Si
b.  a codeword of length li can produce at most (li - 1) danging prefixes, |S| <= Σ(li - 1)
c.  i.  instantaneous
    ii. uniquely decodable
    iii.not uniquely decodable, Suf(10, 01) = {0}, 0 is a codeword
    iv. uniquely decodable
    v.  instantaneous
    vi. not uniquely decodable, Suf(110, 11) = {10, 0}, 10 is a codeword
    vii.not uniquely decodable, is a superset of vi
d.  ii. 0111111..
        can be (0)(11)(11).. or (01)(11)(11)..
    iv. impossible, each 1 determines the meaning of the 0 before it, can be reduced to code {0, 1}
    if an infinite sequence has two interpretations, remove all uniquely decodable codes from the beginning of sequence, 
    at some point the remaining sequence has to be able to be interpreted as starting with two different codewords,
    which means one codeword must be the prefix of another, contradicts the definition of prefix code

5.28
a.  li = ceil(log1/pi), as {pi} is non-increasing, {li} is non-decreasing
    2^-li   = 2^-ceil(log1/pi)
            <= 2^-log1/pi
            = pi
    adding pi to a li bit number changes at least one bit of the first li bits, that bit pattern can never reappear by
    adding more numbers to it, therefore the code is prefix-free
    L   = Σpili
        = Σpi * ceil(log1/pi)
    Σpilog1/pi <= L < Σpi(log1/pi + 1)
    H(X) <= L < H(X) + 1
b.  p = (1/2, 1/4, 1/8, 1/8)
    F = (0, 1/2, 3/4, 7/8)
    C = (0, 10, 110, 111)

5.29
a.  for a dyadic distribution, each step of Huffman procedure combines two groups of probability 2^-i to form a group of
    2^-i+1, so a symbol of probability 2^-i will have codeword length i, li = logpi the Kraft inequality
    at level lmax, all nodes corresponds to codewords and have probability 2^-lmax
    inductively, a node at level l has twice the probability of level l + 1 node, left and right child of a node has the
    same probability
b.  by part a, draw random symbol ~ p(x) is equivalent to take a random path in the Huffman code tree (i.e. start from 
    the root and take left or right child with probability 1/2), the path ends at a codeword at level i with probability
    2^-i = p(x), since left and right children of a node has the same probability each bit in the code sequence has the
    same probability of being 0 or 1
c.  if the optimal code of a set of symbols is compressible, the compressed code is uniquely decodable and has shorter
    average length i.e. the original code is not optimal

5.30
a.  H(p) = 1/2 * 1 + 1/4 * 2 + 1/8 * 3 + 2/16 * 4 = 15/8
    H(q) = 1/2 * 1 + 4/8 * 3 = 16/8 = 2
    H(p||q) = 1/4 * 1 - 2/16 * 1 = 1/8
    H(q||p) = -1/8 * 1 + 2/8 * 1 = 1/8
b.  both cases li = log1/pi for all i, Σpili = E[log1/pi] = H(X)
c.  L = 1/2 * 1 + (1 - 1/2) * 3 = 2, difference is 2 - H(X) = 1/8 = D(p||q)
d.  L = 1/2 * 1 + 1/8 * (2 + 3 + 4 + 4) = 17/8, 17/8 - H(X) = 1/8 = D(q||p)

5.31
a.  by adding STOP to the end of any nonsingular binary code, the resulting ternary code is uniquely decodable and has 
    avarge codeword length L1:1 + 1
        L1:1 + 1 >= H3(X)
        L1:1 >= H(X)/log3 - 1
b.  all instantaneous codes are nonsingular so LINST >= L1:1*, H(X) + 1 >= LINST by Shannon code
c.  let p = (1/3, 1/3, 1/3), a nonsingular code is (0, 1, 00), L = 1/3 * (1 + 1 + 2) = 4/3 <= log3
d.  by greedy algorithm: assume any assignment in which pi is assigned not the shortest codeword, switch pi with any pj
    with shortest codeword or replace its current codeword if the shortest one is not used, the average codeword length
    cannot increase, therefore there exists an optimal assignment where the shortest codeword is assigned to p1, the 
    problem is then reduced to assigning {1, 00, 01, ..} to (p2, p3, ..) which can be solved recursively
e.  L(p, l) = F(p) - l(Σpi - 1)
            = -Σpilogpi - Σpilog(i/2 + 1) - l(Σpi - 1)
    dL/dp   = -logpi - 1/ln2 - log(i/2 + 1) - l = 0
    logpi   = -1/ln2 - log(i/2 + 1) - l
    pi      = 2^(-1/ln2 - l) * 2^-log(i/2 + 1)
            = 2^(-1/ln2 - l) * 1/(i/2 + 1)
            = 2^(-1/ln2 - l) * 2/(2 + i)
    Σpi     = 2^(-1/ln2 - l) * Σ2/(2 + i) = 1
    2^(-1/ln2 - l) = 1/Σ2/(2 + i)
    pi      = 1/Σ2/(2 + j) * 2/(2 + i)
            = 1/Σ1/(2 + j) * 1/(2 + i)
            = 1/(Hm+2 - H2) * 1/(2 + i)
            = c/(2 + i)
f.  F(p)    = -Σpilogpi - Σpilog(i/2 + 1)
            = -Σpilog(pi * (2 + i)/2)
            = -Σpilog(c/(2 + i) * (2 + i)/2)
            = -Σpilog(c/2)
            = 2/c = 2(Hm+2 - H2)

5.32
a.  if the bottles of wine is tasted in order p', the expected number of tastings is 
        1 * p1' + 2 * p2' + 3 * p3 + 4 * p4' + 5 * (p5' + p6')
b.  by part a the bottles of wine should be ordered in non-increasing order of probability, p1 should be tasted first
c.  one tasting is equivalent to a yes-no question of "if one bottle of wine in the set S is bad" for arbitrary set S
    a question scheme is equivalent to a code for the bad wine states, the optimal code given by Huffman procedure is
        (00, 01, 10, 1100, 111, 1101)
d.  the mixture of (p1, p2, p3) or (p4, p5, p6) should be tasted first

5.33
a.  Huffman: (1, 2, 2)
    Shannon: (1, 2, 4)
b.  for D >= 3, Huffman code is (1, 1, 1)
    log(D, 1/0.1) <= 1
    D >= 10

5.34
a.  assume instead of (S1, S2), another pair (Si, Sj) is wired first, where T1 <= Ti, T2 <= Tj, Ti <= Tj
    consider the same circuit but wire (S1, S2) first, the first pair of signals can be handled Tj - T2 >= 0 earlier,
    while Si and Sj replaces S1 and S2 in the original circuit, all signals can be handled in the original time if not
    earlier, new circuit cannot be slower than the original one
b.  the circuit building procedure constructs a tree that, starting from m singleton trees, each time S1 and Sj are
    wired, combine trees rooted at Si and Sj by creating a new node and add these two subtrees as its children
    conversely given a binary tree, the leaves as singleton trees are available at Ti = max(Ti + 0), if a node whose
    children are available at max{i ∈ L}(Ti + li) and max{j ∈ R}(Tj + lj), the node itself is available at time
        max(max(Ti + li), max(Tj + lj)) + 1, where li and lj are measures in their subtrees
        = max{i ∈ L ∪ R}(Ti + li), where li is measured in the combined tree
    therefore the root is available at time max(Ti + li), the tree constructed by the circuit building algorithm must
    be optimal among all trees
c.  let i = argmax{Ti + li}, Tj + lj <= Ti + li for all j,
        2^(Tj + lj) <= 2^(Ti + li)
        2^Tj <= 2^(Ti + li) * 2^-lj
    where Σ2^-lj = 1 by simple induction, therefore
        Σ2^Tj   <= Σ2^(Ti + li) * 2^-lj
                = 2^(Ti + li)
                = 2^C(T)
d.  define pi = 2^Ti / Σ2^Tj, p is a probability vector
    define li = ceil(log1/pi), Σ2^-li <= 1 and by theorem 5.2.1 it's possible to construct a tree with each Ti assigned 
    to a node at level li (if 0 can be assigned to some leaves)
    this tree has
        C(T)    = max(Ti + li)
                = max(Ti + ceil(log1/pi))
                < max(Ti + log1/pi + 1)
                = max(Ti + log(Σ2^Tj/2^Ti) + 1)
                = max(Ti + logΣ2^Tj - Ti + 1)
                = logΣ2^Ti + 1

5.35
// thanks solutions manual
express p as a binary fraction 0.p1p2p3.., the binary fraction 0.Z1Z2Z3.. is uniformly distributed at [0, 1], define a
procedure that:
    generate a fair coin flip Zi
    if pi = 0, Zi = 1, X = 0 (E1)
    if pi = 1, Zi = 0, X = 1 (E2)
    otherwise generate the next fair coin flip 
(E1) is equivalent to 0.Z1Z2Z3.. > 0.p1p2p3.., Pr{E1} = 1 - p = Pr{X = 0}
(E2) is equivalent to 0.Z1Z2Z3.. < 0.p1p2p3.., Pr{E2} = p = Pr{X = 1}
Pr{N = n} = (1/2)^n, E[N] = Σn(1/2)^n = 2

5.36
a.  (1, 2, 2) is possible but (2, 2, 3, 3) is not, (0, 10, 110, 111) is a better prefix code for any |X| = 4
b.  Huffman codewords form the leaves of a binary tree where li is the depth of the codeword in the tree when there are
    more than 1 codeword
    therefore as long as Σli > 1, Σ2^-li = 1, Huffman code must satisfy Kraft inequality with equality

5.37
a.  C2 is instantaneous
    C3 is instantaneous
b.  C2 and C3

5.38
a.  (00, 10, 110, 111, 010, 011), L = 63/25
b.  (0, 1, 20, 21, 22, 23), L = 38/25

5.39
a.  there's a bijection between C(X) and X, H(C(X)) = H(X)
b.  C(X^n) is a function of X^n but the function is not reversible, as f(x) = -xlogx is a convex function, combining
    outcomes always reduces the entropy, H(C(X^n)) < H(X^n)

5.40
a.  H(X) = lim(H(X1, .., Xn)/n) = H(X1) = 3/2
    p is dyadic and (0, 10, 11) is a Huffman code for X, problem 5.29 proves that Zi ~ Bernoulli(1/2) iid. so H(Z) = 1
b.  C is uniquely decodable, there's a bijection between (X1, .., Xn) and (Z1, .., Z2n), H(X1, .., Xn) = H(Z1, .., Z2n)
    lim{n -> ∞}(H(Z1, .., Zn)/n)    = lim{m -> ∞}(H(Z1, .., Z2m)/2m)
                                    = 1/2 * H(X) = 3/4
c.  thanks http://www.ece.tufts.edu/~maivu/ES250/HW3_ES250_sol_a.pdf
    define Yi as
        Yi = 0 if (Z1, .., Zi-1) is a complete codeword sequence, i.e. Zi is the first bit of a new codeword
        Yi = 1 otherwise, i.e. Zn is the second bit of C(1) or C(3)
    Yi is a function of (Z1, .., Zi-1) and given Yi, (Z1, .., Zi-1) and Zi are conditionally independent
    H(Zn|Z1, .., Zn-1)  = H(Zn|Z1, .., Zn-1, Yn)
                        = H(Zn|Yn)
                        = Pr{Yn = 0}H(Zn|Yn = 0) + Pr{Yn = 1}H(Zn|Yn = 1)
                        = Pr{Yn = 0}H(1/4) + Pr{Yn = 1}H(1/3)
    Pr{Yi+1 = 0|Yi = 0} = Pr{X = 2} = 1/4
    Pr{Yi+1 = 0|Yi = 1} = 1
    when n -> ∞, Yn converges to the stationary distribution in which
        u0 = 1/4u0 + u1
        u1 = 3/4u0
        u0 = 4/7, u1 = 3/7
    H(Zn|Z1, .., Zn-1)  -> 4/7 * (1/2 + 3/4(2 - log3)) + 3/7 * (1/3 * log3 + 2/3 * (log3 - 1))
                        = 6/7

5.41
by equation 5.66, L' = L + p10

5.42
a.  cannot be optimal, (00, 01, 02, 1, 2) is a better code for any |X| = 5
b.  for a 3-dic probability distribution (1/9 * 8, 1/27 * 3), these word lengths are log(3, 1/pi) and satisfies Kraft
    inequality with equality, so it must be both possible and optimal

5.43
(A, B, C, A0, A1, B0), is the 6 shortest codewords and uniquely decodable

5.44
by problem 5.24, codeword lengths are 2(100 - 64) = 72 of length 7, 28 of length 6

5.45
a.  Pr{{1, 2} ⊆ Si} = 2^(m - 2) / 2^m = 1/4
    Pr{{1, 2} ∩ Si = ∅} = 2^(m - 2) / 2^m = 1/4
    these two events are disjoint, p = 1/4 + 1/4 = 1/2
    all questions are independent, (1/2)^k for k questions
b.  define indicator Ii that
        Ii = 1 if object i have the same answers as 1 to k questions
        Ii = 0 otherwise
    E[N]    = E[ΣIi]
            = ΣE[Ii]
            = (m - 1)(1/2)^k
c.  (m - 1)(1/2)^(n + sqrt(n))
    = (2^n - 1)(1/2)^(n + sqrt(n))
    = (1/2)^sqrt(n) * (1 - (1/2)^n)
d.  Pr{N >= 1}  = Pr{N >= 1/μ * μ} 
                <= μ
                = (1/2)^sqrt(n) * (1 - (1/2)^n)
    as 1 - (1/2)^n -> 1, (1/2)^sqrt(n) -> 0, Pr{N >= 1} -> 0

6.1
a.  by conservation theorem, W*(p) = logm - H(p) = log3 - 3/2
b.  if horse other than horse 1 wins, the gambler loses all the money
    Pr{gambler go broke before n races} = 1 - Pr{horse 1 wins all n races}
                                        = 1 - (1/2)^n
                                        -> 1

6.2
// thanks http://amber.feld.cvut.cz/bio/konopka/file/5.pdf for the definition of KKT conditions
a.  L(b, l) = Σpilog(b0 + bioi) - l(Σbi - 1)
    dL/dbi  = pioi / ln2(b0 + bioi) - l
    dL/db0  = Σ(pi / ln2(b0 + bioi)) - l
    since 
        constraint Σbi - 1 = 0 is linear, so both Σbi - 1 >= 0 and Σbi - 1 <= 0 are convex
        Σpilog(b0 + bioi) is differentiable and concave on [0, 1]
    a point b satisfying the KKT conditions is a maximum
    the proportional bet b0 = 0, bi = pi, l = 1/ln2 satisfies:
        bi >= 0
        dL/dbi  = pioi / ln2(b0 + pioi) - l
                = pioi / ln2pioi - 1/ln2
                = 0
                = pi * dL/dbi
        dL/db0  = Σ(pi / ln2pioi) - 1/ln2
                = 1/ln2 * Σ(1/oi) - 1/ln2
                < 0
        l >= 0, Σbi - 1 = 0
    therefore the proportional bet is a maximum of the problem
b.  the proportional bet cannot satisfy KKT conditions
    let b = (1, 0, 0, ..)
    b can only be maximum if it satisfy:
        dL/db0  = Σ(pi / ln2) - l
                = 1/ln2 - l
                = 0, l = 1/ln2
        dL/dbi  = pioi/ln2 - 1/ln2 <= 0
        pioi <= 1
    otherwise b0 = 1 is not optimal, 0 < b0 < 1, the gambler has to bet on some of the horses

6.3
a.  H(X1) = H(1/2) = 1
b.  by symmetry, Pr{X2 = 0} = Pr{X2 = 1} = 1/2
    H(X2) = H(1/2) = 1
c.  given (X1, .., Xk-1), Pr{Xk = 1|X1, .., Xk-1} = Pr{Xk+i = 1|X1, .., Xk-1} for any i as the remaining 52 - k + 1
    cards are drawn from the same pool just in different orders
    therefore H(Xk|X1, .., Xk-1) = H(Xk+1|X1, .., Xk-1) >= H(Xk+1|X1, .., Xk), the sequence is non-increasing
d.  by Pr{Xi = 0} = 1/2, all sequences are equally likely, (X1, .., X52) is a uniform distribution on C(52, 26) symbols
    H(X1, .., X52) = logC(52, 26) = 48.817

6.4
E[logSn]    = nlog2 - H(X1, .., Xn)
            = 52 - 48.817 = 3.183

6.5
a.  H(p) = 3/2
b.  with proportional bet (1/2, 1/4, 1/4)
        W(b, p) = Σpilogoi - H(p)
                = (1/2 * 2 + 1/4 * (2 + 1)) - 3/2
                = 7/4 - 3/2 = 1/4
    2^nW = 2^(n/4) -> ∞

6.6
a.  W(b, p) = Σpilogoi - D(p||b) - H(p)
            = -D(p||b) - H(p)
b.  proportional bet, b = (p1, p2, p3)
c.  one that maximizes H(p), (1/3, 1/3, 1/3)

6.7
porportional bet, b = p
    W = Σpilogoi - H(p) = 2 - 7/4 = 1/4
E[S20] = 2^20W = 2^5 = 32
$100 * 32 = $3200 after 20 races

6.8
a.  proportional bet
b.  W(b, p) = Σpilogoi - H(p)
            = 1/8 * (3 + 3 + 2 + 4 + 4 + 4 + 2 + 4) - 3
            = 26/8 - 3
            = 1/4
    log(1000000) / (1/4) = 79.726 days

6.9
W(b, p) = Σpilogoi - H(p) - D(p||q)
when Σpilogoi > Σpilogoi'

6.10
a.  W   = Σpilogoi - H(p) - D(p||q)
        = 1 + 1/2 * log3 - 3/2 - Σpilog(pi/bi)
        = 1/2 * log3 - 3/4
    W'  = Σpilogoi - H(p)
        = 1/2 * log3 - 1/2
    W' - W = 1/4
b.  D(p||q)

6.11
a.  E[X]    = Σ{x, y}xp(x, y)
            = b * 1/2 + 2b * 1/2
            = 3b/2
            = E[Y] by symmetry
b.  E[Y/X]  = 1/2 * 1/2 + 1/2 * 2 = 5/4
c.  Pr{J' = 1} = Pr{J' = 2} = 1/2
    Pr{J = 1} = Pr{J = 2} = 1/2 by symmetry
    Pr{J = 1|J' = 1}    = 1/2 * (1 - p(2b)) + 1/2 * p(b)
                        = 1/2 + 1/2 * (p(b) - p(2b))
    as dp(x)/dx = -2e^2x / (e^2x + 1)^2 is strictly negative, p(b) - p(2b) = 0 only when b = 2b
    therefore Pr{J = 1|J' = 1} != Pr{J = 1}, J and J' are not independent, I(J;J') > 0
d.  E[Z]    = 1/2 * (b(1 - p(b)) + 2bp(b)) + 1/2 * (2b(1 - p(2b)) + bp(2b))
            = 1/2 * (b - bp(b) + 2bp(b) + 2b - 2bp(2b) + bp(2b))
            = 1/2 * (3b + bp(b) - bp(2b))
    as dp(x)/dx < 0, bp(b) - bp(2b) > 0, E[Z] > 3b/2

6.12
a.  proportional bet
b.  W(b, p) = Σpilogoi - D(p||b) - H(p)
            = Σpilogoi - Σpilog(pi/bi) - H(p)
            = Σpilogoi - H(p) + Σpilogbi - H(p)
    minimizing W(b, p) is equivalent to minimizing Σpilogbi
    as logbi can be arbitrarily low, W(b, p) -> -∞

6.13
a.  bo1 = (1 - b)o2
    10b = 30(1 - b), b = 3(1 - b), b = 3/4
    S(X) = 10b = 15/2, logS(X) = 2.907
b.  W*(b, p)    = Σpilogoi - H(p)
                = 1/2 * (log10 + log30) - 1
                = 3.114

6.14
same problem to 6.9

6.15
a.  E[S(X)] = Σpibioi
            = Σbi = 1, wealth won't grow
b.  W   = Σpilogoi - H(p)
        = H(p) - H(p) = 0
c.  ΔW  = I(X;Y)
        = H(X) - H(X|Y)
        = H(p) - (H(p1 + p2) + (p1 + p2)H(p1/(p1 + p2)) + (1 - p1 - p2)H(p3/(1 - p1 - p2), .., pm/(1 - p1 - p2)))
d.  I(X;Y) = ΔW

6.16
a.  d^2Σpiln(1 - bi)/d^2bi  = -pi/(x - 1)^2 <= 0, Σpiln(1 - bi) is concave to bi
    L(b, l) = Σpiln(1 - bi) - l(Σbi - 1)
    dL/dbi  = -pi/(1 - bi) - l = 0
    -pi = l - lbi
    bi = (l + pi)/l = 1 + pi/l
    Σbi = m + Σpi/l = 1
    m + 1/l = 1, lm + 1 = l, l(m - 1) = -1, l = 1/(1 - m)
    bi = 1 + pi(1 - m) is the global maximum
b.  S(X)    = Σpi(1 - bi)
            = Σpi(-pi(1 - m))
            = Σpi^2(m - 1)

6.17
a.  E[X]    = Σ2^k * 2^-k
            = Σ1 = ∞
b.  1/n * logSn = 1/n * logΠ(Xi/c)
                = 1/n * Σlog(Xi/c)
                -> E[log(Xi/c)]
                = Σ{k >= 1}2^-k * log(2^k / c)
                = Σk2^-k - Σ2^-k * logc
                = 2 - logc
    for c > 4, 1/n * logSn = g > 0, Sn = 2^ng -> ∞
    for c < 4, 1/n * logSn = -g < 0, Sn = 2^-ng -> 0
c.  // thanks solutions manual
    only numerical results, the derivative do not have closed form solution
    for c > 3, b* < 1
d.  c -> ∞, b* -> 0
e.  // skipped

6.18
// again only numerical solution
// skipped

7.1
a.  by data processing inequality, X -> Y -> Y', C' = I(X;Y') <= I(X;Y) <= C
b.  I(X;Y|Y') = 0, or X -> Y' -> Y

7.2
let Pr{X = 1} = p
C   = I(X;Y)
    = H(Y) - H(Y|X)
    = H(Y) - pH(Y|X = 1) - (1 - p)H(Y|X = 0)
if α = 0, the channel is lossless, C = log|X| = 1
if α > 0, H(Y|X = 1) = H(Y|X = 0) = 1
C   = H(Y) - 1
if α = 1, Pr{Y = 0} = Pr{X = 0}Pr{Z = 0} = (1 - p)/2, Pr{Y = 2} = Pr{X = 1}Pr{Z = 1} = p/2, Pr{Y = 1} = 1/2
    max{C}  = max{H(Y)} - 1
            = H(1/2) + 1/2 * H(p) - 1
            = 3/2 - 1 = 1/2
if α >= 2, Y = {0, 1, α, α + 1}
    max{C}  = max{H(Y)} - 1 = log4 - 1 = 1

7.3
max{I(X^n;Y^n)} = max{H(X^n) - H(X^n|Y^n)}
                = max{H(X^n) - H(Z^n|Y^n)}  // given Y^n, there's a bijection between X^n and Y^n 
                >= max{H(X^n) - H(Z^n)}
                >= max{H(X^n)} - nH(p)
                = n - nH(p)
                = nC

7.4
a.  C   = max{I(X;Y)}
        = max{H(Y) - H(Y|X)}
        = max{H(Y) - log3}
        = max{H(Y)} - log3
    let X follow a uniform distribution, 
    Pr{Y = i}   = 1/3 * (Pr{X = i - 1} + Pr{X = i - 2} + Pr{X = i - 3})
                = 1/3 * 3/11 = 1/11
    C   = max{H(Y)} - log3
        = log(11/3)
b.  uniform distribution

7.5
I(X;Y)  = I(X1, X2;Y1, Y2)
        = H(Y1, Y2) - H(Y1, Y2|X1, X2)
        <= H(Y1) + H(Y2) - H(Y1|X1, X2) - H(Y2|Y1, X1, X2)
        = H(Y1) - H(Y1|X1) + H(Y2) - H(Y2|X2)
        = I(X1;Y1) + I(X2;Y2)
C   = max{I{X;Y}} 
    = max{I(X1;Y1) + I(X2;Y2)}
    <= C1 + C2

7.6
a.  I(X;Y) = I(X;X) = H(X) = log26
b.  C   = max{I(X;Y)} 
        = max{H(Y) - H(Y|X)}
        = max{H(Y)} - 1
    let X ~ uniform distribution,
    Pr{Y = i}   = 1/2(Pr{X = i} + Pr{X = i - 1 mod 26})
                = 1/2 * 2/26
                = 1/26
    C   = max{H(Y)} - 1
        = log26 - 1 = log13
c.  as described in the text, R = C = log13
    Pr{X = a} = Pr{X = c} = .. = Pr{X = y} = 1/13

7.7
for a single channel, Pe = p = 1/2(1 - (1 - 2p)^1)
assume a cascade of n channels has Pe^n = 1/2(1 - (1 - 2p)^n)
Pe^n+1  = (1 - Pe^n) * p + Pe^n * (1 - p)
        = p - p * Pe^n + Pe^n - p * Pe^n
        = p + (1 - 2p) * Pe^n
        = p + (1 - 2p) * 1/2(1 - (1 - 2p)^n)
        = p + 1/2 - p - (1 - 2p)^n+1
        = 1/2(1 - (1 - 2p)^n+1)

7.8
let Pr{X = 1} = p
C   = max{I(X;Y)}
    = max{H(Y) - H(Y|X)}
    = max{H(Y) - pH(Y|X = 1)}
    = max{H(Y) - p}
    = max{H(p/2) - p}
    = max{-p/2logp/2 - (1 - p/2)log(1 - p/2) - p}
I(X;Y) is concave in p
dI(X;Y)/p   = -1/2 * logp/2 - p/2 * 2/pln2 * 1/2 + 1/2log(1 - p/2) + (1 - p/2) * 1/(1 - p/2)ln2 * 1/2 - 1
            = -1/2 * logp/2 - 1/2ln2 + 1/2log(1 - p/2) + 1/2ln2 - 1
            = -1/2 * log(p/(2 - p)) - 1 = 0
log(p/(2 - p)) = -2
p/(2 - p) = 1/4
4p = 2 - p
p = 2/5
C = 0.322

7.9
by converse of channel coding theorem,
    R   <= C
        = max{I(X;Y)}
        = H(Y) - H(Y|X)
        = H(1/4) - 1/2 * H(1/2)
        = (2 - log3)/4

7.10
a.  I(X;Y)  = H(Y) - H(Y|X)
            = H(Y) - 1
    let X ~ uniform distribution, 
    Pr{Y = i}   = 1/2 * Pr{X = i - 1 mod 5} + 1/2 * Pr{X = i + 1 mod 5}
                = 1/2 * 2/5 = 1/5
    C = max{I(X;Y)} = log5 - 1
b.  // thanks solutions manual
    finding the maximum subset of codewords with disjoint results is equivalent to finding maximum dominating set of
    a graph, possibly an NP-complete problem
    {11, 23, 30, 42, 04} is a set of distinguishable codewords, R = logm / n = log5 / 2 > 1
    and it's the optimal zero-hour capacity of the channel by Lovasz [365]

7.11
I(X^n;Y^n)  = H(Y^n) - H(Y^n|X^n)
            = H(Y^n) - ΣH(Yi|X^n, Y1, .., Yi-1) // chain rule
            = H(Y^n) - ΣH(Yi|Xi)    // conditional independency
            <= ΣH(Y) - ΣH(Yi|Xi)    // assume {Yi} are mutually independent
            = ΣI(Xi;Yi)
equality is achieved when X are iid

7.12
let Pr{X = i} = pi
I(X;Y)  = H(Y) - H(Y|X)
        = H(Y) - (p1 + p3) * H(1/3, 2/3) - p2 * H(1/3, 1/3, 1/3)
        = H(p1 * 2/3 + p2 * 1/3, 1/3, p2 * 1/3 + p3 * 2/3) - (p1 + p3)H(1/3, 2/3) - p2H(1/3, 1/3, 1/3)
where H(Y) = H(p1 * 2/3 + p2 * 1/3, 1/3, p2 * 1/3 + p3 * 2/3) is maximized when
    p1 * 2/3 + p2 * 1/3 = p2 * 1/3 + p3 * 2/3
    p1 = p3
and - (p1 + p3)H(1/3, 2/3) - (1 - p1 - p3)H(1/3, 1/3, 1/3) is maximized when p1 + p3 = 1
therefore optimal p(x) = (1/2, 0, 1/2)
when x = 2, the output is uniform i.e. completely indistinguishable, more x = 2 equals to more noise in the channel

7.13
a.  let Pr{X = 1} = p
    I(X;Y)  = H(Y) - H(Y|X)
            = H(Y) - H(1 - α - ε, α, ε)
            = H((1 - α - ε)(1 - p) + εp, α, ε(1 - p) + (1 - α - ε)p) - H(1 - α - ε, α, ε)
    where H(Y) is maximized when p = 1 - p, p = 1/2
    C   = max{I(X;Y)}
        = H((1 - α)/2, α, (1 - α)/2) - H(1 - α - ε, α, ε)
b.  α = 0
    C   = H(1/2, 1/2) - H(ε)
        = 1 - H(ε)
c.  ε = 0
    C   = H((1 - α)/2, α, (1 - α)/2) - H(α)
        = -(1 - α)(log(1 - α) - 1) - αlogα + (1 - α)log(1 - α) + αlogα
        = 1 - α

7.14
a.  there's a bijection between (X1, X2) and (Y1, Y2)
    hence I(X1, X2;Y1, Y2) = H(X1, X2)
b.  C   = max{I(X1, X2;Y1, Y2)}
        = log|(X1, X2)|
        = 2
c.  maximizing input distribution is a uniform distribution on {00, 01, 10, 11}
    Pr{X1 = 1} = Pr{Y1 = 1} = 1/2
    Pr{X1 = 1, Y1 = 1} = 1/4 = Pr{X1 = 1}Pr{Y1 = 1} and all other combinations, X1 and Y1 are independent
    I(X1;Y1) = 0

7.15
a.  H(X) = H(Y) = 1
    H(X, Y) = H(0.05, 0.05, 0.45, 0.45) = 1.469
    I(X;Y) = 2 - 1.469 = 0.531
b.  Aε^n(X) = {x^n: |-1/nlogp(x^n) - H(X)| < ε}
    as p(x^n) = (1/2)^n, -1/nlogp(x^n) = 1 = H(X), Aε^n(X) = X, all sequences are typical
    similarly Aε^n(Y) = Y
c.  -1/n * log(x^n, y^n) - H(X, Y)
    = -1/n * logp(x^n)p(z^n) - H(X) - H(Y|X)
    = -1/n * logp(x^n) - H(X) - 1/n * logp(z^n) - H(Y|X)
    = -1/n * log(z^n) - H(Y|X)
    where
        H(X, Z) = H(X) + H(Z)   // X and Z are independent
                = H(X, Y)   // bijection between (X, Z) and (X, Y)
                = H(X) + H(Y|X)
        H(Z)    = H(Y|X)
    = -1/n * log(z^n) - H(Z)
    therefore (x^n, y^n) ∈ Aε^n(X, Y) <=> z^n ⊆ Aε^n(Z)
d.  H(Z) = H(Y|X) = H(X, Y) - H(X) = 0.469
    |-1/n * logp(z^n) - H(Z)| < ε
    0.269 < -1/n * log(z^n) < 0.669
    1 <= k <= 4
    |Aε^n(Z)| = 25 + 300 + 2300 + 12650 = 15275
e.  by part c, (x^n, y^n) is jointly typical is equivalent to z^n is typical
    Pr{(X^n, Y^n) ⊆ Aε^n(X, Y)} = Pr{Z^n ⊆ Aε^n(Z)} > 1 - ε for sufficiently large n
    for the example in part d, p(z^n ∈ Aε^n(Z)) = Pr{1 <= k <= 4} = 0.830
f.  Z = X + Y, fixing Y^n, there's a bijection between Z and X
    when X^n is chosen uniformly, 
    Pr{(X^n, y^n) ∈ Aε^n(X, Y)} = |Aε^n(Z)| / |Z|
                                <= 2^n(H(Z) + ε) / |Z|^n
                                = 2^n(H(Z) + ε - log|Z|)
    particularly in the example of part d, 
    Pr{(X^n, y^n) ∈ Aε^n(X, Y)} = |Aε^n(Z)| / |Z|
                                = 15275 / 2^25
                                = 0.000455
g.  by part f, 
    p   = 1 - (1 - 0.000455)^511
        = 0.207
h.  Pr{Error|x^n(1)}    <= Pr{received y^n is not jointly typical with input x^n} 
                        + Pr{received y^n is jointly typical with another x^n}
                        <= 1 - ε + 2^n(H(Z) + ε - log|Z|)
    for the example of part d, 
    Pr{Error|x^n(1)}    <= (1 - 0.830) + 0.207
                        = 0.377

7.16
a.  define the decoder as
        g(y)    = a1 if there are more 0 in the output
                = a2 otherwise
    p'  = C(3, 2)p^2 * (1 - p) + p^3
        = 0.028
b.  C'  = (1 - H(p')) / 3
        = 0.272
c.  C   = (1 - H(p))
        = 0.531
d.  let W and W' be input and output index, W -> X^n -> Y^n -> W'
    I(W;W') <= I(X^n;Y^n)   // data processing inequality
            <= nI(X;Y)
    C'  = max{I(W;W')/n}
        <= max{nI(X;Y)/n}
        = max{I(X;Y)}
        = C

7.17
a.  given any codeword, there's only one codeword differs at 3 places to it
    if the set of codewords contains such a pair (a, b), any other codeword c will have
        dist(a, c) + dist(b, c) = 3
    as dist(a, c), dist(b, c) > 0
        dist(a, c) = 1 or dist(b, c) = 1
    the set of codewords cannot detect 1 bit error sometimes
    the best possible distances between codewords is 2, achievable by a set
        {000, 110, 101, 011}
    any codeword has distance 2 from any other codewords
    p'  = C(3, 2)p^2 = 3p^2 = 0.03
b.  p'  = C(3, 1)p + C(3, 2)p^2 + C(3, 3)p^3
        = 3p + 3p^2 + p^3
        = 0.331
c.  p'  = p^3 / 2 = 0.0005
d.  assume uniform distribution of codewords
    part a:
        if two bits are erased, the code is indistinguishable from another
        if three bits are all erased, all 4 codewords are equally possible
        p = C(3, 2)p^2 / 2 + p^3 * 3/4 = 0.005 + 0.00075 = 0.00575
    part b:
        p   = C(3, 1)p / 2 + C(3, 2)p^2 *  3/4 + p^3 * 7/8
            = 0.173375

7.18
a.  I(X;Y)  = H(Y) - H(Y|X)
            = H(Y) - log3
            <= log3 - log3
            = 0
b.  symmetric channel, C = log3 - log(1/2) = log3 - 1
c.  // thanks https://people.eecs.berkeley.edu/~ananth/229ASpr07/soln_4_229spr07.pdf
    this channel is the sum of two channels BSC(p) and BSC(q)
    let Q be the index of the channel used, Q is a function of X or Y
    I(X;Y)  = I(X,Q;Y)
            = I(Q;Y) + I(X;Y|Q)
            = H(Q) + ΣPr{Q = i}I(X;Y|Q = i)
    C   = max{I(X;Y)} 
        = max{H(Q) + ΣPr{Q = i}I(X;Y|Q = i)}
        = max{H(Q) + ΣPr{Q = i}Ci}
    let ri = 2^Ci / Σ2^Cj, r is a probability vector
    C   = max{-Σqilogqi + Σqilog2^Ci}
        = max{-Σqilogqi + Σqilogri + logΣ2^Ci}
        = max{Σqilog(ri/qi) + logΣ2^Ci}
        = max{logΣ2^Ci - D(q||r)}
        = logΣ2^Ci

7.19
a.  8 * (60 / 5) = 96
b.  equal to a erase channel, C = 3(1 - α)
c.  p(y|x)  = 1 - α + α/8 = 1 - 7α/8 if y = x
            = α/8 if y != x
    I(X;Y)  = H(Y) - H(Y|X)
            = H(Y) - H(1 - α + α/8, 7 of α/8)
            = H(Y) + Σ(1 - 7α/8)log(1 - 7α/8) + 7/8log(α/8)
    C   = max{I(X;Y)}
        = 3 + Σ(1 - 7α/8)log(1 - 7α/8) + 7/8log(α/8)

7.20
a.  2I(X;Y1) - I(Y1;Y2)
    = 2H(Y1) - 2H(Y1|X) - H(Y1) + H(Y1|Y2)
    = H(Y1) - 2H(Y1|X) + H(Y1|Y2)
    = H(Y1, Y2) - H(Y1|X) - H(Y2|X) // given x, Y1 and Y2 are iid
    = H(Y1) - H(Y1, Y2|X) + H(Y1|Y2)
    = H(Y1, Y2) - H(Y1, Y2|X)
    = I(Y1, Y2;X)
b.  C'  = max{I(X;Y1, Y2)}
        = max{2I(X;Y1) - I(Y1;Y2)}
        <= max{2I(X;Y1)}
        = 2C

7.21
a.  by Markov's inequality, Pr{H >= 15} <= E[H] / 15 = 1/3
b.  Pr{H >= 10} <= E[H] / 10 = 1/2
    Pr{W >= 300} <= E[W] / 300 = 1/3
    Pr{H >= 10, W >= 300} <= min(Pr{H >= 10}, Pr{W >= 300}) = 1/3

7.22
the new input symbol can be ignored, the new channel is at least equivalent to the old channel

7.23
a.  the channel transition matrix is 
        [1, 0; 1 - a, a]
    let Pr{X = 1} = p,
    I(X;Y)  = H(Y) - H(Y|X)
            = H(Y) - pH(a)
    C   = max{I(X;Y)}
        = max{-(1 - pa)log(1 - pa) - palogpa - pH(a)}
    dI(X;Y)/dp  = alog(1 - pa) - (1 - pa)/(1 - pa)ln2 * -a - alogpa - pa/paln2 * a - H(a)
                = alog(1 - pa) + a/ln2 - alogpa - a/ln2 - H(a)
                = alog((1 - pa)/pa) - H(a) = 0
    (1 - pa)/pa = 2^(H(a)/a)
    (2^(H(a)/a) + 1)pa = 1
    p = 1/a(2^(H(a)/a) + 1)
    where
        2^(H(a)/a)  = 2^(-loga - (1 - a)/a * log(1 - a))
                    = 1/a * (1/(1 - a))^((1 - a)/a)
        1/p = (1/(1 - a))^((1 - a)/a) + a >= 1
        0 <= p <= 1 is the local maximum
    let c = 2^(H(a)/a), p = 1/a(c + 1)
    C   = H(pa) - pH(a)
        = -1/(c + 1)log(1 / (c + 1)) - c/(c + 1)log(c / (c + 1)) - 1/(c + 1) * H(a)
        = 1/(c + 1)log(c + 1) - c/(c + 1)logc + c/(c + 1)log(c + 1) - 1/(c + 1) * H(a)
        = log(c + 1) - c/(c + 1) * H(a) / a - 1/a(c + 1) * H(a)
        = log(c + 1) - H(a)/a
b.  I(X;Y|Z)    = (1 - a)I(X;Y|Z = 0) + aI(X;Y|Z = 1)
                = aI(X;Y|Z = 1) // when Z = 0, Y is constant, X and Y are independent
                = aI(X;X|Z = 1) // when Z = 1, X = Y
                = aH(X)
    C   = max{I(X;Y|Z)}
        = max{aH(X)}
        = a
    equivalent to a erasure channel BEC(a)

7.24
a.  for Z = {0, 4, 8}, there's a bijection between Y and X, H(Y|X) = 0
    I(X;Y) = H(X), C = max{H(X)} = 2
b.  lemma: capacity of Z = {z1, z2, z3} equals to capacity of Z' = {z1 + k, z2 + k, z3 + k}
        let Y' = X + Z'
        Pr{Y = y, X = x} = Pr{Y' = y + k, X = x} for all x, y
        fixing p(x), I(X;Y) = I(X;Y')
        hence with z1 < z2 < z3, there's always a minimizing Z with z1 = 0
    I(X;Y)  = H(X) - H(X|Y)
    given y, X is a uniform distribution on {0, 1, 2, 3} ∩ {y - z1, y - z2, y - z3}
    let Di = {0, 1, 2, 3} ∩ {y - z1, y - z2, y - z3} given X = i
    by lemma, |Di| <= 4 - i
    Z = {0, 1, 2} maximizes all |Di| thus is the minimizing Z 
    I(X;Y)  = H(Y) - H(Y|X)
            = H(Y) - log3
    where Y = {0, 1, 2, 3, 4, 5}
    let Pr{X = i} = pi, by picking p0 = p3 = 1/2, Y is uniformly distributed
    C   = max{I(X;Y)}
        = max{H(Y)} - log3
        = log6 - log3 = 1

7.25
I(X;Y)  <= I(X;V)
C   = max{I(X;Y)}
    <= max{I(X;V)}
    = max{H(V) - H(V|X)}
    <= log|V| = logk

7.26
a.  a symmetric channel, C = log|Y| - H(r) = 2 - 1 = 1
b.  the new transition matrix is
        [1, 0; 1/2, 1/2; 0, 1; 1/2, 1/2]
    i.  when X = 1, Y ~ U(1, 2), Z ~ U(A, B)
        when X = 3, Pr{Y = 0} = Pr{Y = 3} = 1/2, Z ~ U(A, B)
        I(X;Z)  = H(Z) - H(Z|X)
                = 1 - 1 = 0
    ii. I(X;Z)  = H(Z) - H(Z|X)
                = 1 - 0 = 1
c.  I(X;Z)  = H(Z) - H(Z|X)
            = H(Z) - (p1 + p3)
    by taking Pr{X = 0} = Pr{X = 2} = 1/2, Z is uniformly distributed, p1 + p3 = 0, I(X;Z) is maximized
    C = max{I(X;Z)} = 1
d.  when Z = A, 
        Pr{X = 1|Z = A} = Pr{X = 1, Z = A} / Pr{Z = A} 
                        = 1/4 / 1/2 = 1/2
        Pr{Y = 1|Z = A} = Pr{Y = 1, Z = A} / Pr{Z = A}
                        = 1/2 * 1/2 / 1/2 = 1/2
        Pr{X = 1, Y = 1|Z = A}  = Pr{X = 1, Y = 1, Z = A} / Pr{Z = A}
                                = Pr{X = 1, Y = 1} / Pr{Z = A}
                                = 1/4 / 1/2 = 1/2
    therefore given Z, X and Y are not independent

7.27
I(X;S)  = H(S) - H(S|X)
        = H(a) + (1 - a)H(Y) - ΣPr{X = x}H(S|X = x)
        = H(a) + (1 - a)H(Y) - ΣPr{X = x}(H(a) + (1 - a)H(Y|X = x))
        = H(a) + (1 - a)H(Y) - (1 - a)H(Y|X) - H(a)
        = (1 - a)H(X;Y)
C'  = max{I(X;S)}
    = (1 - a)C

7.28
a.  see problem 7.18.c
b.  the maximum rate R -> C is defined as logM / n, M is the effective number of noise free symbols
    for block size of 1, M = 2^C, M = M1 + M2
    as the two channels have noise free alphabet of size M1 and M2, the union channel has size M1 + M2
c.  C   = log(2^C1 + 2^C2)
        = log(2^(1 - H(p)) + 2^1)
        = log(2 + 2^(1 - H(p)))

7.29
same to problem 7.23

7.30
same to problem 7.24

7.31
by source-channel coding theorem, H(V) = H(a) < C = 1 - H(p)

7.32
same to problem 5.45

7.33
a.  I(X^n;Y^n)  = H(X^n) - H(X^n|Y^n)
                = H(X^n) - ΣH(Xi|Y^n, X1, .., Xi-1)
                = H(X^n)    // Xi = Yi - 1
                = ΣH(Xi|X1, .., Xi-1)
                = ΣH(Xi|Xi-1)   // X1 -> X2 -> .. -> Xn
                = nH(p)
    lim1/n * I(X^n;Y^n) = H(p)
b.  C   = 1 - H(p)
    for H(p) > 1/2, lim1/n * I(X^n;Y^n) = H(p) > C
c.  limI(W;Y^n) = lim(1/n * (H(W) - H(W|Y^n)))
                <= lim(1/n * H(W))
                <= lim(1/n * log|W|)
                = 0

7.34
a.  C   = log(2 * 2^(1 - H(p)))
        = log(2^(2 - H(p)))
        = 2 - H(p)
b.  C   = log(2^(1 - H(p)) + 2)
c.  C1  = max{I(X;Y)}
        = max{H(Y) - H(Y|X)}
        = log(3/2)
    C   = log(3/2 + 2^(1 - H(p)))
d.  erasure channel, C = 1 - 1/3 = 2/3

7.35
a.  equivalent to the union of channel P and a noiseless one bit channel
    C'  = log(2^C + 2)
b.  equivalent to the union of channel P and a noiseless k-bit channel
    C'  = log(2^C + 2^k)

7.36
a.  the transition matrix is
        [1/2, 1/2; 1/2, 1/2]
    therefore C = 0
b.  define a code X^n(W) where
        |W| = 2^(n-1)
        the first bit of the codeword is always 1, X^n(W) = 1||W
    Z can be determined by code of the first bit
    hence W' = g(Y) is a decoder free of errors
    C > R = log|W| / n = (n - 1)/n -> 1
    a noiseless channel has C = 1, hence C = 1

7.37
lemma: |Aε^n| <= 2^n(H(X, Y, Z) + ε)
    1   = Σp(x^n, y^n, z^n)
        >= Σ{Aε^n}p(x^n, y^n, z^n)
        >= Σ{Aε^n}2^-n(H(X, Y, Z) - ε)
        = |Aε^n|2^-n(H(X, Y, Z) - ε)
    |Aε^n| <= 2^n(H(X, Y, Z) - ε)
Pr{(X'^n, Y'^n, X'^n) ∈ Aε^n}   = Σ{Aε^n}p(x^n)p(y^n)p(z^n)
                                <= |Aε^n|2^-n(H(X) - ε)2^-n(H(Y) - ε)2^-n(H(Z) - ε)
                                = 2^n(H(X, Y, Z) - H(X) - H(Y) - H(Z) + 2ε)

8.1
a.  h(x)    = -int(0, ∞, λe^-λx * log(λe^-λx)dx)
            = -int(0, ∞, λe^-λx (logλ + loge * -λx)dx)
            = -logλ + λlogeE[X]
            = loge - logλ
b.  h(x)    = -int(-∞, ∞, 1/2 * λe^-λ|x| * log(1/2 * λe^-λ|x|)dx)
            = -int(0, ∞, λe^-λx * (-1 + logλ - λxloge)dx)
            = 1 - logλ + loge
c.  sum of two normal distributions is a normal distribution (μ1 + μ2, σ1^2 + σ2^2)
    h(x)    = 1/2 * log2πe(σ1^2 + σ2^2)

8.2
Covar(Z) = λK1 + (1 - λ)K2
let Z' ~ N(0, λK1 + (1 - λ)K2)
h(Z')   = 1/2 * log((2πe)^n * |λK1 + (1 - λ)K2|)
        >= h(Z) // multivariate Gaussian distribution maximizes the differential entropy for a covariance matrix
        >= h(Z|θ)
        = λh(X1) + (1 - λ)h(X2)
        = λ/2 * log((2πe)^n * |K1|) + (1 - λ)/2 * log((2πe)^n * |K2|)
        = 1/2 * log((2πe)^nλ * |K1|^λ * (2πe)^(1 - λ) * |K2|^(1 - λ))
|λK1 + (1 - λ)K2| >= |K1|^λ * |K2|^(1 - λ)

8.3
// thanks https://en.wikipedia.org/wiki/Triangular_distribution
a.  pY(y)   = int(-1/2, 1/2, pX(x)pZ(y - x)dx)
            = int(-1/2, 1/2, pZ(y - x)dx)
            = 1/a * Vol([y - 1/2, y + 1/2] ∩ [-a/2, a/2])
    when a < 1,
        pY(y)   = 1/a * (y - (-1 - a)/2) if (-1 - a)/2 <= y < (-1 + a)/2
                = 1 if (-1 + a)/2 <= y <= (1 - a)/2
                = 1/a * ((1 + a)/2 - y) if (1 - a)/2 < y <= (1 + a)/2
                = 0 otherwise
        Pr{(-1 + a)/2 <= Y <= (1 - a)/2} = Vol([(-1 + a)/2, (1 - a)/2]) = 1 - a
        hence with probability 1 - a, Y is uniformly distributed on [(-1 + a)/2, (1 - a)/2]
        with probability a, Y has triangular distribution with base a
        //  even though the two halves of the triangle are not continuous, h(y) is invariant to shifts
        he(Y)   = He(a) + (1 - a)ln(1 - a) + a(1/2 + ln(2a/2))
                = -alna - (1 - a)ln(1 - a) + (1 - a)ln(1 - a) + a/2 + alna
                = a/2
        h(Y)    = he(Y) / ln2 = a/2ln2
        I(X;Y)  = h(Y) - h(Y|X)
                = a/2ln2 - h(Z)
                = a/2ln2 - loga
    when a > 1, 
        pY(y)   = 1/a * (y - (-1 - a)/2)    if (-1 - a)/2 <= y < (1 - a)/2
                = 1/a                       if (1 - a)/2 <= y <= (-1 + a)/2
                = 1/a * ((1 + a)/2 - y)     if (-1 + a)/2 < y <= (1 + a)/2
        Pr{(1 - a)/2 <= y <= (-1 + a)/2} = (a - 1)/a
        he(Y)   = He((a - 1)/a) + (a - 1)/a * ln(a - 1) + 1/a * (1/2 + ln(2/2))
                = -(a - 1)/a * ln((a - 1)/a) - 1/a * log(1/a) + (a - 1)/a * ln(a - 1) + 1/2a
                = lna + 1/2a
        h(Y)    = he(Y) / ln2 
                = loga + 1/2ln2a
        I(X;Y)  = h(Y) - h(Y|X)
                = loga + 1/2ln2a - loga
                = 1/2ln2a
    when a = 1,
        Y ~ Triangular(-1, 1, 0)
        he(Y)   = 1/2 + ln(2/2) = 1/2
        h(Y)    = 1/2ln2
        I(X;Y)  = h(Y) - h(Y|X)
                = 1/2ln2 - loga
                = 1/2ln2
b.  X, Z ∈ [-1/2, 1/2], Y = X + Z ∈ [-1, 1], the support set of Y is a subset of [-1, 1]
    by https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution, uniform distribution has maximum entropy 
    on a given range, the maximizing distribution of Y is U(-1, 1)
    // thanks solutions manual
    let pX(-1/2) = pX(1/2) = 1/2, Y is uniform on almost all places
    h(Y) = log2 = 1, max{I{X;Y}} = 1
c.  // skipped

8.4
decay time of a radioactive particle is usually modeled as exponential distribution
int(0, 80, λe^-λxdx) = 1/2
-e^-80λ + 1 = 1/2
-e^-80λ = 1/2
λ = ln2 / 80
h(X)    = loge - logλ
        = 8.293
H(XΔ)   = h(X) + 3
        = 11.293

8.5
// Advanced Calculus of Several Variables, C. H. Edwards, 2015 contains the proof of the following theorem:
// given a transformation A(x), 
//      dA(x) = |A(x)|dx, where |A(x)| is the Jacobian determinant 
// when A(x) is linear, |A(x)| = |A| is a constant independent to x
assume A is invertible, let Y = AX, X = A^-1Y
int(-∞, +∞, f(x)dx) = 1
int(-∞, +∞, 1/|A| * f(A^-1y)dy) = 1
therefore fY(y) = 1/|A| * fX(A^-1y)
h(Y)    = -int(-∞, +∞, 1/|A| * fX(A^-1y) * log(1/|A| * fX(A^-1y))dy)
        = -int(-∞, +∞, 1/|A| * fX(x) * (-log|A| + logfX(x))|A|dx)
        = log|A| + h(X)

8.6
EQ[logX] - D(Q||P)  = ΣQ(x)logx - ΣQ(x)log(Q(x)/P(x))
                    = ΣQ(x)log(xP(x) / Q(x))
                    = -ΣQ(x)log(Q(x) / xP(x))
                    <= -(ΣQ(x)) * log(ΣQ(x) / ΣxP(x))   // log sum inequality
                    = -log(1 / EP[X])
                    = logEP[X]

8.7
Var(Y)  = Var(X') + Var(U)
        = E[X'^2] - E[X']^2 + 1/12
        = Σ(pi * i^2) - (Σpi * i)^2 + 1/12
as discrete entropy is independent to the value of x, H(X') = H(X)
Y is uniformly distributed on (i, i+1] for each i with probability pi
h(Y)    = H(X) + Σpilog1
        = H(X)
        <= 1/2 * log(2πe)(Σ(pi * i^2) - (Σpi * i)^2 + 1/12)
since entropy of discrete variables do not depend on values of x, {pi} can be permuted arbitrarily

8.8
I(X;Y)  = h(Y) - h(Y|X)
        = h(Y) - h(Z)
        = h(Y) - 1
Y is supported by a subset of [-3, 3]
let Pr{X = -2} = Pr{X = 0} = Pr{X = 2}, Y is uniformly distributed on [-3, 3]
C   = max{I(X;Y)}
    = log6 - 1
    = log3

8.9
I(X;Z)  = h(X) + h(Z) - h(X, Z)
        = 1/2 * log(2πeσx^2) + 1/2 * log(2πeσz^2) - 1/2 * log(2πe)^2|K|
where 
    K   = [σx^2, σxσzρxz, σxσzρxz, σz^2]
    |K| = σx^2σz^2 - (σxσzρxz)^2
        = σx^2σz^2(1 - ρxz^2)
I(X;Z)  = 1/2 * log(σx^2σz^2 / σx^2σz^2(1 - ρxz^2))
        = -1/2 * log(1 - ρxz^2)
independent to E[X], E[Y] and E[Z]
let E[X] = E[Y] = E[Z] = 0
conditional normal distribution is
    X|Y = y ~ N(σxρxy / σy * y, (1 - ρxy^2)σx^2)
ρxz = E[XZ]/σxσz
    = EY[E[XZ|Y]] / σxσz
    = EY[E[X|Y]E[Z|Y]] / σxσz   // markovity
    = EY[σxρxy / σy * Y * σzρyz / σy * Y] / σxσz
    = (σxσzρxyρyz / σy^2) * E[Y^2] / σxσz
    = ρxyρyz
I(X;Y)  = -1/2 * log(1 - (ρxyρyz)^2)

8.10
p(x^n)  = Πp(xi)
        = Πce^(-xi^4)
        = c^ne^(Σ-xi^4)
2^-n(h + ε) <= c^ne^(Σ-xi^4) <= 2^-n(h - ε)
-n(h + ε) <= nlogc - loge * Σxi^4 <= -n(h - ε)
-n(h + logc + ε) <= -loge * Σxi^4 <= -n(h + logc - ε)
n/loge * (h + logc + ε) >= Σxi^4 >= n/loge * (h + logc - ε)

8.11
// skipped

// chapter 9
// math in this chapter is exceedingly hard, especially I have no idea what section 9.3 is talking about

9.1
same to 7.20

9.2
assume like in the text, E[X] = 0
I(X;Y1, Y2) = h(Y1, Y2) - h(Y1, Y2|X)
            = h(Y1, Y2) - h(Z1, Z2|X)
            = h(Y1, Y2) - h(Z1, Z2)
            = h(Y1, Y2) - 1/2 * log(2πe)^2N^2(1 - ρ^2)
where
    Var(X + Z1) = Var(X) + Var(Z1) = P + N
    Covar(X + Z1, X + Z2)   = E[(X + Z1 - E[X + Z1])(X + Z2 - E[X + Z2])]
                            = E[(X + Z1)(X + Z2)]
                            = EX^2 + EZ1Z2 + EXZ1 + EXZ2
                            = P + Nρ + EXEZ1 + EXEZ2 // independence between Z1, Z2 and X
                            = P + Nρ
    h(Y1, Y2) is maximized when (Y1, Y2) ~ N(0, K'), where K = [P + N, P + Nρ, P + Nρ, P + N]
I(X;Y1, Y2) <= 1/2 * log(2πe)^2((P + N)^2 - (P + Nρ)^2) - 1/2 * log(2πe)^2N^2(1 - ρ^2)
            = 1/2 * log(1 + 2P/N(1 + ρ))
            = C
monotonically increasing in P, hence P should be maximized 
the equality is achieved when (Y1, Y2) ~ N(0, K') => X ~ N(0, P)
a.  ρ = 1, C = 1/2 * log(1 + P/N), same to the capacity 
b.  ρ = 0, C = 1/2 * log(1 + 2P/N)
c.  ρ = -1, C = ∞
    as EZ1Z2 = -EZ1^2, 
    Var(Z1 + Z2) = E[(Z1 + Z2)^2] = EZ1^2 + EZ2^2 + 2EZ1Z2 = 0
    Z1 = -Z2 with probability 1
    therefore almost always, Y1 + Y2 = 2X + Z1 + Z2 = 2X, X can be recovered exactly from (Y1, Y2)

9.3
Var(Y) = E[Y^2] - (EY)^2 <= P - (EY)^2, Var(Y) and h(Y) is maximized when EY = 0
Var(Y) = Var(X + Z) = Var(X) + Var(Z) = σx^2 + σ^2, if P < σ^2 the channel is impossible
otherwise the channel is equivalent to a channel with constraint E[X^2] <= P - σ^2
C   = 1/2 * log(1 + (P - σ^2)/σ^2)
    = 1/2 * log(1 + P/σ^2)

9.4
Z ~ Exp(1/μ)
I(X;Y)  = h(Y) - h(Y|X)
        = h(Y) - h(Z)
        = h(Y) - (1 + lnμ)  in Nats
without nonnegative constraint, X can be uniform distribution with arbitrarily wide support, the channel capacity can
be arbitrarily big by picking discrete X's far away from each other
with nonnegative constraint (X >= 0), by chapter 12 example 12.2.5 the entropy maximizing distribution given mean
constraint is exponential distribution
E[Y] = E[X] + E[Z] = λ + μ
h(Y) = 1 + ln(λ + μ)
C = ln((λ + μ) / μ) = ln(1 + λ/μ)

9.5
I(X; Y, V)  = I(X;Y) + I(X;V|Y)
            = I(X;V) + I(X;Y|V) = I(X;Y|V)
as I(X;V|Y) >= 0, I(X;Y|V) >= I(X;Y)

9.6
a.  by section 9.4, the maximizing distribution of power is
        Pi = v - Ni, where Σ(v - Ni)+ = P
    as long as 2P <= σ1^2 - σ2^2, v <= σ1^2, the channel behaves like a single channel σ2^2

    9.7
    I(X;Y)  = h(Y) - h(Y|X)
            = h(Y) - h(X + Z1 + X + Z2|X)
            = h(Y) - h(Z1 + Z2|X)
            = h(Y) - h(Z1 + Z2)
    since (Z1, Z2) ~ N(0, KZ), Z1 + Z2 ~ N(0, 2σ^2 + 2σ^2ρ)
    C   = 1/2 * log(1 + P/2σ^2(1 + ρ))
b.  ρ = 1, C = 1/2 * log(1 + P/4σ^2)
    ρ = 0, C = 1/2 * log(1 + P/2σ^2)
    ρ = -1, C = ∞
        Pr{Z1 = -Z2} = Pr{Y = 2X} = 1, X can be exactly recovered 

9.8
a.  starting from equation 9.73, by KKT conditions, 
    J(P1, .., Pk) = Σ1/2 * log(1 + Pi/Ni) - λ(ΣβiPi - P)
    dJ/dPi  = 1/2 * 1/ln2(1 + Pi/Ni) * 1/Ni - λβi
            = 1/2 * 1/ln2(Pi + Ni) - λβi <= 0
    Pi >= v/βi - Ni for some v that Σ(v - βiNi) <= P
    Pi = (v/βi - Ni)+ is an optimal solution, or βiPi = (v - βiNi)+
    assume β1N1 >= β2N2, as long as P <= β1N1 - β2N2, all power goes to channel 1
b.  β1N1 = 3 < β2N2 = 4
    by water filling, the first unit of P is given to channel 1, remaining power is evenly divided
    β1P1 = β2P2 + 1

9.9
// thanks solutions manual
E[(Z1 + Z2 - Z3)^2] = EZ1^2 + EZ2^2 + EZ3^2 + 2EZ1Z2 - 2EZ1Z3 - 2EZ2Z3
                    = 1 + 1 + 2 - 2 - 2 = 0
Y1 + Y2 - Y3 = X1 + X2 - X3 with probability 1, capacity is infinite

9.10
T = [1, 0; 1 - λdA, λdA]
by problem 7.23, C = log(2^H(λdA)/λdA + 1) - H(λdA)/λdA

9.11
same to problem 8.9

9.12
by equation 9.45, nR = ΣI(Xi;Yi) + nεn
as n -> ∞, Yn = 1/n * Xn + Zn -> Zn independent to Xn, I(Xn;Yn) -> 0
by Cesaro mean, lim{R} = lim{1/n * ΣI(Xi;Yi) + εn} = lim{I(Xn;Yn) + εn} = 0

9.13
without feedback:
    |K - λI| = 0
    (1 - λ)^2 - ρ^2 = 0
    λ1 = 1 + ρ, λ2 = 1 - ρ
    C = 1/2n * Σlog(1 + (λ - λi)+ / λi), where Σ(λ - λi)+ = 2P
with feedback:
    // thanks solutions manual
    // but the solution is very general, no that different to the formula
    // also I don't really understand the math in the solution

9.14
a.  I(X;Y)  = Σp(Z)I(X;Y|Z)
            = 1/10 * I(X;X) + 9/10 * I(X + Z*;X)
            = ∞
b.  encode the entire message as a single binary number and send, 10% of the time the number will be transmitted exactly

9.15
// assume H(X|Y) is defined as int(-∞, +∞, f(y)H(X|y)dy)
a.  when 0 <= y < 1, X must be 0, H(X|y) = 0
    when 1 <= y < a - 1,
        Pr{X = 1, y} = p(a - 1)/a, Pr{X = 0, y} = (1 - p)(a - 1)/a
        Pr{X = 1|y} = p, Pr{X = 0|y} = (1 - p), H(X|y) = H(X)
    when a - 1 <= y, X must be 1, H(X|y) = 0
    Pr{1 <= y < a - 1} = Σ{x}Pr{x, 1 <= y < a - 1} = p(a - 1)/a + (1 - p)(a - 1)/a = (a - 1)/a
    I(X;Y)  = H(X) - H(X|Y)
            = H(p) - (a - 1)/a * H(p)
            = 1/a * H(p)
b.  I(X;Y)  = h(Y) - h(Y|X)
            = h(Y) - h(Z)
            = h(Y) - loga
    where
        f(y)    = (1 - p)/a when 0 <= y < 1
                = 1/a when 1 <= y < a
                = p/a when a <= y < 1 + a
    h(Y)    = -(1 - p)/a * log((1 - p)/a) - (a - 1)/a * log(1/a) - p/a * log(p/a)
            = -(1 - p)/a * (log(1 - p) - loga) + (a - 1)/a * loga - p/a * (logp - loga)
            = H(p)/a + loga
    I(X;Y)  = H(p)/a
c.  I(X;Y) is maximized when p = 1/2, C = 1/a

9.16
same to problem 8.9 and 9.11
what's wrong with this exercise set

9.17
a.  as X2 = X3 = .. = Xn = 0 with probability 1, 
    I(X^n;Y^n) = I(X1;Y^n) = I(X1;Y1)
    C = 1/2 * log(1 + nP/N)
    1/n * C = 1/2n * log(1 + nP/N)
b.  by water-filling
    C = max{I(X^n;Y^n)} = n/2 * log(1 + P/N)
    max{1/n * I(X^n;Y^n)} = 1/2 * log(1 + P/N)
    since 
        1/2n * log(1 + nP/N)
        = 1/2n * (logn + log(1/n + P/N))
        -> 0
    1/2 * log(1 + P/N) > 1/2n * log(1 + nP/N) for sufficiently large n


9.18
a.  by water filling, C = n/2 * log(1 + P/N)
b.  shift does not change entropy, the capacity is same to part a
c.  f(y)    = int(-∞, +∞, f(y, u)du)
            = int(-∞, +∞, Φ(y - u, N)Φ(u, N1)du), where Φ(x, N) is the normal density with mean 0 and variance N
    convolution of two normal distribution is still a normal distribution
    Y ~ N(0, N + N1)
    C = n/2 * log(1 + P/n(N + N1))

9.19
if (λ - λi)+ >= 0, λ >= λi and vice versa, the two sums of P contains the same set of terms
when (λ - λi)+ = 0 or λ <= λi, 1/2 * log(1 + (λ - λi)/λi) = 1/2 * log1 = 0
when (λ - λi)+ > 0, (λ - λi)+ = λ - λi,
    1/2 * log(1 + (λ - λi)/λi)
    = 1/2 * log(λ/λi)
the two sums of C also contains the same set of terms

9.20
a. see problem 9.21
b.  // skipped

9.21
I(X;X + Z*) = h(X + Z*) - h(X + Z*|X)
            = h(X + Z*) - h(Z*)
as Var(X + Z*) = P + N, X* + Z* ~ N(0, P + N), h(X + Z*) <= h(X* + Z*)
by entropy power inequality,
    2^2h(X* + Z) >= 2^2h(X*) + 2^2h(Z)
    h(X* + Z)   >= 1/2 * log(2^2h(X*) + 2^2h(Z))
                = 1/2 * log(2πeP + 2^2h(Z))
I(X*;X* + Z)    = h(X* + Z) - h(Z)
                >= 1/2 * log(2^2h(X*) + 2^2h(Z)) - h(Z)
                = 1/2 * log(2^2h(X*) + 2^2h(Z)) - 1/2 * log2^2h(Z)
                = 1/2 * log(1 + 2πeP/2^2h(Z))
where 2πeP / 2^2h(Z) is minimized when h(Z) = h(Z*), hence I(X*;X* + Z) >= I(X*;X* + Z*)
(X*, Z*) is a Pareto optimality

9.22
by channel coding theorem, as long as C > 0, there exist codes for any R < C that every X^n(w) can be recovered with
arbitrarily low rate of error, then Z^n can be computed from Y^n and X^n

10.1
let Y = X defined on [0, ∞)
E[(Y - Y')^2]   = E[Y^2] - 2Y'E[Y] + Y'^2
is minimized at Y' = E[Y]
E[Y]    = int(0, +∞, yφ(y)dy)
        = int(0, +∞, y * 1/(2π)^1/2 * 1/σ * e^-(x/σ)^2dx)
        = int(0, +∞, d-(2/π)^1/2 * σ * e^-(x/σ)^2)
        = 0 + (2/π)^1/2 * σ
        = sqrt(2/π) * σ
E[(Y - Y')^2] = Var(Y)
X ~ N(0, σ^2) can be seen as a Xθ that X0 = Y and X1 = -Y, θ ~ Bernoulli(1/2)
Var(X)  = E[Var(Xθ|θ)] + Var(E[Xθ|θ])
        = Var(Y) + E[Y]^2
σ^2 = Var(Y) + 2σ^2/π
Var(Y) = (π - 2)/π * σ^2 > σ^2 / 4

10.2
as d(0, 1) = ∞, pr{X' = 1|X = 0} must be 0
q(x'|x) = [1, 0; 1 - p, p] for some Pr{X' = 1|X = 1} = p
Ed(X, X') = p(1)q(0|1)d(1, 0) = (1 - p)/2 <= D, p >= 1 - 2D
I(X;X') = H(X') - H(X'|X)
        = H(p/2) - 1/2 * H(p)
by dH(p)/dp = -logp + log(1 - p), 
dI/dp   = (-log(p/2) + log(1 - p/2)) * 1/2 - 1/2 * (-logp + log(1 - p))
        = 1/2 * (1 + log(1 - p/2)) - 1/2 * log(1 - p)
        = 1/2 + 1/2 * (log(1 - p/2) - log(1 - p))
        >= 0
I is non-decreasing, I(X;X') is minimized at 1 - 2D if D <= 1/2, otherwise I(X;X') is minimized at p = 0
I(X;X') = H(1/2 - D) - 1/2 * H(1 - 2D) if D <= 1/2
        = 0 if D > 1/2

10.3
let q(x'|x) = [1 - p, p; q, 1 - q], Ed(X, X') = ap + bq <= D
I(X;X') = H(X') - H(X'|X)
        = H((1 - p + q)/2) - 1/2 * (H(p) + H(q))
// even wolframalpha gave up

10.4
for a fixed q(x'|x), if Ed(X, X') = D
    Ed'(x, x')  = Σ{x, x'}p(x)q(x'|x)d'(x, x')
                = Σ{x, x'}p(x)q(x'|x)(d(x, x') - w(x))
                = D - Σ{x, x'}p(x)q(x'|x)w(x)
                = D - Σ{x}p(x)w(x)
                = D - w_
any description with rate R achieving d(x, x') = D also achieves d'(x, x') = D - w_ and vice versa
therefore R'(D - w_) = R(D), R'(D) = R(D + w_)
as long as d(xi, xi') < ∞ for some pair (xi, xi'), define d'(x, x') = d(x, x') - d(xi, xi') and the rate distortion
function wrt. d'(x, x') is a known constant away from rate distortion function of d(x, x') 

10.5
// thanks solutions manual
D   = Σp(x, x')d(x, x')
    = Pr{X != X'}
by Fano's inequality, 
    H(D) + Dlog(|X| - 1) >= H(X|X')    
    I(X;X') = H(X) - H(X|X')
            >= logm - H(D) - Dlog(m - 1)
the lower bound can be achieved by
    p(x'|x) = 1 - D if x' = x
            = D/(m - 1) if x' != x
    p(x')   = Σp(x'|x)p(x)
            = 1/m * (D/(m - 1) * (m - 1) + 1 - D)
            = 1/m
    p(x|x') = p(x'|x) * p(x) / p(x')
            = p(x'|x)
    H(X|X') = H(D) + (1 - D)H(0) + Dlog(|X| - 1)
            = H(D) + Dlog(m - 1)
with no description, the best guess is q(x'|x) = q(x') = p(x)
D = ΣPr{X != X'} = 1 - Σ{x = x'}p(x)q(x') = 1 - 1/m
hence R(D)  = logm - H(D) - Dlog(m - 1) if D < 1 - 1/m
            = 0 if D >= 1 - 1/m

10.6
a.  let p1 maximizes Φ(D1), p2 maximizes Φ(D2)
    let p = λp1 + (1 - λ)p2, by linearity of D, Σpidi = λD1 + (1 - λ)D2
    since H(p) is a concave function,
        Φ(λD1 + (1 - λ)D2)  >= H(p)
                            >= λH(p1) + (1 - λ)H(p2)
                            = λΦ(D1) + λΦ(D2)
    and Φ(D) is concave
b.  10.152: definition of mutual information
    10.153: definition of conditional entropy
    10.154: Φ(Dx') = max{H(p)} for all Σp(x|x')d(x, x') = Dx'
    10.155: concavity of Φ(D)
    10.156: Σp(x')ΣpDx' = Σp(x')Σp(x|x')d(x, x') = Ed(X, X') <= D, and Φ(D) is non-decreasing
c.  immediately from part b and equation 10.11
d.  let p* = Φ(D) = argmax{H(p), Σpidi <= D} for some row of d(x, x')
    let p(x') = 1/m, p(x|x') = a permutation of p* that Σp(x|x')d(x, x') = D (always possible as how d is defined)
    I(X;X') = H(X) - H(X|X')
            = H(X) - Σp(x')H(X|X' = x')
            = H(X) - H(p*)
            = H(X) - Φ(D)
    subject to
    Ed(X, X')   = Σ{x, x'}p(x')p(x|x')d(x, x')
                = Σ{x}p*(x)d(x, x')
                = D
    so the lower bound is achievable

10.7
one x' on each row cannot take any probability above 0
the distortion matrix is equivalent to [0, 1;1, 0]
X ~ Bernoulli(1/2) is uniform, distortion matrix is rows of permutations, by problem 10.8,
    R(D)    = H(X) - Φ(D)
            = H(X) - H(p) subject to p <= D
            = 1 - H(p) if D < 1/2 
            = 0 if D >= 1/2
as proven in part d of problem 10.6, define p(x') uniform and p(x|x') = [1 - D, D, 0; 0, 1 - D, D] achieves the bound

10.8
lower bound:
    I(X;X') = h(X) - h(X|X')
            = h(X) - h(X - X'|X')
            >= h(X) - h(X - X')
            >= h(X) - h(N(0, E[(X - X')^2]))
            = h(X) - 1/2 * log2πeE[(X - X')^2]
            >= h(X) - 1/2 * log2πeD
upper bound:
    define X' = (σ^2 - D)/σ^2 * (X + Z), Z ~ (0, Dσ^2/(σ^2 - D)), Z and X independent
    Ed(X, X')   = E[(X' - X)^2]
                = E[(-D/σ^2 * X + (σ^2 - D)/σ^2 * Z)^2]
                = D^2/σ^4 * E[X^2] + (σ^2 - D)^2/σ^4 * E[Z^2]
                = D^2/σ^2 + (Dσ^2 - D^2)/σ^2
                = D
    h(X')   <= h(N(0, (σ^2 - D)^2/σ^4 * (σ^2 + Dσ^2/(σ^2 - D))))
            = h(N(0, σ^2 - D))
            = 1/2 * log2πe(σ^2 - D)
    h(X'|X) = h((σ^2 - D)/σ^2 * Z)
            = h(Z) + log((σ^2 - D)/σ^2)
            = 1/2 * log2πeDσ^2/(σ^2 - D) + log((σ^2 - D)/σ^2)
            = 1/2 * log2πeD(σ^2 - D)/σ^2
    I(X;X') = h(X) - h(X'|X)
            <= 1/2 * log2πe(σ^2 - D) - 1/2 * log2πeD(σ^2 - D)/σ^2
            = 1/2 * log(σ^2/D)
    and any optimal X' can only have rate lower or equal to this
when the source is N(0, σ^2) the upper bound is reached, normal distribution is the hardest to describe

10.9
10.58:  there must be a bijection between X^n and D(f(X^n)) = W, |X|^n = |W|
10.59:  H(f(X^n)|X^n) = 0, f(X^n) is a deterministic function of X^n
10.61:  X' = g(f(X^n)) is a sufficient statistic of f(X^n)
10.65:  Xi -> Xi' -> (X'^n, Xi-1, .., X1)
10.67:  Xi' achieves the minimum rate of Ed(Xi, Xi') individually
10.69:  rate distortion function is linear on the range

10.10
exactly half x' have x - x' odd (or even) given x, d(x, x') is permutations of rows, X is uniform, the Shannon lower
bound can be achieved
Φ(D)    = max{H(p), Σpidi <= D}
        = H(D) + Dlogm + (1 - D)logm
        = H(D) + logm
where the maximizing p* is separately uniform on x - x' is even and x - x' is odd, or
    pi  = D/m if x - i is odd 
        = (1 - D)/m if x - i is even
R(D)    = H(X) - Φ(D)
        = log(2m) - H(D) + logm
        = 1 - H(D)
the verification is given by part d of problem 10.6

10.11
let 1/int(-∞, +∞, e^-x^4dx) = k
h(X)    = -int(-∞, +∞, ke^-x^4log(ke^-x^4)dx)
        = -int(-∞, +∞, ke^-x^4(logk - x^4)dx)
        = -int(-∞, +∞, ke^-x^4logkdx) + int(-∞, +∞, x^4ke^-x^4dx)
        = c - logk
I(X;X') = h(X) - h(X|X')
        = h(X) - h(X - X'|X') 
        = h(X) - h(X - X') subject to E[(X - X')^4] <= D
        >= h(X) - g(D)  // the definition of g(.)
by definition, g(a) is non-decreasing, assume the maximum is achieved on the boundary
by 12.4, the density maximizing h(X) subject to int(-∞, +∞, f(x)x^4dx) = c is f(x) = e^-λ0 * e^λ1x^4 such that
    int(-∞, +∞, x^4 * e^-λ0 * e^λ1x^4dx)) = c
by the definition of c, λ1 = -1, e^-λ0 = k, f(x) = ke^-x^4, h(X) = g(c)
I(X;X') >= g(c) - g(D)

10.12
the new column can be ignored so will never increase R(D)
if the new column has an entry that d(x, x0') < d(x, xk') and p(x|xk') > 0, it can replace xk' given x and decrease 
Ed(X, X'), potentially decrease R(D)
otherwise the new column will not affect R(D)

10.13
a.  the density and distortion satisfies the equality condition of Shannon lower limit 
    Φ(0)    = max{H(p), Σpidi = 0}
            = H(1/2) = 1
    R(0)    = H(X) - Φ(D)
            = 2 - 1 = 1
b.  in any cases, {1, 2} in X and {1, 2} in X' are indistinguishable wrt. d(x, x'), similarly {3, 4} in X and X'
    the problem can be collapsed to X = {1, 2}, X' = {1, 2}, d(x, x') = [0, 1; 1, 0]
    by 10.13,
    R(D)    = 1 - H(D) if D <= 1/2
            = 0 if D >= 1/2
c.  by 10.13, 
    R(D)    = H(p) - H(D) if D < min{p, 1 - p}
            = 0 if D >= min{p, 1 - p}

10.14
a.  I(X, Y;X', Y')  = H(X, Y) + H(X', Y') - H(X, Y, X', Y')
                    = H(X) + H(Y) + H(X') + H(Y') - H(X, X') - H(Y, Y') // independence
                    = I(X;X') + I(Y;Y')
    let p(x', y'|x, y) be the joint conditional distribution minimizing I(X, Y|X', Y') subject to constraints
    Σ{y'}p(x', y'|x, y) = p(x'|x, y) = p(x'|x) satisfies Ed(X, X') <= D1
    Σ{x'}p(x', y'|x, y) = p(y'|x, y) = p(y'|y) satisfies Ed(Y, Y') <= D2
    the two marginal distribution satisfies the constraints of RX(D1) and RY(D2)
    as I(X, Y;X', Y') = I(X;X') + I(Y;Y'), RXY(D1, D2) >= RX(D1) + RX(D2)
b.  given minimizing conditional distributions p(x'|x) and p(y'|y), q(x', y'|x, y) = p(x'|x)p(y'|y) satisfies the 
    constraints of RXY(D1, D2) as the constraints is put on marginal distributions only
    RXY(D1, D2) <= RX(D1) + RX(D2) and hence RXY(D1, D2) = RX(D1) + RX(D2)
    two independent sources cannot be compressed better together than individually

10.15
a.  if R1 <= R2, the feasible set of D(R2) is a superset of D(R1), so it must have D(R2) <= D(R1)
b.  as R(D) is decreasing and convex, its inverse D(R) must be convex
c.  10.162: definition of distortion between sequences
    10.163: linearity of expectations
    10.164: the definition of D(R)
    10.165: convexity of D(R)
    10.166: by the chain rule of mutual information and independence of {Xi}, should be equality here
    10.167: nR = H(i(X^n)) >= H(i(X^n)) - H(i(X^n)|X^n) = I(i(X^n);X^n) >= I(X'^n;X^n)  // data processing inequality
            1/n * I(X^n;X'^n) <= R
            D(1/n * I(X^n;X'^n)) >= D(R)    // D(R) is non-increasing

10.16
// thanks solutions manual
a.  for x^n ∈ Aε^*n, 
        |-H(X) - 1/n * logp(x^n)|   = |Σp(a)logp(a) - 1/n * logΠp(xi)|
                                    = |Σp(a)logp(a) - 1/n * Σlogp(xi)|
                                    = |Σp(a)logp(a) - 1/n * ΣN(a|x^n)logp(a)|
                                    = Σ|p(a) - 1/n * N(a|x^n)||logp(a)|
                                    <= |X| * ε/|X| * max{|logp(a)|}
                                    = ε1
    hence a Aε^*n strong typical set is a Aε1^n weak typical set of X
    |Aε^*n| <= 2^n(H(X) + ε1)
    1/n * lim(|Aε^*n|/2^nH(X))  <= 1/n * lim(2^n(H(X) + ε1) / 2^nH(X))
                                = 1/n * nε1
                                = ε1 -> 0
    or by the method of types:
    for a discrete distribution, by taking ε sufficiently small and for n that np(a) is an integer for all a
    we can achieve N(a|x^n) = p(a) for some ε > 0, Aε^*n is essentially the type of p(a), and by theorem 11.1.3 
        1/(n + 1)^|X| <= |Aε^*n| / 2^nH(X) <= 1
        1/n * lim{log(|Aε^*n| / 2^nH(X))}   >= 1/n * lim{log(1/(n + 1)^|X|)}
                                            = 1/n * lim{-|X|log(n + 1)}
                                            = lim{-|X| * log(n + 1)/n}
                                            = 0
b.  since N(a|x^n) = Σ{b}N(a, b|x^n, y^n), the conditional type is determined by the joint type Pxy, so the number of 
    conditional types cannot exceed the number of joint types, by theorem 11.1.1,
        |Px^n,y^n| <= (n + 1)^|XY| = (n + 1)^|X||Y|
c.  upper bound:
        Pr{y^n|x^n} = Πp(yi|xi)
                    = Πp(b|a)^NXY(a, b)
                    = Πp(b|a)^nPXY(a, b)
                    = Π2^nPXY(a, b)logp(b|a)
                    = 2^nΣ(PXY(a, b)logp(b|a) + PXY(a, b)logV(b|a) - PXY(a, b)logV(b|a))
                    = 2^nΣ(-PXY(a, b)log(V(b|a)/p(b|a)) + PXY(a, b)logV(b|a))
                    = 2^n(-D(V(b|a)||p(b|a)) - H(V|x^n))
        when y^n ∈ TV(x^n), Pr{y^n} = 2^-nH(V|x^n)
        for V(b|a) = p(y|x), 
        1   >= Pr{y^n ∈ TV(x^n)}
            = Pr{y^n ∈ TV(x^n)}
            = Σ{y^n ∈ TV(x^n)}Pr{y^n}
            = |TV(x^n)|2^-nH(p(y|x)|x^n)
            = |TV(x^n)|2^-nH(Y|x^n)
        |TV(x^n)| <= 2^nH(Y|x^n)
    lower bound:
        1   = Σ{V}Pr{y^n ∈ TV(x^n)}
            <= Σ{V}max{V}Pr{y^n ∈ TV(x^n)} 
    let V* = argmax{Pr{y^n ∈ TV(x^n)}}, by inequality 11.23, V*(b|a) = p(b|a)
        1   <= (n + 1)^|X||Y| * |TV(x^n)| * 2^-nH(Y|x^n)
        |TV(x^n)| >= 2^nH(Y|x^n) / (n + 1)^|X||Y|
d.  // informal
    |N(a, b|x^n, y^n)/n - V(b|a)N(a|x^n)/n| <= ε/(|Y| + 1)
    fixing x^n, V(b|a)N(a|x^n) is the expected number of (a, b), V(b|a)N(a|x^n)/n is p(a, b|x^n)
    |N(a, b|x^n, y^n)/n - p(a, b|x^n)| <= ε/(|Y| + 1)
    sum over a ∈ X, 
    |N(b|y^n)/n - p(b|x^n)| <= ε|X|/(|Y| + 1) = ε1/(|Y| + 1)
    by part a, |Aε^*n(Y|x^n)| = 2^nH(Y|x^n) to the first order of exponential
e.  // informal
    conditioning on x^n, 
        |N(a, b|x^n, y^n)/n - p(a, b|x^n)| <= ε/|X||Y|
    similar to part d, |Aε^*n(X, Y)| = 2^nH(Y|x^n) to the first order of exponential
f.  by summing over a or b, 
        |N(a, b|x^n, y^n)/n - p(a, b)| <= ε/|X||Y| imples
        |N(a|x^n) - p(a)| <= ε/|X| and 
        |N(b|y^n) - p(b)| <= ε/|Y|
    if (x^n, y^n) is strongly jointly typical, x^n and y^n are strongly typical
    Pr{(x^n, Y^n) ∈ Aε^*n(X, Y)}    
    = Σ{(x^n, y^n) ∈ Aε^*n(X, Y)}p(y^n)
    <= |Aε^*n(X, Y)| * 2^-n(H(Y) + ε1)
    <= 2^n(H(Y|X) + ε2) * 2^-n(H(Y) + ε1)
    = 2^-n(I(X;Y) - ε3)
    similarly Pr{(x^n, Y^n) ∈ Aε^*n(X, Y)} >= 2^-n(I(X;Y) + ε3)

10.17
a.  assume d(v, v') is bounded, for some C > R > R(D) there exists an encoding V -> W -> V', |W| = 2^nR that
        Ed(V, V') <= D + δ (through a noiseless channel)
    as C > R, W -> X^n -> Y^n -> W' can be transmitted through the channel with arbitrarily low error rate Pe
        Ed(V, V') <= D + δ + Pe * d_max -> D + δ as Pe -> 0
b.  for any setup V^n -> X^n -> Y^n -> V'^n, fixing p(y|x) and p(v), constraint on Ed(V, V') <= D,
        R(D) < I(V;V') <= I(X;Y) <= C (the computation of C is not constrained by distortion rate i.e. max of superset)
    therefore the channel capacity must be greater than R(D)

10.18
a.  Ed'(X, X') <= D <=> E(d(X, X') + a) <= D
                    <=> Ed(X, X') <= D - a
    hence R'(D) = R(D - a)
b.  Ed'(X, X') <= D <=> E(bd(X, X')) <= D
                    <=> Ed(X, X') <= D/b
    R'(D) = R(D/b)
c.  when d(x, x') = (x - x')^2, R(D) = 1/2 * log(σ^2/D)
    so R''(D) = R'(D - 3) = R((D - 3)/5) = 1/2 * log(σ^2 / ((D - 3)/5))

10.19
X^n -> W -> (X1^n, X2^n), |W| = 2^nR, let R(D1, D2) = min{I(X;X1, X2), Ed(X, X1) <= D1, Ed(X, X2) <= D2}
converse:
    R(D1, D2) is non-increasing in both D1 and D2
    R(D1, D2) is convex:
        let p1 achieves R(D11, D12), p2 achieves R(D12, D22)
        let p = λp1 + (1 - λ)p2
        d1 and d2 are linear in distribution, Ep[d1(x, x1)] = λD11 + (1 - λD12), Ep[d2(x, x2)] = λD12 + (1 - λ)D22
        I(X;X1, X2) is convex in p(x1, x2|x)
        R(λD11 + (1 - λ)D21, λD12 + (1 - λ)D22) <= Ip(X;X1, X2)
                                                <= λIp1(X;X1, X2) + (1 - λ)Ip2(X;X1, X2)
                                                = λR(D11, D21) + (1 - λ)R(D12, D22)
    for some Ed(X^n, X1^n) <= D1 and Ed(X^n, X2^n) <= D2,
    nR  >= H(W)
        >= H(W) - H(W|X^n)
        = I(X^n;W)
        >= I(X^n;X1^n, X2^n)
        = H(X^n) - H(X^n|X1^n, X2^n)
        = ΣH(Xi) - ΣH(Xi|X1^n, X2^n, X1, .., Xi-1)
        >= ΣH(Xi) - ΣH(Xi|X1i, X2i)
        = ΣI(Xi;X1i, X2i)
        >= ΣR(Ed(Xi, X1i), Ed(Xi, X2i)) // definition of R(D1, D2)
        = n(1/n * ΣR(Ed(Xi, X1i), Ed(Xi, X2i)))
        >= nR(1/n * ΣEd(Xi, X1i), 1/n * ΣEd(Xi, X2i))
        = nR(Ed(X^n, X1^n), Ed(X^n, X2^n))
        >= nR(D1, D2)
    R   >= R(D1, D2)
achievability:
    consider (X1, X2) as a single random variable on X1 x X2
    expand the last condition of distortion typical set as
        |d(x^n, x1^n) - Ed(X, X1)| < ε,
        |d(x^n, x2^n) - Ed(X, X2)| < ε
    so Ed(X^n, X1^n(X^n)) <= D1 + ε + Pe * d_max, Ed(X^n, X2^n(X^n)) <= D2 + ε + Pe * d_max
    the bound of Pe doesn't involve the last condition of distortion typical set, thus
        Pe -> 0 as n -> ∞ if R > I(X;X1, X2) + 3ε 
    for any R > R(D1, D2), (R, D1, D2) is achievable

10.20
a.  let q(x'|x) = argmin{I(X;X'), Ed2(X, X') <= D}
    Eq[d1(X, X')] <= Eq[d2(X, X')] = D
    q(x'|x) is in the feasible set of R1(D), R1(D) <= R2(D)
b.  see problem 10.19

11.1
a.  D(f1||f2) 
    = int(-∞, +∞, f1(x)log(f1(x) / f2(x))dx)
    = int(-∞, +∞, f1(x)log(σ2/σ1 * e^(1/2)(-(x/σ1)^2 + (x/σ2)^2))dx)
    = log(σ2/σ1) + 1/2ln2 * (E1[-(x/σ1)^2] + E1[(x/σ2)^2])
    = log(σ2/σ1) + 1/2ln2 * (-1 + (σ1/σ2)^2)
b.  D(f1||f2)
    = int(-∞, +∞, f1(x)log(f1(x) / f2(x))dx)
    = int(-∞, +∞, f1(x)log(λ1/λ2 + ln2(-λ1x + λ2x))dx)
    = log(λ1/λ2) + ln2(-1 + λ2/λ1)
c.  infinite, f2(x) = 0 on [0, a)
d.  infinite, p2(T) = 0

11.2
// thanks wolframalpha
let T = P - Q, ln(P/Q) = ln(1 + T/Q), the taylor expansion of ln(1 + T/Q) is 
    T/Q - T^2/2Q^2 + ..
D(P||Q) = int(-∞, +∞, (T + Q)(T/Q - T^2/2Q^2 + ..))
        = int(-∞, +∞, T^2/Q + T - T^3/2Q^2 - T^2/2Q + ..)
        = int(-∞, +∞, T + T^2/2Q + ..)  // other terms have higher order of 1/Q
        = int(-∞, +∞, T) + int(-∞, +∞, T^2/2Q) + ..
        = 0 + χ^2 / 2 + ..

11.3
a.  L(p, l) = Σpilog(pi/qi) - l1(Σpi - 1) - l2(-Σpilogpi - R)
    dL/dpi  = log(pi/qi) + 1/ln2 - l1 + l2logpi + l2/ln2 = 0
    logpi - logqi + loge - l1 + l2logpi + l2loge = 0
    (1 + l2)logpi = -(1 + l2)loge + l1 + logqi
    logpi = -loge + l1/(1 + l2) + logqi/(1 + l2)
    pi  = e * 2^(l1/(1 + l2)) * qi^(1/(1 + l2))
        = a * qi^b for some a, b constant
    Σpi = Σaqi^b = 1, a = 1/Σqi^b, pi = qi^b / Σqi^b for some b satisfying H(P) = R
b.  let p = min{Pr{X = 0}, Pr{X = 1}}
    H(p) = -plogp - (1 - p)log(1 - p) < R

11.4
// thanks solutions manual
assume all constraints are active, by equation 11.110,
    P*(x) = c1Q(x)e^Σaihi(x)
where {ai} is chosen to satisfy ΣP*(x)hi(x) <= αi
similarly
    R*(x) = c2Q(x)e^(Σbihi(x) + Σvigi(x))
let R'(x) minimize D(R'||P*) subject to ΣR'(x)hi(x) <= αi and ΣR'(x)gi(x) <= βi
    R'(x)   = c3P*(x)e^(Σdihi(x) + Σwigi(x))
            = c1c3Q(x)e^(Σ(ai + di)hi(x) + Σwigi(x))
which has the same form and constraints to R*(X), hence R'(x) = R*(x)

11.5
lemma: if f(x) = g(x) to first order of exponents, h(x) is polynomial, f(x)h(x) = g(x) to first order of exponents
    1/n * lim(log(f(x)h(x) / g(x)))
    = 1/n * lim(log(f(x)/g(x)) + logh(x))
    = 0 + 1/n * lim(logh(x))
    = 0
let Px(a) = N(a|x^n)/n, 
1/n * Σg(xi) >= α   <=> 1/n * ΣN(a|x^n)g(a) >= α
                    <=> ΣPx(a)g(a) >= α
let A be the set of probability that ΣPx(a)g(a) >= α
Σ{P ∈ A}|T(P)|  = Σ{P ∈ A}2^nH(P)   // to first order 
                <= Σ{P ∈ A}2^nH(P*)
                <= (n + 1)^|X| * 2^nH(P*)
                = 2^nH(P*)  // to first order
also 
Σ{P ∈ A}|T(P)|  = Σ{P ∈ A}2^nH(P)
                >= 2^nH(P*)

11.6
a.  E[1/n * ΣXi - μ] = 1/n * ΣEμ[Xi] - μ = 0
b.  1/n * E[Σ(Xi - 1/n * ΣXi)^2]
    = 1/n * E[Σ(Xi - μ - (1/n * ΣXi - μ))^2]
    = 1/n * E[Σ(Xi - μ)^2 - 2(1/n * ΣXi - μ)Σ(Xi - μ) + n(1/n * ΣXi - μ)^2]
    = 1/n * (nσ^2 - 2n(1/n * ΣXi - μ)^2 + n(1/n * ΣXi - μ)^2)
    = 1/n * (nσ^2 - σ^2)
    = (n - 1)σ^2/n < σ^2
    1/(n - 1) * E[Σ(Xi - 1/n * ΣXi)^2] = σ^2
c.  since nSn^2 / σ^2 ~ χ^2(n - 1), Sn^2 = (n - 1)/n * Sn-1^2, (n - 1)Sn-1^2 / σ^2 ~ χ^2(n - 1)
    E[(Sn-1^2 - σ^2)^2] = Var(Sn-1^2) = σ^4 / (n - 1)^2 * Var(χ^2(n - 1)) = 2σ^4 / (n - 1)
    similarly
    E[(Sn^2 - σ^2)^2]   = E[(Sn^2 - (n - 1)/n * σ^2 - 1/n * σ^2)^2]
                        = E[(Sn^2 - E[Sn^2])^2] - 2/n * σ^2 * E[Sn^2 - E[Sn^2]] + σ^4/n^2 
                        = σ^4(2n - 2) / n^2 + σ^4/n^2
                        = σ^4(2n - 1) / n^2
    where 
    2/(n - 1) - (2n - 1)/n^2
    = (2n^2 - 2n^2 + 3n - 1) / n^2(n - 1)
    = (3n - 1) / n^2(n - 1)
    > 0 for n > 1

11.7
lim{θ' -> θ, 1/(θ' - θ)^2 * int(-∞, +∞, D(pθ||pθ'))}
= lim{θ' -> θ, 1/(θ' - θ)^2 * 1/ln2 * int(-∞, +∞, pθ(x) * ln(pθ(x) / pθ'(x)))}
= lim{h -> 0, 1/h^2 * 1/ln2 * int(-∞, +∞, pθ(x) * ln(pθ(x) / pθ+h(x)))}
let g(h) = pθ(x) * (lnpθ(x) - lnpθ+h(x))
g(0)    = 0
g'(h)   = -pθ(x) / pθ+h(x) * dpθ+h(x)/dh
        = -pθ(x) / pθ+h(x) * dpθ+h(x)/dθ * dθ/dh
        = pθ(x) / pθ+h(x) * dpθ+h(x)/dθ
g'(0)   = dpθ(x)/dθ
g''(h)  = -pθ(x) * (-1/pθ+h(x)^2 * (dpθ+h(x)/dh)^2 + 1/pθ+h(x) * d^2pθ+h(x)/d^2h)
g''(0)  = 1/pθ(x) * (dpθ(x)/dθ)^2 + d^2pθ(x)/d^2θ
also
int(-∞, +∞, dpθ(x)/dθdx) = dint(-∞, +∞, pθ(x)dx)/dθ = 0, similarly
int(-∞, +∞, d^2pθ(x)/d^2θdx) = 0
by Taylor expansion around h = 0,
lim{h -> 0, 1/h^2 * 1/ln2 * int(-∞, +∞, pθ(x) * ln(pθ(x) / pθ+h(x)))}
= lim{h -> 0, 1/h^2 * 1/ln2 * int(-∞, +∞, 1/pθ(x) * (dpθ(x)/dθ)^2 * h^2 / 2 + O(h^3))}
= lim{h -> 0, 1/2ln2 * int(-∞, +∞, 1/pθ(x) * (dpθ(x)/dθ)^2 + O(h))}
= lim{h -> 0, 1/ln4 * int(-∞, +∞, pθ(x) * (dln(pθ(x))/dθ)^2 + O(h))}
= 1/ln4 * J(θ)

11.8
a.  dfθ(x)/dθ   = -1/2 * (2πθ)^(-3/2) * 2π * e^(x^2/θ)/2 + 1/(2πθ)^1/2 * e^(x^2/θ)/2 * x^2/2θ^2
    dfθ(x)/dθ / fθ(x)   = -1/2θ + x^2/2θ^2
    J(θ)    = E[(-1/2θ + X^2/2θ^2)^2]
            = 1/4θ^2 + -2E[X^2]/4θ^3 + E[X^4]/4θ^4
            = 1/2θ^2
b.  dfθ(x)/dθ   = 1/θ - x
    J(θ)    = E[(1/θ - X)^2]
            = 1/θ^2 - 2E[X]/θ + E[X^2]
            = 1/θ^2 - 2/θ^2 + 2/θ^2
            = 1/θ^2
c.  a.  var(T) >= 1/J(θ) = 2θ^2
    b.  var(T) >= 1/J(θ) = θ^2

11.9
immediate consequence of equation 11.279

11.10
a.  L(P(x, y), l)   = D(P||Q0) - l1(ΣP(x, y)logQ(x) - H(X)) - l2(ΣP(x, y)logQ(y) - H(Y)) - l3(ΣP(x, y)logQ(x, y) - H(X, Y))
    dL/dP(x, y) = logP(x, y) - logQ(x, y) + 1/ln2 - l1logQ(x) - l2logQ(y) - l3logQ(x, y) = 0
    logP(x, y)  = l1logQ(x) + l2logQ(y) + (l3 + 1)logQ(x, y) - loge
    P(x, y)     = 2^(l1logQ(x) + l2logQ(y) + (l3 + 1)logQ(x, y) - loge)
                = Q(x, y) * e^(l0 + l1logQ(x) + l2logQ(y) + l3logQ(x, y))
    and D(P||Q0) is strictly convex in P
b.  P*(x, y)    = Q0(x, y)e^(l0 + l1logQ(x) + l2logQ(y) + l3logQ(x, y))
                = Q(x)Q(y)2^(l0' + l1logQ(x) + l2logQ(y) + l3logQ(x, y))
                = cQ(x)^(l1 + 1) * Q(y)^(l2 + 1) * Q(x, y)^l3
    setting c = 1, l1 - =1, l2 = -1, l3 = 1,
                = Q(x, y)
    and the constraints are just be formula of H(X), H(Y) and H(X, Y) when P(x, y) = Q(x, y)

11.11
let T' = T - bT(θ), T' is unbiased estimator of θ
by the same argument in the text, 
E[VT']^2 <= J(θ)var(T'), where
E[VT']  = E[V(T - bT(θ))]
        = E[VT] - bT(θ)E[V]
        = dE[T]/dθ
        = d(bT(θ) + θ))/dθ
        = bT'(θ) + 1
var(T') >= E[VT']^2 / J(θ)
        = (1 + bT'(θ))^2 / J(θ)
E[(T - θ)^2]    = E[(T' - θ + bT(θ))^2]
                = var(T') + bT(θ)^2
                >= (1 + bT'(θ))^2 / J(θ) + bT(θ)^2

11.12
let Pr{Decide H1|H2 is true} = β, by Chernoff-Stein lemma, 
lim{1/n * logβ} = -D(P1||P2)
                = -(1/2 * 1 + 0 + 1/4 * -1)
                = -1/4
logβ    ≒ -n/4 when n is large

11.13
11.326 is the pmf of binomial distribution
1.  as p > 1/2, floor(np) >= n/2, C(n, k) is a decreasing function starting from k = n/2
    as q > 1/2, q^i(1 - q)^(n - i) is a decreasing function of i
2.  that term is approximately the probability of type T(P) in Q^n, by equation 11.54,
    Q^n(T(P)) <= 2^-nD
3.  Pr{X: avg(X) >= p}  <= Σ2^-nD
                        <= n2^-nD
    Pr{X: avg(X) >= p}  >= C(n, i)q^i(1 - q)^(n - i) where i = ceil(np)
                        >= 1/(n + 1)^|X| * 2^-nD
    log(1/(n + 1)^|X|) - nD <= logPr{X: avg(X) >= p} <= logn - nD
    -1/n * logPr{X: avg(X) >= p} -> D

11.14
a.  D(P||Q) = int(-∞, +∞, p(x)log(p(x) / 1/sqrt(2πσ^2)e^-1/2(x/σ)^2))
            = int(-∞, +∞, p(x)logp(x)) + 1/2 * log2πσ^2 + 1/2 * E[X^2]/σ^2
            = -h(P) + 1/2 * log2πσ^2 + 1/2 * E[X^2]/σ^2
    subject to E[X^2] >= α^2, -h(P) is minimized by normal distribution, P ~ N(0, α^2)
    D(P1||P2)   = log(σ2/σ1) - loge/2 * (1 - σ1^2 / σ2^2)
    dD/dσ1  = -loge/σ1 + loge * σ1/σ2^2 > 0 when σ1 > σ2
    avg(Xi^2) -> E[Xi^2] = σ^2 as n -> ∞, σ^2 >= α^2 with large n
    for α^2 > σ^2, the minimizing P* is N(0, α^2), 
    logPr{avg(Xi^2) >= α} -> -nD(P*||P) = log(σ/α) - loge/2 * (1 - α^2/σ^2)
b.  P* = N(0, α^2)

11.15
a.  L(p, l) = D(P||Q) - l1(p1 - 2p2) - l2(Σpi - 1)
            = Σpilog6pi - l1(p1 - 2p2) - l2(Σpi - 1)
    dL/dp1  = log6 + logp1 + loge - l1 - l2 = 0
    logp1   = l1 + l2 - ln6 - loge
    p1  = 2^(l1 + l2 - log6 - loge) 
        = 2^(l1 + l2')
    dL/dp2  = log6 + logp2 + loge + 2l1 - l2 = 0
    p2  = 2^(-2l1 + l2')
    dL/dpi  = log6 + logpi + loge - l2 = 0 for i in {3 .. 6}
    pi  = 2^l2'
    p1 / p2 = 2^3l1 = 2, l1 = 1/3
    let 2^l2' = k, 4k + k(2^1/3 + 2^-2/3) = 1, k = 0.170
    p1 = 0.214
    p2 = 0.107
    p3 = .. = p6 = 0.170
    D(P||Q) = 0.0282
    logPr{E}    -> -0.0282n
b.  by conditional limit theorem, X1|X^n ∈ E ~ {pi} calculated above

11.16
a.  D(p0||p1)   = Σ2^-x * log(2^-x / qp^(x - 1))
                = Σ2^-x * (-x - logq - (x - 1)logp)
                = -2 - logq - 2logp + logp
                = -2 - logp - logq
b.  by equation 11.240, the test is {x: P1(x) / P2(x) > 1}

11.17
a.  Eθ0[lθ(X^n)]    = int(-∞, +∞, lnΠfθ(xi) * Πfθ0(xi)dx^n)
                    = int(-∞, +∞, Σlnfθ(xi) * Πfθ0(xi)dx^n)
                    = Σint(-∞, +∞, lnfθ(xi) * Πfθ0(xi)dx^n)
                    = Σint(-∞, +∞, lnfθ(xi) * fθ0(xi)dxi)
                    = n * int(-∞, +∞, fθ0(x)ln(fθ(x) / fθ0(x) * fθ0(x))dx)
                    = n(-D(fθ0||fθ) - H(fθ0))
b.  -D(fθ0||fθ) is maximized by θ = θ0

11.18
a.  1/n * ΣXi = ΣxPx(x)
    D(Q||P) = Σq(x)log(q(x) / p^x-1(1 - p))
            = -H(Q) - (E[X] - 1)logp - log(1 - p), where logp and log(1 - p) < 0
    subject to E[X] >= α, -h(Q) is minimized by exponential distribution Exp(1/α)
    but X is discrete so the minimizing distribution may be the discrete counterpart of Exp(1/α) which is Geo(1/α)
    L(q, l) = Σq(x)log(q(x)/p(x)) - l1(Σq(x) - 1) - l2(Σxq(x) - α)
    dL/dq   = logq(x) - logp(x) + 1/ln2 - l1 - l2x = 0
    logq(x) = logp(x) + l1 + l2x + 1/ln2
    q(x)    = p(x) * 2^(l1 + l2x + 1/ln2)
            = p(x) * 2^(l1' + l2x)
            = p^(x - 1) * (1 - p) * c1 * c2^x
            = (pc2)^(x - 1) * (1 - p)c1c2
    is a geometric distribution subject to E[X] >= α, Q = Geometric(1/α)
    q(x)    = (1 - 1/α)^(x - 1) * 1/α
    D(Q||P) = -H(Q) - (E[X] - 1)logp - log(1 - p)
            = -αH(1/α) - (α - 1)logp - log(1 - p)
            = -1/n * lim{logPr{avg(X) >= α}}
b.  by conditional limit theorem,
    Pr{X1 = k|1/n * ΣXi >= α}   -> Q(k)
                                = (1 - 1/α)^(k - 1) * 1/α
c.  Pr{1/n * ΣXi >= 4}  = 2^-nD(Q||P)
                        = 2^-0.755n to first order
    Pr{X1 = k|1/n * ΣXi >= 4}   = (3/4)^(k - 1) * 1/4

11.19
// how is it possible to do integration by parts when the functions are derivatives of θ while the integration is of x?
// tracing another proof given by Theory of Point Estimation, Lehmann & Casella, 1998
-E[d^2lnfθ(x)/dθ^2] = -E[-(dfθ(x)/dθ / fθ(x))^2 + d^2fθ(x)/dθ^2 / fθ(x)]
                    = E[(dfθ(x)/dθ / fθ(x))^2] - d/dθ int(-∞, +∞, dfθ(x)/dθ dx)
                    = J(θ) - 0 = J(θ)

11.20
ln(n!)  = ln(Π{2 <= i <= n}i)
        = Σ{2 <= i <= n - 1}ln(i) + ln(n)
        <= int(2, n - 1, ln(x)dx) + ln(n)   // approximation of integrals by sum, ln(x) is non-decreasing
        = (n - 1)ln(n - 1) - n + 3 - ln4 + ln(n)
when ln(n - 1) >= 3 - ln4, n >= 7,
        <= nln(n - 1) - n + ln(n)
        <= nln(n) - n + ln(n)
n!  <= e^(nln(n) - n + ln(n))
    = n(n/e)^n
ln(n!)  = Σ{1 <= i <= n}ln(i)
        >= int(0, n, ln(x)dx)
        = nln(n) - n
n!  >= e^(nln(n) - n)
    = (n/e)^n

11.21
1/n * logC(n, np)   <= 1/n * log(n(n/e)^n / ((np/e)^np) * ((n - np)/e)^(n - np))
                    = 1/n * log(n^n+1 / (np^np * (n - np)^(n - np)))
                    = 1/n * ((n + 1)logn - np(logn + logp) - (n - np)(logn - log(1 - p)))
                    = 1/n * (logn - n(plogp + (1 - p)log(1 - p)))
                    -> H(p)
1/n * logC(n, np)   >= 1/n * log((n/e)^n / (np * (np/e)^np * (n - np) * ((n - np)/e)^(n - np)))
                    = 1/n * log(n^n / (np^(np + 1) * (n - np)^(n - np + 1)))
                    = 1/n * (nlogn - (np + 1)(logn + logp) - (n - np + 1)(logn + log(1 - p))
                    = 1/n * (-2logn - n((p + 1/n)logp + (1 - p + 1/n)log(1 - p)))
                    -> H(p)
therefore 1/n * logC(n, np) -> H(p)
similarly 1/n * logC(n, np1, .., npn) -> H(p)

11.22
E   = {(x, y): Σxi - Σyi >= nt}
    = {Pxy: ΣxPx(x) - ΣyPy(y) >= t}
by Sanov's theorem, 1/n * logPr{E} -> -D(R*(x, y)||P(x)Q(y)), where R* ∈ E minimizes D(R||PQ)
L(rxy, l)   = Σrxylog(rxy / P(x)Q(y)) - l1(Σrxy - 1) - l2(Σ(x - y)rxy - t)
dL/rxy  = logrxy - logP(x)Q(y) + loge - l1 - l2(x - y) = 0
logrxy  = logP(x)Q(y) + l1' + l2(x - y)
rxy     = P(x)Q(y)2^(l1' + l2(x - y)) for some l1' and l2 satisfying the constraints
D(R||PQ)    = ΣP(x)Q(y)log2^(l1' + l2(x - y))
            = l1' + l2(E[X] - E[Y])
            = l1' + l2t
where l1' = 1/ΣP(x)Q(y)e^l2(x - y)

11.23
α = Q^n(1/n * log(P(X) / Q(X)) > 0) = Q^n(E) is the probability of error of the first kind of (log) likelihood test
by Sanov's theorem, α = 2^-nD(Q*||P) to the first order, Q* is the closest distribution to P in ~E
by equation 11.200, Q* is of the form:
    Q*(x)   = (P(x)^k * Q(x)^(1 - k)) / Σ(P(x)^k * Q(x)^(1 - k))
            = c(P(x)^k * Q(x)^(1 - k)) for some c and k
D(Q*||P)    = Σc(P^kQ^(1 - k))log(cP^kQ^(1 - k) / P)
            = Σc(P^kQ^(1 - k))(logc + (k - 1)logP + (1 - k)logQ)
            = logc + Σc(P^kQ^(1 - k))(k - 1)(logP - logQ)
            = logc + (k - 1)EP*[logP - logQ]
if constraints are active, EP*[log(P/Q)] = 0,
D(Q*||P)    = logc

11.24
a.  Fθ(x)   = ΣPr{Z = i}Pr{X <= x|Z = i}
            = Pr{Z = 0}F0(x) + Pr{Z = 1}F1(x)
            = (1 - θ)F0(x) + θF1(x)
    fθ(x)   = dFθ(x)/dx
            = (1 - θ)f0(x) + θf1(x)
b.  J(θ)    = Eθ[(dfθ(x)/fθ(x))^2]
            = int(-∞, +∞, (-f0(x) + f1(x))^2 / ((1 - θ)f0(x) + θf1(x)) dx)
c.  var(T)  >= 1/J(θ)
d.  // thanks solutions manual and On the Estimation of Mixing Distributions, D. C. Boes, 1966
    // https://projecteuclid.org/download/pdf_1/euclid.aoms/1177699607
    for any set B ⊆ R,
    Pθ(B)   = (1 - θ)P0(B) + θP1(B)
            = P0(B) + θ(P1(B) - P0(B))
    θ   = (Pθ(B) - P0(B)) / (P1(B) - P0(B))
    hence take any B = (-∞, y], let I(y) = Pθ{X <= y}, E[I(y)] = int(-∞, y, fθ(x)dx) = Fθ(y), then
        T = (I(y) - F0(y)) / (F1(y) - F0(y))
    is an unbiased estimator of θ

11.25
let E   = {x: 1/n * ΣXi >= α}
        = {Px: ΣaPx(a) >= α}
Pr{X1 = k|E} = P*(k) where P* minimizes D(P||Q), Q = Binomial(m, q), P ∈ E
L(p, l) = D(P||Q) - l1(Σp(x) - 1) - l2(xp(x) - α)
        = Σp(x)log(p(x) / q(x)) - l1(Σp(x) - 1) - l2(xp(x) - α)
by problem 11.18,
p(x)    = q(x) * c1 * c2^x
        = C(m, x)q^x(1 - q)^(m - x) * c1 * c2^x
as p(x) <= 1, c2 <= 1,
let c2 = p(1 - q) / q(1 - p) for some p <= 1 // always possible by continuity
p(x)    = C(m, x)p^x(1 - p)^(m - x) * c1
by binomial formula, c1 = 1,
p(x)    = C(m, x)p^x(1 - p)^(m - x) for some p, p(x) = Binomial(m, p)
when constraints are active, EP[X] = α = np, p = α/n

11.26
a.  Pr{E}   = C(n, n/4)(2/3)^(n/4) * (1/3)^(3n/4)
b.  by conditional limit theorem, {X1 = k|E} -> P*(k), P* ∈ E minimizes D(P||Q)
    by equation 11.150,
    P*(x)   = 1/3 * e^(λx^2) / Σ(1/3 * e^(λx^2)) for some λ that Σx^2P*(x) = 1/2
    P*(x)   = k/(2k + 1) if x = ±1
            = 1/(2k + 1) if x = 0
    Σx^2P*(x) = 2k/(2k + 1) = 1/2, k = 1/2
    P*  = (1/4, 1/2, 1/4)
    Pr{X1 = ±1|E} = 1/2

11.27
L(Q, l) = EQ[logX] - D(Q||P) - l(ΣQ(x) - 1)
        = ΣQ(x)logx - ΣQ(x)log(Q(x)/P(x)) - l(ΣQ(x) - 1)
dL/dQ   = logx - logQ(x) + logP(x) - loge - l = 0
logQ(x) = logx - logP(x) - loge - l
Q(x)    = cxP(x), since ΣQ(x) = cEP[X] = 1, c = 1/EP[X],
        = xP(x)/EP[X]
EQ[logX] - D(Q||P)  = ΣxP(x)/EP[X] * logx - ΣxP(x)/EP[X] * log(x/EP[X])
                    = (ΣxP(x)(logx - logx + logEP[X])) / EP[X]
                    = EP[X]logEP[X] / EP[X]
                    = logEP[X]

11.28
a.  1/n * ΣXi^2 - (1/n * ΣXi)^2 <= α
    1/n * Σa^2N(a|x^n) - (1/n * ΣaN(a|x^n))^2 <= α
    Σa^2Px(a) - (ΣaPx(a))^2 <= α
b.  let E = {Px: Σa^2Px(a) - (ΣaPx(a))^2 <= α} 
    1/n * logQ^n(E) -> -D(P||Q) where P minimizes D(P||Q) subject to P ∈ E
    L(P, l) = ΣP(x)log(P(x)/Q(x)) - l1(ΣP(x) - 1) - l2(Σx^2P(x) - (ΣxP(x))^2 - α)
    dL/dP   = logP(x) - logQ(x) - loge - l1 - l2(x^2 - Σx^2P(x)) = 0
    logP(x) = logQ(x) + l1' + l2(x^2 - EP[X^2])
    P(x)    = c1Q(x)2^(x^2 - EP[X^2])
    where c1 = 1/ΣQ(x)2^(x^2 - EP[X^2])
    D(P||Q) = ΣP(x)log(c1 * 2^(x^2 - EP[X^2]))
            = ΣP(x)(logc1 + x^2 - EP[X^2])
            = logc1 + EP[X^2] - EP[X^2]
            = logc1

11.29
a.  when n = 2, the simplex is {(x, 1 - x): 0 <= x <= 1}
    Pr{x =< c} = c for any 0 <= c <= 1
    when X = Y1 / (Y1 + Y2),
    Pr{X <= c}  = Pr{Y1/(Y1 + Y2) <= c}
                = Pr{(1 - c)Y1 <= cY2}
                = Pr{Y1 <= cY2/(1 - c)}
                = int(0, 1, int(0, cy2/(1 - c), dy1)dy2) for c/(1 - c) <= 1
                = int(0, 1, cy2/(1 - c)dy2)
                = c/2(1 - c)
    when c = 1/4, Pr{X <= c} = 1/6 != 1/4, this sampling is not uniform
b.  // skipped
    
// again have no idea what section 12.4 is talking about
12.1
f(x) = e^(λ0 + λ1x + λ2lnx) = c1 * x^c2 * e^c3x
is the Gamma distribution

12.2
a.  L(P, l) = ΣP(x)log(P(x)/Q(x)) - l0(ΣP(x) - 1) - Σli(ΣP(x)gi(x) - αi)
    dL/dP   = logP(x) - logQ(x) + loge - l0 - Σligi(x) = 0
    P(x)    = Q(x)2^(l0' + Σligi(x))
            = Q(x)e^(c0 + Σcigi(x))
b.  D(P||Q) = ΣP(x)log(P(x)/Q(x))
            = ΣP(x)log(P(x)/P*(x) * P*(x)/Q(x))
            = D(P||P*) + ΣP(x)log(P*(x)/Q(x))
            >= ΣP(x)(c0 + Σcigi(x))
            = ΣP*(x)(c0 + Σcigi(x))
            = D(P*||Q)

12.3
a.  by Burg's theorem, Xi ~ N(0, 1)
b.  by Burg's theorem, Xi = aXi-1 + Zi, Zi ~ N(0, N)
    E[XiXi+1]   = E[aXi^2 + XiZi+1]
                = aE[Xi^2] + E[Xi]E[Zi+1]
                = a = 1/2
    E[Xi^2] = E[(aXi-1 + Zi)^2]
            = a^2E[Xi-1^2] + E[Zi^2]
            = a^2 + N = 1
    N   = 1 - 1/4 = 3/4
c.  // what is maximum entropy spectrum?
    a.  independent Gaussian ~ N(0, N), H(X) = 1/2 * log2πe
    b.  multivariate Gaussian, 
        H(X)    = lim{H(Xn|X^n-1)}
                = H(Xi+1|Xi)
                = H(Zi+1)
                = 1/2 * log(3πe/2)

12.4
H(X, Y) = H(X) + H(Y) - I(X;Y), where H(X) and H(Y) are fixed by marginal distribution and I(X;Y) >= 0 where equality
is achieved when X and Y are independent
therefore p(x, y) = p(x)p(y) maximizes the entropy

12.5
for any joint distribution of X^n,
h(X^n)  = Σh(Xi|X^i-1)
        <= h(X1) + Σh(Xi|Xi-1)
and h(Xi|Xi-1) is determined by the marginal fXi-1,Xi, Σh(Xi|Xi-1) is the entropy of a first order Markov chain with
time varying transform 
since fXi-1,Xi are marginals, it must have int(-∞, +∞, fXi-1,Xi dxi-1) = int(-∞, +∞, fXi,Xi+1 dxi+1) = fXi
let X1 ~ fX1 = int(-∞, +∞, fx1x2 dx2), Xi|Xi-1 ~ fXi|Xi-1 = fXi,Xi-1 / fXi-1
X^n ~ fX1 * ΠXi|Xi-1  = fX1 * Π(fXi,Xi-1 / fXi-1)
the upper bound is achievable with the first order time varying Markov chain defined as above

12.6
g(x)    = e^(λ0 + λ1lnf0(x))
        = c1 * f0^λ, c1 = 1/int(-∞, +∞, f0^λdx)
int(-∞, +∞, g(x)lnf0(x)dx)  = c1 * int(-∞, +∞, f0^λlnf0dx) = α0
let α0 = -he(f0), the constraint is satisfied by λ = 1, g(x) = f0(x)

12.7
// thanks solutions manual
let the bias of Xn' be bX = E[Xn'] - E[Xn]
E[(Xn - Xn')^2] = E[((Xn - E[Xn]) - (Xn' - E[Xn] - bX) - bX)^2]
                = E[(Xn - E[Xn])^2 + (Xn' - E[Xn'])^2 + bX^2 
                - 2(Xn - E[Xn])(Xn' - E[Xn']) - 2bX(Xn - E[Xn]) + 2bx(Xn' - E[Xn'])]
where 
    E[(Xn - E[Xn)^2] = Var(Xn)
    E[(Xn' - E[Xn'])^2] = E[(Σ(biXn-i - E[biXn-i]))^2] is determined by bi, mean and covariance
    E[(Xn - E[Xn])(Xn' - E[Xn'])]   = E[(Xn - E[Xn])Σ(bi(Xn-i - E[Xn-i])]
                                    = ΣbiE[(Xn - E[Xn])(Xn-i - E[Xn-i])]
    is determined by mean and covariance
thus E[(Xn - Xn')^2] is determined only by the mean vector and covariance matrix
assume without loss of generality that f(x^n) is multivariate Gaussian, the optimal MSE estimator of Xn
    T(x^n-1) = E[Xn|X^n-1 = x^n-1]
is unbiased and linear in X^n-1 
// a proof can be found here: http://www.math.chalmers.se/~rootzen/highdimensional/SSP4SE-appA.pdf
let σ^2 = E[(Xn - Xn')^2] = Var(Xn|X^n-1), since Xn|X^n-1 is also normal, 
h(Xn|X^n-1) = 1/2 * log2πeσ^2
hence the maximizing f(x^n) must maximize h(Xn|X^n-1)

12.8
a.  f(x) = e^(λ0 + λ1cos(u0x)) subject to constraints
b.  f(x) = e^(λ0 + λ1sin(u0x)) subject to constraints
c.  Φ(u0)   = int(0, a, f(x) * e^iu0x dx)
            = int(0, a, f(x) * (cos(u0x) + isin(u0x)) dx)
            = int(0, a, f(x) * cos(u0x) dx) + i * int(0, a, f(x) * sin(u0x) dx)
            = α1 + iα2
    f(x) = e^(λ0 + λ1cos(u0x) + λ2sin(u0x))
d.  since f(x) and cos(u0x) is periodical, the integral constraint is also periodical, the limit at  a -> ∞ may not be 
    well defined

12.9
a.  H(X^n)  = ΣH(Xi+1|X^i-1)
            = H(X1) + ΣH(Xi+1|X^i-1)
            <= H(X1) + ΣH(Xi+1|Xi)
            = H(X1) + Σ(H(1/3) + 1/3 * H(Xi+1|Xi+1 = Xi) + 2/3 * H(Xi+1|Xi+1 != Xi))
            = H(X1) + nH(1/3)
    so the entropy of X^n is maximized by a Markov process that
    X1 ~ Bernoulli(1/2), Xi+1|Xi ~ p(xi+1|xi) where
        p(xi+1|xi)  = 1/3 if xi+1 = xi
                    = 2/3 if xi+1 != xi
b.  lim{H(Xn|Xn-1)} = H(1/3)

12.10
a.  Var(Y) = Var(X1) + Var(X2) <= P1 + P2
    with variance constraint h(Y) is maximized by Y ~ N(0, P1 + P2), X1 ~ N(0, P1), X2 ~ N(0, P2)
    this limit is achievable with X1 ~ N(0, P1) and X2 ~ N(0, P2) 
b.  Var(Y)  = Var(X1) + Var(X2) + 2Covar(X1, X2)
            <= P1 + P2 + 2Covar(X1, X2)
            <= P1 + P2 + 2sqrt(P1 + P2) // Cauchy-Schwarz inequality
    and the upper bound can be achieved by X2 = sqrt(P2/P1) * X1
    h(Y) is maximized by Y ~ N(0, P1 + P2 + 2sqrt(P1 + P2)), which is achievable by X1 ~ N(0, P1)
c.  ??

12.11
a.  H(X^n)  = ΣH(Xi|X^i-1)
            <= H(X1) + ΣH(Xi|Xi-1)
            <= nlog3
    where the upper bound can be achieved by Xi ~ iid U({1, 2, 3})
b.  let {Xi} = Y1, Z1, Y2, Z2, ..
    I(X2n+2;X2n) = I(Yn+1;Yn) = α = H(Yn+1) - H(Yn+1|Yn)
    H(Yn+1|Yn) is maximized when H(Yn+1) = log3
    constraint on marginals Xn+1, Xn ~ U({1, 2, 3}), H(Xn+1|Xn) = log3 - α can be achieved by let P(Xn+1|Xn = 1) have
    H(Xn+1|Xn = 1) = log3 - α (possible by continuity of entropy and 0 <= α <= log3) and let P(Xn+1|Xn = 2) and 
    P(Xn+1|Xn = 3) be its permutations
    let Z^n be independent to Y^n with the same distribution and transition
    H(X^2n) = H(Y^n) + H(Z^n)
    lim{1/2n * h(X^2n)} = lim{1/2n * (h(Y^n) + h(Z^n))}
                        = lim{h(Yn|Y^n-1)}/2 + lim{h(Zn|Z^n-1)}/2
                        = h(Yn|Yn-1)
                        = log3 - α

12.12
a.  E[E[Xn+1|X^n]] = E[Xn+1], the estimator is unbiased
    EX^n[EXn+1[(Xn+1 - Xn+1')^2|X^n]]
    = EX^n+1[(Xn+1 - Xn+1')^2]
    = Var(Xn+1|X^n) = σ^2, where
    h(Xn+1|X^n) <= 1/2 * log2πeσ^2
    log2πeσ^2   >= 2h(Xn+1|X^n)
    2πeσ^2      >= e^2h(Xn+1|X^n)
    σ^2         >= e^2h(Xn+1|X^n) / 2πe
b.  if X^n follows a multivariate Gaussian distribution, the conditional distribution is also Gaussian so yes

12.13
by problem 12.5, there's always a first order time varying Markov chain of the same marginals with no lower entropy
H(X^n)  = H(X1) + ΣH(Xi|Xi-1)
        <= H(X1) + (n - 1) * max{H(Xi|Xi-1), Pr{Xi = 0|Xi-1 = 0} = 0}
where max{H(X)} = max{H(Xi|Xi-1), Pr{Xi = 0|Xi-1 = 0} = 0} is calculated in problem 4.7.d

12.14
a.  f(x) = e^(λ0 + λ1x^8 + λ2x^16)
b.  f(x) = e^(λ0 + λ1(x^8 + x^16))
c.  the feasible set of part a is a subset of the feasible set of part b, maximum entropy of part b is no lower

12.15
f(x) = e^(λ0 + λ1e^-x), where
λ0 = 1/int(-∞, +∞, e^(λ1e^-x)dx)
int(-∞, +∞, e^(λ0 + λ1e^-x) * e^-x dx) = α

12.16
by Burg's theorem, the entropy rate is maximized by a first order Gauss-Markov process where 
    (Xn, Xn+1) ~ N(0, K), K = [1, 1/2; 1/2, 1]
H(X)    = lim{h(Xn|X^n-1)}
        = h(X2|X1)
        = h(X1, X2) - h(X1)
        = 1/2 * log(2πe)^2|K| - 1/2 * log2πe
        = 1/2 * log(3πe/4)

12.17
a.  by problem 12.13, the entropy can be maximized by a first order Markov chain subject to the same constraints
    the stationary joint distribution is [p, q; q, r] subject to p + r - 2q = 1/2 and p + 2q + r = 1
    4q = 1/2, q = 1/8
    the maximum H(X2|X1) = H(X1, X2) - H(X1) is achieved by p = r = 3/8
b.  H(X) = H(X2|X1) = H(1/4)
c.  when Xi ~ iid Bernoulli(p), 
    EX1X2 = (1 - p)^2 + p^2 - 2p(1 - p) = 1/2
    4p^2 - 4p + 1 = 1/2
    (2p - 1)^2 = 1/2
    2p - 1 = sqrt(2) / 2
    p = 1/2 + sqrt(2) / 4 > 3/4

12.18
f(z, v) = e^(λ0 + λ1(1/2 * m(vx^2 + vy^2 + vz^2) + mgz)
that is, the joint distribution of 3 independent N(0, -1/λ1m) and 1 independent Exp(-λ1mg), hence
E[1/2 * m(Vx^2 + Vy^2 + Vz^2)] = 1/2 * m * 3 * -1/λ1m = -3/2 * 1/λ1
E[mgZ] = mg * -1/λ1mg = -1/λ1
E[1/2 * m||V||^2] = 3/2 * E[mgZ] = 3/5 * E0
E[mgZ] = 2/5 * E0

12.19
same to problem 12.9

12.20
same to problem 12.10

12.21
a.  by problem 12.11, let X^2n = Y1, Z1, Y2, Z2, ..
    Y1 is constrained by EY1^2 = 1, EY1Y2 = α
    by Burg's theorem, h(Y^n) is maximized by a first order Gauss-Markov process with K = [1, α; α, 1] as well as Z^n
    the initial distribution won't affect the entropy rate and can be arbitrary
    h(X^2n) = h(Y^n) + h(Z^n|Y^n)
            <= h(Y^n) + h(Z^n)
    where equality is achieved only if Y^n and Z^n are independent and h(Y^n) and h(Z^n) can be maximized separately by
    the Gauss-Markov process above
b.  by problem 12.11, 
    lim{1/n * h(X^n)}   = h(Yn|Yn-1)
                        = h(Yn-1, Yn) - h(Yn-1)
                        = 1/2 * log(2πe)^2|K| - 1/2 * log2πe
                        = 1/2 * log2πe(1 - α^2)
c.  since Y^n and Z^n are independent, EXiXi-1 = 0 for all i

12.22
a.  given EX = α, h(X) following the constraints is maximized by Exp(1/α), h(X) = loge + logα
    hence logα >= h - loge, α > (2^h)/e
b.  let Y = X - a, Y is supported by [0, +∞), h(Y) = h(X), Y is constrained by E[Y] = α - a
    maximum of h(X) = h(Y) = loge + log(α - a) where Y ~ Exp(1/(α - a))
    log(α - a) >= h - loge
    α - a >= (2^h)/e
    α >= (2^h)/e + a

13.1
a.  [1 - α, α, 0; 0, α, 1 - α] is a erasure channel
    D* = C = 1 - α
b.  C is achieved by uniform distribution of i
    p(v) = ((1 - α)/2, α, (1 - α)/2)
c.  same as proved in the text

13.2
a.  the channel [Pa; Pb; Pc] is symmetric, D* = C = log3 - log(Pa) = 0.428
    P = (1/3, 1/3, 1/3) since symmetric channel capacity is maximized by uniform output
    li = log3
b.  same to part a

13.3
by stationarity, 3/4 * p0 + 1/4 * p1 = p0, p0 = p1 = 1/2
F(X^1)  = Pr{X1 = 0} * 1 = 1/2 = 0.1
F(X^2)  = F(X^1) + p(10) * 0 = 1/2 = 0.1
F(X^3)  = F(X^2) + p(100) = 1/2 + 3/32 = 1/2 + 1/16 + 1/32 = 0.10011
F(X^4)  = F(X^3) = 0.10011
F(X^5)  = F(X^4) + p(10100) = 1/2 + 3/32 + 3/256 = 0.10011011
F(X^∞)  <= F(X^3||1)
        = F(1011)
        = F(X^3) + p(1010)
        = 0.10011 + 2^-7
        = 0.100111
therefore F1F2F3 = 100

13.4
a.  by stationarity, p0 = p1 = 1/2
    F(01110)    = p(00) + p(010) + p(0110)
                = 1/6 + 2/9 + 2/27
                = 25/54
                = 0.01110110...
b.  F(01110||X) <= F(01111)    
                = F(01110) + p(01110)
                = 25/54 + 2/81
                = 79/162
                = 0.01111100...
    the first 4 bits are known to be 0111

13.5
0|00|000|1|10|101|0000|01|1010|1
(0, 0), (1, 0), (2, 0), (0, 1), (4, 0), (5, 1), (3, 0), (1, 1), (6, 0), (4, None)

13.6
1|11|111|1111|1..
(1 + c - 1)(c - 1) / 2 <= n
(c - 1)^2 / 2 <= n
c <= sqrt(2n) + 1
c/n <= sqrt(2n) / n + 1/n -> 0 as n -> ∞

13.7
a.  Rn-1 can be described by logRn-1 + 2loglogRn-1 + O(1) bits
    Xn can be described by ceil(log|X|) <= log|X| + 1 bits
    with a constant length separator, (Rn-1, Xn) can be described by logRn-1 + 2loglogRn-1 + log|X| + O(1) bits in total
b.  1.  add log|X| / n -> 0
    2.  Rn is also a valid Rn-1, so Rn-1 <= Rn

13.8
a.  |P| < log|W| + 1
    |L| < log|M| + 1
b.  log|W| + log|M| + 2 < 8L
    L > (log|W| + log|M| + 2)/8
c.  |P| = 12, |L| = 8, L > 20/8 or L >= 3

13.9
a.  0,00,001,000,0001,1,01,011,1
b.  all distinct elements of X^1, then X^2, then X^3 ..
c.  the fixed sequence in problem 13.6
    for a phrase vi, if max{|vj|, 1 <= j <= i - 1} = k, then |vi| <= k + 1
    for a given c(n), n is bounded by 
        n   = Σ|vi|
            <= Σ(max{|vj|, 1 <= j < i}) + 1
            <= Σi
            = (c^2 + c)/2
    the sequence 1111111.. achieves this upper bound

13.10
a.  since X is iid, the entropy rate is the entropy a single symbol H(X)
    by definition of typical set, 
        |-1/l * logp(x^l) - H(X)| < δ
        p(x^l) >= 2^-l(H + δ)
    E[Rl(X^l)|X^l = x^l]    = 1/p(x^l)  // equation 13.73
                            <= 2^l(H + δ)
b.  take any δ < ε,
    Pr{Rl(X^l) > 2^l(H + ε)}    = ΣPr{Rl(X^l) > 2^l(H + ε)|X^l = x^l}p(x^l)
                                = Σ{Aδ^l}Pr{Rl(X^l) > 2^l(H + ε)|X^l = x^l}p(x^l)
                                + Σ{~Aδ^l}Pr{Rl(X^l) > 2^l(H + ε)|X^l = x^l}p(x^l)
                                <= Σ{Aδ^l}E[Rl(X^l)|X^l = x^l]/2^l(H + ε) * p(x^l) + Pr{~Aδ^l}
                                <= Σ2^l(δ - ε) * p(x^l) + Pr{~Aδ^l}
                                <= 2^l(δ - ε) + Pr{~Aδ^l}
                                -> 0
c.  D1:
        by definition of δ-typical set,
        |Aδ^l| <= 2^l(H + δ), |D1| = l|Aδ^l| <= l2^l(H + δ)
        describe Rl with fixed number of bits, 
        ||Rl|| = ceil(log|D1|) < logl + l(H + δ) + 1
        ||Rl||/l = H + δ + o(1) -> H + δ for any δ > 0
        E[L(X^l)]/l = Σ{Aδ^l}(O(1) + ||Rl||)p(x^l)/l + Σ{~Aδ^l}(llogA + O(1))/l
                    = (O(1) + ||Rl||)/l * Pr{Aδ^l} + (logA + o(1))Pr{~Aδ^l}
                    -> H + δ
    D2:
        ||Rl|| = ceil(log|D2|) < l(H + δ) + 1, ||Rl||/l < H + δ + o(1)
        E[L(X^l)]/l = Pr{Rl(X^l) <= 2^l(H + δ)} * (O(1) + ||Rl||)/l + Pr{Rl(X^l) > 2^l(H + δ)} * (O(1) + llogA)/l
                    -> H + δ

13.11
a.  // thanks solutions manual
    let L be the prefix of X^n in AD, then 
    nH(X)   = H(X^n)
            = H(X^L, L, X^{L+1,n})
            = H(X^L) + H(L|X^L) + H(X^{L+1,n}|L, X^L)
    since L is fixed given X^L, X^L and X^{L+1,n} are independent given L, 
    H(X^{L+1,n}|L)  = ΣPr{L = l}H(X^{L+1,n}|L = l)
                    = ΣPr{L = l}(n - l)H(X)
                    = (n - EL)H(X)
                    = nH(X) - H(X^L)
    H(X^L)  = E[L]H(X)
    since X^L ∈ AD, H(X^L) <= log|AD|
    log|AD| >= E[L]H(X)
    log|AD|/E[L] >= H(X)
b.  // skipped, the proof is quite complicated

14.1
let px and py be two programs of x and y that K(x) = l(px) and K(y) = l(py)
let some p be a simulator program of U itself that after halting the first time, initializes all the states, and run
the rest of the tape as another program, let l(p) = c
p||px||py is a program for (x, y), hence K(x, y) <= K(x) + K(y) + c

14.2
a.  proved in the text
b.  problem 14.1 and an additional subroutine that adds the two numbers on the working tape
c.  for some incompressible x^n, 10^n - x^n must also be incompressible (otherwise K(x^n) = K(10^n - x^n) + c for some 
    program that computes x^n from 10^n - x^n)
    but the sum of the two number has K(10^n) = K(n) + c ≒ K(log(x^n)) + c

14.3
a.  a horizontal line is defined by the line number 1 <= k <= n
    K(x|n) = K(k|n) + c = logn + c' in average and lower if k is special
b.  a box is defined by two points (x1, y1), (x2, y2)
    K(x|n) = 4logn + c in average
c.  K(x|n) = 2logn + c

14.4
a program to output a string of length n:
    print the next n bits exactly: ...
have to describe the n-bit string and the number n, l(p) <= logn + 2loglogn + n + c
the first logn + 2loglogn + c bits in the program must be fixed, other n bits can be arbitrary
let I be a random variable that I = 1 if the program P is of the form described above, I = 0 otherwise
let N be the number of arbitrary bits
I and N is determined by the first logn + loglogn + c bits of the program
H(X)    >= H(X|I, N)
        >= ΣPr{I = 1, N = n}H(X|I = 1, N = n)
        = 2^-(logn + loglogn + c)H(X|I = 1, N = n)  // N is a function of X
        = Σ1/cn(logn)^2 * n
        = Σ1/c(logn)^2 -> ∞

14.5
Pu(x) ~ 2^-K(x)
a.  a program that prints 0 forever
    Pu(x) = 2^-c for some c
b.  a program that computes π to infinite precision
    same to part a
c.  K(x) = K(n) + c <= logn + 2loglogn + c
    Pu(x) ~ 1/cn(logn)^2
d.  K(x) = K(w^n) + c >= n - c
    Pu(x) ~ 2^(n - c)
e.  2^-c

14.6
a.  theorem 14.2.2 is independent of arity
b.  for a ternary computer, there are < 3^k programs of length < k, each of them either doesn't halt or produces a fixed
    output x

14.7
let k0, k1, k2 be the number of 0, 1 and 2 in the string of length n
by the same method in example 14.2.8, K(x^n|n) <= c + 2log3(n) + log3(C(n, k0, k1, k2))
by Stirling's approximation,
    log3(C(n, k0, k1, k2))  ≒ log3((n/e)^n / ((k0/e)^k0 * (k1/e)^k1 * (k2/e)^k2)))
                            = log3((n/k0)^k0 * (n/k1)^k1 * (n/k2)^k2)
                            = -Σ(ki * log3(ki/n))
so K(x^n|n)/n   <= c/n + 2log3(n)/n - Σ(ki/n * log3(ki/n))
                = c/n + 2log3(n)/n + H3(k0/n, k1/n, k2/n)
                -> H3(θ)
where θ is the empirical distribution of x^n
since K(x^n|n)/n >= 1, lim{H3(θ)} >= 1
but H3(θ) <= log3(|X|) = log3(3) = 1, so H3(θ) -> 1, θ ~ U({0, 1, 2}) as n -> ∞

14.8
a.  K(~A|n) <= K(A|n) + c
    given a program of A and n, ~A can be computed from A with an additional, constant-length procedure
b.  K(A ∪ B|n) <= K(A|n) + K(b|n) + c
    a simulator running two programs back to back + a program computing union of two sets on the working tape has 
    constant length
c.  K(A ∩ B|n) <= K(A|n) + K(b|n) + c
    similar to part b

14.9
the simplest program that outputs n bits of 1/sqrt(2) for sufficiently large n is a program that outputs 1/sqrt(2) to
infinite precision, p0 = 2^-K(p) = c for some constant c that is a sum over all program that computes 1/sqrt(2)
if the next bit is not the corresponding bit of 1/sqrt(2), the output is a description of n (i.e. describes n by output
n bits of 1/sqrt(2) followed with a wrong bit), hence K(x) >= K(n) ~ 2^-(logn + o(logn)) ~ 1/n
therefore it's the same to equation 14.86, p(1|1^n) = cn/(cn + 1)

14.10
a.  K(x|m) <= m^2/2 + c
    and K(x|m) ~ m^2/2 + c for most patterns
b.  the cell can be described by its location on the image
    K(x|m) <= m^2/2 + 2logm + c

14.11
a.  each rectangle is 4 degrees of freedom, K(x|n) <= 8logn + c
b.  two degrees of freedom vanished, K(x|n) <= 6logm + c
c.  given one rectangle, the other rectangle must be the same shape at different location (2 dof)
    K(x|n) <= 6logm + c
d.  given one rectangle, the other rectangle have 3 dof (the last is defined by the area)
    K(x|n) <= 7logm + c
e.  the entire grid, K(u|n) ~ c
f.  an incompressible string of length n^2, K(x|n) ~ n^2 + c

14.12
a.  some constant c, the encryption map has length 27
b.  K(y^n) <= K(x^n) + c = n/4 + c
    a program can compute the encrypted text from the encryption map and the plain text
c.  there are 27! possible encryption maps, K(m) ~ log27!
    the data must have a Kolmogorov complexity at least K(m)

14.13
the lowest Kolmogorov complexity of a number can be a constant
if a number have the bit pattern of an incompressible string, K(n) >= n
K(n + k) - K(n) >= n + k - c

14.14
M(n) can be almost unbounded (e.g. Graham's number)
2^n + c0 >= S(n) >= 2^n - c1 since K(2^n) ~ log2^n + o(log2^n) ~ n and there's incompressible number of 2^n+1 bits
therefore S(n) << M(n)
both K(M(n)) and K(S(n)) ~ n since a shorter program can be extended to a longer program by no-ops

