Lecture notes will be taken from a video playlist of 2020 MIT 6.824 lectures, which are public, has
good audo quality and nicely segmented:
    https://www.youtube.com/watch?v=cQP8WApzIQQ&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB
but the skeleton code for labs in the 2020 repository doesn't work with the latest version of Golang
(1.19), the 2021 repository is used instead:
    git clone git://g.csail.mit.edu/6.824-golabs-2021 6.824

Lab code is located at
    ./MIT6.824-2020/labs
in the same structure of the 2021 repo, however only modified code are committed.
    
Golang notes
References:
    a tour of go: https://go.dev/tour/welcome/1
    std doc: https://pkg.go.dev/std
    effective go: https://go.dev/doc/effective_go

The official go std documentation cannot be searched alone: the result is always polluted by 3rd
party packages and cannot be filtered for std only. Use devdocs.io instead:
    https://devdocs.io/go/

By default every type in Golang is byte-copyable, certain types that cannot be safely cloned must be
immediately hidden behind a reference e.g. sync.Mutex 

Zero-sized unit type in Golang is struct{}, the only value of it is struct{}{}.

Because `map[K]V` is magic, its documentation is not included in the documentation of Golang std. It
supports two operations:
    insert: map[key] = value
    get:    value, exists = map[key]

Only bare function calls can be deferred, channel insert, variable assignment and other statements
cannot be deferred. A possible solution is to wrap the statements in an immediate anonymous
function, javascript IIFE style:
    defer func() { c <- true }()

Golang integer literal in place of time.Duration is implicitly converted to nanoseconds, which means
    time.Sleep(1000)
put the thread to sleep for 1000 nanoseconds = 1 microsecond

Unintuitive for loop semantics:
    https://github.com/golang/go/discussions/56010

Golang doesn't have sum type nor proper enums: enums in Golang are new type constants sharing
namespace with any other constants in the same package, the obvious drawback of this approach is
that Golang compiler does not and cannot check if a value is valid for an enum. The following code
will not trigger any compiler error:
    type Bool int
    const (
        False   Bool = 0
        True    Bool = 1
    )
    var b Bool = 100
for the same reason Golang compiler cannot tell if a switch on an enum is exhaustive or not by
design.

Golang `range` iterator doesn not iterate on references: the value of the slice are copied to the
loop variable on start of each iteration, modification to the loop variable is not reflected to the
slice. 

Wrapper types (e.g. bufio.Writer) doesn't have a method exposing the underlying raw type, references
to the raw type have to be kept somewhere if clean up is necessary.

When passing a struct as RPC argument / reply, every single recursive field must be public
(Capitalized), otherwise ___the field will not be passed but zero-initialized at callee___
    type Args struct { V int }
will be passed normally, but
    type Args struct { v int }
means v == 0

Official Golang documentation does not list any implementors of an interface.

The global random number generator in Golang is, actually, global (across threads), it shouldn't be
used in a performance-critical multi-threaded program.

Lecture 1. Introduction
Topics of distributed systems:
    - Scalability: 
        increase in the number of nodes (computers) in the system must be translated to increase in
        performance fairly linearly, not constrained by any bottlenecks
    - Fail tolerance:
        - Availability: 
            the system should keep operating under certain kind of failures on a small number of
            nodes
        - Recoverability: 
            the system should be able to recover from a failure without loss of correctness
    - Consistency:
        different flavors of consistency models, strongest consistency model (linearizability) is
        intentionally avoided because of costs

MapReduce: see also notes for Designing Data-Intensive Applications
---NOTE START---
The computing environment at Google can be characterized as:
    - commodity machines: Linux, 2-4 GB memory dual core x86 processor
    - commodity network: 100MB or 1GB interface, even less bisection bandwidth
    - hundreds to thousands of machines in a cluster, failures are common
    - inexpensive IDE hard drives, running distributed and replicated file system (GFS), on the same
        worker machine 
    - jobs are submitted to then mapped by a single scheduler 

Master tracks the state of each pair of Map and Reduce task:
    enum State {
        Idle,
        InProgress(WorkerId),
        Completed(WorkerId),
    }
for completed Map tasks the Master additionally stores the location and size of intermediate file
produced by a Map worker. The information is later propagated to in-progress reduce tasks.

Master pings workers periodically and deems them failed if no response for a certain period.
Completed Reduce task on failed workers doesn't have to be re-executed, however Map task on failed
workers are rescheduled even if it's completed because Reduce worker has to later access
intermediate data on the failed machine. Master is responsible for broadcasting the new Map worker 
to all relevant Reduce workers.

State of the Master is periodically backed up to the NV storage, failed master can restart from a
checkpoint or simply abort all ongoing MapReduce tasks.

When the tasks are not deterministic, the final output may not be consistent. Consider the following
sequence of events:
    1.  A single Map worker M1 produces two intermediate value R1 and R2 
    2.  Reduce worker 1 executes on R1, produces e(R1)
    3.  M1 fails, rescheduled and re-executed on M2, M2 produces R1' and R2'
    4.  Reduce worker 2 executes on R2', produces e(R2')
if Map task is not deterministic, there may be inconsistency among e(R1) and e(R2'). Otherwise
consistency is largely ensured by atomicity of operations on GFS.

The main bottleneck of the MapReduce system in Google was their network: Map function has to fetch
data from the underlying file system, Reduce function requires all value associated with the same
key that have to be collected from all Map workers. Capacity is only 50MB/s per machine on the top
level switch. The single leader in the system had to schedule works closed to the files they are
operating on.

Master spawns more Map and Reduce tasks (denoted by M and R) than available workers the same way
multi-thread algorithms spawn more threads than available parallelism to achieve load balance. In
practice a reasonable configuration has M = 200_000, R = 5000 on 2000 worker machines.

Master duplicates still in-progress tasks to different workers at the very end of a MapReduce
operation as a means to migrate from staggering workers, reportedly it improves performance of some
workload by 44%.

Refinements on the basic MapReduce framework:
    - User-defined partitioning function:
        user may control how output of Reduce are batched into files based on intermediate key
    - Ordering guarantees:
        Google implementation of MapReduces guarantees orders of key among a partition
    - Combiner function:
        execute after Map on Map worker, combine intermediate (key, value) pairs in similar fashion
        to the Reduce function, aimed to reduce network load
    - Custom input and output types:
        define how data is read from file or how data is written to output
    - Side effects:
        side product of Map / Reduce function to file, no consistency guarantees
    - Skipping bad records:
        exception handler on workers send the error raising record id to Master, if the same record
        id is encountered twice Master re-schedule the same task with the record skipped
    - Status information:
        Master report states through an internal HTTP server
    - Counters:
        named counter objects on workers propagated to Master in Ping response, a very limited form
        of logging
---NOTE END---

Lecture 2. RPC and threads
./MIT6.824-2020/questions/web_crawler.go

Reason to use threads:
    - IO Concurrency:
        allows the OS to better utilize CPU when part of the program is blocking on IO events (file
        read / write, mouse / keyboard input, etc.), may be solved otherwise by event-driven design
    - parallelism:
        threads can be assigned to and executed on different CPU cores
    - convenience:
        there's concepts that are most naturally expressed as threads, e.g. periodical jobs and
        watchdogs 

Most non-trivial instructions in x86 are not atomic, even seemingly simple instruction like INC is
translated to several micro codes hence vulnerable to race condition. Shared data has to be
explicitly atomic or protected by locks.

Reasons it's not always a good idea for data structures to manage their own locks thus hiding it
from users:
    1.  it's meaningless overhead when the data structure is not shared
    2.  when multiple data structures are interlinked, it's possible that the only way to solve
        potential deadlock is to lift their locks to a higher scope 
    
Synchronization primitives in Golang:
    - sync.Mutex:
        every day mutex, copying a sync.Mutex is unsafe after first use, most of the case should
        immediately be put behind a reference and never dereferenced
    - channels:
        by default has a capacity of 0, sending thread is blocked immediately, can be constructed
        with arbitrary capacity: make(chan T, CAPACITY)
    - sync.Cond:
        ordinary conditional variable with ordinary API, slightly generalized to wait on both mutex
        and RW lock
    - sync.WaitGroup:
        barrier implemented in Golang, API allows arbitrary delta to the counter
    
The Golang for loop semantic mentioned at 57:10 is about to be fixed:
    https://github.com/golang/go/discussions/56010
in some future version (maybe Go 2),
    vs := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
    rs := make([]*int, 0, 10)
    for _, v := range vs {
        rs = append(rs, &v)
    }
    for _, r := range rs {
        fmt.Println(*r)
    }
will print 1 to 10.

RPC (Remote Procedure Call) is only mentioned in 2021 lecture video:
    http://nil.csail.mit.edu/6.824/2021/schedule.html
Sequence of events of an RPC:
    1.  client calls the RPC, pass arguments to client-side stub function
    2.  client-side stub function marshalls (serializes) arguments to a message, type information
        along only if the message format is self describing (protobuf, the message format of gRPC,
        is not self-describing for example)
    3.  message passed to server, unmarshalled by server-side stub function
    4.  server-side stub function calls the actual target function 
    5.  server-side stub function marshalls the return value to a message
    6.  message passed to client, client-side stub function unmarshalls the message to values
    7.  client-side stub function returns value to client
most of the steps (stubs, communication protocol, message format) are handled by code auto-generated
from a schema.

RPC semantics under failures:
    - at least once:
        retry until the operations is verifiably performed at least once, only appropriate for
        idempotent operations e.g. insert into a map
    - at most once:
        duplicate requests are filtered based on something (unique tags)? in the message or a lower
        level (TCP?), server may still receive zero request, the RPC model of Golang
    - exactly once:
        expensive, rarely deployed
     
Server-side RPC framework must also serve as a local name server to distribute RPC calls to
corresponding stubs on the same server node. For a bigger RPC system in which calls may land on
different server nodes a dedicated name server is required.

Golang registers RPC services by runtime reflection magic:
    rpc.Server.Register registers method of a struct as RPC service iff the method:
        - is exported method of exported type
        - has two arguments, both of exported type
        - the second argument is a pointer
        - one return value, of type error
    the service name will be
        fmt.Sprintf("%s.%s", struct_type_name, struct_method_name)
    https://pkg.go.dev/net/rpc@go1.19.3#Server.Register

// why is arguments of put in kv.go hard coded to { "subject", "6.824" }?

// 2021 code example on synchronization primitives, not mentioned anywhere in 2020 lecture 2
vote-count-1.go
    data race and spin wait

vote-count-2.go
    data race on `count` and `finished` prevented by a mutex
    still spin wait

vote-count-3.go
    spin wait on a lower resolution by unconditional time.Sleep

vote-count-4.go
    wait on conditional variable, finish condition only checked on worker termination. Unlike in
    other languages, Golang conditional variables do not suffer from spontaneous wake up
    https://pkg.go.dev/sync#Cond.Wait

vote-count-5.go
    no shared data, votes are passed back to master thread through a channel, channel function as
    both a synchronized data structure and a barrier 

vote-count-6.go
    patches a major bug in 5: collect all votes no matter the result, otherwise worker threads would
    block forever on 0 capacity channel
    now think about it, the channel should have a capacity of 10 so the worker threads never block

Lecture 3. GFS
paper: The Google File System, see notes for Designing Data-Intensive Applications
Question: Describe a sequence of events that would result in a client reading stale data from the
Google File System.
    1.  a chunk server C fails 
    2.  master detects the failure, re-replicate all its chunks on other chunk servers
    3.  a mutation happens on chunks originally replicated on C
    4.  C back online
    5.  a client with chunk C in its cache accesses staled data

Bad replication design: multiple servers, clients push request to all of them
    no global order of events, no consistency

On chunk server outage, Master cannot designate another primary for a chunk before its lease has
expired due to the risk of "spit brain", i.e. two primaries handling client requests in
un-synchronized fashion.

Lecture 4. Primary-Backup Replication
paper: The Design of a Practical System for Fault-Tolerant Virtual Machines
---NOTE START---
Two approaches to keep a server and its backup synchronized:
    1.  replicate state of CPU, memory and IO devices, unrealistic because of the requirement of
        high bandwidth
    2.  start primary and backup at the same initial state then replicate deterministic execution,
        unrealistic on physical servers because of non-identical devices, much more manageable on
        VMs
This paper describes an implementation of the second approach (deterministic replay) on x86: VMWare
vSphere Fault Tolerance (FT). FT supports single core only, how to apply state transfer on
multi-core machines is an open question yet to be answered.

FT only attempts to deal with fail-stop failures, i.e. failures are detected before externally
visible incorrect actions

Primary and backup share nothing but a network storage, connected by a VM -> VM logging channel,
only primary is visible through network or to the users. Executions are replicated to the backup but
the outputs are dropped.

Challenges:
    1.  correctly capturing all the input and non-deterministic info necessary to ensure
        deterministic execution, x86 is non-deterministic (rdtsc, rdrand)
    2.  correctly apply captured input on the backup
    3.  minimal performance impact all the while

Output requirement, the fundamental requirement of FT:
    "if the backup VM ever takes over after a failure of the primary, the backup VM will continue
    executing in a way that is entirely consistent with all outputs that the primary VM has sent to
    the external world."

To ensure that, output is delayed until the backup has acknowledged receiving logs of all inputs
prior to and including the output operation. On failover (backup VM take over) the backup will
replay its logs up to the output operation then go live. Note that only the output but not the
execution of primary is delayed.

FT does not guarantee exactly-once output, application code should handle missing or duplicated
packets.

Failure detection:
    - UDP heartbeat between servers
    - VM monitoring log channel (should have steady stream of traffic because of timer interrupt)
    - atomic test-and-set (a lock) on shared storage to make sure no split brain

Steps to start an FT backup VM
    1.  copy the running primary VM, < 1 second pause with VMWare VMotion
    2.  choose a server connected to the same shared storage, again outsourced to some internal
        tools at VMWare
    3.  setup FT and log channel 

Cycle of an input event:
    1.  primary hypervisor receives input event 
    2.  primary hypervisor passes input event to primary VM, simultaneously pushes a log entry to
        the log buffer
    2.  primary hypervisor flushes log buffer to log channel 
    3.  backup hypervisor buffers log entries from log channel
    4.  backup hypervisor sends acknowledgement back to primary hypervisor
    5.  (maybe simultaneously with 4) backup hypervisor recreates the input event from the log
        entry, passes it to backup VM

Primary VM is paused when its log buffer is full, backup VM is paused when its log buffer is empty.
To avoid exceeding execution lag and, thereby, long failover lag, primary hypervisor monitors
execution lag as part of FT protocol and proceeds to throttle CPU allocation of primary VM.

Most VM operations, i.e. VM power off, CPU reallocation, and other operations beyond the scope of
guest OS, is logged as control entry and replicated by backup hypervisor. VMotion of backup VM
additionally requires cooperation from primary.

implementation issues for disk IOs:
    - concurrent writes are non-deterministic: 
        must be linearized
    - DMA on memory-mapped device visible to guest OS is data race: 
        write on hypervisor managed "bounce buffer" invisible to guest OS, copy to DMA buffer later
    - failed disk IOs are inconsistent: 
        IOs free from concurrency and data race is idempotent

implementation issued for network IOs:
    - asynchronous network operations are non-deterministic:
        asynchronous network operations disabled, always trap to hypervisor
    - synchronous network IO is slow and laggy:
        - batch outgoing / incoming packets
        - eliminate context switches by attaching log acknowledgement handling to TCP stack as
            callback
        - hypervisor flush log entries more frequently

FT with non-shared storage
    pros: 
        - disk IOs are no longer outputs, do not have to be delayed per Output Rule
        - primary and backup may be physically distanced
    cons:
        - extra expensive disk sync steps on start up
        - IO failures creates inconsistency
        - extra facility to deal with split brain 

backup VM read from disk:
    pros:
        - disk input from disk, much less traffic on log channel
    cons:
        - slower execution, disk read vs. network buffer read
        - read result must be transmitted whenever either primary or backup fails
        - primary write on the same location must be delayed

FT performance highlights:
    - CPU impact < 10%
    - log channel bandwidth < 20Mb/s for non-network applications
    - heavy impact on network bandwidth for network applications
---NOTE END---

question: How does VM FT handle network partitions?
    atomic test-and-set a flag on shared storage, only one server may go live

Primary-backup replication: deals with independent fail-stop faults on a single machine. 
Examples of correlated faults:
    - whole data center power shortage
    - whole data center overheat
    - earthquake
Examples of fail-stop faults: 
    - power shortage
    - network shortage
    - CPU overheat
    - kernel panic
    - (when coded to) network package error caught by checksum
    - (when coded to) disk error caught by checksum
Examples of not fail-stop faults 
    - logical software bugs
    - memory corruption
    - malicious intruders

Two replication schemes:
    1.  state transfer
        send the whole state (memory, disk, ..), or at least the delta, to backups
    2.  replicated state machine
        start from the same initial state, send only external inputs to backups

Characteristics of replication scheme:
    - what is the state to be replicated
    - how close is the primary and backups synced
    - how cut-off (failover) is performed
    - what's the visible anomaly during a cut-off
    - how new backups are set up

FT log entry contains:
    - instruction number (since backup start up) to insert interrupt
    - type and data of event
    - (if the operation is non-deterministic) result of the operation
FT requires CPU hardware support to insert interrupt on specific instruction number

Obviously FT backup VM executes up to the instruction number in the next log entry and blocks on an
empty log buffer, or it will overrun an input event yet to arrive.

Lecture 5. Go, Threads, and Raft
paper: The Go Memory Model
---NOTE START---
Worth reading:
    - Foundations of the C++ Concurrency Memory Model
        https://www.hpl.hp.com/techreports/2008/HPL-2008-56.pdf
    - How to Make a Correct Multiprocess Program Execute Correctly on a Multiprocessor
        https://lamport.azurewebsites.net/pubs/lamport-how-to-make.pdf

A data race is defined as a write to a memory location and a read / write to that same location
happens concurrently, unless all accesses are atomic data accesses.

A memory operation is defined by four criteria:
    1.  kind, including
        - ordinary read
        - ordinary write 
        - synchronizing operation (atomic data access, mutex operation, channel operation, etc.)
    2.  location in the program
    3.  memory location or variable being accessed
    4.  values read / written by the operation
A memory operation may be read-like, write-like, or both.

A Golang program execution is modeled as a set of goroutine executions together with a mapping 
    W: ReadLike -> WriteLike
specifying the write-like operation that each read-like operation reads from. The execution (mainly
the mapping W) may not be the same each time for the same program.

"sequenced before" is a partial order defined on:
    Seq ⊆ OrdinaryWrite x OrdinaryRead
    // inherent to the structure of the program
"synchronized before" is a partial order similarly defined on read- / write-like synchronization
operations:
    Sync ⊆ SyncWriteLike x SyncReadLike
    ∀(w, r) ∈ Sync, W(r) = w
"happens before" relation is the transitive closure of the union of "sequenced before" and
"synchronized before":
    H ⊆ WriteLike x ReadLike
    H = (Seq ∪ Sync)^+

Sequential consistency is defined as (L. Lamport, "How to make a multiprocessor computer that
correctly executes multiprocess programs", 1979):
    H is a total order on memory operations for all possible executions

Correctness (data race free) requirements:
    1.  "memory operations in each goroutine must correspond to a correct sequential execution of
        that goroutine."
        notably order of evaluation in struct literals is undefined:
            m := map[int]{1: 1, 1: 2}
        may be {1: 1} or {1: 2}
    2.  "For a given program execution, the mapping W, when limited to synchronizing operations,
        must be explainable by some implicit total order of the synchronizing operations that is
        consistent with sequencing and the values read and written by those operations."
    3.  "For an ordinary (non-synchronizing) data read r on a memory location x, W(r) must be a
        write w that is visible to r, where visible means that both of the following hold:"
            - w happens before r
            - w does not happen before any other w' to the same location that happens before r

A read-write data race on memory location x consists of a read-like operation r and a write-like
operation w on x unordered by happened before:
    (w, r) ∉ H

A write-write data race on memory location x consists of two write-like operations on x unordered by
happened before:
    (w, w') ∉ H & (w', w) ∉ H

A Golang program free of data races is sequentially consistent. Word / sub-word memory access in
Golang is always atomic: a correct implementation of Golang should abort on data race or observe a
value actually written to the same location on single / sub word memory access that satisfies
correctness requirement 3. Races on multi-word data structures leave undefined value.

Synchronized before relations:
// denote 
//  "synchronized before" as <S
//  "happens before" as <H
    - Initialization
        If package p imports package q, completion of p's init functions <H the start of of any of
        q's. 
        The completion of all init functions <S the start of main.main.
    - Goroutine creation
        The Golang statement that starts a new goroutine <S the start of the goroutine. Destruction
        of a goroutine has no order guarantee.
    - Channel communication
        A send on a channel <S the completion of the corresponding receive.
        The closing of a channel <S a receive that returns a zero value because of the close .
        The kth receive on a channel with capacity C <S the completion of k+Cth send.
    - Locks
        For both l sync.Mutex and sync.RWMutex, n < m, nth call to l.Unlock() <S mth call to return
        from l.Lock().
        For any call to l.RLock() on sync.RWMutex, ∃n that
            nth call to l.Unlock() <S return from l.RLock()
            matching call to l.RUnlock() <S return from n+1th call to l.Lock()
        A successful call to l.TryLock() or l.TryRLock() is equivalent to l.Lock() or l.RLock(), an
        unsuccessful call has no ordering.
    - Once
        completion of f() from once.Do(f) <S return from any call to once.Do(f)
    - Atomic values
        If effect of an atomic operation A is observed by another atomic operation B, A <S B. The
        only atomic memory order ordering in Golang is SeqCst.
    - Finalizers
        SetFinalizer(x, f) <S the finalization call f(x).
    - Cond
        call to c.Broadcast() <S any return from c.Wait() it unblocks, no spontaneous awake in
        Golang.
---NOTE END---

question:
---CODE START---
var a string
var done bool

type Unit struct{}

func setup(done chan Unit) {
	a = "hello, world"          // 1
	done <- Unit{}              // 2
}

func main() {
	done := make(chan Unit)     // 3
	go setup(done)              // 4
	<-done                      // 5
	print(a)                    // 6
}
---CODE END---
By order of evaluation:
    (1, 2), (3, 4), (4, 5), (5, 6) ∈ H
By synchronized before relations:
    (4, 1),     // goroutine creation
    (2, 5),     // channel communication
    ∈ H
H is a total order:
    3 <H 4 <H 1 <H 2 <H 5 <H 6
the program is sequentially consistent.

// first half of the lecture is about golang sync primitives, nothing remarkable
// second half of the lecture is about obvious bugs in Raft implementation, nothing remarkable

Lecture 6. Fault Tolerance: Raft (1)
paper: In Search of an Understandable Consensus Algorithm (Extended Version), up to Section 5
---NOTE START---
Worth reading:
    - [1] The Raft Consensus Algorithm
        https://raft.github.io/
        interactive Raft system demonstration
    - [2] Consensus: Bridging Theory and Practice
        https://github.com/ongardie/dissertation/blob/master/stanford.pdf?raw=true
        PhD dissertation of the same author, much more detailed than the Raft paper
    - [3] Students' Guide to Raft
        https://thesquareplanet.com/blog/students-guide-to-raft/
        notes from one of the TA of MIT6.824 2016, pitfalls on implementing Raft in Golang

Raft consistency and performance guarantees:
    1.  Safety: never return incorrect result under non-Byzantine conditions, including network
        delays / partitions and packet loss / duplication / reordering.
    2.  Functional: assuming fail-stop, as long as a simple majority of the replicas are operational
        the system is available.
    3.  Consistency doesn't rely on accurate system clocks
    4.  A minority of slow servers will not affect response time

Raft protocol invariants:
    1.  Election safety: 
            at most one leader can be elected in a given term
    2.  Leader append-only: 
            a leader never overwrites or deletes entries in its log
    3.  Log matching: 
            if two logs contain an entry with the same index and term, the all log entries up to the
            index are identical
    4.  Leader completeness:
            if a log entry is committed in a given term, then that entry will be present in the logs
            of the leaders at the same index of all higher-numbered terms
    5.  State machine safety:
            if a server applied a log entry at an index to its state machine, no other server will
            ever apply a different log entry at the same index

A Raft server is in one of three states:
    - Leader, handles all client requests
    - Follower, passively respond to leaders and candidates
    - Candidate, entered after Follower timeout, starts election

Raft server divides time into terms. One or no leader is elected each term. Term increases
monotonically as a logical clock.

A server with currentTerm smaller than the term it received in an RPC call updates currentTerm to
the larget one. A leader received a larger term in an RPC call reverts to Follower state. A leader
received a smaller term in a request rejects the request.

Raft servers communicate with each other using RPCs, timed out RPCs are retried (consistent to
Golang RPC model), RPCs should be issued in parallel.

Raft election process:
    0.  Follower is informed the presence of a Leader by periodic heartbeats (empty AppendEntries RPC)
    1.  A timed out Follower turns into a Candidate, increments currentTerm
    2.  Candidate vote for itself, issues RequestVotes RPC to other servers in the cluster until one
        of the three happens:
        1.  it wins the election (simple majority of the responses voted for it) 
            => become the new Leader
        2.  another server is established as leader (heartbeat with a term >= currentTerm)
            => revert to Follower
        3.  neither 1 or 2 happened until timeout
            => increment currentTerm, send RequestVotes again 
        timeout is randomized between 150 - 300ms, typically a lucky candidate will timeout early
        and win the election 
    3.  The newly elected leader immediately sends the initial AppendEntries heartbeat to all other
        servers in the cluster

Heartbeat interval of the Leader is not mentioned in the paper, naturally it should be much smaller
than the minimal election timeout so occasional packet loss / delay will not start an unnecessary
election, at the same time not too small to the point of visibly consuming the network bandwidth.
50ms (1/3 of minimum election timeout) is used in the interactive Raft demonstration [1].

An log entry is committed after been applied to the state on Leader. A log entry is committed once
the Leader that created the entry in the current term has replicated it on a majority of the servers
(i.e. received successful AppendEntries from more than half peers). A Follower applies a log entry
on its state after it's committed (informed by heartbeat). 

Leader maintains a nextIndex value for each Follower, initialized to 1 + last log index. After
failed AppendEntries, leader decrements nextIndex for that Follower then retry AppendEntries.
Eventually (prevLogIndex, prevLogTerm) in AppendEntries will match that on Follower, inconsistent
log entries after prevLogIndex will be purged. 

Raft new command process:
    1.  client send command to Leader
    2.  Leader appends (command, term) to its log, send AppendEntries to Followers
        retried until nextIndex points to empty or first inconsistent log entry on Follower, then
        new entries are appended to log on Follower
    3.  After receiving success from a majority of peers (including Leader itself), Leader updates
        commitIndex, apply new command to its state then update lastApplied
    4.  Followers will apply the command to their states the same way after been informed of
        commitIndex on Leader by heartbeat

As stated in invariant 2, a Leader never overwrites or deletes its log entry, such events happens
only after a new Leader is elected in a new term.

To prevent an out of date newly elected Leader overwriting committed log entries, RequestVotes RPC
includes (lastLogTerm, lastLogIndex) from the Candidate. Followers reject Candidates less up-to-date
than itself. Up-to-date-ness of (Term, Index) is defined as lexicographical order of the tuple:
    (T0, I0) is more up-to-date than (T1, I1) iff:
        T0 > T1, or
        T0 == T1 and I0 > I1

It's not safe to commit a last log entry when it has a term smaller than currentTerm, such log
entries must be indirectly committed by a future command in the currentTerm.

Proof of Leader Completeness Property (invariant 3) by contradiction:
    Assume for a term T, U is the smallest term U > T that an entry E committed by Leader[T] of term T
    is not stored by Leader[U] of term U
    By invariant 2, the entry must be absent since the election of term U
    By definition of commit, Leader[T] replicated the entry on a majority of peers
    By process of election, Leader[U] received vote from a majority of peers
    These two majorities must overlap, there's a peer P replicated the entry and voted for Leader[U]
    P stored the entry when voting for Leader[U] since:
        ∀T' that T <= T' < U, Leader[T'] stores the entry by definition of U
        Followers only remove entries if they are inconsistent with the Leader
    Leader[U] must be at least as up-to-date as P during election of term U 
    if Leader[U] and P has the same lastLogTerm:
        let lastLogTerm be T', T <= T' < U
        by how AppendEntries RPC is defined (Log Matching Property), both log of P and Leader[U]
        must be a prefix of Leader[T'] at the end of term T'
        lastLogIndex of Leader[U] should be at least the same as P
        log of P is a prefix of Leader[U], Leader[U] contains E 
    if Leader[U] has greater lastLogTerm than P:
        let T' be lastLogTerm of Leader[U], T <= T' < U
        Leader[T'] during its term contained E
        by Log Matching Property, Leader[U] also contains E
    both cases contradicts with the assumption, such U doesn't exist,
    ∀U>T, log of Leader[T] is a prefix of log of Leader[U] 

Proof of State Machine Safety (invariant 5):
    Assume a peer P applies an entry E to its state machine
    By how AppendEntries works, logs of P and current Leader are identical up through E
    Per protocol, E must be committed
    By Leader Completeness Property, the log of P up through E is a prefix of logs of all future
    leaders
---NOTE END---

Raft implementation details:
    - Never do anything time consuming while holding the lock. Release the lock and reacquire it
        later during RPC calls.
    - Most change in peer states, like a role shift, should cancel all timeouts issued before it,
        but the deciding vote handler should cancel only other vote handlers, one approach to this
        is a hierarchy of cancellable contexts.

Wrong replicated system:
    - clients send request to / expect acknowledgement from all servers 
        exponentially lower availability
    - clients send request to reachable servers
        different server may be reachable from different clients (network partition), split brain,
        causes inconsistency

Solution: odd number of servers, majority vote required to make any progress
    - at most one partition may function normally 
    - system can withstand less than half node failures
    - any two majorities overlap at at least one server

Raft doesn't manage state, the application code built on top of it replicates commands in Raft but
states are managed by the application itself.

Raft interface exposed to application code:
    - func Start(Command) (Index, Term)
        replicate the command in the raft cluster, return index and term of the command in leader
    - func ApplyCh <-ApplyMsg{Command, Index}
        a channel sending out committed commands and its index in the Raft cluster 

One possible network failure that halts the (most naive) Raft system to a full stop: Leader can
send out heartbeats but cannot receive any message, no new election will occur
