Lecture notes will be taken from a video playlist of 2020 MIT 6.824 lectures, which are public, has
good audio quality and nicely segmented:
    https://www.youtube.com/watch?v=cQP8WApzIQQ&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB
but the skeleton code for labs in the 2020 repository doesn't work with the latest version of Golang
(1.19), the 2021 repository is used instead:
    git clone git://g.csail.mit.edu/6.824-golabs-2021 6.824

Lab code is located at
    ./MIT6.824-2020/labs
in the same structure of the 2021 repo, however only modified files are committed.
    
Golang notes
References:
    a tour of go: https://go.dev/tour/welcome/1
    std doc: https://pkg.go.dev/std
    effective go: https://go.dev/doc/effective_go

The official go std documentation cannot be searched alone: the result is always polluted by 3rd
party packages and cannot be filtered for std only. Use devdocs.io instead:
    https://devdocs.io/go/

By default every type in Golang is byte-copyable, certain types that cannot be safely cloned must be
immediately hidden behind a reference e.g. sync.Mutex 

Zero-sized unit type in Golang is struct{}, the only value of it is struct{}{}.

Because `map[K]V` is magic, its documentation is not included in the documentation of Golang std. It
supports two operations:
    insert: map[key] = value
    get:    value, exists = map[key]

Only bare function calls can be deferred, channel insert, variable assignment and other statements
cannot be deferred. A possible solution is to wrap the statements in an immediate anonymous
function, javascript IIFE style:
    defer func() { c <- true }()

Golang integer literal in place of time.Duration is implicitly converted to nanoseconds, which means
    time.Sleep(1000)
put the thread to sleep for 1000 nanoseconds = 1 microsecond

Unintuitive for loop semantics:
    https://github.com/golang/go/discussions/56010

Golang doesn't have sum type nor proper enums: enums in Golang are new type constants sharing
namespace with any other constants in the same package, the obvious drawback of this approach is
that Golang compiler does not and cannot check if a value is valid for an enum. The following code
will not trigger any compiler error:
    type Bool int
    const (
        False   Bool = 0
        True    Bool = 1
    )
    var b Bool = 100
for the same reason Golang compiler cannot tell if a switch on an enum is exhaustive or not by
design.

Golang `range` iterator doesn not iterate on references: the value of the slice are copied to the
loop variable on start of each iteration, modification to the loop variable is not reflected to the
slice. 

Wrapper types (e.g. bufio.Writer) doesn't have a method exposing the underlying raw type, references
to the raw type have to be kept somewhere if clean up is necessary.

When passing a struct as RPC argument / reply, every single recursive field must be public
(Capitalized), otherwise ___the field will not be passed but zero-initialized at callee___
    type Args struct { V int }
will be passed normally, but
    type Args struct { v int }
means v == 0

Official Golang documentation does not list any implementors of an interface.

The global random number generator in Golang is, actually, global (across threads), it shouldn't be
used in a performance-critical multi-threaded program.

Golang slices are views to arrays with subtle copy-on-append behavior: 
	a := [4]int{1, 2, 3, 4}
	b := a[:2]
	b = append(b, 5)                // overwrites a[2]
	fmt.Println("a=", a, "b=", b)   // a=[1 2 5 4] b=[1 2 5]
    c := a[2:]
    c = append(c, 6)                // exceeds the capacity of a, copy on append
    a[3] = 7
    fmt.Println("a=", a, "c=", c)   // a=[1 2 5 7] c=[5 4 6]
    fmt.Println("cap(c)=", cap(c))  // cap(c)=4, capacity grows to power of 2

Golang can correctly infer unreachable memory behind a slice, the program below doesn't OOM:
    s := make([]int, 0)
    for {
        s = append(s, 1)
        s = s[1:]
    }

Lecture 1. Introduction
Topics of distributed systems:
    - Scalability: 
        increase in the number of nodes (computers) in the system must be translated to increase in
        performance fairly linearly, not constrained by any bottlenecks
    - Fail tolerance:
        - Availability: 
            the system should keep operating under certain kind of failures on a small number of
            nodes
        - Recoverability: 
            the system should be able to recover from a failure without loss of correctness
    - Consistency:
        different flavors of consistency models, strongest consistency model (linearizability) is
        intentionally avoided because of costs

MapReduce: see also notes for Designing Data-Intensive Applications
---NOTE START---
The computing environment at Google can be characterized as:
    - commodity machines: Linux, 2-4 GB memory dual core x86 processor
    - commodity network: 100MB or 1GB interface, even less bisection bandwidth
    - hundreds to thousands of machines in a cluster, failures are common
    - inexpensive IDE hard drives, running distributed and replicated file system (GFS), on the same
        worker machine 
    - jobs are submitted to then mapped by a single scheduler 

Master tracks the state of each pair of Map and Reduce task:
    enum State {
        Idle,
        InProgress(WorkerId),
        Completed(WorkerId),
    }
for completed Map tasks the Master additionally stores the location and size of intermediate file
produced by a Map worker. The information is later propagated to in-progress reduce tasks.

Master pings workers periodically and deems them failed if no response for a certain period.
Completed Reduce task on failed workers doesn't have to be re-executed, however Map task on failed
workers are rescheduled even if it's completed because Reduce worker has to later access
intermediate data on the failed machine. Master is responsible for broadcasting the new Map worker 
to all relevant Reduce workers.

State of the Master is periodically backed up to the NV storage, failed master can restart from a
checkpoint or simply abort all ongoing MapReduce tasks.

When the tasks are not deterministic, the final output may not be consistent. Consider the following
sequence of events:
    1.  A single Map worker M1 produces two intermediate value R1 and R2 
    2.  Reduce worker 1 executes on R1, produces e(R1)
    3.  M1 fails, rescheduled and re-executed on M2, M2 produces R1' and R2'
    4.  Reduce worker 2 executes on R2', produces e(R2')
if Map task is not deterministic, there may be inconsistency among e(R1) and e(R2'). Otherwise
consistency is largely ensured by atomicity of operations on GFS.

The main bottleneck of the MapReduce system in Google was their network: Map function has to fetch
data from the underlying file system, Reduce function requires all value associated with the same
key that have to be collected from all Map workers. Capacity is only 50MB/s per machine on the top
level switch. The single leader in the system had to schedule works closed to the files they are
operating on.

Master spawns more Map and Reduce tasks (denoted by M and R) than available workers the same way
multi-thread algorithms spawn more threads than available parallelism to achieve load balance. In
practice a reasonable configuration has M = 200_000, R = 5000 on 2000 worker machines.

Master duplicates still in-progress tasks to different workers at the very end of a MapReduce
operation as a means to migrate from staggering workers, reportedly it improves performance of some
workload by 44%.

Refinements on the basic MapReduce framework:
    - User-defined partitioning function:
        user may control how output of Reduce are batched into files based on intermediate key
    - Ordering guarantees:
        Google implementation of MapReduces guarantees orders of key among a partition
    - Combiner function:
        execute after Map on Map worker, combine intermediate (key, value) pairs in similar fashion
        to the Reduce function, aimed to reduce network load
    - Custom input and output types:
        define how data is read from file or how data is written to output
    - Side effects:
        side product of Map / Reduce function to file, no consistency guarantees
    - Skipping bad records:
        exception handler on workers send the error raising record id to Master, if the same record
        id is encountered twice Master re-schedule the same task with the record skipped
    - Status information:
        Master report states through an internal HTTP server
    - Counters:
        named counter objects on workers propagated to Master in Ping response, a very limited form
        of logging
---NOTE END---

Lecture 2. RPC and threads
./MIT6.824-2020/questions/web_crawler.go

Reason to use threads:
    - IO Concurrency:
        allows the OS to better utilize CPU when part of the program is blocking on IO events (file
        read / write, mouse / keyboard input, etc.), may be solved otherwise by event-driven design
    - parallelism:
        threads can be assigned to and executed on different CPU cores
    - convenience:
        there's concepts that are most naturally expressed as threads, e.g. periodical jobs and
        watchdogs 

Most non-trivial instructions in x86 are not atomic, even seemingly simple instruction like INC is
translated to several micro codes hence vulnerable to race condition. Shared data has to be
explicitly atomic or protected by locks.

Reasons it's not always a good idea for data structures to manage their own locks thus hiding it
from users:
    1.  it's meaningless overhead when the data structure is not shared
    2.  when multiple data structures are interlinked, it's possible that the only way to solve
        potential deadlock is to lift their locks to a higher scope 
    
Synchronization primitives in Golang:
    - sync.Mutex:
        every day mutex, copying a sync.Mutex is unsafe after first use, most of the case should
        immediately be put behind a reference and never dereferenced
    - channels:
        by default has a capacity of 0, sending thread is blocked immediately, can be constructed
        with arbitrary capacity: make(chan T, CAPACITY)
    - sync.Cond:
        ordinary conditional variable with ordinary API, slightly generalized to wait on both mutex
        and RW lock
    - sync.WaitGroup:
        barrier implemented in Golang, API allows arbitrary delta to the counter
    
The Golang for loop semantic mentioned at 57:10 is about to be fixed:
    https://github.com/golang/go/discussions/56010
in some future version (maybe Go 2),
    vs := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
    rs := make([]*int, 0, 10)
    for _, v := range vs {
        rs = append(rs, &v)
    }
    for _, r := range rs {
        fmt.Println(*r)
    }
will print 1 to 10.

RPC (Remote Procedure Call) is only mentioned in 2021 lecture video:
    http://nil.csail.mit.edu/6.824/2021/schedule.html
Sequence of events of an RPC:
    1.  client calls the RPC, pass arguments to client-side stub function
    2.  client-side stub function marshalls (serializes) arguments to a message, type information
        along only if the message format is self describing (protobuf, the message format of gRPC,
        is not self-describing for example)
    3.  message passed to server, unmarshalled by server-side stub function
    4.  server-side stub function calls the actual target function 
    5.  server-side stub function marshalls the return value to a message
    6.  message passed to client, client-side stub function unmarshalls the message to values
    7.  client-side stub function returns value to client
most of the steps (stubs, communication protocol, message format) are handled by code auto-generated
from a schema.

RPC semantics under failures:
    - at least once:
        retry until the operations is verifiably performed at least once, only appropriate for
        idempotent operations e.g. insert into a map
    - at most once:
        duplicate requests are filtered based on something (unique tags)? in the message or a lower
        level (TCP?), server may still receive zero request, the RPC model of Golang
    - exactly once:
        expensive, rarely deployed
     
Server-side RPC framework must also serve as a local name server to distribute RPC calls to
corresponding stubs on the same server node. For a bigger RPC system in which calls may land on
different server nodes a dedicated name server is required.

Golang registers RPC services by runtime reflection magic:
    rpc.Server.Register registers method of a struct as RPC service iff the method:
        - is exported method of exported type
        - has two arguments, both of exported type
        - the second argument is a pointer
        - one return value, of type error
    the service name will be
        fmt.Sprintf("%s.%s", struct_type_name, struct_method_name)
    https://pkg.go.dev/net/rpc@go1.19.3#Server.Register

// why is arguments of put in kv.go hard coded to { "subject", "6.824" }?

// 2021 code example on synchronization primitives, not mentioned anywhere in 2020 lecture 2
vote-count-1.go
    data race and spin wait

vote-count-2.go
    data race on `count` and `finished` prevented by a mutex
    still spin wait

vote-count-3.go
    spin wait on a lower resolution by unconditional time.Sleep

vote-count-4.go
    wait on conditional variable, finish condition only checked on worker termination. Unlike in
    other languages, Golang conditional variables do not suffer from spontaneous wake up
    https://pkg.go.dev/sync#Cond.Wait

vote-count-5.go
    no shared data, votes are passed back to master thread through a channel, channel function as
    both a synchronized data structure and a barrier 

vote-count-6.go
    patches a major bug in 5: collect all votes no matter the result, otherwise worker threads would
    block forever on 0 capacity channel
    now think about it, the channel should have a capacity of 10 so the worker threads never block

Lecture 3. GFS
paper: The Google File System, see notes for Designing Data-Intensive Applications
Question: Describe a sequence of events that would result in a client reading stale data from the
Google File System.
    1.  a chunk server C fails 
    2.  master detects the failure, re-replicate all its chunks on other chunk servers
    3.  a mutation happens on chunks originally replicated on C
    4.  C back online
    5.  a client with chunk C in its cache accesses staled data

Bad replication design: multiple servers, clients push request to all of them
    no global order of events, no consistency

On chunk server outage, Master cannot designate another primary for a chunk before its lease has
expired due to the risk of "spit brain", i.e. two primaries handling client requests in
un-synchronized fashion.

Lecture 4. Primary-Backup Replication
paper: The Design of a Practical System for Fault-Tolerant Virtual Machines
---NOTE START---
Two approaches to keep a server and its backup synchronized:
    1.  replicate state of CPU, memory and IO devices, unrealistic because of the requirement of
        high bandwidth
    2.  start primary and backup at the same initial state then replicate deterministic execution,
        unrealistic on physical servers because of non-identical devices, much more manageable on
        VMs
This paper describes an implementation of the second approach (deterministic replay) on x86: VMWare
vSphere Fault Tolerance (FT). FT supports single core only, how to apply state transfer on
multi-core machines is an open question yet to be answered.

FT only attempts to deal with fail-stop failures, i.e. failures are detected before externally
visible incorrect actions

Primary and backup share nothing but a network storage, connected by a VM -> VM logging channel,
only primary is visible through network or to the users. Executions are replicated to the backup but
the outputs are dropped.

Challenges:
    1.  correctly capturing all the input and non-deterministic info necessary to ensure
        deterministic execution, x86 is non-deterministic (rdtsc, rdrand)
    2.  correctly apply captured input on the backup
    3.  minimal performance impact all the while

Output requirement, the fundamental requirement of FT:
    "if the backup VM ever takes over after a failure of the primary, the backup VM will continue
    executing in a way that is entirely consistent with all outputs that the primary VM has sent to
    the external world."

To ensure that, output is delayed until the backup has acknowledged receiving logs of all inputs
prior to and including the output operation. On failover (backup VM take over) the backup will
replay its logs up to the output operation then go live. Note that only the output but not the
execution of primary is delayed.

FT does not guarantee exactly-once output, application code should handle missing or duplicated
packets.

Failure detection:
    - UDP heartbeat between servers
    - VM monitoring log channel (should have steady stream of traffic because of timer interrupt)
    - atomic test-and-set (a lock) on shared storage to make sure no split brain

Steps to start an FT backup VM
    1.  copy the running primary VM, < 1 second pause with VMWare VMotion
    2.  choose a server connected to the same shared storage, again outsourced to some internal
        tools at VMWare
    3.  setup FT and log channel 

Cycle of an input event:
    1.  primary hypervisor receives input event 
    2.  primary hypervisor passes input event to primary VM, simultaneously pushes a log entry to
        the log buffer
    2.  primary hypervisor flushes log buffer to log channel 
    3.  backup hypervisor buffers log entries from log channel
    4.  backup hypervisor sends acknowledgement back to primary hypervisor
    5.  (maybe simultaneously with 4) backup hypervisor recreates the input event from the log
        entry, passes it to backup VM

Primary VM is paused when its log buffer is full, backup VM is paused when its log buffer is empty.
To avoid exceeding execution lag and, thereby, long failover lag, primary hypervisor monitors
execution lag as part of FT protocol and proceeds to throttle CPU allocation of primary VM.

Most VM operations, i.e. VM power off, CPU reallocation, and other operations beyond the scope of
guest OS, is logged as control entry and replicated by backup hypervisor. VMotion of backup VM
additionally requires cooperation from primary.

implementation issues for disk IOs:
    - concurrent writes are non-deterministic: 
        must be linearized
    - DMA on memory-mapped device visible to guest OS is data race: 
        write on hypervisor managed "bounce buffer" invisible to guest OS, copy to DMA buffer later
    - failed disk IOs are inconsistent: 
        IOs free from concurrency and data race is idempotent

implementation issued for network IOs:
    - asynchronous network operations are non-deterministic:
        asynchronous network operations disabled, always trap to hypervisor
    - synchronous network IO is slow and laggy:
        - batch outgoing / incoming packets
        - eliminate context switches by attaching log acknowledgement handling to TCP stack as
            callback
        - hypervisor flush log entries more frequently

FT with non-shared storage
    pros: 
        - disk IOs are no longer outputs, do not have to be delayed per Output Rule
        - primary and backup may be physically distanced
    cons:
        - extra expensive disk sync steps on start up
        - IO failures creates inconsistency
        - extra facility to deal with split brain 

backup VM read from disk:
    pros:
        - disk input from disk, much less traffic on log channel
    cons:
        - slower execution, disk read vs. network buffer read
        - read result must be transmitted whenever either primary or backup fails
        - primary write on the same location must be delayed

FT performance highlights:
    - CPU impact < 10%
    - log channel bandwidth < 20Mb/s for non-network applications
    - heavy impact on network bandwidth for network applications
---NOTE END---

question: How does VM FT handle network partitions?
    atomic test-and-set a flag on shared storage, only one server may go live

Primary-backup replication: deals with independent fail-stop faults on a single machine. 
Examples of correlated faults:
    - whole data center power shortage
    - whole data center overheat
    - earthquake
Examples of fail-stop faults: 
    - power shortage
    - network shortage
    - CPU overheat
    - kernel panic
    - (when coded to) network package error caught by checksum
    - (when coded to) disk error caught by checksum
Examples of not fail-stop faults 
    - logical software bugs
    - memory corruption
    - malicious intruders

Two replication schemes:
    1.  state transfer
        send the whole state (memory, disk, ..), or at least the delta, to backups
    2.  replicated state machine
        start from the same initial state, send only external inputs to backups

Characteristics of replication scheme:
    - what is the state to be replicated
    - how close is the primary and backups synced
    - how cut-off (failover) is performed
    - what's the visible anomaly during a cut-off
    - how new backups are set up

FT log entry contains:
    - instruction number (since backup start up) to insert interrupt
    - type and data of event
    - (if the operation is non-deterministic) result of the operation
FT requires CPU hardware support to insert interrupt on specific instruction number

Obviously FT backup VM executes up to the instruction number in the next log entry and blocks on an
empty log buffer, or it will overrun an input event yet to arrive.

Lecture 5. Go, Threads, and Raft
paper: The Go Memory Model
---NOTE START---
Worth reading:
    - Foundations of the C++ Concurrency Memory Model
        https://www.hpl.hp.com/techreports/2008/HPL-2008-56.pdf
    - How to Make a Correct Multiprocess Program Execute Correctly on a Multiprocessor
        https://lamport.azurewebsites.net/pubs/lamport-how-to-make.pdf

A data race is defined as a write to a memory location and a read / write to that same location
happens concurrently, unless all accesses are atomic data accesses.

A memory operation is defined by four criteria:
    1.  kind, including
        - ordinary read
        - ordinary write 
        - synchronizing operation (atomic data access, mutex operation, channel operation, etc.)
    2.  location in the program
    3.  memory location or variable being accessed
    4.  values read / written by the operation
A memory operation may be read-like, write-like, or both.

A Golang program execution is modeled as a set of goroutine executions together with a mapping 
    W: ReadLike -> WriteLike
specifying the write-like operation that each read-like operation reads from. The execution (mainly
the mapping W) may not be the same each time for the same program.

"sequenced before" is a partial order defined on:
    Seq ⊆ OrdinaryWrite x OrdinaryRead
    // inherent to the structure of the program
"synchronized before" is a partial order similarly defined on read- / write-like synchronization
operations:
    Sync ⊆ SyncWriteLike x SyncReadLike
    ∀(w, r) ∈ Sync, W(r) = w
"happens before" relation is the transitive closure of the union of "sequenced before" and
"synchronized before":
    H ⊆ WriteLike x ReadLike
    H = (Seq ∪ Sync)^+

Sequential consistency is defined as (L. Lamport, "How to make a multiprocessor computer that
correctly executes multiprocess programs", 1979):
    H is a total order on memory operations for all possible executions

Correctness (data race free) requirements:
    1.  "memory operations in each goroutine must correspond to a correct sequential execution of
        that goroutine."
        notably order of evaluation in struct literals is undefined:
            m := map[int]{1: 1, 1: 2}
        may be {1: 1} or {1: 2}
    2.  "For a given program execution, the mapping W, when limited to synchronizing operations,
        must be explainable by some implicit total order of the synchronizing operations that is
        consistent with sequencing and the values read and written by those operations."
    3.  "For an ordinary (non-synchronizing) data read r on a memory location x, W(r) must be a
        write w that is visible to r, where visible means that both of the following hold:"
            - w happens before r
            - w does not happen before any other w' to the same location that happens before r

A read-write data race on memory location x consists of a read-like operation r and a write-like
operation w on x unordered by happened before:
    (w, r) ∉ H

A write-write data race on memory location x consists of two write-like operations on x unordered by
happened before:
    (w, w') ∉ H & (w', w) ∉ H

A Golang program free of data races is sequentially consistent. Word / sub-word memory access in
Golang is always atomic: a correct implementation of Golang should abort on data race or observe a
value actually written to the same location on single / sub word memory access that satisfies
correctness requirement 3. Races on multi-word data structures leave undefined value.

Synchronized before relations:
// denote 
//  "synchronized before" as <S
//  "happens before" as <H
    - Initialization
        If package p imports package q, completion of p's init functions <H the start of of any of
        q's. 
        The completion of all init functions <S the start of main.main.
    - Goroutine creation
        The Golang statement that starts a new goroutine <S the start of the goroutine. Destruction
        of a goroutine has no order guarantee.
    - Channel communication
        A send on a channel <S the completion of the corresponding receive.
        The closing of a channel <S a receive that returns a zero value because of the close .
        The kth receive on a channel with capacity C <S the completion of k+Cth send.
    - Locks
        For both l sync.Mutex and sync.RWMutex, n < m, nth call to l.Unlock() <S mth call to return
        from l.Lock().
        For any call to l.RLock() on sync.RWMutex, ∃n that
            nth call to l.Unlock() <S return from l.RLock()
            matching call to l.RUnlock() <S return from n+1th call to l.Lock()
        A successful call to l.TryLock() or l.TryRLock() is equivalent to l.Lock() or l.RLock(), an
        unsuccessful call has no ordering.
    - Once
        completion of f() from once.Do(f) <S return from any call to once.Do(f)
    - Atomic values
        If effect of an atomic operation A is observed by another atomic operation B, A <S B. The
        only atomic memory order ordering in Golang is SeqCst.
    - Finalizers
        SetFinalizer(x, f) <S the finalization call f(x).
    - Cond
        call to c.Broadcast() <S any return from c.Wait() it unblocks, no spontaneous awake in
        Golang.
---NOTE END---

question:
---CODE START---
type Unit struct{}

var a string
var done chan Unit

func setup(done chan Unit) {
	a = "hello, world"          // 1
	done <- Unit{}              // 2
}

func main() {
	done := make(chan Unit)     // 3
	go setup(done)              // 4
	<-done                      // 5
	print(a)                    // 6
}
---CODE END---
By order of evaluation:
    (1, 2), (3, 4), (4, 5), (5, 6) ∈ H
By synchronized before relations:
    (4, 1),     // goroutine creation
    (2, 5),     // channel communication
    ∈ H
H is a total order:
    3 <H 4 <H 1 <H 2 <H 5 <H 6
the program is sequentially consistent.

// first half of the lecture is about golang sync primitives, nothing remarkable
// second half of the lecture is about obvious bugs in Raft implementation, nothing remarkable

Lecture 6. Fault Tolerance: Raft (1)
paper: In Search of an Understandable Consensus Algorithm (Extended Version), up to Section 5
---NOTE START---
Worth reading:
    - [1] The Raft Consensus Algorithm
        https://raft.github.io/
        interactive Raft system demonstration
    - [2] Consensus: Bridging Theory and Practice
        https://github.com/ongardie/dissertation/blob/master/stanford.pdf?raw=true
        PhD dissertation of the same author, much more detailed than the Raft paper
    - [3] Students' Guide to Raft
        https://thesquareplanet.com/blog/students-guide-to-raft/
        notes from one of the TA of MIT6.824 2016, pitfalls on implementing Raft in Golang

Raft consistency and performance guarantees:
    1.  Safety: never return incorrect result under non-Byzantine conditions, including network
        delays / partitions and packet loss / duplication / reordering.
    2.  Functional: assuming fail-stop, as long as a simple majority of the replicas are operational
        the system is available.
    3.  Consistency doesn't rely on accurate system clocks
    4.  A minority of slow servers will not affect response time

Raft protocol invariants:
    1.  Election safety: 
            at most one leader can be elected in a given term
    2.  Leader append-only: 
            a leader never overwrites or deletes entries in its log
    3.  Log matching: 
            if two logs contain an entry with the same index and term, the all log entries up to the
            index are identical
    4.  Leader completeness:
            if a log entry is committed in a given term, then that entry will be present in the logs
            of the leaders at the same index of all higher-numbered terms
    5.  State machine safety:
            if a server applied a log entry at an index to its state machine, no other server will
            ever apply a different log entry at the same index

A Raft server is in one of three states:
    - Leader, handles all client requests
    - Follower, passively respond to leaders and candidates
    - Candidate, entered after Follower timeout, starts election

Raft server divides time into terms. One or no leader is elected each term. Term increases
monotonically as a logical clock.

A server with currentTerm smaller than the term it received in an RPC call updates currentTerm to
the larget one. A leader received a larger term in an RPC call reverts to Follower state. A leader
received a smaller term in a request rejects the request.

Raft servers communicate with each other using RPCs, timed out RPCs are retried (consistent to
Golang RPC model), RPCs should be issued in parallel.

Raft election process:
    0.  Follower is informed the presence of a Leader by periodic heartbeats (empty AppendEntries RPC)
    1.  A timed out Follower turns into a Candidate, increments currentTerm
    2.  Candidate vote for itself, issues RequestVotes RPC to other servers in the cluster until one
        of the three happens:
        1.  it wins the election (simple majority of the responses voted for it) 
            => become the new Leader
        2.  another server is established as leader (heartbeat with a term >= currentTerm)
            => revert to Follower
        3.  neither 1 or 2 happened until timeout
            => increment currentTerm, send RequestVotes again 
        timeout is randomized between 150 - 300ms, typically a lucky candidate will timeout early
        and win the election 
    3.  The newly elected leader immediately sends the initial AppendEntries heartbeat to all other
        servers in the cluster

Heartbeat interval of the Leader is not mentioned in the paper, naturally it should be much smaller
than the minimal election timeout so occasional packet loss / delay will not start an unnecessary
election, at the same time not too small to the point of visibly consuming the network bandwidth.
50ms (1/3 of minimum election timeout) is used in the interactive Raft demonstration [1].

An log entry is committed after been applied to the state on Leader. A log entry is committed once
the Leader that created the entry in the current term has replicated it on a majority of the servers
(i.e. received successful AppendEntries from more than half peers). A Follower applies a log entry
on its state after it's committed (informed by heartbeat). 

Leader maintains a nextIndex value for each Follower, initialized to 1 + last log index. After
failed AppendEntries, leader decrements nextIndex for that Follower then retry AppendEntries.
Eventually (prevLogIndex, prevLogTerm) in AppendEntries will match that on Follower, inconsistent
log entries after prevLogIndex will be purged. 

Raft new command process:
    1.  client send command to Leader
    2.  Leader appends (command, term) to its log, send AppendEntries to Followers
        retried until nextIndex points to empty or first inconsistent log entry on Follower, then
        new entries are appended to log on Follower
    3.  After receiving success from a majority of peers (including Leader itself), Leader updates
        commitIndex, apply new command to its state then update lastApplied
    4.  Followers will apply the command to their states the same way after been informed of
        commitIndex on Leader by heartbeat

As stated in invariant 2, a Leader never overwrites or deletes its log entry, such events happens
only after a new Leader is elected in a new term.

To prevent an out of date newly elected Leader from overwriting committed log entries, RequestVotes
RPC includes (lastLogTerm, lastLogIndex) from the Candidate. Followers reject Candidates less
up-to-date than itself. Up-to-date-ness of (Term, Index) is defined as lexicographical order of the
tuple:
    (T0, I0) is more up-to-date than (T1, I1) iff:
        T0 > T1, or
        T0 == T1 and I0 > I1

It's not safe to commit a last log entry when it has a term smaller than currentTerm, such log
entries must be indirectly committed by a future command in the currentTerm.

Proof of Leader Completeness Property (invariant 3) by contradiction:
    Assume for a term T, U is the smallest term U > T that an entry E committed by Leader[T] of term T
    is not stored by Leader[U] of term U
    By invariant 2, the entry must be absent since the election of term U
    By definition of commit, Leader[T] replicated the entry on a majority of peers
    By process of election, Leader[U] received vote from a majority of peers
    These two majorities must overlap, there's a peer P replicated the entry and voted for Leader[U]
    P stored the entry when voting for Leader[U] since:
        ∀T' that T <= T' < U, Leader[T'] stores the entry by definition of U
        Followers only remove entries if they are inconsistent with the Leader
    Leader[U] must be at least as up-to-date as P during election of term U 
    if Leader[U] and P has the same lastLogTerm:
        let lastLogTerm be T', T <= T' < U
        by how AppendEntries RPC is defined (Log Matching Property), both log of P and Leader[U]
        must be a prefix of Leader[T'] at the end of term T'
        lastLogIndex of Leader[U] should be at least the same as P
        log of P is a prefix of Leader[U], Leader[U] contains E 
    if Leader[U] has greater lastLogTerm than P:
        let T' be lastLogTerm of Leader[U], T <= T' < U
        Leader[T'] during its term contained E
        by Log Matching Property, Leader[U] also contains E
    both cases contradicts with the assumption, such U doesn't exist,
    ∀U>T, log of Leader[T] is a prefix of log of Leader[U] 

Proof of State Machine Safety (invariant 5):
    Assume a peer P applies an entry E to its state machine
    By how AppendEntries works, logs of P and current Leader are identical up through E
    Per protocol, E must be committed
    By Leader Completeness Property, the log of P up through E is a prefix of logs of all future
    leaders
---NOTE END---

question: 
    currentTerm >= lastLogTerm on each Follower, the exact value of currentTerm is uncertain for any
    server, but a majority of them should be 8 by the election of term 8. Since RequestVotes RPC
    doesn't change the logs, any server may be assumed to have currentTerm = 8.
    candidate       possible voters
    (a)             (b), (e), (f)
    (b)             (f)
    (c)             (a), (b), (e), (f)
    (d)             (a), (b), (c), (e), (f)
    (e)             (b), (f)
    (f)             

Raft implementation details:
    - Never do anything time consuming while holding the lock. Release the lock and reacquire it
        later during RPC calls.
    - Most change in peer states, like a role shift, should cancel all timeouts issued before it,
        but the deciding vote handler should cancel only other vote handlers, one approach to this
        is a hierarchy of cancellable contexts.
    - applyCh must be unbuffered, otherwise the client may crash and loss buffered ApplyMsg in
        the channel. Sending messages to applyCh could be time consuming
    - Requests and replies should not be skipped simply because they are from an earlier term /
        context, follow Figure 2 to letters 

Wrong replicated system:
    - clients send request to / expect acknowledgement from all servers 
        exponentially lower availability
    - clients send request to reachable servers
        different server may be reachable from different clients (network partition), split brain,
        causes inconsistency

Solution: odd number of servers, majority vote required to make any progress
    - at most one partition may function normally 
    - system can withstand less than half node failures
    - any two majorities overlap at at least one server

Raft doesn't manage state, the application code built on top of it replicates commands in Raft but
states are managed by the application itself (except snapshots).

Raft interface exposed to application code:
    - func Start(Command) (Index, Term)
        replicate the command in the raft cluster, return index and term of the command in leader
    - func ApplyCh <-ApplyMsg{Command, Index}
        a channel sending out committed commands and its index in the Raft cluster 

One possible network failure that halts the (most naive) Raft system to a full stop: Leader can
send out heartbeats but cannot receive any message, no new election will occur

Lecture 7. Fault Tolerance: Raft (2)
paper: In Search of an Understandable Consensus Algorithm (Extended Version), Section 7 to end
---NOTE START---
Logs cannot grow unbounded, server will one day run out of memeory: Raft needs log compaction.
Snapshotting is preferred log compaction method in the paper.

Log compaction usually can be performed independently on each server unless a Follower lags far
behind the Leader or a new server joins the cluster, in which case Leader has to manually send its
committed snapshot to Followers. The snapshot should be labelled with last included index and term
to support up-to-date check in AppendEntries RPC.

InstallSnapshot may be triggered when nextIndex for a Follower is reduced to a log index that has
already be compacted on Leader. Leader then have to send InstallSnapshot instead of AppendEntries to
the Follower.

Clients start with a randomly picked Raft peer, then redirected from a Follower to the last recorded
Leader. If Leader crashes, clients will try again with randomly picked servers. The exact RPC
suggested by the paper is slightly more complicated than necessary: check the lab 2D requirements of
MIT6.824 2021 course.

Raft may commit the same command twice:
    1.  client request command
    2.  command committed to majority of servers
    3.  Leader crashes before replying to client
    4.  client retry request, succeed again
Possible solution is to assign unique serial numbers to every command.

Read-only commands doesn't affect the state managed by Raft, but application must not bypass the
Raft replication scheme entirely on read-only commands otherwise clients may read stale data.
Leaders may commit an extra no-op command at start of their term to stay up-to-date all the time,
read-only commands on an always up-to-date Leader can be handled without being appended to the log.
---NOTE END---

question: Could a received InstallSnapshot RPC cause the state machine to go backwards in time?
    No, because:
        - InstallSnapshot doesn't overwrite log entries more up-to-date than the last entry in the
            snapshot, as described in step 6 of InstallSnapshot RPC receiver implementation.
        - A log entry must be committed before being applied to the state, snapshot constructed from
            committed entries on Leader contains no more and no less commands up to the last index.

Fast backup of nextIndex: instead of 1 at a time, AppendEntries may be modified to return:
    xTerm: the first term that contains conflicting log entries (may be -1 == None)
    xIndex: the first log index of xterm (may be -1 == None)
    xLen: the length of logs, index of the last log + 1
That's enough information for Leader to backup a term at a time:
    case 1: Leader has no entries in xTerm
        nextIndex = xIndex, the entire term should be removed
    case 2: Leader has entries in xTerm
        nextIndex = last log index with xTerm on Leaders + 1, only conflicting logs should be removed 
    case 3: xTerm == xIndex == -1
        nextIndex = xLen, no conflicting terms, send all missing logs to Follower

Persistent states should be stored on non-volatile memory in a verifiable way, three pieces of
information has to be persisted for Raft to function correctly after a crash:
    - log
        otherwise the application won't be able to replay the state after a crash
    - currentTerm
        otherwise servers may conduct another election for a past term  
    - votedFor
        otherwise a crashed server may vote twice to different candidates during an election  
New value of these states should be persisted before the change is externally visible, to clients or
peers through RPCs.  

Storing even a small piece of data to a mechanical disk is expansive, a slow persistent storage
would be the main bottleneck of Raft system.

Followers should (have to) ignore InstallSnapshot if Leader snapshot is a prefix of a locally
compacted snapshot.

Linearizability defined in this lecture (see also notes for Designing Data-Intensive Applications):
    ∃R∈(MemoryOps x MemoryOps), there's a total order R over memory ops that
        1.  if an operation A finished before another operation B started,
                A <_R B
        2.  if a read A sees the value written by a write B,
                A <_R B
                ~∃B'∈WriteOps, B' != B & A <_R B' <_R B
            (B is the immediate write preceding A)

Lecture 8. ZooKeeper
paper: ZooKeeper: Wait-free coordination for Internet-scale systems
---NOTE START---
ZooKeeper: wait-free FIFO linearizable coordination kernel 
    - wait-free: ZooKeeper API manipulates wait-free (non-locking) objects similar to a file system,
        higher performance compared to similar technologies (~100k ops/s if most ops are read)
    - FIFO: application requests are served first in, first out, with extra mechanism to help
        clients managing their cache
    - linearizable: writes to ZooKeeper managed objects has linearizability
    - coordination kernel: application developers implement their own sync primitives on top of
        ZooKeeper API, ZooKeeper does not understand Leader / Follower or configuration change

The paper focuses on three topics:
    - Coordination kernel:
        ZooKeeper, a wait-free coordination service with relaxed consistency guarantees
    - Coordination recipes:
        how sync primitives can be built on top of ZooKeeper
    - Experience with coordination:
        case study and performance highlights

ZooKeeper Terminology:
    - client: user of the ZooKeeper service
    - znode: ZooKeeper in-memory data node, with time stamp and version number
    - data tree: znodes organized in a hierarchical namespace 
    - update / write: operation on the state of the data tree
    - session: established by client on connection to ZooKeeper, through which client issue requests

ZooKeeper exposes an API similar to a file system:
    data tree   <=> hierarchical file system
    znode       <=> file with hierarchical path
    znode       <=> directory (znode is able to have children and data)
The major difference is that ZooKeeper supports only full read and write.

Two types of znode, specified on creation:
    - Regular:
        explicitly created and deleted
    - Ephemeral:
        automatically deleted on session termination
Additionally a file created with the "sequential" flag will have a monotonically increasing counter
appended to its name. A new znode created under a parent p will have counter not smaller (should be
greater?) than any znode ever created under p.

If a client issues a read request on a znode with the "watch" flag, a one-time notification that
triggers whenever the file is modified will be associated with the session. The notification does
nothing more than informing the client (contains no data).

ZooKeeper session is closed by client calling close on the session handle or by server on timeout.
Session is duplex:
    client -> server: client operations
    server -> client: state change notification reflecting execution of client operations
Session can be transparently moved from one server to another.

ZooKeeper client API:
// https://zookeeper.apache.org/doc/r3.4.6/api/org/apache/zookeeper/ZooKeeper.html
    - create(path, data, flags) -> String | Error
        create a znode with content data[] at path
        return path if znode is created by the request, return error if znode already exists
    - delete(path, version)
        delete the file at path if the version number matches
    - exists(path, watch) -> Stat | null
        test whether a znode at path exists, if watch flag is set subscribe to one-time
        state change notification on the file, including znode creation
        return stats of the znode or null if no such znode exists
    - getData(path, watch) -> byte[]
        return data and metadata of the znode at path, watch flag will be ignored if the
        znode doesn't exist
    - setData(path, data, version) -> Stat
        overwrite content of znode at path with data[] if the version number matches
        return the stats of the znode
    - getChildren(path, watch) -> List<String>
        return children of znode at path
    - sync(path /* ignored */)
        wait until all pending operation (before calling sync) is propagated to server
Each method has both sync (block until propagated to server) and async version. All version check
may be bypassed with a version number -1.

ZooKeeper order guarantees:
    1.  A-linearizable writes, a stronger condition than linearizability that allows concurrent
        requests from a single thread (client), but only for write-like operations
    2.  Write requests from a single thread is served in FIFO (send by client) order
    3.  Read requests from a single thread is each associated with a logical time stamp on writes
        (zxid), which obeys the same FIFO order as read requests are issued, i.e. server state
        perceivable to a client cannot go back in time, even across multiple sessions (server
        switches)
 
Example scenario: leader update configs
    - other processes may access the configs when leader is updating them
        solution: a "ready" znode is created under a designated path, leader delete "ready" before
        changing config and create it after completion, FIFO and linearizability guarantees that
        async write to configs will finish before the creation of "ready"
    - leader may crash while modifying the configs 
        solution: same as above
    - TOCTOU bug, leader may delete "ready" after a process tests its existence and after the
      process tries to read configs
        solution: instead of tests, processes subscribe to watch notifications on "ready", on
        both deletion and creation the event precedes the state change, the existence of "ready" can
        then be cached and checked locally
    - private channels, process A may change a znode then notify process B, if B's replica is
      lagging the change may not be in place
        solution: process B calls sync then read, all pending write (wrt. a real timer?) must be
        resolved before read

ZooKeeper availability guarantees:
    1.  The service is available is a majority of replicas are alive and reachable
    2.  If the service responses successfully, the change persists after any number of failures as
        long as a quorum (majority?) of servers eventually recover

Examples of primitives built on ZooKeeper:
    - Config management:
        Shared config in a znode, clients subscribe to watch notification.
    - Group membership:
        Create a znode z_g for the group, each process member create a sequential ephemeral child of
        z_g on start up. Group membership will be revoked on process termination. Use getChildren to
        query group membership.
    - Simple exclusive locks:
        (Similar to lock files in a ordinary file system) A process create ephemeral file on an
        agreed path, other files block on delete event on watch notification.
        Like Cond.Broadcast in Golang, Watch notification awakes all blocking processes despite that
        only one of them may acquire the lock ("herd effect").
    - Simple exclusive locks free from herd effect:
        For a znode l on agreed path, a process lock the mutex by:
            1.  n = create(l, "/lock-", EPHEMERAL|SEQUENTIAL) 
            2.  call getChildren(l, false)
            3.  if n is the child of l with lowest sequence number, acquire the lock
            4.  if not, let p be the child of l immediately precedes n
            5.  call exists(p, true), if p exists, wait on watch event
            6.  goto 2
        unlock the mutex by:
            1.  delete(n)
        A protocol abiding process cannot create a child with sequence number lower than or equal to
        an existing child, once lock step 5 returns false it will always return false in future, no
        TOCTOU bugs here
    - Read / Write locks:
        Start from a znode l on agreed path, acquire write lock:
            // almost identical to mutex lock
            // sequence number shared by write / read znodes
            1.  n = create(l, "/write-", EPHEMERAL|SEQUENTIAL) 
            2.  call getChildren(l, false)
            3.  if n has lowest sequence number, acquire write lock
            4.  if not, let p be the child of l immediately precedes n
            5.  call exists(p, true), if p exists, wait on watch event
            6.  goto 2
        Acquire read lock:
            1.  n = create(l, "/read-", EPHEMERAL|SEQUENTIAL) 
            2.  call getChildren(l, false)
            3.  if there's no "l/write-" child with a lower sequence number, acquire read lock
            4.  if not, let p be that write child
            5.  call exists(p, true), if p exists, wait on watch event
            6.  goto 2
        unlock (both read and write):
            1.  delete(n)
    - Double barrier (Barrier with thresholds on entrance and exit):
        Enter (implied by the high level description in the paper):
            // start from a znode b on agreed path
            1.  n = create(b, "/barrier-", EPHEMERAL|SEQUENTIAL)
            2.  call getChildren(b, false)
            3.  if number of children >= threshold, create(b, "/ready"), enter the barrier 
            4.  call exists(b + "/ready", true)
            5.  if true enter the barrier, if not goto 2
        Exit:
            1.  delete(n)
            2.  call getChildren(b, false)
            3.  if b has no "barrier-" child, exit the barrier
            4.  if not, call exist(c, true) on a random child c of b, wait on watch event
            5.  goto 2

Examples of ZooKeeper applications and their primitives:
    - The Fetching Service (config metadata, leader election):
        part of the crawler at Yahoo!, use ZooKeeper to redirect clients to healthy server and elect
        new leader on master failure
    - Katta (group membership, leader election, config management):
        distributed indexer at Yahoo!, master assign works to slaves (config management), both
        master and slaves may fail (leader election), slaves frequently join and leave the cluster
        (group membership)
    - Yahoo! Message Broker (config metadata, failure detection, group membership, leader election):
        distributed publish-subscribe system, thousands of topics replicated among a set of servers
        with a primary-backup scheme, use ZooKeeper to manage distribution of topics (config
        metadata) and deal with failed replicas (failure detection, group membership, leader
        election)

ZooKeeper components:
    - Request processor:
        transforms update requests to idempotent transactions 
    - Atomic broadcast:
        agreement protocol used to replicate the ZooKeeper data tree, tolerates less than majority
        server failure, guarantees order of broadcasts from a leader, all changes from a previous
        leader is delivered to new leader before the new leader starts broadcasting its own changes,
        followers never diverge but may stale
    - Replicated database:
        in-memory replica of the entire ZooKeeper data tree, persisted to a write-ahead log and
        periodically to a snapshot, snapshot of the database is fuzzy in the sense that the database
        is not locked when the snapshot is taken, the snapshot may not correspond to a legal state,
        replaying a few idempotent updates from the log fixes it (possible because updates are
        idempotent)

Write requests goes through request processor -> atomic broadcast -> replicated database, read
requests are served locally by the replicated database alone (may be stale).

Watch events are triggered and served locally, by the server the client is connected to.

sync() is an async primitive that guarantees the order of operations after it, all operations
following a sync() is guaranteed to executed after all pending requests before sync() is applied to
the local replicated database.

Responses from ZooKeeper server contains a zxid as a total order on updates. Session reestablishment
to a new server is refused until the server caught up to the last zxid of the client. Clients send
heartbeats on idle.

Adding server to a ZooKeeper cluster negatively impacts write overall throughput as writes must be
both replicated to all servers and persisted on non-volatile storage, while overall throughput of
purely read operations grows linearly to the number of servers as reads are served locally. In
practice write throughput is improved by partitioning terms to different ZooKeeper clusters.

Typical write request latency of ZooKeeper is 1-2ms.
---NOTE END---
question: Why isn't possible for two clients to acquire the same lock?
    Sequential znode creation enforces a strict total order on acquire attempts, let p and q be two
    processes trying to acquire the lock, p_n and q_n be the corresponding znodes they created at
    step 1, either p_n < q_n or p_n > q_n, w/o loss of generality assume p_n < q_n, by step 2-6 q
    cannot enter critical section until p_n is deleted (unlocked). Order of events is well-defined
    as implied by linearizability.
    On client crash or network failure, client ceases to send requests and heartbeats, after session
    timeout server determines that the client has failed then deletes the ephemeral files created by
    it, in this case the lock file created by the client hence unlocks the mutex.

Definition of linearizability in the lectures relies on global real time, which is not present in
the original definition by Herlihy (1990). The original definition only requires sequential order
per process, a condition stronger than a thread-local real time.

Typical non-linearizable configuration:
    - clients perceive two different writes in different order, assume 
            (unordered) Wx1 Wx2
            C1: Rx1 Rx2
            C2: Rx2 Rx1
        assume w/o loss of generality in an total order H, Wx1 <_H Wx2, by H|C2:
            Wx1 <_H Rx1 <_H Wx2
            Rx2 <_H Rx1 <_H Wx2
        Rx2 doesn't have a corresponding write, non-linearizable
    - clients perceive stale write, let x-axis be the real time: 
            S1: Wx1
            S2:     Wx2
            C1:         Rx1
        because of the extra global real time, Rx1 is not immediately preceded by a corresponding
        write, non-linearizable

Linearizability implies that a distributed system has to determine whether retransmission is or is
not transparent to the clients, for instance:
    C1: |--Wx1--| |--Wx2--|
    C2:     |------Rx?- (reply lost) |--Rx?--] (retransmission)
if retransmission is transparent to clients, history of C2 is 
    C2:     |------Rx?-----------------------]
both Rx1 and Rx2 are possible and legal replies according to linearizability. 
If retransmission is not transparent to clients:
    C2:     |------Rx?-              |--Rx2--]
the only legal reply is Rx2 according to the global time order.
One viable solution is for servers to record replies to client responses then replay the reply on
duplicate (per sequence number) request.

Interesting aspects of ZooKeeper:
    - API of a stand-alone general purpose coordination service
    - how does the performance of replicated service scale

ZAB, the single-leader consensus protocol deployed by ZooKeeper servers, has similar performance
characteristics to Raft that, adding more server _worsen_ the performance of updates as the leader
has to wait on more peers.

Read requests may be served locally by a follower, but a follower in a Raft-like system is not
guaranteed to be up-to-date. Linearizability forbids this approach. ZooKeeper allows clients to read
stale data, read performance is scalable at the sacrifice of linearizability.

Analysis of ZooKeeper atomic config update example in the paper, on two config files:
Operations performed by the leader on config update:
    1.  delete(ready)
    2.  update config 1
    3.  update config 2
    4.  create(ready)
Operations performed by a follower attempting to read the config:
    1.  while exist(ready, watch = true) == false
            wait on event
    2.  read config 1
    3.  read config 2
    4.  if at any point between 1 and the return of 3 the event triggered, goto 1
Watch notification is guaranteed to be delivered before the state change is perceivable to the
clients, in this particular scenario if the reader is not informed of the deletion of ready, step 1
ensures that reads are atomic in general (by monotonicity of perceivable state) and step 4 ensures
that reads are atomic during the operation (by notification timing).

Lecture 9. More Replication, CRAQ
paper: Object Storage on CRAQ
---NOTE START---
CRAQ (Chain Repllication with Apportioned Queries): distributed chain replication object storage
system providing strong consistency and scalable read throughput at the same time 
    - chain replication: 
        chain head handles writes, chain tail handles reads, writes are propagated down the chain
    - object storage: 
        key-value storage with flat namespace, whole-object modifications, possibly no consistency
        guarantees across multiple objects 
    - apportioned queries: 
        read operations in the chain replication system are handled by all nodes

Contributions of this paper:
    - scalable read operations on CR (chain replication) without sacrificing strong consistency  
    - lower-latency eventually consistent read mode, clients may specify the maximum staleness they
        are willing to tolerate 
    - a system design for building CRAQ across geo-diverse clusters that preserve strong locality,
        on top of ZooKeeper
    - possible extensions to CRAQ, including atomic multi-object updates and the use of multicast to
        improve write performance

Interface of a typical object storage:
    - write(objID, V)
        stores the value V associated with object identifier objID 
    - V <- read(objID)
        retrieves the value V associated with object id objID

Consistency models in CRAQ:
    - strong consistency:
        read and write to a single object is are executed in the same sequential order on all nodes
        (linearizability?), read always sees up-to-date value
    - eventual consistency (with or without maximum-bounded staleness):
        writes are executed in the same sequential order on all nodes, reads may return stale data,
        state perceivable to clients are monotonic among a single session but not across sessions

Characteristics of CR:
    - strong read / write consistency
    - head node handles all writes, writes are propagated down the chain, a write arrived at the
        tail node is committed, acknowledgement of write of commit is propagated in reverse
        direction, from tail to head, then finally to the client as a reply to the write operation
    - tail node handles all reads, returns only committed values
        natural strong consistency
    - multiple concurrent writes can be pipelined down the chain, write performance is competitive
        or superior to primary / backup replication

Characteristics of CRAQ:
    - read may be handled by any node in the chain, not just the tail
    - a node in CRAQ stores multiple versions of an object, each tagged by a monotonically
        increasing version number and a binary flag "clean" or "dirty", all versions start as clean
    - when a node receives a new version of an object,
        1.  the node appends this latest version to the list of versions for the object
        2.  if node is not the tail, marks the version as dirty, propagates the write to successor
        3.  if node is tail, marks the now committed version as clean, propagate acknowledgement
            backwards up the chain
    - when a node receives an acknowledgement, 
        1.  mark the object version as clean, (allowed to) delete all previous versions
        2.  propagate the acknowledgement to predecessor 
    - when a node receives a read request to an object,
        1.  if the latest known version of the object is clean, return its value
        2.  if not, ask tail's last committed version number of the object, return that version of
            the object, which must be there based on how writes are committed

CRAQ read throughput is higher than CR in:
    - Read-heavy workloads:
        most reads are clean, scales linearly with number of servers
    - Write-heavy workloads:
        version queries are lighter-weight than full read operation, tail is less of a bottleneck,
        additionally the system could forbid reads on tail node when the chain is long enough

A read operation may be annotated with one of three permissive consistency model:
    - strong consistency:
        clean read as describe above
    - (unbounded) eventual consistency:
        allows a node to return latest (clean or dirty) version of an object, different nodes may
        stale to different degrees 
    - eventual consistency with maximum-bounded inconsistency:
        return value is allowed to have a maximum inconsistency period, defined over real time or
        versioning

Each chain node knows its predecessor, successor, head and tail. When the head fails it's successor
takes over as the new head, when the tail fails it's predecessor takes over as the new tail. CRAQ
leverages ZooKeeper to keep track of chain membership.

Workload assumptions behind common chain placement strategies:
    - most of all writes to an object originate in a single datacenter
    - some objects are only relevant to a subset of datacenters
    - different objects are accessed in different frequency

A CRAQ configuration consists of multiple chains, object identifier in CRAQ is a two-tuple:
    (chain identifier, key identifier)
where hash of chain identifier determines which node in a datacenter stores the chain, key
identifier is only unique among the chain. Key identifier can grow unbounded. Alternatively
assignment of chain identifier to chains could be dynamic, managed by some other coordination
service.

The paper described 3 ways of specifying chain placement:
    - implicit datacenters & global chain size: { num_datacenters, chain_size }
        num_datacenters: number of datacenters the nodes of the chain will be distributed to
        chain_size: length of the chain within a single datacenter
    - explicit datacenters & global chain size { chain_size, dc_1, dc_2, .., dc_N }:
        dc1, .., dcN: a sequence of datacenters in the chain, last node in dc_{k} is predecessor of
            first node in dc_{k+1}, dc1 stores the head, dcN stores the tail
        chain_size: same as above, if chain_size == 0 all nodes in a datacenter is included
    - explicit datacenter chain sizes { dc_1, chain_size_1, .., dc_N, chain_size_N }
        same as above except chain size is specified separately

When using explicit datacenters configs, dc_1 may be appointed as the master datacenter: write to
the chain will be rejected if all nodes in dc_1 is unreachable. Otherwise some node in dc_2 could
become the new head when dc_1 is disconnected and so on. Writes are only served in a partition if a
majority of nodes is in it, read would still be served up to maximum-bounded inconsistent period as
aforementioned.

Explicit datacenter configs improves write latency by ensuring that write operations cross
datacenter boundaries exactly twice: once as new version to an object, another as acknowledgement.
Despite that, when enough datacenters are added to the chain, write latency are still significantly
higher than primary / replica systems.

CRAQ uses ZooKeeper to track membership changes and chain metadata. Since ZooKeeper is not designed
for multi-datacenter situation, the example deployment of CRAQ puts in extra effort to make sure
chain nodes in each datacenter are connected to local ZooKeeper nodes.

Extended API of CRAQ on top of typical object storage:
    - prepend / append:
        adds data to start or end of an object's current value
        head applies the modification to the (maybe dirty) latest version, then propagates a full
        write to successor
    - increment / decrement:
        adds or subtracts to an object, interpreted as an integer
        implementation similar to prepend / append
    - test-and-set:
        update an object iff its version number equals the input
        head node checks if most recent committed version number matches the input and there's no
        uncommitted versions, if not rejects the request without any further chain action

Multi-object atomic update ("mini-transition") typically could be implemented as a two-phase commit
operation (one pass locks the objects, the second pass commits the update), in single-chain CRAQ the
two passes could be fused into one as head is the only node handling write requests thus is able to
lock all objects.  Even in multi-chain CRAQ only head nodes are involved in such an operation.

Write could be broadcasted to all nodes in a chain in help of a multicast network protocol, then
instead of the whole value only the metadata (objId and version?) needs to be propagated down the
chain. If a node didn't receive the multicast, it should block the acknowledgement until it acquired
the value in other means, e.g. from its predecessor.
The paper also suggests to broadcast acknowledgement through the same multicast protocol, author
didn't explain why this is safe.

ZooKeeper znode hierarchy of CRAQ:
    - /nodes/dc_name
        one unique name for each datacenter, nodes in a datacenter subscribe to changes to its
        children 
    - /nodes/dc_name/node_id
        each node creates an ephemeral znode with unique identifier under their designated
        datacenter on initialization 
        contains IP address and CRAQ port numbers
    - /nodes/chain_id
        one unique identifier for each chain, nodes in a chain subscribe to changes to its metadata
        contains chain config but no membership data
Based on the assumptions:
    - number of chains is at least an order of magnitude higher than the number of datacenters
    - chain membership is much more dynamic than datacenter membership
Alternatively node membership could be organized under chain_id if deployment deviates from the
above assumptions.

Nodes within a datacenter form a one-hop DHT (distributed hash table, a peer-to-peer protocol
utilized by e.g. bittorrent), CRAQ successor and predecessor are defined in terms of the DHT ring.
Chain membership is tracked by this peer-to-peer protocol, not ZooKeeper. 

Nodes opens a pool of TCP connections with each of its predecessor, successor and tail, RPC messages
are pipelined and round-robin-ed across these connections.

Furthermore first and last node in a datacenter watches node membership of the other datacenter they
are connected to.

A second form of write propagation, the back propagation, is triggered on membership change or node
failure. When a new node joins the chain at the middle it may receive write propagation from both
direction for a period. Both forward and back write propagation could send full history of an object
(both clean and dirty versions) during recovery of a predecessor / successor.
More precisely, let
    L_C:    configured, fixed length of chain C
    N:      an existing node in chain C
On addition of a node A to the chain C:
    - if A is N's successor:
        1.  N propagates full history of all objects in C to A
        2.  if A has been in the chain before, instead of the entire object store only the
            difference is calculated and communicated by a un-specified reconciliation algorithm
    - if A is N's predecessor:
        1.  if N was not the head, N back propagates full history of all objects in C to A
            (what happens when A is inserted as the new head?)
        2.  A take over as the tail if N is the previous tail
        3.  N becomes the tail if N's successor is previously the tail
        4.  A take over as the head if N is the previous head
    - if A is within L_C predecessors of N:
        1.  if N was the tail, N retires from tail duties, mark its data in chain C as removable
        2.  if N's successor is the tail, N assumes tail duties
On deletion of a node D:
    - if D was N's successor:
        1.  N reconciles with its new successor, as D could be in the process of propagating a write
            down the chain
    - if D was N's predecessor:
        1.  N reconciles with its new predecessor, as D could be in the process of propagating an
            acknowledgement up the chain
        2.  if D was the head, N assumes head duties
        3.  if N was the tail, retires from tail duties, propagate full history of all notes to N's
            new successor
    - if D was within L_C predecessors of N
        1.  if N was the tail, retires from tail duties, propagate full history of all notes to N's
            new successor
(again, presented without any prove of correctness)

Performance of CRAQ:
    - throughput:
        - 500-byte read throughput scales linearly with number of nodes, ~100k read ops/s on CRAQ-5
            (7-node CRAQ)
        - 500-byte write throughput decreased gradually as chain length increased, ~5k ops/s on
            CRAQ-5 
        - 4-byte test-and-set throughput linear to the reciprocal of chain length, as one
            test-and-set operation blocks the entire chain, ~400 ops/s on CRAQ-5
        - read throughput decreases gradually in response to more concurrent write operations, still
            more performant than CR as the tail node in CRAQ handles thinner version queries
        - read from non-tail node approaches 100% dirty as concurrent writes increases 
    - latency:
        - dirty read has higher latency than clean read (obvious)
        - write latency linear to chain size
        - both read and write latency are negatively impacted by heavy load and object size
        - when tail is in a remote datacenter, dirty read latency is dominated by RTT across
            datacenters 
    - failure recovery:
        - affect of node failure on read throughput is 1/C, where C is the chain length
        - node failure slightly increases read latency as read has to be retried
        - write cannot be committed until a new node is assigned to the chain, the major drawback of
            CRAQ in comparison to a consensus protocol like Raft
---NOTE END---

question: Explain how this could lead to violations of linearizability
    let 
        N1 and N2 be two nodes in CRAQ chain, N2 is further down the chain than N1
        C be a client
    in the scenario below:
        1.  C connects to N1
        2.  C reads a dirty value V2
        3.  C disconnects, connects with N2 instead
        4.  V1 haven't be propagated to N2 yet, C reads an earlier value V1
    the state perceivable to C went backwards in time, non-linearizable

A distributed counter, implemented on ZooKeeper:
    loop {
        x, v = getData("cnt")
        if setData("cnt", x + 1, v) {
            break
        }
    }
Short coming: Ω(N^2) complexity with N contending clients
possible solution: 
    - randomized exponential backoff (similar to Raft election timeout)
    - mutex without herd effect as described in ZooKeeper paper 

ZooKeeper mutex doesn't provide atomicity over multiple znodes, it's still possible for a client to
crash in the middle of editing one of the files, the next client will see garbage data with or
without locking.

Advantage of CQ over Raft:
    - straightforward fail patterns: 
        update flows linearly through all nodes, a failed node may be easily recovered or replaced
    - better read / write throughput: 
        head node replicates update to a single other node instead of all of them, head node is
        capable to handle multiple ongoing writes, read and write are handled by different nodes
Disadvantage of CQ compared to Raft:
    - CQ by itself doesn't prevent split brain, relies on another coordination service
    - every single lagging replica slows down the entire system

Lecture 10. Cloud Replicated DB, Aurora
paper: Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases
---NOTE START---
Amazon Aurora: high throughput DB for OLTP workloads with a focus on network capacity
    DB: 
        not an object store, supports SQL (presumably not the entirety of SQL)
    OLTP: 
        OnLine Transaction Processing, non-blocking, atomic change of state across multiple objects
        / database records
    focus on network capacity:
        author believes that network capacity, not computing nor storage, will be the main
        bottleneck for distributed systems, indeed IO in e.g. a sharded object store is spread
        across multiple physical devices, in contrary almost all replication schemes amplifies
        network stress

Issues of traditional replicated RDBMS:
    - network stress
    - cache miss blocks reading thread during slow disk operation
    - multi-phase transaction protocols (e.g. 2PC) are high latency and intolerant to failures 
Advantages of Aurora over traditional RDBMS:
    - storage is an independent fault-tolerant and self-healing geo-diverse service, treats
        failure in durability (physical failures), availability and performance variation uniformly 
    - network IOPs are reduced by an order of magnitude by only writing redo log records to storage 
    - move expansive functions (backup, redo recovery) from database engine to continuous async
        operations distributed to a cluster

Contributions of this paper:
    - distributed storage system tolerant to correlated physical failure 
    - offload traditional low level functions of a database to a distributed storage
    - eliminate blocking operations in RDBMS (2PC, crash recovery, checkpointing)

Two quorum rules in Aurora:
    let 
        V:      number of replicas in the cluster 
        V_r:    quorum required for read operations
        V_w:    quorum required for write operations
    1.  V_r + V_w > V
        read overlaps with most recent write, read is up-to-date
    2.  V_w > V / 2
        each write overlaps with the last write, same as Raft
A common assignment of V = 3, V_w = V_r = 2 is inadequate: replicated system should be tolerant to a
major failure on a replica and constant "background noise" failures among other replicas, a 2/3
quorum would frequently be unavailable in that scenario. The recommended quorum assignment by the
paper is V = 6, V_w = 4, V_r = 3.

AZ (Availability Zone): a subset of replicas loosely connected with other replicas with low latency
links but otherwise isolated for most physical faults (power, intra-AZ networking, software
deployments, flooding etc.). Replication target of Aurora is
    1.  losing an entire AZ and one additional node (AZ+1) doesn't cause data loss
    2.  losing an entire AZ doesn't cause write unavailability
The (6, 4, 3) model mentioned above with 2 replicas in each AZ satisfies both requirements.

"AZ+1" quorum is sufficient iff MTTF (Mean Time to Failure) of a uncorrelated double fault is
sufficiently lower than the MTTR (Mean Time to Repair) of a single node. MTTF of uncorrelated faults
is hard to reduce after a certain point, instead Amazon focused on reducing MTTR.

Hierarchy of a typical deployment:
    volume:                     up to 64TB, divided to PGs 10GB in size each
    PG (Protection Groups):     replicated 6-way to 3 AZs
    AZ:                         two replicated 10GB segments in each AZ
    segment:                    stored on separate Amazon EC2 VM
A 10GB segment can be repaired in 10 seconds over a 10GB link. A failure beyond the capacity of AZ+1
quorum is at least fault of 1 entire AZ and 2 node in the same 10s MTTR.

Aurora treats all unavailability as a fault, including disk overheating, OS patch and software
upgrades (which is executed on different timing in different AZ).

A traditional RDBMS on write request has to write to:
    - transaction replay log
    - SQL statement binary log  
    - the actual data page 
    - a second temporary data page, in case of write failure
    - DB metadata
These writes are fully and sequentially replicated from 
    - primary   -> replicas
    - DBs       -> storages
    - storages  -> mirrors
multiple links in the chain are sync so the latency is additive. Quorum of a traditional replicated
RDBMS is the total number of replicas, means outliers severely impact the overall performance.

In Aurora, the only write replicated are redo log records and metadata from 
    - primary   -> replicas 
    - primary   -> a quorum of storages
    - storages  -> mirrors
DB tier of Aurora doesn't write pages: not for background bookkeeping, not for checkpoints, not for
cache miss, instead a dedicated log applier in storage tier generates DB pages on background or on
demand. In fact in perspective of the engine, the log is the DB and the materialized DB pages are
merely a cache.

As a result DB on startup / crash recovery no longer have to replay logs from a previous checkpoint. 

In one of their test (Table 1) Aurora is able to sustain 35x more transactions than mirrored MySQL,
each transaction caused 7.7x fewer IOs.

Life cycle of a log record in the storage layer of Aurora:
    1.  received, placed in in-memory incoming queue
    2.  taken out from the queue, persisted on non-volatile update queue
        (acknowledgement sent to primary)
    3.  sorted, grouped, placed in hot log list
    4.  gaps in hot log list filled by gossiping with replicas
    5.  log records coalesced into new data pages 
    6.  periodically
            - stage logs and pages to backup
            - garbage collect old versions
            - validate checksum on pages
Every step is async, only steps 1 - 2 adds latency to the foreground.

Aurora tags each log record with a monotonically increasing LSN (Log Sequence Number). The storage
service keeps track of:
    - VCL (Volume Complete LSN): the greatest LSN that all previous log records are available 
    - CPLs (Consistency Point LSNs): LSN of last log record of MTRs (Mini-TRansactions, composed of
        continuous log records)
    - VDL (Volume durable LSN): greatest CPL <= VCL
During storage recovery the log records are truncated to VDL. Atomicity of MTRs is thus restored by
the storage service, while atomicity of DB-level transactions are managed by the DB layer.

Write to the storage is throttled by an LAL (LSN Allocation Limit), DB cannot assign LSN to newly
generated log records greater than LAL + VDL.

Log of each segment of each PG are not necessarily continuous in storage, log records contains a
backlink to the preceding log record in the same segment. Metadata for each segment contains a SCL
(Segment Complete LSN) that's used by storage nodes on log exchange.

Transaction commits are completed async: acknowledgement are sent to clients once the latest VDL
exceeds the commit LSN.

A cache page is evicted from memory iff its page LSN >= VDL (why?). Aurora employs a mechanism
similar to zxid in ZooKeeper to guarantee consistency of read results.

A storage volume can be shared by a single write replica and 15 read replicas. Reader applies log
records relative to its cache pages up to VDL. Log records of a MTR is always applied atomically.
Reader applies log records async to the writer with a ~20ms lag.

After a crash DB contacts with a read quorum of storage replicas for each PG. As V_r + V_w > V, it
can recalculate the latest VDL before crash. Logs after VDL is truncated, and the truncation
operations is persisted to the storage in case the DB crashes again during recovery.

Performance of Aurora:
    - throughput of both read-only and write-only workloads scales linearly to instance size (number
        of vCPUs and memory), scales way better than MySQL
    - 34x faster than MySQL with out-of-cache write-only workload on a 1TB database
    - scales 10x better than MySQL on concurrent connections, up to 5000 users
        (write-only workload doesn't scale with concurrent users: Aurora is a single writer system)
    - replica lag behind primary in range of 1 - 10ms, in contrary of 300_000ms with MySQL
    - 2-15x throughput than MySQL on hot row contention
    - drastically reduced latency in real applications
---NOTE END---

question: What runtime state does the database need to maintain?
    (mentioned in 4.2.3) a read-point, i.e. the VDL at the time the request is issued, in the same
    manner of zxid in ZooKeeper

Lineup of Amazon services:
    - EC2: virtual machine for rent
    - S3: physically detached storage backing EC2, supports periodical / manual snapshot
        risks data loss between snapshot period
    - EBS: chain-replicated storage backing EC2, cannot be shared between concurrent EC2 instances
        storages are physically close to each other, vulnerable to correlated faults
    - RDS: fully replicated EC2 + EBS on 2 AZs
        the scheme described by Figure 2 in the paper, write to record of few bytes are amplified to
        a remote page write of 4KB or more, slow and high latency
    - Aurora: log replicated quorum system across 3 AZs 

Data pages in Aurora function as a kind of snapshot, storage service stores old pages and a series
of relevant log records for each page.

PGs ~ shards, logs are only sent to a PG if the operation modifies data in that PG.

When a storage server goes down, each of the segments stored on it is replicated to a separate
living storage server in parallel, hence PGs may recover from a single failed machine in 10s.

Aurora writer sends log records and transaction-respecting commit index (VDL?) to active readers so
they can update local cache pages without talking to storage services.

Lecture 11. Cache Consistency: Frangipani
paper: Frangipani: A Scalable Distributed File System
---NOTE START---
Frangipani: A 2-layer adaptive consistent distributed file system
    - 2-layer: a distributed storage layer (Petal) and a file system layer (Frangipani)
    - adaptive: servers can be added to the cluster without changing configs on existing servers
    - consistent: access to the storage service is protected by locks, tolerates machine, network
        and disk failures without intervention, backup could be taken on the fly

Frangipani registers itself as a normal file system in Unix Kernel, users access files in Frangipani
as they would a local file system. Modifications are cached in the kernel's buffer pool only to be
flushed to Frangipani on fsync() syscall; the metadata however may be guaranteed non-volatile by the
time write() returns.

Petal behaves just like a local physical drive, any Unix file system may use Petal as the backing
storage, but only Frangipani (with the help of a lock service) provides coherent access to the data. 

Petal / Frangipani deployment is secure iff:
    - Petal, Frangipani and the lock service run on trusted systems
    - they authenticate themselves to one another
    - network connecting them cannot be eavesdropped
    - untrusted client accesses the file system by talking to a Frangipani server through separated
        network 

Petal servers share a 2^64 byte virtual disk address space, used part of which is committed to
physical disk in 64 KB chunks. Each region stores:
    - 0-0 TB: config parameters
    - 1-1 TB: WALs (Write Ahead Logs) of metadata, 4GB for each server, supports 256 servers
        maximum, user data is not logged
    - 2-4 TB: allocation bitmaps for the rest of the disk address space, dynamically partitioned to
        blocks exclusive to each servers
    - 5-5 TB: inodes, 512 bytes each in size, one-one corresponds to indices in allocation bitmaps,
        shared read / write access from all servers
    - 6-133 TB: small data blocks, 4KB each in size
    - 133- TB: large data blocks, 1TB each in size

The metadata log records are written out to Petal async in the order of requests.  Frangipani can be
configured to write out log records sync. Metadata is only updated after log is persisted in Petal.
Logs frequently fill up the allocated space (128KB), each time the oldest 25% of logs are reclaimed
(and written out to Petal if hasn't).

On failure of a Frangipani server, ownership of it's logs are implicitly (by the lock service?)
given to the recovery daemon, which will then replay the incomplete portion of the logs on Petal.
The failed Frangipani server could later be restarted with an empty log.

Each log is tagged with a monotonically increasing sequence number, the last log record is the
log record with a sequence number greater than the succeeding record.

Correctness of metadata recovery relies on:
    - the locking protocol serializes updates to the same data: 
        write lock covering dirty data is only released after the data has been written to Petal, by
        the original holder or the recovery daemon 
    - recovery daemon never replays a log describing completed update: 
        each 512-byte metadata block is tagged with a monotonically increasing version number, each
        log record contains the new version number, a log is replayed iff the block version number
        is less than the record version number
    - at any time only one recovery daemon is trying to replay the logs of a failed server
Consistency of file data blocks is not guaranteed.

Access to data blocks are protected by a read / write lock:
    - a read lock allows a server to read the associated data from disk and cache it, the server
        must invalidate its cache before releasing the read lock
    - a write lock allows a server to read and write the associated data from disk and cache it, the
        cache deviates from the data on disk iff the server holding the write lock updated it, the
        server must write out the dirty data to disk before releasing or downgrading the write lock
Dirty data is never forwarded from writing server to reading server for simplicity.

There is a read / write lock for each:
    - log record
    - segment of bitmap space (no contention on file creation)
    - data block or inode not currently allocated to a file (shares the same lock with the bitmap
        space pointing to it)
    - file, directory or symbolic link (shares the same lock with its inode)
Locks are acquired in globally defined total order and in two phase to avoid deadlock:
    1.  in 1st phase, server looks up all locks necessary to the operation, may involve acquiring and
        releasing locks, examine their content
    2.  in 2nd phase, locks is sorted and acquired in order
    3.  if any file is modified since phase 1, release all locks and go to step 1
    4.  if not, perform the operation on cache and write logs, locks are retained until dirty blocks
        are written out to the disk
Appropriate to a workload in which concurrent shared writing is rare (not appropriate to e.g.
    MapReduce)

Each lock is leased with an expiration time of 30s, clients (Frangipani servers) must renew their
locks periodically, if failed to do so the clients invalidates its cache, return error to all
requests until the file system is unmounted.

Iterations of lock service:
    1.  a centralized lock server, causes large performance glitch on failure
    2.  on the same petal virtual disk, common case performance is poor
    3.  a distributed lock service based on Paxos
Locks are divided into 100 disjoint groups then assigned to lock servers, the global state
maintained by the consensus protocol consists of
    - a list of lock servers
    - assignment of lock group to lock servers
    - a list of operating clerks (lock module linked into Frangipani server)
Assignment of lock groups are adjusted  for load balance, the reassignment happens in 2 phases:
    1.  lock servers losing locks discard them from the internal state
    (some agreement achieved between 1 and 2?)
    2.  lock servers gaining locks contact clerks holding the locks, recover the state from them

Both the lock servers and the Petal servers can tolerate less than majority failures and network
partitions.  

Currently this scheme contains a bug: Petal servers doesn't check the up-to-date-ness of Frangipani
read / write locks, a write request issued before the lease expiration and arrived at a Petal server
after the expiration may cause inconsistency. A possible solution (not yet implemented) is tag each
request with an expiration timestamp.

Frangipani server can be added to the cluster with minimal administrative effort and removed
effortlessly. Both addition and removal of Petal servers and lock servers are transparent.

Petal async creates read-only snapshot of the whole virtual disk by copy-on-write, but snapshot
created this way is not consistent with the file system. Alternatively Frangipani may facilitate the
lock server to create a consistent sync snapshot.

Performance of Frangipani:
    - on a single machine: comparable performance to a local installation of AdvFS, both with and
        without NVRAM cache
    - on a single machine: comparable big file read / write throughput to AdvFS
    - on a single machine: comparable read / write latency to AdvFS, thanks to async handling of
        logs
    - adding Frangipani servers to a no-shared-write workload has minimal negative impact on latency 
    - uncached read scales almost linearly to the number of Frangipani servers
    - non-shared write scales gradually until the network is saturated
    - in presence of a writer, read scales much worse than linearly, flatted out at 2 readers with
        read-ahead (reader invalidates its cache when dropping the lock, read-ahead is unhelpful
        when there's contention)
    - in presence of a writer, reader performance improves as the amount of shared data decreases:
        even though reader invalidates its entire cache on dropping locks, writer would has less
        data to write out
    - in presence of multiple writers, write performance is abysmal as nearly each write syscall is
        also a fsync: lock is acquired and dropped during every write, causing dirty data to be
        write out
---NOTE END---