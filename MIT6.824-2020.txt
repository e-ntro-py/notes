Lecture notes will be taken from a video playlist of 2020 MIT 6.824 lectures, which are public, has
good audo quality and nicely segmented:
    https://www.youtube.com/watch?v=cQP8WApzIQQ&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB
but the skeleton code for labs in the 2020 repository doesn't work with the latest version of Golang
(1.19), the 2021 repository is used instead:
    git clone git://g.csail.mit.edu/6.824-golabs-2021 6.824
    
Golang notes
References:
    a tour of go: https://go.dev/tour/welcome/1
    std doc: https://pkg.go.dev/std
    effective go: https://go.dev/doc/effective_go

The official go std documentation cannot be searched alone: the result is always polluted by 3rd
party packages and cannot be filtered for std only. Use devdocs.io instead:
    https://devdocs.io/go/

By default every type in Golang is byte-copyable, certain types that cannot be safely cloned must be
immediately hidden behind a reference e.g. sync.Mutex 

Because `map[K]V` is magic, its documentation is not included in the documentation of Golang std. It
supports two operations:
    insert: map[key] = value
    get:    value, exists = map[key]

Only bare function calls can be deferred, channel insert, variable assignment and other statements
cannot be deferred. A possible solution is to wrap the statements in an immediate anonymous
function, javascript IIFE style:
    defer func() { c <- true }()

Golang integer literal in place of time.Duration is implicitly converted to nanoseconds, which means
    time.Sleep(1000)
put the thread to sleep for 1000 nanoseconds = 1 microsecond

Unintuitive for loop semantics:
    https://github.com/golang/go/discussions/56010

Golang doesn't have sum type nor proper enums: enums in Golang are new type constants sharing
namespace with any other constants in the same package, the obvious drawback of this approach is
that Golang compiler does not and cannot check if a value is valid for an enum. The following code
will not trigger any compiler error:
    type Bool int
    const (
        False   Bool = 0
        True    Bool = 1
    )
    var b Bool = 100
for the same reason Golang compiler cannot tell if a switch on an enum is exhaustive or not by
design.

Golang `range` iterator doesn not iterate on references: the value of the slice are copied to the
loop variable on start of each iteration, modification to the loop variable is not reflected to the
slice. 

Wrapper types (e.g. bufio.Writer) doesn't have a method exposing the underlying raw type, references
to the raw type have to be kept somewhere if clean up is necessary.

When passing a struct as RPC argument / reply, every single recursive field must be public
(Capitalized), otherwise ___the field will not be passed but zero-initialized at callee___
    type Args struct { V int }
will be passed normally, but
    type Args struct { v int }
means v == 0

Lecture 1. Introduction
Topics of distributed systems:
    - Scalability: 
        increase in the number of nodes (computers) in the system must be translated to increase in
        performance fairly linearly, not constrained by any bottlenecks
    - Fail tolerance:
        - Availability: 
            the system should keep operating under certain kind of failures on a small number of
            nodes
        - Recoverability: 
            the system should be able to recover from a failure without loss of correctness
    - Consistency:
        different flavors of consistency models, strongest consistency model (linearizability) is
        intentionally avoided because of costs

MapReduce: see also notes for Designing Data-Intensive Applications
---NOTE START---
The computing environment at Google can be characterized as:
    - commodity machines: Linux, 2-4 GB memory dual core x86 processor
    - commodity network: 100MB or 1GB interface, even less bisection bandwidth
    - hundreds to thousands of machines in a cluster, failures are common
    - inexpensive IDE hard drives, running distributed and replicated file system (GFS), on the same
        worker machine 
    - jobs are submitted to then mapped by a single scheduler 

Master tracks the state of each pair of Map and Reduce task:
    enum State {
        Idle,
        InProgress(WorkerId),
        Completed(WorkerId),
    }
for completed Map tasks the Master additionally stores the location and size of intermediate file
produced by a Map worker. The information is later propagated to in-progress reduce tasks.

Master pings workers periodically and deems them failed if no response for a certain period.
Completed Reduce task on failed workers doesn't have to be re-executed, however Map task on failed
workers are rescheduled even if it's completed because Reduce worker has to later access
intermediate data on the failed machine. Master is responsible for broadcasting the new Map worker 
to all relevant Reduce workers.

State of the Master is periodically backed up to the NV storage, failed master can restart from a
checkpoint or simply abort all ongoing MapReduce tasks.

When the tasks are not deterministic, the final output may not be consistent. Consider the following
sequence of events:
    1.  A single Map worker M1 produces two intermediate value R1 and R2 
    2.  Reduce worker 1 executes on R1, produces e(R1)
    3.  M1 fails, rescheduled and re-executed on M2, M2 produces R1' and R2'
    4.  Reduce worker 2 executes on R2', produces e(R2')
if Map task is not deterministic, there may be inconsistency among e(R1) and e(R2'). Otherwise
consistency is largely ensured by atomicity of operations on GFS.

The main bottleneck of the MapReduce system in Google was their network: Map function has to fetch
data from the underlying file system, Reduce function requires all value associated with the same
key that have to be collected from all Map workers. Capacity is only 50MB/s per machine on the top
level switch. The single leader in the system had to schedule works closed to the files they are
operating on.

Master spawns more Map and Reduce tasks (denoted by M and R) than available workers the same way
multi-thread algorithms spawn more threads than available parallelism to achieve load balance. In
practice a reasonable configuration has M = 200_000, R = 5000 on 2000 worker machines.

Master duplicates still in-progress tasks to different workers at the very end of a MapReduce
operation as a means to migrate from staggering workers, reportedly it improves performance of some
workload by 44%.

Refinements on the basic MapReduce framework:
    - User-defined partitioning function:
        user may control how output of Reduce are batched into files based on intermediate key
    - Ordering guarantees:
        Google implementation of MapReduces guarantees orders of key among a partition
    - Combiner function:
        execute after Map on Map worker, combine intermediate (key, value) pairs in similar fashion
        to the Reduce function, aimed to reduce network load
    - Custom input and output types:
        define how data is read from file or how data is written to output
    - Side effects:
        side product of Map / Reduce function to file, no consistency guarantees
    - Skipping bad records:
        exception handler on workers send the error raising record id to Master, if the same record
        id is encountered twice Master re-schedule the same task with the record skipped
    - Status information:
        Master report states through an internal HTTP server
    - Counters:
        named counter objects on workers propagated to Master in Ping response, a very limited form
        of logging
---NOTE END---

Lecture 2. RPC and threads
./MIT6.824-2020/questions/web_crawler.go

Reason to use threads:
    - IO Concurrency:
        allows the OS to better utilize CPU when part of the program is blocking on IO events (file
        read / write, mouse / keyboard input, etc.), may be solved otherwise by event-driven design
    - parallelism:
        threads can be assigned to and executed on different CPU cores
    - convenience:
        there's concepts that are most naturally expressed as threads, e.g. periodical jobs and
        watchdogs 

Most non-trivial instructions in x86 are not atomic, even seemingly simple instruction like INC is
translated to several micro codes hence vulnerable to race condition. Shared data has to be
explicitly atomic or protected by locks.

Reasons it's not always a good idea for data structures to manage their own locks thus hiding it
from users:
    1.  it's meaningless overhead when the data structure is not shared
    2.  when multiple data structures are interlinked, it's possible that the only way to solve
        potential deadlock is to lift their locks to a higher scope 
    
Synchronization primitives in Golang:
    - sync.Mutex:
        every day mutex, copying a sync.Mutex is unsafe after first use, most of the case should
        immediately be put behind a reference and never dereferenced
    - channels:
        by default has a capacity of 0, sending thread is blocked immediately, can be constructed
        with arbitrary capacity: make(chan T, CAPACITY)
    - sync.Cond:
        ordinary conditional variable with ordinary API, slightly generalized to wait on both mutex
        and RW lock
    - sync.WaitGroup:
        barrier implemented in Golang, API allows arbitrary delta to the counter
    
The Golang for loop semantic mentioned at 57:10 is about to be fixed:
    https://github.com/golang/go/discussions/56010
in some future version (maybe Go 2),
    vs := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
    rs := make([]*int, 0, 10)
    for _, v := range vs {
        rs = append(rs, &v)
    }
    for _, r := range rs {
        fmt.Println(*r)
    }
will print 1 to 10.

RPC (Remote Procedure Call) is only mentioned in 2021 lecture video:
    http://nil.csail.mit.edu/6.824/2021/schedule.html
Sequence of events of an RPC:
    1.  client calls the RPC, pass arguments to client-side stub function
    2.  client-side stub function marshalls (serializes) arguments to a message, type information
        along only if the message format is self describing (protobuf, the message format of gRPC,
        is not self-describing for example)
    3.  message passed to server, unmarshalled by server-side stub function
    4.  server-side stub function calls the actual target function 
    5.  server-side stub function marshalls the return value to a message
    6.  message passed to client, client-side stub function unmarshalls the message to values
    7.  client-side stub function returns value to client
most of the steps (stubs, communication protocol, message format) are handled by code auto-generated
from a schema.

RPC semantics under failures:
    - at least once:
        retry until the operations is verifiably performed at least once, only appropriate for
        idempotent operations e.g. insert into a map
    - at most once:
        duplicate requests are filtered based on something (unique tags)? in the message or a lower
        level (TCP?), server may still receive zero request, the RPC model of Golang
    - exactly once:
        expensive, rarely deployed
     
Server-side RPC framework must also serve as a local name server to distribute RPC calls to
corresponding stubs on the same server node. For a bigger RPC system in which calls may land on
different server nodes a dedicated name server is required.

Golang registers RPC services by runtime reflection magic:
    rpc.Server.Register registers method of a struct as RPC service iff the method:
        - is exported method of exported type
        - has two arguments, both of exported type
        - the second argument is a pointer
        - one return value, of type error
    the service name will be
        fmt.Sprintf("%s.%s", struct_type_name, struct_method_name)
    https://pkg.go.dev/net/rpc@go1.19.3#Server.Register

// why is arguments of put in kv.go hard coded to { "subject", "6.824" }?

// 2021 code example on synchronization primitives, not mentioned anywhere in the lectures
vote-count-1.go
    data race and spin wait

vote-count-2.go
    data race on `count` and `finished` prevented by a mutex
    still spin wait

vote-count-3.go
    spin wait on a lower resolution by unconditional time.Sleep

vote-count-4.go
    wait on conditional variable, finish condition only checked on worker termination. Unlike in
    other languages, Golang conditional variables do not suffer from spontaneous wake up
    https://pkg.go.dev/sync#Cond.Wait

vote-count-5.go
    no shared data, votes are passed back to master thread through a channel, channel function as
    both a synchronized data structure and a barrier 

vote-count-6.go
    patches a major bug in 5: collect all votes no matter the result, otherwise worker threads would
    block forever on 0 capacity channel
    now think about it, the channel should have a capacity of 10 so the worker threads never block

Lecture 3. GFS
paper: The Google File System, see notes for Designing Data-Intensive Applications
Question: Describe a sequence of events that would result in a client reading stale data from the
Google File System.
    1.  a chunk server C fails 
    2.  master detects the failure, re-replicate all its chunks on other chunk servers
    3.  a mutation happens on chunks originally replicated on C
    4.  C back online
    5.  a client with chunk C in its cache accesses staled data

Bad replication design: multiple servers, clients push request to all of them
    no global order of events, no consistency

On chunk server outage, Master cannot designate another primary for a chunk before its lease has
expired due to the risk of "spit brain", i.e. two primaries handling client requests in
un-synchronized fashion.

Lecture 4. Primary-Backup Replication
The Design of a Practical System for Fault-Tolerant Virtual Machines
---NOTE START---
Two approaches to keep a server and its backup synchronized:
    1.  replicate state of CPU, memory and IO devices, unrealistic because of the requirement of
        high bandwidth
    2.  start primary and backup at the same initial state then replicate deterministic execution,
        unrealistic on physical servers because of non-identical devices, much more manageable on
        VMs
This paper describes an implementation of the second approach (deterministic replay) on x86: VMWare
vSphere Fault Tolerance (FT). FT supports single core only, how to apply state transfer on
multi-core machines is an open question yet to be answered.

FT only attempts to deal with fail-stop failures, i.e. failures are detected before externally
visible incorrect actions

Primary and backup share nothing but a network storage, connected by a VM -> VM logging channel,
only primary is visible through network or to the users. Executions are replicated to the backup but
the outputs are dropped.

Challenges:
    1.  correctly capturing all the input and non-deterministic info necessary to ensure
        deterministic execution, x86 is non-deterministic (rdtsc, rdrand)
    2.  correctly apply captured input on the backup
    3.  minimal performance impact all the while

Output requirement, the fundamental requirement of FT:
    "if the backup VM ever takes over after a failure of the primary, the backup VM will continue
    executing in a way that is entirely consistent with all outputs that the primary VM has sent to
    the external world."

To ensure that, output is delayed until the backup has acknowledged receiving logs of all inputs
prior to and including the output operation. On failover (backup VM take over) the backup will
replay its logs up to the output operation then go live. Note that only the output but not the
execution of primary is delayed.

FT does not guarantee exactly-once output, application code should handle missing or duplicated
packets.

Failure detection:
    - UDP heartbeat between servers
    - VM monitoring log channel (should have steady stream of traffic because of timer interrupt)
    - atomic test-and-set (a lock) on shared storage to make sure no split brain

Steps to start an FT backup VM
    1.  copy the running primary VM, < 1 second pause with VMWare VMotion
    2.  choose a server connected to the same shared storage, again outsourced to some internal
        tools at VMWare
    3.  setup FT and log channel 

Cycle of an input event:
    1.  primary hypervisor receives input event 
    2.  primary hypervisor passes input event to primary VM, simultaneously pushes a log entry to
        the log buffer
    2.  primary hypervisor flushes log buffer to log channel 
    3.  backup hypervisor buffers log entries from log channel
    4.  backup hypervisor sends acknowledgement back to primary hypervisor
    5.  (maybe simultaneously with 4) backup hypervisor recreates the input event from the log
        entry, passes it to backup VM

Primary VM is paused when its log buffer is full, backup VM is paused when its log buffer is empty.
To avoid exceeding execution lag and, thereby, long failover lag, primary hypervisor monitors
execution lag as part of FT protocol and proceeds to throttle CPU allocation of primary VM.

Most VM operations, i.e. VM power off, CPU reallocation, and other operations beyond the scope of
guest OS, is logged as control entry and replicated by backup hypervisor. VMotion of backup VM
additionally requires cooperation from primary.

implementation issues for disk IOs:
    - concurrent writes are non-deterministic: 
        must be linearized
    - DMA on memory-mapped device visible to guest OS is data race: 
        write on hypervisor managed "bounce buffer" invisible to guest OS, copy to DMA buffer later
    - failed disk IOs are inconsistent: 
        IOs free from concurrency and data race is idempotent

implementation issued for network IOs:
    - asynchronous network operations are non-deterministic:
        asynchronous network operations disabled, always trap to hypervisor
    - synchronous network IO is slow and laggy:
        - batch outgoing / incoming packets
        - eliminate context switches by attaching log acknowledgement handling to TCP stack as
            callback
        - hypervisor flush log entries more frequently

FT with non-shared storage
    pros: 
        - disk IOs are no longer outputs, do not have to be delayed per Output Rule
        - primary and backup may be physically distanced
    cons:
        - extra expensive disk sync steps on start up
        - IO failures creates inconsistency
        - extra facility to deal with split brain 

backup VM read from disk:
    pros:
        - disk input from disk, much less traffic on log channel
    cons:
        - slower execution, disk read vs. network buffer read
        - read result must be transmitted whenever either primary or backup fails
        - primary write on the same location must be delayed

FT performance highlights:
    - CPU impact < 10%
    - log channel bandwidth < 20Mb/s for non-network applications
    - heavy impact on network bandwidth for network applications
---NOTE END---

question: How does VM FT handle network partitions?
    atomic test-and-set a flag on shared storage, only one server may go live

Primary-backup replication: deals with independent fail-stop faults on a single machine. 
Examples of correlated faults:
    - whole data center power shortage
    - whole data center overheat
    - earthquake
Examples of fail-stop faults: 
    - power shortage
    - network shortage
    - CPU overheat
    - kernel panic
    - (when coded to) network package error caught by checksum
    - (when coded to) disk error caught by checksum
Examples of not fail-stop faults 
    - logical software bugs
    - memory corruption
    - malicious intruders

Two replication schemes:
    1.  state transfer
        send the whole state (memory, disk, ..), or at least the delta, to backups
    2.  replicated state machine
        start from the same initial state, send only external inputs to backups

Characteristics of replication scheme:
    - what is the state to be replicated
    - how close is the primary and backups synced
    - how cut-off (failover) is performed
    - what's the visible anomaly during a cut-off
    - how new backups are set up

FT log entry contains:
    - instruction number (since backup start up) to insert interrupt
    - type and data of event
    - (if the operation is non-deterministic) result of the operation
FT requires CPU hardware support to insert interrupt on specific instruction number

Obviously FT backup VM executes up to the instruction number in the next log entry and blocks on an
empty log buffer, or it will overrun an input event yet to arrive.