// whenever possible, programming homeworks will be done in Rust instead of C.

4. The Abstraction: The Process
section 4.5, starting from Oct 2008, struct context in xv6 is:
    struct context {
        uint edi;
        uint esi;
        uint ebx;
        uint ebp;
        uint eip;
    };
ecx and edx are caller saved, esp the stack pointer is encoded in the address of the context, none
of them has to be saved explicitly

Questions
4.1
100%, no process issues IO, unless the kernel intentionally blocks execution for no reason the cpu
will always be assigned to a ready process 

4.2
// length of the IO operation determined by -L includes the tick in which the IO operation is
// issued, an IO operation of length 5 issued at tick 1 is finished at the start of tick 6, the
// entirety of tick 6 may be scheduled to some other process, but the IO operation is still only
// considered as complete at tick 6, not tick 5
10 cycles: 
    PID 0 is run to completion (4 cycles), context switch to PID 1
    PID 1 issues IO (1 cycle) 
    PID 1 blocks on IO (4 + 1 cycles, + 1 since the process is considered complete at tick 10)

4.3
6 cycles:
    PID 0 issues IO (1 cycle), blocks on IO, context switch
    PID 1 is run to complete (4 cycles)
    PID 0 blocks on IO (1 + 1 cycle)
The scheduler is not preemptive, a process will not be de-scheduled unless it invoked a syscall 

4.4
9 cycles:
    PID 0 issues IO (1 cycles)
    PID 0 blocks on IO (4 cycles), context switch at start of tick 5
    PID 1 runs to completion (4 cycles)

4.5
same to 4.3

4.6
27 cycles:
    PID 0 issues IO (1 cycle), context switch to PID 1
    PID 1 runs to completion (5 cycles), context switch to PID 2
    PID 2 runs to completion (5 cycles), context switch to PID 3
    PID 3 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle)
    PID 0 blocks on IO (4 cycles)
    PID 0 issues IO (1 cycle)
    PID 0 blocks on IO (4 + 1 cycles)

4.7
18 cycles:
    PID 0 issues IO (1 cycle), context switch to PID 1
    PID 1 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle), context switch to PID 2
    PID 2 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle), context switch to PID 3
    PID 3 runs to completion (5 cycles)
A process issued an IO operation is likely to issue more in near future (e.g. keyboard press in a
document processing software, mouse click in a GUI), immediately switch back to the process after IO
completion increases utilization of the IO device and CPU as well as responsiveness to user input.
In modern preemptive OS an IO heavy process may be assigned higher priority, compared to computation
bounded processes it will be scheduled more frequently but with smaller time quantum each time. 

4.8
in general, -I IO_RUN_IMMEDIATE should be roughly more efficient than -I IO_RUN_LATER, -S
SWITCH_ON_IO should be strictly more efficient than -S SWITCH_ON_END

Remaining questions: 
- after control being yielded to user process, how could a process be preempted without invoking
  syscalls by itself? (A: with interruption tied to a physical timer)

5.Interlude: Process API
wait() will return prematurely when:
    - the argument (*stat_loc) is not a proper pointer or NULL
    - the child process is stopped (not terminated) by a signal, in which case it can be restarted
      later

meaning of suffixes of exec:
    - l: variadic functions accepting arbitrary number of arguments as arguments to the program
    - p: search program in PATH environment variable
    - v: arguments
    - e: environment variables
    
Questions 
5.1
./ostep/chapter_5/q1.c
Fork() copies the entire state of the parent process to the child process, x as an auto variable on
the stack or in register is copied to the child process with the initial value 100. After that x in
child and parent processes are two different entities hence modification to them does not interfere
with each other.

5.2
./ostep/chapter_5/q2.c
the file descriptor is available to both parent and child processes, write from the parent process
always seems to happen before write from the child process, maybe fork() returns earlier to the
parent process compared to the child?

5.3
./ostep/chapter_5/q3.c
the parent process can sleep for a long enough duration, the two processes may communicate with some
form of shared memory, pipe, lock or signal, above all wait() still is the most natural way to
express this pattern.

5.4
./ostep/chapter_5/q4.c
All variants of exec are built upon a single syscall `execve`. At the beginning the C programming
language was an implementation detail of UNIX system, in the 80s C gained popularity among various
vendors despite having no standard and kept evolving that way even after ANSI C. UNIX before Linux
is a blanket term for a dozen competing incompatible operating systems. The current state of exec
variants may be an effort to maintain backward compatibility with all of them.

5.5
./ostep/chapter_5/q5.c
On success wait() returns process id of the terminated child. When the process calling wait() has no
child process ECHILD is assigned to the global variable errno.

5.6
./ostep/chapter_5/q6.c
Parent process block on the termination of a selected child process instead of any one of them.
Behavior of the function may be modified by the additional options argument, e.g. return immediately
when there's no terminated child process. 

5.7
./ostep/chapter_5/q7.c
printf() silently succeeds without actually printing anything to stdout.

5.8
./ostep/chapter_5/q8.c

6. Mechanism: Limited Direct Execution
Measurement (in WSL)
syscall:
    ./ostep/chapter_6/measure_syscall.c
    getpid(), 48 nanoseconds
context switch:
    ./ostep_chapter_6/measure_switch.c
    20 microseconds (without sched_setaffinity)
    1 microsecond (with sched_setaffinity)

7. Scheduling: Introduction
Proofs of the optimality of SJF and STCF can be found in problem 16-2 of introduction to algorithms

Questions
7.1
SJF:
    avg. turnaround: (200 + 400 + 600)/3 = 400
    avg. response: (0 + 200 + 400)/3 = 200
FIFO:
    same to SJF

7.2
SJF:
    avg. turnaround: (100 + 300 + 600)/3 = 333.33
    avg. response: (0 + 100 + 300)/3 = 133.33
FIFO:
    same to SJF

7.3
RR, same length:
    avg. turnaround: (598 + 599 + 600)/3 = 599
    avg. response: (0 + 1 + 2)/3 = 1
RR, different lengths:
    avg. turnaround: (298 + 499 + 600)/3 = 465.67
    avg. response: (0 + 1 + 2)/3 = 1

7.4
jobs ordered by ascending length, in other cases FIFO is not optimal

7.5
all except the longest job must be shorter than the quantum length:
    if there's two jobs with lengths longer than quantum length, no matter what is scheduled first
    the other will has a longer response time than in RR

7.6
response time in SJF is linear in job lengths
p./scheduler.py -p SJF -j 3 -l 3,2,1 -c
    Average -- Response: 1.33  Turnaround 3.33  Wait 1.33
p./scheduler.py -p SJF -j 3 -l 30,20,10 -c
    Average -- Response: 13.33  Turnaround 33.33  Wait 13.33
p./scheduler.py -p SJF -j 3 -l 300,200,100 -c
    Average -- Response: 133.33  Turnaround 333.33  Wait 133.33

7.7
response time with RR is linear in quantum length
let quantum length be T_q, worst response time is:
    (N - 1)T_q
    
8. Scheduling: The Multi-Level Feedback Queue
Questions
8.1
./mlfq.py -j 2 -n 2 -c -m 50 -M 0

8.2
Example 1: A Single Long-Running Job
    ./mlfq.py -n 3 -q 10 -l 0,200,0 -c
Example 2: Along Came A Short Job
    ./mlfq.py -n 3 -q 10 -l 0,200,0:100,20,0 -c
// io frequency is defined as "number of CPU ticks between two IO requests" in the simulator 
Example 3: What About I/O?
    ./mlfq.py -S -n 3 -q 10 -l 0,200,0:100,20,1.1 -c
Attempt #2: The Priority Boost
    without boost:
    ./mlfq.py -n 3 -q 10 -S -I -i 1 -l 0,200,0:100,50,1:100,50,1 -c
    with boost:
    ./mlfq.py -n 3 -q 10 -S -B 100 -I -i 1 -l 0,200,0:100,50,1:100,50,1 -c
Attempt #3: Better Accounting
    without allotment accounting:
    ./mlfq.py -q 10 -S -i 1 -l 0,200,0:100,100,9 -c
    with allotment accounting:
    ./mlfq.py -q 10 -i 1 -l 0,200,0:100,100,9 -c
Tuning MLFQ And Other Issues
    ./mlfq.py -Q 10,20,40 -l 0,140,0:0,140,0 -c

8.3
one queue, no allotment accounting, no IO reset
./mlfq.py -n 1 -j 10 -c -m 20

8.4
./mlfq.py -q 100 -S -i 1 -l 0,10000,99:0,10000,0 -c

8.5
200ms, assume after each boost the job will be run at least once (i.e. the system is not crowded
with high priority jobs)

8.6
lower response time in sacrifice of IO throughput. In CFQ (the default Linux IO scheduler between
2.6 - 5.0) the scheduler will introduce an artificial delay after an IO request in hope to catch more
IO requests from the same process to be performed in batch. Such a mechanism is majorly meaningless
with an SSD. CFQ was then removed from Linux in 5.0.

[B+18] “The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS”
additional details of CFS: 
    - fairness is ensured among applications instead of threads
    - a new thread is created with maximum vruntime in the queue
    - a thread waking up doesn't preempt the current thread if the vruntime is close enough
    - threads are distributed to CPUs by their utilizations 
    - load balancing happens every 4ms, cores steal threads from each other

9. Scheduling: Proportional Share
Weighting (Niceness), logarithm of prio_to_weight is roughly linear, i.e.
    log(Wi) - Log(Wj) ~ i - j
    e^(log(Wi) - log(Wj)) ~ e^(i - j)
    Wi / Wj ~ e^(i - j)
    
Remaining questions:
    - how are these dynamic data structures, especially the priority queue in stride scheduling
      implemented in the kernel where dynamic allocation is not taken as given?

Questions
9.1
// too long, omitted 

9.2
Job 0 has 1% chance to be run before completion of job 1. With such imbalance in ticket distribution
the scheduler is no longer fair or even preemptive.

9.3
Finish time T of the first job is governed by the negative binomial distribution with a constraint
    X ~ NB(100, 0.5), X <= 200 
    E[X] = Σ_{k = 0}^100 C(k + 100, k) * 0.5^100 * 0.5^k * (100 + k)
By WolframAlpha the expectation is around 189.67, expected fairness is 189.67/200 = 0.95

9.4
fairness decreases as quantum increases, when -q 2 the distribution is equivalent to
    X ~ NB(50, 0.5), X <= 100
    E[X] = 92.96, fairness = 0.93
when -q 5,
    X ~ NB(20, 0.5), X <= 40
    E[X] = 35.86, fairness = 0.90
when -q 10,
    X ~ NB(10, 0.5), X <= 20
    E[X] = 17.30, fairness = 0.87
when -q 100, the first job runs to completion before the start of the other, fairness = 0.5

9.5
The unfairness by low resolution impacts stride scheduler as well but the difference between finish
time will be limited to one quantum. Another interesting topic would be the balance between
resolution and scheduling efficiency, which is beyond the capacity of this simulator.

10. Multiprocessor Scheduling
Questions
10.1
30 ticks, no other threads competing for CPUs, the cache is too small for the job to run efficiently

10.2
20 ticks, 10 cold tick and 10 warm ticks

10.3
the job runs twice as fast in warm mode, as the default value of -r.

10.4
by the trace
    9   a [ 20] cache[w]
the cache was warmed at the end of 10th tick, as the default value of -w.

10.5
150 ticks, the jobs are scheduled as
    CPU 0   ACBA...
    CPU 1   BACB...
as A has a working set as big as the cache, it invalidates all previous content in the cache, hence
no jobs are running in warm mode ever.

10.6
(10 + 90/2) * 2 = 110 ticks, this time a, b and c run in warm mode after the first 10 ticks, where b
and c shares the same CPU, the current affinity is the only one where
    1.  all CPUs are working
    2.  all working sets fit in the cache
therefore there's no other affinity setting that runs faster than the current one.

10.7
-M 50
    -n 1: 300 ticks
    -n 2: 150 ticks
    -n 3: 100 ticks
-M 100
    -n 1: 300 ticks
    -n 2: 150 ticks
    -n 3: 55 ticks
only with -M 100 -n 3 every job will run in warm mode, 10 + (90 / 2) = 55 ticks

10.8
100 ticks instead of 110, CPU utilization is slightly better
with no job stealing (-P 0) the running time is 200 ticks, with more job stealing (-P 1) the running
time improves slightly to 95 ticks as the simulator doesn't account for context switch and lock
costs. Per CPU scheduler scales worse than the single queue round-robin scheduler as the simulator
suggests, yet again because the simulator is over-simplified.

10.9
// omitted

13. The Abstraction: Address Spaces
Questions
13.1
free has an option to print memory in power of 1000 instead of 1024. Unlike HDDs Memory nowadays is
never marketed or discussed in power of 1000, maybe that's not yet an universal agreement back when
free is created.

13.2
WSL doesn't have full access to physical memory, out of 16GB installed 4GB is invisible inside WSL.
100MB used memory among which 2.4MB is reported by top, leaves 97.6MB occupied by the kernel.

13.3
how to use:
    install the x86_64-unknown-linux-musl build target by:
        rustup target add x86_64-unknown-linux-musl
    build with musl target and rust-lld:
        rustc --target x86_64-unknown-linux-musl -C linker=rust-lld memory-user.rs
    run the memory user in background:
        ./memory-user 100 &

13.4
A certain amount of memory is occupied immediately at the beginning of execution, for large
allocations the used amount of memory gradually increases over time. Both case the used memory is
freed the instance the program is killed. A page is only written out after the first write, in this
case by compiler magic the zero-initialization is not treated as a write.

13.5
The behavior of pmap is controlled by a config file (~/.pmaprc by default) instead of command line
arguments. As the format of its output varies more frequent than other UNIX tools it seems to be a
reasonable design.

13.6
// not a question

13.7
result of zsh:
    hundreds of segments, mostly shared libraries and memory mapped files 
    2964K r---- locale-archive  // memory-mapped localized strings used by the system
    2816K rw---   [ anon ]  // heap?
    2364K r--s- Unix.zwc    // auto-completion utilities of unix tools
    316K rw---   [ stack ]  // stack

13.8
result of memory-user:
    10+ segments, no shared libraries, libc and std statically linked 
    00007fde2f299000 16384004K rw---   [ anon ]
    00007fe2172b4000    292K r-x-- memory-user
    00007ffc31960000    136K rw---   [ stack ]

14. Interlude: Memory API
Questions
14.1
segmentation fault

14.2
Program received signal SIGSEGV, Segmentation fault.
0x0000555555555150 in main (argc=1, argv=0x7fffffffd6f8) at null.c:7
7           printf("%u", *ptr);

14.3
==1379== Invalid read of size 4
==1379==    at 0x109150: main (null.c:7)
==1379==  Address 0x0 is not stack'd, malloc'd or (recently) free'd

14.4
gdb didn't find any problem. Report from valgrind:
==1507== 40 bytes in 1 blocks are definitely lost in loss record 1 of 1
==1507==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1507==    by 0x10914D: main (leak.c:5)

14.5
GDB didn't find any problem. Report from valgrind:
==1727== Invalid write of size 4
==1727==    at 0x10916C: main (q5.c:6)
==1727==  Address 0x4a0e1d0 is 0 bytes after a block of size 400 alloc'd
==1727==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1727==    by 0x10915D: main (q5.c:5)
This is rather concerning. The allocator must have allocated a memory block larger than requested,
probably the next power of 2 (128 bytes), and GDB as the language agnostic debugger it is didn't
notice the original allocation size in C code.

14.6
GDB didn't find any problem. Report from valgrind:
==1898== Invalid read of size 4
==1898==    at 0x109182: main (q6.c:9)
==1898==  Address 0x4a0e040 is 0 bytes inside a block of size 400 free'd
==1898==    at 0x48369AB: free (vg_replace_malloc.c:530)
==1898==    by 0x10917D: main (q6.c:7)
==1898==  Block was alloc'd at
==1898==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1898==    by 0x10916D: main (q6.c:6)
Even more concerning. The allocator must didn't immediately return the allocated memory to the
system with brk, rather it reserves the memory for further allocation. GDB again was totally fine
with this illegal access to freed memory. Valgrind, however, hijacks calls to malloc and free to run
on its own allocator, thus had a chance to do more precise memory analysis.

14.7
GDB:
    Starting program: /mnt/d/Software/Textbook/playground/notes/ostep/chapter_14/main
    free(): invalid pointer
Valgrind:
    ==2063== Invalid free() / delete / delete[] / realloc()
    ==2063==    at 0x48369AB: free (vg_replace_malloc.c:530)
    ==2063==    by 0x109171: main (q7.c:6)
    ==2063==  Address 0x4a0e068 is 40 bytes inside a block of size 400 alloc'd
    ==2063==    at 0x483577F: malloc (vg_replace_malloc.c:299)
    ==2063==    by 0x10915D: main (q7.c:5)

14.8
./ostep/chapter_14/q8.c

14.9
... or create a saner interface to heap allocated structures, one that automatically frees the
memory when it's no longer needed, does bound check on each access, tracks the lifetime of the
references so the freed memory cannot be accessed again. Or even better, create a new language in
which these rules are enforced at compilation.

15. Mechanism: Address Translation
Questions 
15.1
-s 0
    0x000001ae --> 0x00003230
    0x00000109 --> 0x0000318b
    0x0000020b --> SIGSIGV
    0x0000019e --> 0x00003220
    0x00000322 --> SIGSIGV
-s 1
    0x0000030e --> SIGSIGV
    0x00000105 --> 0x00003741
    0x000001fb --> SIGSIGV
    0x000001cc --> SIGSIGV
    0x0000029b --> SIGSIGV
-s 2
    0x00000039 --> 0x00003ce2
    0x00000056 --> 0x00003cee
    0x00000357 --> SIGSIGV
    0x000002f1 --> SIGSIGV
    0x000002ad --> SIGSIGV

15.2
max(VA) + 1 = 930, +1 since limit is the length of the segment, limit must be one bigger than the
biggest offset to accommodate all the virtual addresses.

15.3
16 * 1024 - (100 - 1) = 0x00003f9d
irrelevant to virtual addresses, the size of data stored on these addresses are not specified

15.4
// skipped

15.5
fix base register and physical memory size, the expectation of fraction of randomly generated
addresses is exactly
    value of limit register / size of address space 

16. Segmentation
Questions
16.1
for a positively growing segment,
    PA = offset(VA) + base if offset(VA) < limit
for a negatively growing segment,
    PA = base - ((1 << OFFSET_BITS) - offset(VA)) if (1 << OFFSET_BITS) - offset(VA) <= limit
(0 ..= 63) --> Segment 0
(64 ..= 127) --> Segment 1
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 0
    VA  0: 0x0000006c (decimal:  108) --> seg 1, 0x000001ec
    VA  1: 0x00000061 (decimal:   97) --> seg 1, SIGSIGV
    VA  2: 0x00000035 (decimal:   53) --> seg 0, SIGSIGV
    VA  3: 0x00000021 (decimal:   33) --> seg 0, SIGSIGV
    VA  4: 0x00000041 (decimal:   65) --> seg 1, SIGSIGV
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 1
    VA  0: 0x00000011 (decimal:   17) --> seg 1, 0x00000011
    VA  1: 0x0000006c (decimal:  108) --> seg 0, 0x000001ec
    VA  2: 0x00000061 (decimal:   97) --> seg 1, SIGSIGV
    VA  3: 0x00000020 (decimal:   32) --> seg 0, SIGSIGV
    VA  4: 0x0000003f (decimal:   63) --> seg 0, SIGSIGV
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2
Virtual Address Trace
    VA  0: 0x0000007a (decimal:  122) --> seg 1, 0x000001fa
    VA  1: 0x00000079 (decimal:  121) --> seg 1, 0x000001f9
    VA  2: 0x00000007 (decimal:    7) --> seg 0, 0x00000007
    VA  3: 0x0000000a (decimal:   10) --> seg 0, 0x0000000a
    VA  4: 0x0000006a (decimal:  106) --> seg 1, SIGSIGV

16.2
highest legal virtual address in segment 0
    0x0000013
lowest legal virtual address in segment 1
    0x00001ec
lowest illegal address
    0x000006c
highest illegal address
    0x000006b
// Modified the script to recognize hexadecimal addresses. The script is typical dynamic typed
// madness, the variable called `vStr` may actually be an integer, when instanceof(vStr, int) is
// false the string can be parsed with int(vStr, 0) which infers the base from the prefix.
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -A 0x13,0x14,0x6b,0x6c -c

16.3
--b0 0 --l0 2 --b1 (anything >= 4) --l1 2

16.4
SIZE=128;LIMIT=$(($SIZE / 2 * 9 / 10));./segmentation.py -a $SIZE -p 512 -b 0 -l $LIMIT -B 512 -L

16.5
./segmentation.py -a 128 -p 512 -b 0 -l 0 -B 512 -L 0 -c

17. Free-space Management
"The solution is simple: go through the list and merge neighboring chunks; when finished, the heap
will be whole again."
This coalesce algorithm is O(n). A common O(1) solution to the coalesce problem is to write not only
a header, but also a footer to the allocated chunk, hence on freeing a chunk the allocator may
access both the chunks before and after the chunk and perform the necessary coalesce.

[S15] "Understanding glibc malloc" by Sploitfun
---NOTES START---
ptmalloc2, the allocator in glibc since 2006, maintains multiple arenas for different threads, up to
a limit which is 8 * NUMBER_OF_CPU on 64-bit platforms, after that threads have to share arenas as
in the older dlmalloc.

The arena in the main thread is created with sbrk, all other thread arenas are created by mmap. Both
requires more than necessary space from the operating system, also do not return space immediately
to the operating system when it's free'd. Arenas grow whenever a malloc request cannot be fulfilled.

The allocator maintains three kinds of data structures:
    - heap_info: the heap header. There may be multiple heaps (contiguous chunk of memory in virtual
      memory space) in a single arena, as the arena grows overtime as a result of malloc requests,
      the memory under a single arena may not be contiguous. Main thread doesn't have multiple heaps
      (it's memory can always be moved to a larger contiguous chunk with sbrk)
    - malloc_state: the arena header. contains information about bins, top chunk, last remainder
      chunk, etc. For thread arenas malloc_state is part of the heap, while malloc_state of the main
      thread is a global variable of glibc.
    - malloc_chunk: the chunk header. Different type of chunks has different headers.

Small enough chunks (16 - 64 bytes) are kept in their own free lists, or "fast bins" in glibc term.
They are not coalesced nor sorted on free, just goes back the bin they came from. Each bin holds
free chunks of the same size, 8 bytes apart in size from one bin to another.

Chunks of other sizes when free'd are first added to an "unsorted bin", allowing them to be reused
quickly. Small Bins (< 512 bytes) are 8 bytes apart. Large Bins (>= 512 bytes) have exponentially
increasing sizes and chunks in each bin do not have the same size. Both of them are coalesced when
two free chunks are next to each other. 

Chunk at the top border (the last in virtual address, the initial free chunk) is called top chunk,
top chunk is not in any bins, large and top chunks are the only chunks that will be split on user
request. Top chunk is split when there's no other free blocks in any of the bins.

Last Remainder Chunk points to the last free chunk as the result of a split. Last Remainder Chunk is
prioritized in the unsorted bin to serve consecutive small allocations. These requests are more
likely to be allocated next to each other and coalesced back to a single chunk on free if Last
Reminder Chunk is split on consecutive calls to malloc. 
---NOTES END---

Questions
17.1
./malloc.py -n 10 -H 0 -p BEST -s 0
10 operations, no headers, best fit, no coalesce
ptr[0] = Alloc(3)
    return 1000
    List [(1003, 97)]
Free(ptr[0])
    return 0
    List [(1000, 3), (1003, 97)]
ptr[1] = Alloc(5)
    return 1003
    List [(1000, 3), (1008, 92)]
Free(ptr[1])
    return 0
    List [(1000, 3), (1003, 5), (1008, 92)]
ptr[2] = Alloc(8)
    return 1008
    List [(1000, 3), (1003, 5), (1016, 84)]
Free(ptr[2])
    return 0
    List [(1000, 3), (1003, 5), (1008, 8), (1016, 84)]
ptr[3] = Alloc(8)
    return 1008
    List [(1000, 3), (1003, 5), (1016, 84)]
free(ptr[3])
    return 0
    List [(1000, 3), (1003, 5), (1008, 8), (1016, 84)]
ptr[4] = Alloc(2)
    return 1000
    List [(1002, 1), (1003, 5), (1008, 8), (1016, 84)]
ptr[5] = Alloc(7)
    return 1008
    List [(1002, 1), (1003, 5), (1015, 1), (1016, 84)]
the free space is fragmented badly, neighboring free chunks are not coalesced 

17.2
Free list at the end
    [ addr:1000 sz:3 ] [ addr:1003 sz:5 ] [ addr:1008 sz:8 ] [ addr:1016 sz:8 ] [ addr:1033 sz:67 ]
Free space are even more fragmented, the biggest chunk is now 76 bytes instead of 84, other small
fragments increased in size.

17.3
Free space is fragmented much as the BEST strategy but not as bad as WORST. Allocation is still O(n)
but should be much faster than BEST and WORST, allocator no longer has to inspect the entire free
list.

17.4
allocation time comparison
-l ADDRSORT
    FIRST < BEST = WORST
-l SIZESORT+
    FIRST = BEST < WORST
-l SIZESORT-
    FIRST = WORST < BEST

17.5
With no coalesce most allocation in the long run fails, the free space is fragmented to the extreme
that it contains 100 chunks of length 1. With coalesce most allocation still succeeds in long run,
the free list never grows beyond 10 nodes. 

17.6
In theory FIRST fit with SIZESORT+ is equivalent to BEST fit, FIRST fit with SIZESORT- is equivalent
to BEST fit, while ordering should not affect the performance of BEST and WORST by too much (maybe
some minor effect when the order is used to break ties), but the simulator is written in a poor way
that the free space will not be correctly coalesced unless the free list is ordered by ADDRSORT,
both SIZESORT- and SIZESORT+ will cause more fragmentation.

17.7
For any fit strategy and free list order, 
    1.  allocate a series of chunks of size (n + 1) until the free space is totally exhausted
        (assuming the size of free space is a multiple of (n + 1))
    2.  for each chunk allocated at step 1
        - free a chunk of size (n + 1)
        - allocate a chunk of size n 
        - allocate a chunk of size 1
    3.  free all chunks of size n
at the end, only 1/(n + 1) of the free space is used, nevertheless there's no free node with length
longer than n.

18. Paging: Introduction
Questions
18.1
-a ASIZE
    address space size
-P PAGESIZE
    page size
page table size = ASIZE / PAGESIZE
larger pages incur more internal fragmentation

18.2
1k page size, lowest 10 bits are page offset
16k / 1k = 16 = 2^4, highest 4 bits are virtual page number
-u 50
    VA 0x00003385 (decimal:    13189) --> (VPN 12)  0x00003f85
    VA 0x0000231d (decimal:     8989) --> (VPN 8)   INVALID
    VA 0x000000e6 (decimal:      230) --> (VPN 0)   0x000060e6
    VA 0x00002e0f (decimal:    11791) --> (VPN 11)  INVALID
    VA 0x00001986 (decimal:     6534) --> (VPN 6)   0x00007586
more virtual addresses are valid as -u increases

18.3
-P 8 -a 32 -p 1024 -v -s 1
    page size too small, virtual address space too small compared to physical address space, most of
    physical address space is not addressable from a single process
-P 8k -a 32k -p 1m -v -s 2
    nothing different, only the page size is more reasonable
-P 1m -a 256m -p 512m -v -s 3
    overall reasonable, page size too large for light threads

18.4
-a 32k -p 16k
    Error: physical memory size must be GREATER than address space size (for this simulation)
-p 2g
    Error: must use smaller sizes (less than 1 GB) for this simulation.
-a 5k
    Error in argument: address space must be a power of 2
-P 5k
    Error in argument: page size must be a power of 2
-a 2k -P 4k
    // page size should not be greater than virtual address space size, didn't catch the error 

19. Paging: Faster Translations (TLBs)
Questions
19.1 
Instant::now() timer in Rust std, backed by clock_gettime(CLOCK_MONOTONIC) on Linux. The resolution
reported by clock_getres() is 1ns, but after that much patches to Spectre and other vulnerabilities
there's no way any system timer can still have sub-microsecond precision. Without any knowledge of
the deviation of the timer nor the memory operation I'd say at least a few thousand iterations would
be necessary.

19.2
how to use:
    install the x86_64-unknown-linux-musl build target by:
        rustup target add x86_64-unknown-linux-musl
    build with musl target and rust-lld:
        rustc --target x86_64-unknown-linux-musl -C linker=rust-lld -O -o tlb-musl tlb.rs
    run in linux:
        ./tlb-musl NUM_OF_PAGES PEPEAT
    build on windows:
        rustc --target x86_64-pc-windows-msvc -O -o tlb-msvc tlb.rs
    run in windows:
        ./tlb-msvc NUM_OF_PAGES PEPEAT

19.3
