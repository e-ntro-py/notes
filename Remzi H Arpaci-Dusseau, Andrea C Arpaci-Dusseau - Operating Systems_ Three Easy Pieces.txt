// whenever possible, programming homeworks will be done in Rust instead of C.

4. The Abstraction: The Process
section 4.5, starting from Oct 2008, struct context in xv6 is:
    struct context {
        uint edi;
        uint esi;
        uint ebx;
        uint ebp;
        uint eip;
    };
ecx and edx are caller saved, esp the stack pointer is encoded in the address of the context, none
of them has to be saved explicitly

Questions
4.1
100%, no process issues IO, unless the kernel intentionally blocks execution for no reason the cpu
will always be assigned to a ready process 

4.2
// length of the IO operation determined by -L includes the tick in which the IO operation is
// issued, an IO operation of length 5 issued at tick 1 is finished at the start of tick 6, the
// entirety of tick 6 may be scheduled to some other process, but the IO operation is still only
// considered as complete at tick 6, not tick 5
10 cycles: 
    PID 0 is run to completion (4 cycles), context switch to PID 1
    PID 1 issues IO (1 cycle) 
    PID 1 blocks on IO (4 + 1 cycles, + 1 since the process is considered complete at tick 10)

4.3
6 cycles:
    PID 0 issues IO (1 cycle), blocks on IO, context switch
    PID 1 is run to complete (4 cycles)
    PID 0 blocks on IO (1 + 1 cycle)
The scheduler is not preemptive, a process will not be de-scheduled unless it invoked a syscall 

4.4
9 cycles:
    PID 0 issues IO (1 cycles)
    PID 0 blocks on IO (4 cycles), context switch at start of tick 5
    PID 1 runs to completion (4 cycles)

4.5
same to 4.3

4.6
27 cycles:
    PID 0 issues IO (1 cycle), context switch to PID 1
    PID 1 runs to completion (5 cycles), context switch to PID 2
    PID 2 runs to completion (5 cycles), context switch to PID 3
    PID 3 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle)
    PID 0 blocks on IO (4 cycles)
    PID 0 issues IO (1 cycle)
    PID 0 blocks on IO (4 + 1 cycles)

4.7
18 cycles:
    PID 0 issues IO (1 cycle), context switch to PID 1
    PID 1 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle), context switch to PID 2
    PID 2 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle), context switch to PID 3
    PID 3 runs to completion (5 cycles)
A process issued an IO operation is likely to issue more in near future (e.g. keyboard press in a
document processing software, mouse click in a GUI), immediately switch back to the process after IO
completion increases utilization of the IO device and CPU as well as responsiveness to user input.
In modern preemptive OS an IO heavy process may be assigned higher priority, compared to computation
bounded processes it will be scheduled more frequently but with smaller time quantum each time. 

4.8
in general, -I IO_RUN_IMMEDIATE should be roughly more efficient than -I IO_RUN_LATER, -S
SWITCH_ON_IO should be strictly more efficient than -S SWITCH_ON_END

Remaining questions: 
- after control being yielded to user process, how could a process be preempted without invoking
  syscalls by itself? (A: with interruption tied to a physical timer)

5.Interlude: Process API
wait() will return prematurely when:
    - the argument (*stat_loc) is not a proper pointer or NULL
    - the child process is stopped (not terminated) by a signal, in which case it can be restarted
      later

meaning of suffixes of exec:
    - l: variadic functions accepting arbitrary number of arguments as arguments to the program
    - p: search program in PATH environment variable
    - v: arguments
    - e: environment variables
    
Questions 
5.1
./ostep/chapter_5/q1.c
Fork() copies the entire state of the parent process to the child process, x as an auto variable on
the stack or in register is copied to the child process with the initial value 100. After that x in
child and parent processes are two different entities hence modification to them does not interfere
with each other.

5.2
./ostep/chapter_5/q2.c
the file descriptor is available to both parent and child processes, write from the parent process
always seems to happen before write from the child process, maybe fork() returns earlier to the
parent process compared to the child?

5.3
./ostep/chapter_5/q3.c
the parent process can sleep for a long enough duration, the two processes may communicate with some
form of shared memory, pipe, lock or signal, above all wait() still is the most natural way to
express this pattern.

5.4
./ostep/chapter_5/q4.c
All variants of exec are built upon a single syscall `execve`. At the beginning the C programming
language was an implementation detail of UNIX system, in the 80s C gained popularity among various
vendors despite having no standard and kept evolving that way even after ANSI C. UNIX before Linux
is a blanket term for a dozen competing incompatible operating systems. The current state of exec
variants may be an effort to maintain backward compatibility with all of them.

5.5
./ostep/chapter_5/q5.c
On success wait() returns process id of the terminated child. When the process calling wait() has no
child process ECHILD is assigned to the global variable errno.

5.6
./ostep/chapter_5/q6.c
Parent process block on the termination of a selected child process instead of any one of them.
Behavior of the function may be modified by the additional options argument, e.g. return immediately
when there's no terminated child process. 

5.7
./ostep/chapter_5/q7.c
printf() silently succeeds without actually printing anything to stdout.

5.8
./ostep/chapter_5/q8.c

6. Mechanism: Limited Direct Execution
Measurement (in WSL)
syscall:
    ./ostep/chapter_6/measure_syscall.c
    getpid(), 48 nanoseconds
context switch:
    ./ostep_chapter_6/measure_switch.c
    20 microseconds (without sched_setaffinity)
    1 microsecond (with sched_setaffinity)

7. Scheduling: Introduction
Proofs of the optimality of SJF and STCF can be found in problem 16-2 of introduction to algorithms

Questions
7.1
SJF:
    avg. turnaround: (200 + 400 + 600)/3 = 400
    avg. response: (0 + 200 + 400)/3 = 200
FIFO:
    same to SJF

7.2
SJF:
    avg. turnaround: (100 + 300 + 600)/3 = 333.33
    avg. response: (0 + 100 + 300)/3 = 133.33
FIFO:
    same to SJF

7.3
RR, same length:
    avg. turnaround: (598 + 599 + 600)/3 = 599
    avg. response: (0 + 1 + 2)/3 = 1
RR, different lengths:
    avg. turnaround: (298 + 499 + 600)/3 = 465.67
    avg. response: (0 + 1 + 2)/3 = 1

7.4
jobs ordered by ascending length, in other cases FIFO is not optimal

7.5
all except the longest job must be shorter than the quantum length:
    if there's two jobs with lengths longer than quantum length, no matter what is scheduled first
    the other will has a longer response time than in RR

7.6
response time in SJF is linear in job lengths
p./scheduler.py -p SJF -j 3 -l 3,2,1 -c
    Average -- Response: 1.33  Turnaround 3.33  Wait 1.33
p./scheduler.py -p SJF -j 3 -l 30,20,10 -c
    Average -- Response: 13.33  Turnaround 33.33  Wait 13.33
p./scheduler.py -p SJF -j 3 -l 300,200,100 -c
    Average -- Response: 133.33  Turnaround 333.33  Wait 133.33

7.7
response time with RR is linear in quantum length
let quantum length be T_q, worst response time is:
    (N - 1)T_q
    
8. Scheduling: The Multi-Level Feedback Queue
Questions
8.1
./mlfq.py -j 2 -n 2 -c -m 50 -M 0

8.2
Example 1: A Single Long-Running Job
    ./mlfq.py -n 3 -q 10 -l 0,200,0 -c
Example 2: Along Came A Short Job
    ./mlfq.py -n 3 -q 10 -l 0,200,0:100,20,0 -c
// io frequency is defined as "number of CPU ticks between two IO requests" in the simulator 
Example 3: What About I/O?
    ./mlfq.py -S -n 3 -q 10 -l 0,200,0:100,20,1.1 -c
Attempt #2: The Priority Boost
    without boost:
    ./mlfq.py -n 3 -q 10 -S -I -i 1 -l 0,200,0:100,50,1:100,50,1 -c
    with boost:
    ./mlfq.py -n 3 -q 10 -S -B 100 -I -i 1 -l 0,200,0:100,50,1:100,50,1 -c
Attempt #3: Better Accounting
    without allotment accounting:
    ./mlfq.py -q 10 -S -i 1 -l 0,200,0:100,100,9 -c
    with allotment accounting:
    ./mlfq.py -q 10 -i 1 -l 0,200,0:100,100,9 -c
Tuning MLFQ And Other Issues
    ./mlfq.py -Q 10,20,40 -l 0,140,0:0,140,0 -c

8.3
one queue, no allotment accounting, no IO reset
./mlfq.py -n 1 -j 10 -c -m 20

8.4
./mlfq.py -q 100 -S -i 1 -l 0,10000,99:0,10000,0 -c

8.5
200ms, assume after each boost the job will be run at least once (i.e. the system is not crowded
with high priority jobs)

8.6
lower response time in sacrifice of IO throughput. In CFQ (the default Linux IO scheduler between
2.6 - 5.0) the scheduler will introduce an artificial delay after an IO request in hope to catch more
IO requests from the same process to be performed in batch. Such a mechanism is majorly meaningless
with an SSD. CFQ was then removed from Linux in 5.0.

[B+18] “The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS”
additional details of CFS: 
    - fairness is ensured among applications instead of threads
    - a new thread is created with maximum vruntime in the queue
    - a thread waking up doesn't preempt the current thread if the vruntime is close enough
    - threads are distributed to CPUs by their utilizations 
    - load balancing happens every 4ms, cores steal threads from each other

9. Scheduling: Proportional Share
Weighting (Niceness), logarithm of prio_to_weight is roughly linear, i.e.
    log(Wi) - Log(Wj) ~ i - j
    e^(log(Wi) - log(Wj)) ~ e^(i - j)
    Wi / Wj ~ e^(i - j)
    
Remaining questions:
    - how are these dynamic data structures, especially the priority queue in stride scheduling
      implemented in the kernel where dynamic allocation is not taken as given?

Questions
9.1
// too long, omitted 

9.2
Job 0 has 1% chance to be run before completion of job 1. With such imbalance in ticket distribution
the scheduler is no longer fair or even preemptive.

9.3
Finish time T of the first job is governed by the negative binomial distribution with a constraint
    X ~ NB(100, 0.5), X <= 200 
    E[X] = Σ_{k = 0}^100 C(k + 100, k) * 0.5^100 * 0.5^k * (100 + k)
By WolframAlpha the expectation is around 189.67, expected fairness is 189.67/200 = 0.95

9.4
fairness decreases as quantum increases, when -q 2 the distribution is equivalent to
    X ~ NB(50, 0.5), X <= 100
    E[X] = 92.96, fairness = 0.93
when -q 5,
    X ~ NB(20, 0.5), X <= 40
    E[X] = 35.86, fairness = 0.90
when -q 10,
    X ~ NB(10, 0.5), X <= 20
    E[X] = 17.30, fairness = 0.87
when -q 100, the first job runs to completion before the start of the other, fairness = 0.5

9.5
The unfairness by low resolution impacts stride scheduler as well but the difference between finish
time will be limited to one quantum. Another interesting topic would be the balance between
resolution and scheduling efficiency, which is beyond the capacity of this simulator.

10. Multiprocessor Scheduling
Questions
10.1
30 ticks, no other threads competing for CPUs, the cache is too small for the job to run efficiently

10.2
20 ticks, 10 cold tick and 10 warm ticks

10.3
the job runs twice as fast in warm mode, as the default value of -r.

10.4
by the trace
    9   a [ 20] cache[w]
the cache was warmed at the end of 10th tick, as the default value of -w.

10.5
150 ticks, the jobs are scheduled as
    CPU 0   ACBA...
    CPU 1   BACB...
as A has a working set as big as the cache, it invalidates all previous content in the cache, hence
no jobs are running in warm mode ever.

10.6
(10 + 90/2) * 2 = 110 ticks, this time a, b and c run in warm mode after the first 10 ticks, where b
and c shares the same CPU, the current affinity is the only one where
    1.  all CPUs are working
    2.  all working sets fit in the cache
therefore there's no other affinity setting that runs faster than the current one.

10.7
-M 50
    -n 1: 300 ticks
    -n 2: 150 ticks
    -n 3: 100 ticks
-M 100
    -n 1: 300 ticks
    -n 2: 150 ticks
    -n 3: 55 ticks
only with -M 100 -n 3 every job will run in warm mode, 10 + (90 / 2) = 55 ticks

10.8
100 ticks instead of 110, CPU utilization is slightly better
with no job stealing (-P 0) the running time is 200 ticks, with more job stealing (-P 1) the running
time improves slightly to 95 ticks as the simulator doesn't account for context switch and lock
costs. Per CPU scheduler scales worse than the single queue round-robin scheduler as the simulator
suggests, yet again because the simulator is over-simplified.

10.9
// omitted

13. The Abstraction: Address Spaces
Questions
13.1
free has an option to print memory in power of 1000 instead of 1024. Unlike HDDs Memory nowadays is
never marketed or discussed in power of 1000, maybe that's not yet an universal agreement back when
free is created.

13.2
WSL doesn't have full access to physical memory, out of 16GB installed 4GB is invisible inside WSL.
100MB used memory among which 2.4MB is reported by top, leaves 97.6MB occupied by the kernel.

13.3
how to use:
    install the x86_64-unknown-linux-musl build target by:
        rustup target add x86_64-unknown-linux-musl
    build with musl target and rust-lld:
        rustc --target x86_64-unknown-linux-musl -C linker=rust-lld memory-user.rs
    run the memory user in background:
        ./memory-user 100 &

13.4
A certain amount of memory is occupied immediately at the beginning of execution, for large
allocations the used amount of memory gradually increases over time. Both case the used memory is
freed the instance the program is killed. A page is only written out after the first write, in this
case by compiler magic the zero-initialization is not treated as a write.

13.5
The behavior of pmap is controlled by a config file (~/.pmaprc by default) instead of command line
arguments. As the format of its output varies more frequent than other UNIX tools it seems to be a
reasonable design.

13.6
// not a question

13.7
result of zsh:
    hundreds of segments, mostly shared libraries and memory mapped files 
    2964K r---- locale-archive  // memory-mapped localized strings used by the system
    2816K rw---   [ anon ]  // heap?
    2364K r--s- Unix.zwc    // auto-completion utilities of unix tools
    316K rw---   [ stack ]  // stack

13.8
result of memory-user:
    10+ segments, no shared libraries, libc and std statically linked 
    00007fde2f299000 16384004K rw---   [ anon ]
    00007fe2172b4000    292K r-x-- memory-user
    00007ffc31960000    136K rw---   [ stack ]

14. Interlude: Memory API
Questions
14.1
segmentation fault

14.2
Program received signal SIGSEGV, Segmentation fault.
0x0000555555555150 in main (argc=1, argv=0x7fffffffd6f8) at null.c:7
7           printf("%u", *ptr);

14.3
==1379== Invalid read of size 4
==1379==    at 0x109150: main (null.c:7)
==1379==  Address 0x0 is not stack'd, malloc'd or (recently) free'd

14.4
gdb didn't find any problem. Report from valgrind:
==1507== 40 bytes in 1 blocks are definitely lost in loss record 1 of 1
==1507==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1507==    by 0x10914D: main (leak.c:5)

14.5
GDB didn't find any problem. Report from valgrind:
==1727== Invalid write of size 4
==1727==    at 0x10916C: main (q5.c:6)
==1727==  Address 0x4a0e1d0 is 0 bytes after a block of size 400 alloc'd
==1727==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1727==    by 0x10915D: main (q5.c:5)
This is rather concerning. The allocator must have allocated a memory block larger than requested,
probably the next power of 2 (128 bytes), and GDB as the language agnostic debugger it is didn't
notice the original allocation size in C code.

14.6
GDB didn't find any problem. Report from valgrind:
==1898== Invalid read of size 4
==1898==    at 0x109182: main (q6.c:9)
==1898==  Address 0x4a0e040 is 0 bytes inside a block of size 400 free'd
==1898==    at 0x48369AB: free (vg_replace_malloc.c:530)
==1898==    by 0x10917D: main (q6.c:7)
==1898==  Block was alloc'd at
==1898==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1898==    by 0x10916D: main (q6.c:6)
Even more concerning. The allocator must didn't immediately return the allocated memory to the
system with brk, rather it reserves the memory for further allocation. GDB again was totally fine
with this illegal access to freed memory. Valgrind, however, hijacks calls to malloc and free to run
on its own allocator, thus had a chance to do more precise memory analysis.

14.7
GDB:
    Starting program: /mnt/d/Software/Textbook/playground/notes/ostep/chapter_14/main
    free(): invalid pointer
Valgrind:
    ==2063== Invalid free() / delete / delete[] / realloc()
    ==2063==    at 0x48369AB: free (vg_replace_malloc.c:530)
    ==2063==    by 0x109171: main (q7.c:6)
    ==2063==  Address 0x4a0e068 is 40 bytes inside a block of size 400 alloc'd
    ==2063==    at 0x483577F: malloc (vg_replace_malloc.c:299)
    ==2063==    by 0x10915D: main (q7.c:5)

14.8
./ostep/chapter_14/q8.c

14.9
... or create a saner interface to heap allocated structures, one that automatically frees the
memory when it's no longer needed, does bound check on each access, tracks the lifetime of the
references so the freed memory cannot be accessed again. Or even better, create a new language in
which these rules are enforced at compilation.

15. Mechanism: Address Translation
Questions 
15.1
-s 0
    0x000001ae --> 0x00003230
    0x00000109 --> 0x0000318b
    0x0000020b --> SIGSIGV
    0x0000019e --> 0x00003220
    0x00000322 --> SIGSIGV
-s 1
    0x0000030e --> SIGSIGV
    0x00000105 --> 0x00003741
    0x000001fb --> SIGSIGV
    0x000001cc --> SIGSIGV
    0x0000029b --> SIGSIGV
-s 2
    0x00000039 --> 0x00003ce2
    0x00000056 --> 0x00003cee
    0x00000357 --> SIGSIGV
    0x000002f1 --> SIGSIGV
    0x000002ad --> SIGSIGV

15.2
max(VA) + 1 = 930, +1 since limit is the length of the segment, limit must be one bigger than the
biggest offset to accommodate all the virtual addresses.

15.3
16 * 1024 - (100 - 1) = 0x00003f9d
irrelevant to virtual addresses, the size of data stored on these addresses are not specified

15.4
// skipped

15.5
fix base register and physical memory size, the expectation of fraction of randomly generated
addresses is exactly
    value of limit register / size of address space 

16. Segmentation
Questions
16.1
for a positively growing segment,
    PA = offset(VA) + base if offset(VA) < limit
for a negatively growing segment,
    PA = base - ((1 << OFFSET_BITS) - offset(VA)) if (1 << OFFSET_BITS) - offset(VA) <= limit
(0 ..= 63) --> Segment 0
(64 ..= 127) --> Segment 1
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 0
    VA  0: 0x0000006c (decimal:  108) --> seg 1, 0x000001ec
    VA  1: 0x00000061 (decimal:   97) --> seg 1, SIGSIGV
    VA  2: 0x00000035 (decimal:   53) --> seg 0, SIGSIGV
    VA  3: 0x00000021 (decimal:   33) --> seg 0, SIGSIGV
    VA  4: 0x00000041 (decimal:   65) --> seg 1, SIGSIGV
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 1
    VA  0: 0x00000011 (decimal:   17) --> seg 1, 0x00000011
    VA  1: 0x0000006c (decimal:  108) --> seg 0, 0x000001ec
    VA  2: 0x00000061 (decimal:   97) --> seg 1, SIGSIGV
    VA  3: 0x00000020 (decimal:   32) --> seg 0, SIGSIGV
    VA  4: 0x0000003f (decimal:   63) --> seg 0, SIGSIGV
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2
Virtual Address Trace
    VA  0: 0x0000007a (decimal:  122) --> seg 1, 0x000001fa
    VA  1: 0x00000079 (decimal:  121) --> seg 1, 0x000001f9
    VA  2: 0x00000007 (decimal:    7) --> seg 0, 0x00000007
    VA  3: 0x0000000a (decimal:   10) --> seg 0, 0x0000000a
    VA  4: 0x0000006a (decimal:  106) --> seg 1, SIGSIGV

16.2
highest legal virtual address in segment 0
    0x0000013
lowest legal virtual address in segment 1
    0x00001ec
lowest illegal address
    0x000006c
highest illegal address
    0x000006b
// Modified the script to recognize hexadecimal addresses. The script is typical dynamic typed
// madness, the variable called `vStr` may actually be an integer, when instanceof(vStr, int) is
// false the string can be parsed with int(vStr, 0) which infers the base from the prefix.
./segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -A 0x13,0x14,0x6b,0x6c -c

16.3
--b0 0 --l0 2 --b1 (anything >= 4) --l1 2

16.4
SIZE=128;LIMIT=$(($SIZE / 2 * 9 / 10));./segmentation.py -a $SIZE -p 512 -b 0 -l $LIMIT -B 512 -L

16.5
./segmentation.py -a 128 -p 512 -b 0 -l 0 -B 512 -L 0 -c

17. Free-space Management
"The solution is simple: go through the list and merge neighboring chunks; when finished, the heap
will be whole again."
This coalesce algorithm is O(n). A common O(1) solution to the coalesce problem is to write not only
a header, but also a footer to the allocated chunk, hence on freeing a chunk the allocator may
access both the chunks before and after the chunk and perform the necessary coalesce.

[S15] "Understanding glibc malloc" by Sploitfun
---NOTE START---
ptmalloc2, the allocator in glibc since 2006, maintains multiple arenas for different threads, up to
a limit which is 8 * NUMBER_OF_CPU on 64-bit platforms, after that threads have to share arenas as
in the older dlmalloc.

The arena in the main thread is created with sbrk, all other thread arenas are created by mmap. Both
requires more than necessary space from the operating system, also do not return space immediately
to the operating system when it's free'd. Arenas grow whenever a malloc request cannot be fulfilled.

The allocator maintains three kinds of data structures:
    - heap_info: the heap header. There may be multiple heaps (contiguous chunk of memory in virtual
      memory space) in a single arena, as the arena grows overtime as a result of malloc requests,
      the memory under a single arena may not be contiguous. Main thread doesn't have multiple heaps
      (it's memory can always be moved to a larger contiguous chunk with sbrk)
    - malloc_state: the arena header. contains information about bins, top chunk, last remainder
      chunk, etc. For thread arenas malloc_state is part of the heap, while malloc_state of the main
      thread is a global variable of glibc.
    - malloc_chunk: the chunk header. Different type of chunks has different headers.

Small enough chunks (16 - 64 bytes) are kept in their own free lists, or "fast bins" in glibc term.
They are not coalesced nor sorted on free, just goes back the bin they came from. Each bin holds
free chunks of the same size, 8 bytes apart in size from one bin to another.

Chunks of other sizes when free'd are first added to an "unsorted bin", allowing them to be reused
quickly. Small Bins (< 512 bytes) are 8 bytes apart. Large Bins (>= 512 bytes) have exponentially
increasing sizes and chunks in each bin do not have the same size. Both of them are coalesced when
two free chunks are next to each other. 

Chunk at the top border (the last in virtual address, the initial free chunk) is called top chunk,
top chunk is not in any bins, large and top chunks are the only chunks that will be split on user
request. Top chunk is split when there's no other free blocks in any of the bins.

Last Remainder Chunk points to the last free chunk as the result of a split. Last Remainder Chunk is
prioritized in the unsorted bin to serve consecutive small allocations. These requests are more
likely to be allocated next to each other and coalesced back to a single chunk on free if Last
Reminder Chunk is split on consecutive calls to malloc. 
---NOTE END---

Questions
17.1
./malloc.py -n 10 -H 0 -p BEST -s 0
10 operations, no headers, best fit, no coalesce
ptr[0] = Alloc(3)
    return 1000
    List [(1003, 97)]
Free(ptr[0])
    return 0
    List [(1000, 3), (1003, 97)]
ptr[1] = Alloc(5)
    return 1003
    List [(1000, 3), (1008, 92)]
Free(ptr[1])
    return 0
    List [(1000, 3), (1003, 5), (1008, 92)]
ptr[2] = Alloc(8)
    return 1008
    List [(1000, 3), (1003, 5), (1016, 84)]
Free(ptr[2])
    return 0
    List [(1000, 3), (1003, 5), (1008, 8), (1016, 84)]
ptr[3] = Alloc(8)
    return 1008
    List [(1000, 3), (1003, 5), (1016, 84)]
free(ptr[3])
    return 0
    List [(1000, 3), (1003, 5), (1008, 8), (1016, 84)]
ptr[4] = Alloc(2)
    return 1000
    List [(1002, 1), (1003, 5), (1008, 8), (1016, 84)]
ptr[5] = Alloc(7)
    return 1008
    List [(1002, 1), (1003, 5), (1015, 1), (1016, 84)]
the free space is fragmented badly, neighboring free chunks are not coalesced 

17.2
Free list at the end
    [ addr:1000 sz:3 ] [ addr:1003 sz:5 ] [ addr:1008 sz:8 ] [ addr:1016 sz:8 ] [ addr:1033 sz:67 ]
Free space are even more fragmented, the biggest chunk is now 76 bytes instead of 84, other small
fragments increased in size.

17.3
Free space is fragmented much as the BEST strategy but not as bad as WORST. Allocation is still O(n)
but should be much faster than BEST and WORST, allocator no longer has to inspect the entire free
list.

17.4
allocation time comparison
-l ADDRSORT
    FIRST < BEST = WORST
-l SIZESORT+
    FIRST = BEST < WORST
-l SIZESORT-
    FIRST = WORST < BEST

17.5
With no coalesce most allocation in the long run fails, the free space is fragmented to the extreme
that it contains 100 chunks of length 1. With coalesce most allocation still succeeds in long run,
the free list never grows beyond 10 nodes. 

17.6
In theory FIRST fit with SIZESORT+ is equivalent to BEST fit, FIRST fit with SIZESORT- is equivalent
to BEST fit, while ordering should not affect the performance of BEST and WORST by too much (maybe
some minor effect when the order is used to break ties), but the simulator is written in a poor way
that the free space will not be correctly coalesced unless the free list is ordered by ADDRSORT,
both SIZESORT- and SIZESORT+ will cause more fragmentation.

17.7
For any fit strategy and free list order, 
    1.  allocate a series of chunks of size (n + 1) until the free space is totally exhausted
        (assuming the size of free space is a multiple of (n + 1))
    2.  for each chunk allocated at step 1
        - free a chunk of size (n + 1)
        - allocate a chunk of size n 
        - allocate a chunk of size 1
    3.  free all chunks of size n
at the end, only 1/(n + 1) of the free space is used, nevertheless there's no free node with length
longer than n.

18. Paging: Introduction
Questions
18.1
-a ASIZE
    address space size
-P PAGESIZE
    page size
page table size = ASIZE / PAGESIZE
larger pages incur more internal fragmentation

18.2
1k page size, lowest 10 bits are page offset
16k / 1k = 16 = 2^4, highest 4 bits are virtual page number
-u 50
    VA 0x00003385 (decimal:    13189) --> (VPN 12)  0x00003f85
    VA 0x0000231d (decimal:     8989) --> (VPN 8)   INVALID
    VA 0x000000e6 (decimal:      230) --> (VPN 0)   0x000060e6
    VA 0x00002e0f (decimal:    11791) --> (VPN 11)  INVALID
    VA 0x00001986 (decimal:     6534) --> (VPN 6)   0x00007586
more virtual addresses are valid as -u increases

18.3
-P 8 -a 32 -p 1024 -v -s 1
    page size too small, virtual address space too small compared to physical address space, most of
    physical address space is not addressable from a single process
-P 8k -a 32k -p 1m -v -s 2
    nothing different, only the page size is more reasonable
-P 1m -a 256m -p 512m -v -s 3
    overall reasonable, page size too large for light threads

18.4
-a 32k -p 16k
    Error: physical memory size must be GREATER than address space size (for this simulation)
-p 2g
    Error: must use smaller sizes (less than 1 GB) for this simulation.
-a 5k
    Error in argument: address space must be a power of 2
-P 5k
    Error in argument: page size must be a power of 2
-a 2k -P 4k
    // page size should not be greater than virtual address space size, didn't catch the error 

19. Paging: Faster Translations (TLBs)
Questions
19.1 
Instant::now() timer in Rust std, backed by clock_gettime(CLOCK_MONOTONIC) on Linux. The resolution
reported by clock_getres() is 1ns, but after that much patches to Spectre and other vulnerabilities
there's no way any system timer can still have sub-microsecond precision. Without any knowledge of
the deviation of the timer nor the memory operation I'd say at least a few thousand iterations would
be necessary.

19.2
how to use:
    install the x86_64-unknown-linux-musl build target by:
        rustup target add x86_64-unknown-linux-musl
    build with musl target and rust-lld:
        rustc --target x86_64-unknown-linux-musl -C linker=rust-lld -O -o tlb-musl tlb.rs
    run in linux:
        ./tlb-musl NUM_OF_PAGES PEPEAT
    build on windows:
        rustc --target x86_64-pc-windows-msvc -O -o tlb-msvc tlb.rs
    run in windows:
        ./tlb-msvc NUM_OF_PAGES PEPEAT

19.3
how to use:
    // depends on /bin/bash
    ./run.sh PATH_TO_EXEC
5000 trials seems to be enough

19.4
text is clear enough in this case, there's no much data and the trend is simple.

19.5
instead of plain array access the array is modified with volatile_write:
    let ptr = &mut vec[i] as *mut u32;
    unsafe { write_volatile(ptr, 1); }
volatile writes are never elided nor reordered by the Rust compiler.

19.6
not significant so far in measurements 

19.7
whatever overhead the first access incurs will be averaged to almost nothing after 5000 trials,
adding manual volatile initialization before measuring didn't change time per access.

20. Paging: Smaller Tables
Questions
20.1
Still one: the PDBR. Locations of lower levels are stored in memory not a register.

20.2
// would do 1 instead of 30 translations in a table of 4096 raw bytes
// -s 0, the first virtual address
PDBR = 108
VA = 0x611c = 0b11000_01000_11100
PDIndex = 0b11000 = 24
PDE = 0xa1 = 0b1_0100001, VDE.Valid = True
PDE.PFN = 0b0100001 = 33
PTIndex = 0b01000 = 8
PTE = 0xb5 = 0b1_0110101, PTE.Valid = True
PTE.PFN = 0b0110101 = 53
Offset = 0b11100 = 28
Value = 0x08
each lookup follows 3 memory references in total
    one to the PDE
    one to the PTE
    one to the physical address in PTE

20.3
Modern CPU brings a whole page or something close to the cache on memory access, after the first
access to a memory location the highest level PDT and the page tables down from there for the VA
would have been bought into cache, following accesses up to a common low level VPN would have cache
hits. If the Page table hierarchy is deep to the point that not all page directory tables used to
translate a single VA could be fit into the cache there would be significantly more cache misses.

21. Beyond Physical Memory: Mechanisms
Questions
21.1
./mem 1024
    free, The amount of free memory decreased by 1_000_000. the unit seems to be KB.
    in, The number of interrupts per second increased by 3x. Mostly page memory accesses.
    r, The number of runnable processes increased by 1
    us, user time is 8%, a process constantly fault on memory access is impossible to saturate a
        modern CPU
All of the stats above are linear in the number of mem running

21.2
swpd remains 0, free increased then decreased by 1GB.

21.3
./mem $((13 * 1024))
1.  Before the main loop, an array bigger than available memory is allocated, pages that doesn't fit
    in the main memory is swapped out to the disk, for a brief second so skyrocketed to 1.5GB/s. This
    number is significantly over the capacity of the disk, which indicates the swap files are merely
    created by the file system and contains uninitialized data at this point. 
2.  At the beginning of loop 0 most VAs accessed by the main loop is in physical memory, after a
    while the program reaches the swapped out region of the array, generated a constant stream of
    page faults. si remained constant at ~100MB/s. For some reason the OS swapped out more than
    necessary pages out to the disk on initialization, as swpd decreased from 1.5GB to 1GB with no
    pages swapped out in this period.
3.  Starting from loop 1, each page access is now a page fault (maybe due to the page evict policy),
    the same amount of pages are swapped, in and out every second, si == so == bandwidth, no change
    in swpd anymore.

21.4
bi == si, bo == so as expected, no other process utilizing the disk
cs == 2 * in, each interrupt is handled by a context switch from the process to kernel and a context
switch from kernel to the process
the CPU is entirely idle, the user process is blocking on IO, the kernel has no ready process to
schedule, only the disk is busily swapping pages in and out.

21.5
./mem $((4 * 1024))
    The first loop is slower than subsequent loops as the memory is initialized on the fly. After
    that the bandwidth is 9000MB/s constant.
./mem $((13 * 1024))
    The first loop is faster than subsequent loops as a major part of the memory it accesses is not
    in swap space yet. After that the bandwidth is 100MB/s constant.
The graph should be like a step function: any x <= 12 maps to 9000, any x > 12 maps to 100.

21.6
the entire system freezes after a certain size
the allocation fails once array size > memory available to user process + swap file size

21.7
// is it even possible to configure swap space from WSL?

22. Beyond Physical Memory: Policies
"However, in many cases the importance of said algorithms has decreased, as the discrepancy between
memory-access and disk-access times has increased."
A reasonably available NVMe storage now is only 1 magnitude slower than DDR4 in terms of both speed
and IOPS.

Questions
22.1
-p FIFO -s 0 -n 10
    10% hit rate 
-p LRU -s 0 -n 10
    20% hit rate
-p RAND -s 0 -n 10
    0% hit rate
-p OPT -s 0 -n 10
    40% hit rate
-p CLOCK -s 0 -n 10
    10% hit rate
the seemingly random sequence (8 7 4 2 5 4 7 3 4 5) and the small default cache size (3 pages)
nullified the potential benefit of LRU and CLOCK.

22.2
FIFO
    a repeated sequential access to a pattern of length > 5, 0% hit rate. Would be dramatically
    improved to 100% once the cache size is equal or greater to the length of the pattern
LRU
    the same to FIFO
MRU
    (1 2 3 4 5 6 5 6 5 6 ...), the last two pages 5 and 6 will be evicted each time on the access of
    the other, 0% hit rate, would be dramatically improved to 100% once cache size is increased by 1

22.3
same to Figure 22.6 , given the trace is long enough

22.4
hot / cold pages, same to Figure 22.7, given the trace is long enough

22.5
The processed list of page numbers exceeds the length limit of argument list (getconf ARG_MAX, in
this case 2097152 bytes). The output is limited to the first 10_000 accesses. The program ls has a
working set so tiny that even a single cached page will cover more than 60% memory accesses,
regardless of policy in use. The first 10_000 memory access of a Linux core utility must be a bad
sample as the trace of vmstat gives the exact same results.

23. complete virtual memory systems
[ll82] “virtual memory management in the vax/vms operating system”
---NOTE START---
A common low-end processor in 1970s had <1 mhz clock speed, as a result, vax/vms is optimized
towards reduced usage of processor on cost of memory requirements.

Two system parameters, a low limit and a high limit, are associated with the global modified list of
pages. Once the list size reaches the high limit, the OS writes out pages until the size of the
modified list is now the lower limit, during which process contiguous pages in virtual memory is
written to contiguous regions of the disk, next time the process faults on one of the pages (and by
spatial locality, would fault on consecutive pages) the OS may cluster the read to a sequential
access to the disk. 

Whenever a process is removed from the memory (e.g. on context switch), its resident set is swapped
out to the disk. A process is never swapped in unless there's enough memory for its entire resident
set at the point it's swapped out. A process could lock itself and its pages in memory so it's never
swapped out, a privileged syscall in modern Linux, which should also be the case of VAX/VMS (not
mentioned in the paper).
---NOTE END---

26. Concurrency: An Introduction
Questions
26.1
All general registers, if not specified otherwise, has initial value 0. The program will halt after
the first iteration of the loop, on exit %dx = -1.

26.2
The state of registers are not shared between threads, no matter what is the interrupt frequency the
two threads will be decremented by 1 each iteration, after 3 iterations both threads will halt.
Since interrupt frequency is set to a value much larger than the cycles required to run a thread to
completion, the second thread will start only after when the first thread halted.

26.3
As stated above in 26.2, the program is deterministic and the interrupt frequency will not change
the result of execution.

26.4
In C terms,
    int *ptr = 2000;
    do {
        *ptr += 1;
    } while (b > 0);
b = 0, *ptr = 0 at the beginning of the program, value = *ptr is incremented once from 0 to 1 by
instruction 1004 then the program halts.

26.5
The default interrupt frequency is once between 50 instructions, both threads are not interrupted
while in the critical section, the number of iterations is controlled by the initial value of the
loop counter %bx which is set to 3, the final value of memory position 2000 is 6.

26.6
-s 0
    final value is 2, both threads are not interrupted in critical section
-s 1
    Thread 0 is interrupted right after instruction 1000, the same value 1 is written twice by
    thread 0 and 1 to the memory position 2000, final value is 1
-s 2
    final value is 2, both threads are not interrupted in critical section
critical section is instruction 1000 to 1002 inclusive, if a thread is interrupted after instruction
1000 and before instruction 1003 the execution order is unsafe.

26.7
-i 1
    final value is 1, both thread loads the initial value of (2000) to %ax
-i 2 
    final value is 1, both thread loads the initial value of (2000) to %ax
-i 3
    final value is 2, the store operation (instruction 1002) of thread 0 happens before the context
    switch, threads 1 read value 1 from the memory location
for i >= 3, the answer is correct for a single iteration

26.8
There's 6 instructions in the loop, if a = b mod 6, using a or b as the interrupt frequency should
yield the same correctness.
-i 1, -i 2 is incorrect as stated above in 26.7
-i 3 is correct
    length of the critical section is 3, the execution order is:
        critical section of thread 0
        critical section of thread 1
        jump test of thread 0
        jump test of thread 1
        (repeat)
    both threads are never interrupted in critical section
-i 4 is incorrect
    thread 0 is interrupted in the critical section of the 2nd iteration
-i 5 is incorrect
    thread 0 is interrupted in the critical section of the 4th iteration
-i 6 is correct
    each thread runs for a whole iteration of the loop before being interrupted

26.9
The memory location 2000 is used as a spin lock, thread 1 initialized with ax = 1 releases the lock
by storing 1 to the memory location, thread 0 initialized with ax = 0 busily waits for the store in
the other thread. the final value of (2000) is always 1 on exit.

26.10
As long as there's only one signaller, the behavior of the program is deterministic, both threads
doesn't have a critical section assume the instructions are atomic and the threads are scheduled on
a single CPU. On multiple CPUs the behavior of the program is less consistent, the modification done
by one CPU may not be immediately visible to another CPU. 
The memory location is used as a spin lock, the waiter loop at full speed until the lock is released
by the signaller, that's definitely not an efficient use of the CPU.
