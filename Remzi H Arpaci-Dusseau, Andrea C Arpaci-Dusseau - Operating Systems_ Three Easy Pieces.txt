// whenever possible, programming homeworks will be done in Rust instead of C.

4. The Abstraction: The Process
section 4.5, starting from Oct 2008, struct context in xv6 is:
    struct context {
        uint edi;
        uint esi;
        uint ebx;
        uint ebp;
        uint eip;
    };
ecx and edx are caller saved, esp the stack pointer is encoded in the address of the context, none
of them has to be saved explicitly

Questions
4.1
100%, no process issues IO, unless the kernel intentionally blocks execution for no reason the cpu
will always be assigned to a ready process 

4.2
// length of the IO operation determined by -L includes the tick in which the IO operation is
// issued, an IO operation of length 5 issued at tick 1 is finished at the start of tick 6, the
// entirety of tick 6 may be scheduled to some other process, but the IO operation is still only
// considered as complete at tick 6, not tick 5
10 cycles: 
    PID 0 is run to completion (4 cycles), context switch to PID 1
    PID 1 issues IO (1 cycle) 
    PID 1 blocks on IO (4 + 1 cycles, + 1 since the process is considered complete at tick 10)

4.3
6 cycles:
    PID 0 issues IO (1 cycle), blocks on IO, context switch
    PID 1 is run to complete (4 cycles)
    PID 0 blocks on IO (1 + 1 cycle)
The scheduler is not preemptive, a process will not be descheduled unless it invoked a syscall 

4.4
9 cycles:
    PID 0 issues IO (1 cycles)
    PID 0 blocks on IO (4 cycles), context switch at start of tick 5
    PID 1 runs to completion (4 cycles)

4.5
same to 4.3

4.6
27 cycles:
    PID 0 issues IO (1 cycle), context switch to PID 1
    PID 1 runs to completion (5 cycles), context switch to PID 2
    PID 2 runs to completion (5 cycles), context switch to PID 3
    PID 3 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle)
    PID 0 blocks on IO (4 cycles)
    PID 0 issues IO (1 cycle)
    PID 0 blocks on IO (4 + 1 cycles)

4.7
18 cycles:
    PID 0 issues IO (1 cycle), context switch to PID 1
    PID 1 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle), context switch to PID 2
    PID 2 runs to completion (5 cycles), context switch to PID 0
    PID 0 issues IO (1 cycle), context switch to PID 3
    PID 3 runs to completion (5 cycles)
A process issued an IO operation is likely to issue more in near future (e.g. keyboard press in a
document processing software, mouse click in a GUI), immediately switch back to the process after IO
completion increases utilization of the IO device and CPU as well as responsiveness to user input.
In modern preemptive OS an IO heavy process may be assigned higher priority, compared to computation
bounded processes it will be scheduled more frequently but with smaller time quantum each time. 

4.8
in general, -I IO_RUN_IMMEDIATE should be roughly more efficient than -I IO_RUN_LATER, -S
SWITCH_ON_IO should be strictly more efficient than -S SWITCH_ON_END

Remaining questions: 
- after control being yielded to user process, how could a process be preempted without invoking
  syscalls by itself? (A: with interruption tied to a physical timer)

5.Interlude: Process API
wait() will return prematurely when:
    - the argument (*stat_loc) is not a proper pointer or NULL
    - the child process is stopped (not terminated) by a signal, in which case it can be restarted
      later

meaning of suffixes of exec:
    - l: variadic functions accepting arbitrary number of arguments as arguments to the program
    - p: search program in PATH environment variable
    - v: arguments
    - e: environment variables
    
Questions 
5.1
./ostep/chapter_5/q1.c
Fork() copies the entire state of the parent process to the child process, x as an auto variable on
the stack or in register is copied to the child process with the initial value 100. After that x in
child and parent processes are two different entities hence modification to them does not interfere
with each other.

5.2
./ostep/chapter_5/q2.c
the file descriptor is available to both parent and child processes, write from the parent process
always seems to happen before write from the child process, maybe fork() returns earlier to the
parent process compared to the child?

5.3
./ostep/chapter_5/q3.c
the parent process can sleep for a long enough duration, the two processes may communicate with some
form of shared memory, pipe, lock or signal, above all wait() still is the most natural way to
express this pattern.

5.4
./ostep/chapter_5/q4.c
All variants of exec are built upon a single syscall `execve`. At the beginning the C programming
language was an implementation detail of UNIX system, in the 80s C gained popularity among various
vendors despite having no standard and kept evolving that way even after ANSI C. UNIX before Linux
is a branket term for a dozen competing imcompatible operating systems. The current state of exec
variants may be an effort to maintain backward compatbility with all of them.

5.5
./ostep/chapter_5/q5.c
On success wait() returns process id of the terminted child. When the process calling wait() has no
child process ECHILD is assigned to the global variable errno.

5.6
./ostep/chapter_5/q6.c
Parent process block on the termination of a selected child process instead of any one of them.
Behavior of the function may be modified by the additional options argument, e.g. return immediately
when there's no terminated child process. 

5.7
./ostep/chapter_5/q7.c
printf() sliently succeeds without actually printing anything to stdout.

5.8
./ostep/chapter_5/q8.c

6. Mechanism: Limited Direct Execution
Measurement (in WSL)
syscall:
    ./ostep/chapter_6/measure_syscall.c
    getpid(), 48 nanoseconds
context switch:
    ./ostep_chapter_6/measure_switch.c
    20 microseconds (without sched_setaffinity)
    1 microsecond (with sched_setaffinity)

7. Scheduling: Introduction
Proofs of the optimality of SJF and STCF can be found in problem 16-2 of introduction to algorithms

Questions
7.1
SJF:
    avg. turnaround: (200 + 400 + 600)/3 = 400
    avg. response: (0 + 200 + 400)/3 = 200
FIFO:
    same to SJF

7.2
SJF:
    avg. turnaround: (100 + 300 + 600)/3 = 333.33
    avg. response: (0 + 100 + 300)/3 = 133.33
FIFO:
    same to SJF

7.3
RR, same length:
    avg. turnaround: (598 + 599 + 600)/3 = 599
    avg. response: (0 + 1 + 2)/3 = 1
RR, different lengths:
    avg. turnaround: (298 + 499 + 600)/3 = 465.67
    avg. response: (0 + 1 + 2)/3 = 1

7.4
jobs ordered by ascending length, in other cases FIFO is not optimal

7.5
all except the longest job must be shorter than the quantum length:
    if there's two jobs with lengths longer than quantum length, no matter what is scheduled first
    the other will has a longer response time than in RR

7.6
response time in SJF is linear in job lengths
p./scheduler.py -p SJF -j 3 -l 3,2,1 -c
    Average -- Response: 1.33  Turnaround 3.33  Wait 1.33
p./scheduler.py -p SJF -j 3 -l 30,20,10 -c
    Average -- Response: 13.33  Turnaround 33.33  Wait 13.33
p./scheduler.py -p SJF -j 3 -l 300,200,100 -c
    Average -- Response: 133.33  Turnaround 333.33  Wait 133.33

7.7
response time with RR is linear in quantum length
let quantum length be T_q, worst response time is:
    (N - 1)T_q
    
8. Scheduling: The Multi-Level Feedback Queue
Questions
8.1
./mlfq.py -j 2 -n 2 -c -m 50 -M 0

8.2
Example 1: A Single Long-Running Job
    ./mlfq.py -n 3 -q 10 -l 0,200,0 -c
Example 2: Along Came A Short Job
    ./mlfq.py -n 3 -q 10 -l 0,200,0:100,20,0 -c
// io frequency is defined as "number of CPU ticks between two IO requests" in the simulator 
Example 3: What About I/O?
    ./mlfq.py -S -n 3 -q 10 -l 0,200,0:100,20,1.1 -c
Attempt #2: The Priority Boost
    without boost:
    ./mlfq.py -n 3 -q 10 -S -I -i 1 -l 0,200,0:100,50,1:100,50,1 -c
    with boost:
    ./mlfq.py -n 3 -q 10 -S -B 100 -I -i 1 -l 0,200,0:100,50,1:100,50,1 -c
Attempt #3: Better Accounting
    without allotment accounting:
    ./mlfq.py -q 10 -S -i 1 -l 0,200,0:100,100,9 -c
    with allotment accounting:
    ./mlfq.py -q 10 -i 1 -l 0,200,0:100,100,9 -c
Tuning MLFQ And Other Issues
    ./mlfq.py -Q 10,20,40 -l 0,140,0:0,140,0 -c

8.3
one queue, no allotment accounting, no IO reset
./mlfq.py -n 1 -j 10 -c -m 20

8.4
./mlfq.py -q 100 -S -i 1 -l 0,10000,99:0,10000,0 -c

8.5
200ms, assume after each boost the job will be run at least once (i.e. the system is not croweded
with high priority jobs)

8.6
lower response time in sacrifice of IO throughput. In CFQ (the default Linux IO scheduler between
2.6 - 5.0) the scheduler will introduce an artifical delay after an IO request in hope to catch more
IO requests from the same process to be performed in batch. Such a mechanism is majorly meaningless
with an SSD. CFQ was then removed from Linux in 5.0.

[B+18] “The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS”
additional details of CFS: 
    - fairness is ensured among applications instead of threads
    - a new thread is created with maximum vruntime in the queue
    - a thread waking up doesn't preempt the current thread if the vruntime is close enough
    - threads are distributed to CPUs by their utilizations 
    - load balancing happens every 4ms, cores steal threads from each other

9. Scheduling: Proportional Share
Weighting (Niceness), logarithm of prio_to_weight is roughly linear, i.e.
    log(Wi) - Log(Wj) ~ i - j
    e^(log(Wi) - log(Wj)) ~ e^(i - j)
    Wi / Wj ~ e^(i - j)
    
Remaining questions:
    - how are these dynamic data structures, especially the priority queue in stride scheduling
      implemented in the kernel where dynamic allocation is not taken as given?

Questions
9.1
// too long, omitted 

9.2
Job 0 has 1% chance to be run before completion of job 1. With such imbalance in ticket distribution
the scheduler is no longer fair or even preemptive.

9.3
Finish time T of the first job is governed by the negative binomial distribution with a constraint
    X ~ NB(100, 0.5), X <= 200 
    E[X] = Σ_{k = 0}^100 C(k + 100, k) * 0.5^100 * 0.5^k * (100 + k)
By WolframAlpha the expectation is around 189.67, expected fairness is 189.67/200 = 0.95

9.4
fairness decreases as quantum increases, when -q 2 the distribution is equivalent to
    X ~ NB(50, 0.5), X <= 100
    E[X] = 92.96, fairness = 0.93
when -q 5,
    X ~ NB(20, 0.5), X <= 40
    E[X] = 35.86, fairness = 0.90
when -q 10,
    X ~ NB(10, 0.5), X <= 20
    E[X] = 17.30, fairness = 0.87
when -q 100, the first job runs to completion before the start of the other, fairness = 0.5

9.5
The unfairness by low resolution impacts stride scheduler as well but the difference between finish
time will be limited to one quantum. Another interesting topic would be the balance between
resolution and scheduling efficiency, which is beyond the capacity of this simulator.

10. Multiprocessor Scheduling
Questions
10.1
30 ticks, no other threads competing for CPUs, the cache is too small for the job to run efficiently

10.2
20 ticks, 10 cold tick and 10 warm ticks

10.3
the job runs twice as fast in warm mode, as the default value of -r.

10.4
by the trace
    9   a [ 20] cache[w]
the cache was warmed at the end of 10th tick, as the default value of -w.

10.5
// it's not documented but default time slice is 10
150 ticks, the jobs are scheduled as
    CPU 0   ACBA...
    CPU 1   BACB...
as A has a working set as big as the cache, it invalidates all previous content in the cache, hence
no jobs are running in warm mode ever.

10.6
(10 + 90/2) * 2 = 110 ticks, this time a, b and c run in warm mode after the first 10 ticks, where b
and c shares the same CPU, the current affinity is the only one where
    1.  all CPUs are working
    2.  all working sets fit in the cache
therefore there's no other affinity setting that runs faster than the current one.

10.7
-M 50
    -n 1: 300 ticks
    -n 2: 150 ticks
    -n 3: 100 ticks
-M 100
    -n 1: 300 ticks
    -n 2: 150 ticks
    -n 3: 55 ticks
only with -M 100 -n 3 every job will run in warm mode, 10 + (90 / 2) = 55 ticks

10.8
100 ticks instead of 110, CPU utilization is slightly better
// it's not documented but default peek interval is 30 ticks
with no job stealing (-P 0) the running time is 200 ticks, with more job stealing (-P 1) the running
time improves slightly to 95 ticks as the simulator doesn't account for context swtich and lock
costs. Per CPU scheduler scales worse than the single queue round-robin scheduler as the simulator
suggests, yet again because the simulator is over-simplified.

10.9
// omitted

13. The Abstraction: Address Spaces
Questions
13.1
free has an option to print memory in power of 1000 instead of 1024. Unlike HDDs Memory nowadays is
never marketed or discussed in power of 1000, maybe that's not yet an universal agreement back when
free is created.

13.2
WSL doesn't have full access to physical memory, out of 16GB installed 4GB is invisible inside WSL.
100MB used memory among which 2.4MB is reported by top, leaves 97.6MB occupied by the kernel.

13.3
how to use:
    install the x86_64-unknown-linux-musl build target by:
        rustup target add x86_64-unknown-linux-musl
    build with musl target and rust-lld:
        rustc --target x86_64-unknown-linux-musl -C linker=rust-lld memory-user.rs
    run the memory user in background:
        ./memory-user 100 &

13.4
A certain amount of memory is occupied immediately at the beginning of execution, for large
allocations the used amount of memory gradually increases over time. Both case the used memory is
freed the instance the program is killed. A page is only written out after the first write, in this
case by compiler magic the zero-initialization is not treated as a write.

13.5
The behavior of pmap is controlled by a config file (~/.pmaprc by default) instead of command line
arguments. As the format of its output varies more frequent than other UNIX tools it seems to be a
reasonable design.

13.6
// not a question

13.7
result of zsh:
    hundreds of segments, mostly shared libraries and memory mapped files 
    2964K r---- locale-archive  // memory-mapped localized strings used by the system
    2816K rw---   [ anon ]  // heap?
    2364K r--s- Unix.zwc    // auto-completion utilities of unix tools
    316K rw---   [ stack ]  // stack

13.8
result of memory-user:
    10+ segments, no shared libraries, libc and std statically linked 
    00007fde2f299000 16384004K rw---   [ anon ]
    00007fe2172b4000    292K r-x-- memory-user
    00007ffc31960000    136K rw---   [ stack ]

14. Interlude: Memory API
Questions
14.1
segmentation fault

14.2
Program received signal SIGSEGV, Segmentation fault.
0x0000555555555150 in main (argc=1, argv=0x7fffffffd6f8) at null.c:7
7           printf("%u", *ptr);

14.3
==1379== Invalid read of size 4
==1379==    at 0x109150: main (null.c:7)
==1379==  Address 0x0 is not stack'd, malloc'd or (recently) free'd

14.4
gdb didn't find any problem. Report from valgrind:
==1507== 40 bytes in 1 blocks are definitely lost in loss record 1 of 1
==1507==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1507==    by 0x10914D: main (leak.c:5)

14.5
GDB didn't find any problem. Report from valgrind:
==1727== Invalid write of size 4
==1727==    at 0x10916C: main (q5.c:6)
==1727==  Address 0x4a0e1d0 is 0 bytes after a block of size 400 alloc'd
==1727==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1727==    by 0x10915D: main (q5.c:5)
This is rather concerning. The allocator must have allocated a memory block larger than requested,
probably the next power of 2 (128 bytes), and GDB as the language agnostic debugger it is didn't
notice the original allocation size in C code.

14.6
GDB didn't find any problem. Report from valgrind:
==1898== Invalid read of size 4
==1898==    at 0x109182: main (q6.c:9)
==1898==  Address 0x4a0e040 is 0 bytes inside a block of size 400 free'd
==1898==    at 0x48369AB: free (vg_replace_malloc.c:530)
==1898==    by 0x10917D: main (q6.c:7)
==1898==  Block was alloc'd at
==1898==    at 0x483577F: malloc (vg_replace_malloc.c:299)
==1898==    by 0x10916D: main (q6.c:6)
Even more concerning. The allocator must didn't immediately return the allocated memory to the
system with brk, rather it reserves the memory for further allocation. GDB again was totally fine
with this illegal access to freed memory. Valgrind, however, hijacks calls to malloc and free to run
on its own allocator, thus had a chance to do more precise memory analysis.

14.7
GDB:
    Starting program: /mnt/d/Software/Textbook/playground/notes/ostep/chapter_14/main
    free(): invalid pointer
Valgrind:
    ==2063== Invalid free() / delete / delete[] / realloc()
    ==2063==    at 0x48369AB: free (vg_replace_malloc.c:530)
    ==2063==    by 0x109171: main (q7.c:6)
    ==2063==  Address 0x4a0e068 is 40 bytes inside a block of size 400 alloc'd
    ==2063==    at 0x483577F: malloc (vg_replace_malloc.c:299)
    ==2063==    by 0x10915D: main (q7.c:5)

14.8
./ostep/chapter_14/q8.c

14.9
... or create a saner interface to heap allocated structures, one that automatically frees the
memory when it's no longer needed, does bound check on each access, tracks the lifetime of the
references so the freed memory cannot be accessed again.