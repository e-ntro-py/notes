// rereading, lost all the notes / code of the previous reading

Chapter 2
2.1-1
[ 31, 41, 59, 59, 41, 58 ]
[ 31, 41, 41, 59, 41, 58 ]
[ 31, 31, 41, 59, 41, 58 ]
[ 26, 31, 41, 59, 59, 58 ]
[ 26, 31, 41, 41, 59, 59 ]
[ 26, 31, 41, 41, 58, 59 ]

2.1-2
change line 5 to
    while i > 0 and A[i] < key

2.1-3
./CLRS/start/index.ts#linearSearch
loop invariant: 
    none of the first i items in the array equals to v
initialization:
    i == 0, loop invariant holds trivially
maintenance:
    every iteration, A[i] == v is checked
    the loop continues only when A[i] == v is false
termination:
    if the loop terminated early, then A[i] == v and the first i elements in the array are not equal to v
    therefore arr[i] is the first element equals to v
    if the loop terminated on loop condition, i = arr.length, the first i elements are the whole array
    none of the items in the array equals to v, so function returns NIL
    
2.1-4
Input: Two n-element sequence of 0 and 1s, representing two numbers a and b in binary form
Output: a (n+1)-element sequence of 0 and 1s, representing a number c == a + b in binary form
assumes little-endian
./CLRS/start/index.ts#addBinary

2.2-1
Θ(n^3)

2.2-2
invariant: 
    each of the the first i elements of the array are the ith smallest elements
the last element is greater or equal to all the previous items, thereby must be the greatest element
best-case and worst-case both Θ(n^2)

2.2-3
Σip(i) = (Σi) / n = (1 + n) * n / 2n = (1 + n) / 2 on average, Θ(n)
n on worst case, Θ(n)
assuming comparsion of integers takes constant time

2.2-4
hard-code the solution of a specific input in the program 
if input equals to the specific input, returns that solution immediately
base-case running time reduced to time required to check the input

2.3-1
[ 3, 41, 52, 26, 38, 57, 9, 49 ]
[ 3, 41, 26, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 9, 38, 49, 57 ]
[ 3, 9, 26, 38, 41, 49, 52, 57 ]
[ 3, 9, 26, 38, 41, 49, 52, 57 ]

2.3-2
./CLRS/start/mergesort.ts#mergeNoSentinel

2.3-3
when n = 2,
    T(n) = 2 = 2lg2
assume for n = 2^k, T(n) = nlgn = 2^klg(2^k) = k * 2^k
    T(2n)   = T(2^(k+1))
            = 2T(2^k) + 2^(k+1)
            = 2 * 2^k * k + 2^(k+1)
            = k * 2^(k+1) + 2^(k+1)
            = (k+1) * 2^(k+1)
            = (2n)lg(2n)
therefore for n = 2^k, k > 1,
    T(n) = nlgn

2.3-4
T(n)    = Θ(1)          when n = 1
        = T(n-1) + Θ(n) when n > 1

2.3-5
./CLRS/start/index.ts#binarySearch
T(n)    = Θ(1)          when n = 1
        = Θ(n/2) + Θ(1) when n > 1
by master theorem T(n) = Θ(lgn)

2.3-6
cannot improve
each iteration in insertion sort has to shift Θ(n) elements one place to the right
even if the right position to insert the element can be find in time Θ(lgn), shifting still takes Θ(n)
overall time still Θ(n^2)

2.3-7
first sort the array by mergesort
then for every element g in the sorted array, binary search x - g
(if the index of x - g equals the index of g, x = 2g, and such a pair doesn't exist)
./CLRS/index.ts#pairSum
mergesort: Θ(nlgn)
binary search: n * Θ(lgn) = Θ(nlgn)
overall: Θ(nlgn) + Θ(nlgn) = Θ(nlgn)

2-1
a.  insertion sort on an array of length k takes time Θ(k^2)
    there are n/k such arrays, overall running time Θ(k^2) * n / k = Θ(nk)
b.  the same with figure 2.5
    the height of the tree is reduced to lg(n/k)
    base level performs n/2k merges with arrays of length k, n/2k * Θ(k) = Θ(n) in total
    overall running time Θ(nlg(n/k))
c.  T(n) = Θ(nk + nlg(n/k))
    if k = ω(lgn), Θ(nk) = ω(nlgn), the running time of the modified algorithm then is asymptotically greater
d.  k = O(lgn), the precise value of k depends on cache size, hidden constant factors in algorithms, etc.

2-2
a.  it terminates in finite time on all inputs
b.  invariant:
        at the end of each iteration, the smallest element in slice A[j-1 .. A.length] is in A[j-1]
    initialization:
        j = A.length, the first iteration compares A[A.length] to A[A.length - 1], put the smaller in A[A.length - 1]
        which is the smallest in A[A.length - 1 .. A.length]
    maintenance:
        the smallest number is compared with A[j-1], the smaller of the two is swapped to A[j-1]
        inductively A[j-1] now holds the smallest number in A[j-1 .. A.length]
    termination:
        j = i+1, A[i] holds the smallest number in A[i .. A.length]
c.  invariant:
        at the end of each iteration, A[1..i] consists of ith smallest elements and is sorted in non-decreasing order
    initialization:
        at the end of the first iteration, the smallest element in A[1 .. A.length] is placed in A[1]
        it is the smallest element, A[1 .. 1] is trivially sorted
    maintenance:
        the inner loop finds the smallest number in A[i .. A.length] and puts it in A[i]
        A[1 .. i-1] consists of the i-1 smallest elements in A, therefore A[i] is the ith smallest number
        by definition A[i] >= A[i-1], A[1..i] is sorted 
    termination:
        A[1..A.length - 1] consists of A.length - 1 smallest numbers and is sorted
        thereby A[A.length] must be the greatest element in A
        A is sorted in non-decreasing order
d.  inner loop Θ(n)
    outer loop run n - 1 times
    overall running time (n-1) * Θ(n) = Θ(n^2)
    both the best and worst case of bubble sort performs Θ(n^2) comparsions
    while insertion sort in best case has running time Θ(n)

2-3
a.  the algorithm performs n additions and n multiplications
    assuming both are contant time, T(n) = Θ(n)
b.  ./CLRS/start/index.ts#naivePolynomial
    k = Θ(n), Math.pow(x, k) = x^k if implemented naively takes time Θ(k), if not takes time Θ(lgk)
    overall running time Θ(n^2) or Θ(nlgn)
c.  initialization:
        initially y = 0, the sum of an empty sequence
    maintenance:
        y = Σ(k ∈ {0..n-(i+1)})a(k+i+1)x^k = a(i+1) + a(i+2)x + .. + an * x^(n-(i+1)) at the start
        y = ai + xy = ai + a(i+1)x + .. + an * x^(n-i) = Σ(k ∈ {0..n-i})a(k+i)x^k
    termination:
        the end of the last iteration ends with i = 0, the next iteration (not executed) starts with i = -1
        y = Σ(k ∈ {0..n})ak * x^k
d.  as above

2-4
a.  (0, 4), (1, 4), (2, 3), (2, 4), (3, 4)  // zero-based index
b.  [n, n-1 .. 1], every pair is an inversion
    there are C(n, 2) = n(n-1)/2 pairs
c.  every assignment in the insertion sort reduces the total number of inversions by 1
    the sorted array has no inversions
    therefore the number of inversions in an array is a lower bound of the running time of insertion sort
d.  assume an array A[p..r] and some q that p < q < r, then the total number of inversions in A[p..r] is the sum of
    1.  the number of inersions in A[p..q]
    2.  the number of inversions in A[q+1..r]
    3.  the number of inversions as pairs (i, j) such that
            i in {p..q}, j in {q+1..r}
    for every index i in {p..q}, the number of type 3 inversions is exactly the number of indices j in {q+1..r} that
        A[i] > A[j]
    which can be calculated in a slightly modified merge function
    ./CLRS/start/mergesort.ts#inversionCount

Chapter 3
3.1-1
max(f(n), g(n)) <= f(n) + g(n) for all n
assume f(n) >= g(n), max(f(n), g(n)) = f(n) = (f(n) + f(n)) / 2 >= (f(n) + g(n)) / 2
symmetrically when g(n) >= f(n), max(f(n), g(n)) >= (f(n) + g(n)) / 2
therefore let c1 = 1, c2 = 1/2,
    1/2(f(n) + g(n)) <= max(f(n), g(n)) <= f(n) + g(n)
    max(f(n), g(n)) = Θ(f(n) + g(n))

3.1-2
(n + a)^b = Σ(k ∈ {0..b})(C(b, k) * n^k * a^(b-k))
the term with highest order is n^b, therefore (n + a)^b = Θ(n^b)

3.1-3
O(n^2) is an upper bound, the set O(n^2) contains all functions from constant to quadratic
denote the running time of algorithm A by T(n)
the statement can then be translated to "T(n) is at least any constant (including 0) for large enough n"
which is meaningless for a asymptotically non-negative function

3.1-4
2^(n+1) = 2 * 2^n = Θ(2^n)
2^2n / 2^n = 2^n, no constant can be asymptotically larger than 2^n

3.1-5
=>: f(n) = Θ(g(n)), c1g(n) <= f(n) <= c2g(n) for n >= n0, then trivally f(n) = O(g(n)) and f(n) = Ω(g(n))
<=: f(n) = O(g(n)) implies f(n) <= c1g(n) for n >= n1
    f(n) = Ω(g(n)) implies f(n) >= c2g(n) for n >= n2
    take n3 = max(n1, n2), c1g(n) <= f(n) <= c2g(n) for n >= n3, f(n) = Θ(g(n))
therefore f(n) = Θ(n) <=> f(n) = O(g(n)) and f(n) = Ω(g(n))

3.1-6
f(n) = Θ(g(n)) => f(n) = O(g(n)), for all inputs g(n) is an upper bound of f(n)
so even in the worst case f(n) = O(g(n))
similarly in the best case f(n) = Ω(g(n))

3.1-7
assume f(n) = o(g(n)) and f(n) = ω(g(n)), then
lim(f(n) / g(n)) = ∞ and lim(f(n) / g(n)) = 0, contradiction
so no such f(n) exist, o(g(n)) ∩ ω(g(n)) = ∅

3.1-8
Ω(g(n, m)) = {f(n, m):  there exist positive constants c, n0, m0 such that
                        cg(n, m) <= f(n, m) for all n >= n0 or m >= m0 }
Θ(g(n, m)) = O(g(n, m)) ∩ Ω(g(n, m))

3.2-1
for n1 >= n2, f(n1) >= f(n2), g(n1) >= g(n2), f(n1) + g(n1) >= f(n2) + g(n2)
thereby f(n) + g(n) is monotonically increasing
for n1 >= n2, g(n1) >= g(n2), f(g(n1)) >= f(g(n2))
thereby f(g(n)) is monotonically increasing

3.2-2
a^log(b, c) = (b^log(b, a))^log(b, c)
            = (b^log(b, c))^log(b, a)
            = c^log(b, a)

3.2-3
n! = (2πn)^(1/2) * (n/e)^n * (1 + Θ(1/n))
lg(n!)  = lg((2πn)^(1/2)) + lg((n/e)^n) + lg(1 + Θ(1/n))
        = 1/2lg(2πn) + nlg(n/e) + lg(1 + Θ(1/n))
        = Θ(lgn) + Θ(nlgn) + O(lgn)
        = Θ(nlgn)
n! = Πn, n! / 2^n = Π(k/2), lim(Π(k/2)) -> ∞, n! = ω(2^n)
n! / n^n = Π(k/n) <= 1/n, lim(1/n) -> 0, n! = o(n^n)

3.2-4
lg((lgn)!) = Θ(lgn * lglgn)
for large enough n,
c1(lgn * lglgn) <= lg((lgn)!) <= c2(lgn * lglgn)
e^c1(lgn * lglgn) <= (lgn)!
n^(c1 * lglgn) <= (lgn)!
as c1 * lglgn is not a constant, (lgn)! cannot be polynomially bounded
thanks https://ita.skanev.com/03/02/04.html
lg((lglgn)!)    = Θ(lglgn * lglglgn)
                = o(lglgn * lglgn)
                = o((lglgn)^2)
                = o(lgn)
asymptotically,
lg((lglgn)!) < lgn
(lglgn)! < e^lgn = n
therefore (lglgn)! is polynomially bounded

3.2-5
by definition, 
lg*lgn = lg*n - 1 = Θ(lg*n)
lg(lg*n) = Θ(lg(lg*n))
therefore lg*lgn is asymptotically larger

3.2-6
((1 + 5^(1/2)) / 2)^2 = (1 + 2 * 5^(1/2) + 5) / 4 = (3 + 5^(1/2)) / 2 = (1 + 5^(1/2)) / 2 + 1
((1 - 5^(1/2)) / 2)^2 = (1 - 2 * 5^(1/2) + 5) / 4 = (3 - 5^(1/2)) / 2 = (1 - 5^(1/2)) / 2 + 1

3.2-7
let g and g' denote golden ratio and its conjugate, r5 denotes square root of 5
g - g' = r5
g^2 = g + 1, g'^2 = g' + 1
F1 = 1 = (g - g') / r5
F2 = 1 = (g^2 - g'^2) / r5
assume Fi = (g^i - g'^i) / r5, Fi+1 = (g^(i+1) - g'^(i+1)) / r5
    (g^(i+2) - g'^(i+2)) / r5   = (g^i * g^2 - g'^i * g'^2) / r5
                                = (g^i * (g + 1) - g'^i * (g' + 1)) / r5
                                = (g^i - g'^i) / r5 + (g^(i+1) - g'^(i+1)) / r5
                                = Fi + Fi+1
                                = Fi+2

3.2-8
by symmetry law
    n = Θ(klnk)
as ln(n) is monotonically increasing
    c1klnk <= n <= c2klnk
    ln(c1klnk) <= ln(n) <= ln(c2klnk)
    lnk + lnlnk + c1' <= ln(n) <= lnk + lnlnk + c2'
    ln(n) = Θ(lnk)
    lnk = Θ(ln(n))
    k = Θ(n) / lnk = Θ(n) / Θ(ln(n)) = Θ(n / ln(n))

3-1
a.  k >= d, p(n) / n^k = Σ(ai * n^(i - k))
    each i - k <= 0, therefore lim(p(n) / n^k) <= ad
    take c = ad + ε, where ε is an arbitrary positive number
    there must exist n0 such that for n >= n0, lim(p(n) / cn^k) < 1, p(n) = O(n^k)
b.  k <= d, p(n) / n^k = Σ(ai * n^(i - k))
    lim(p(n) / n^k) >= ad
    take c = ad/2, with large enough n, cn^k <= p(n), p(n) = Ω(n^k)
c.  k == d <=> k <= d && k >= d
    combine a and b, p(n) = O(n^k) && p(n) = Ω(n^k) => p(n) = Θ(n^k)
d.  if k > d, lim(p(n) / n^k) = 0, p(n) = o(n^k)
e.  if k < d, lim(p(n) / n^k) = ∞, p(n) = ω(n^k)

3-2
a.  y   y   n   n   n
b.  y   y   n   n   n
c.  n   n   n   n   n
    n^sin(n) swings between 1/n and n
d.  n   n   y   y   n
    lim(2^n / 2^(n/2)) = lim(2^(n/2)) = ∞
e.  y   n   y   n   y 
    by 3.16, n^lgc = c^lgn
f.  y   n   y   n   y
    lg(n!) = Θ(nlgn)
    lg(n^n) = nlgn = Θ(nlgn)

3-3
a.  1
    lg(lg*n)
    lg*(lgn) = lg*n - 1
    lg*n
    lnln(n)
    // polylogarithmics:
    2^lg*n // unsure
    (lgn)^(1/2)
    ln(n)
    (lgn)! = Θ(lgn * lglgn)
    (lgn)^2
    2^((2lgn)^(1/2)) // unsure
    // polynomials:
    n^(1/lgn)
    (2^(1/2))^lgn = n^(1/2)
    n 
    2^lgn = n
    nlgn
    lg(n!) = Θ(nlgn)
    n^2
    4^lgn = n^2
    n^3
    // exponentials:
    (lgn)^lgn = n^lglgn
    n^lglgn
    (3/2)^n
    2^n
    n * 2^n
    e^n
    // super-exponentials:
    n!
    (n+1)!
    2^(2^n)
    2^(2^(n+1))
b.  the fastest growing function here is 2^(2^(n+1))
    2^(2^(n+2)) * |sin(n)|

3-4
a.  false, n = O(n^2), n^2 != O(n)
b.  false, let f(n) = n^2, g(n) = n, min(f(n), g(n)) = n, f(n) + g(n) = Θ(n^2) != Θ(n)
c.  f(n) = O(g(n))
    f(n) <= c1g(n)
    lg(f(n)) <= lg(c1(g(n))) = lg(g(n)) + c2 = O(lg(g(n)))
d.  false
    f(n) = O(g(n))
    f(n) <= c1g(n)
    2^f(n) <= 2^c1g(n) = (2^c1)^g(n)
    let f(n) = 2n, g(n) = n
    2^f(n) = 2^2n, 2^g(n) = 2^n, 2^2n != O(2^n)
e.  false, let f(n) = 1/n, f^2(n) = (1/n)^2 = 1/n^2
    lim(f(n) / f^2(n)) = ∞, f(n) = ω(f^2(n))
f.  f(n) = O(g(n))
    f(n) <= c1g(n)
    (1/c1)f(n) <= g(n)
    g(n) = Ω(f(n))
g.  false, let f(n) = 2^n, f(n/2) = 2^(n/2), lim(f(n) / f(n/2)) = ∞, f(n) = ω(f(n/2))
h.  for any g(n) ∈ o(f(n)), lim(g(n) / f(n)) = 0
    for large enough n, lim((f(n) + g(n)) / f(n)) = 1
    let c1 = 1, c2 = 1 + ε, there must exist n0 such that for n >= n0,
        c1f(n) <= f(n) + g(n) <= c2f(n)
    and f(n) + g(n) = Θ(f(n))

3-5
a.  if f(n) != O(g(n)), there's no n0 and c that cg(n) >= f(n) for all n >= n0
    let c be an arbitrary constant, if only for finitely many n, f(n) >= cg(n)
    then take n0 be the greatest of such n, then for n >= n0+1, f(n) < cg(n), f(n) = O(g(n))
    thereby there must be infinitely many n such that f(n) >= cg(n) >= 0, f(n) = ∞Ω(g(n))
    so f(n) != O(g(n)) => f(n) = ∞Ω(g(n)) and f(n) != ∞Ω(g(n)) => f(n) = O(g(n))
    if f(n) = n and g(n) = n * |sin(n)|, then neither f(n) = O(g(n)) or f(n) = Ω(g(n))
b.  it is the complement of set O(g(n)), may work smoother in certain proofs
    it's harder to reason and less intuitive, f(n) = ∞Ω(g(n)) no longer gives an lower bound of f(n)
c.  =>: f(n) = Θ(g(n))  => c1g(n) <= f(n) <= c2g(n)
                        => f(n) = Ω(g(n)) and f(n) <= c2g(n)
        let f(n) = -n, g(n) = -n, take c1 = c2 = 1, f(n) = Θ(g(n))
        but |f(n)| = n > cg(n) for all positive constant c, f(n) != O'(g(n))
    <=: |f(n)| <= cg(n) => f(n) <= cg(n), so f(n) = O'(g(n)) implies f(n) = O(g(n))
        therefore f(n) = O'(g(n)) && f(n) = Ω(g(n)) => f(n) = Θ(g(n))
d.  ~Ωg(n) = {f(n): there exist positive constants c, k and n0 that
                    0 <= cg(n)lg^k(n) <= f(n) for all n >= n0 }
    ~Θg(n) = {f(n): there exist positive constants c1, c2, k1, k2 and n0 that
                    c1g(n)lg^k1(n) <= f(n) <= c2g(n)lg^k2(n) for all n >= n0 }
    =>: f(n) = ~Θ(g(n)) => c1g(n)lg^k1(n) <= f(n) <= c2g(n)lg^k2(n) for all n >= n0
        instantly f(n) = ~O(g(n)) and f(n) = ~Ω(g(n))
    <=: similar, take n0 be the maximum of the two

3-6
a.  n - c
b.  lg*(n)
c.  lg(n)
d.  lg(n) - 1
e.  lglg(n)
f.  ∞
g.  lglg(n)
h.  iter 0: n
    iter 1: n / lgn = (iter 0) / lgn
    iter 2: (n / lgn) / lg(n / lgn) = (iter 1) / (lgn - lglgn)
    iter 3: ((n / lgn) / (lgn - lglgn)) / lg((n / lgn) / (lgn - lglgn))
            = (iter 2) / ((lgn - lglgn) - lg(lgn - lglgn))
    the divisor is never greater than lgn
    therefore lower bound is Ω(log(lgn, n)) = Ω(lgn / lglgn)
    upper bound unknown

Chapter 4
4.1-1
still the maximum subarray, the left and right sum is initialized to -Infinity

4.1-2
./CLRS/start/max-subarray.ts#findMaximumSubarrayBrute

4.1-3
crossover happens between 1024 <= n0 <= 2048
mixing recursive and brute-force algorithms then the crossover is no longer noticeable

4.1-4
return tuple of (Option<Idx>, Option<Idx>, Sum)
initialize left-sum and right-sum to 0, left-max and right-max to None
if all array elements between low and high are non-positive, left-max and right-max will not be reassigned
return value will be (None, None, 0)

4.1-5
if tail_max is the maximum subarray of the form A[i .. j] in an array A[0 .. j]
then all subarrays of the form A[k .. i-1] where k <= i-1 sum to non-positive value
all subarrays of the form A[i .. k] where k < j sum to non-negative value
considering A[k .. j+1],
    if k < i
        the slice can be divided to A[k .. i-1] and A[i .. j+1], where A[k .. i-1] sums to non-positive
        sum of this slice is not greater than A[i .. j+1]
    if i < k <= j
        the sum of the slice can be expressed as sum(A[i .. j+1]) - sum(A[i .. k-1])
        where k - 1 < j and sum(A[i .. k-1]) >= 0
        sum of this slice is not greater than A[i .. j+1]
    if k = j+1
        the sum equals to A[j+1], and A[j+1] > sum(A[i .. j+1]) only if sum(A[i .. j]) < 0
therefore the new tail_max is either A[i .. j+1] or A[j+1], depends on whether sum(A[i .. j]) < 0
./CLRS/start/max-subarray.ts#findMaximumSubarrayLinear

4.2-1
((1, 3), (7, 5)) * ((6, 8), (4, 2))
S1 = B12 - B22 = 6;
S2 = A11 + A12 = 4;
S3 = A21 + A22 = 12;
S4 = B21 - B11 = -2;
S5 = A11 + A22 = 6;
S6 = B11 + B22 = 8;
S7 = A12 - A22 = -2;
S8 = B21 + B22 = 6;
S9 = A11 - A21 = -6;
S10 = B11 + B12 = 14;
P1 = A11 . S1 = 6;
P2 = S2 . B22 = 8;
P3 = S3 . B11 = 72;
P4 = A22 . S4 = -10;
P5 = S5 . S6 = 48;
P6 = S7 . S8 = -12;
P7 = S9 . S10 = -84;
C11 = P5 + P4 - P2 + P6 = 18;
C12 = P1 + P2 = 14;
C21 = P3 + P4 = 62;
C22 = P5 + P1 - P3 - P7 = 66;

4.2-2
./CLRS/start/matrix-mul.ts#strassen

4.2-3
extend size of matrix to the next power of 2
assume size of matrix A is n x n, the next power of 2 is 2^b, the extended matrix looks like
    A   0   and     B   0
    0T  1           0T  1
where 1 denotes (2^b - n) x (2^b - n) unit matrix, 0 denotes n x (2^b - n) zero metrix
the multiplication will be
    AB  0
    0T  1
then AB can be extracted from the result
as n is not a power of 2, 2^b <= 2n
the extension takes time at most (2^b)^2 = O((2n)^2) = O(n^2)
O(n^2) + T(2^b) <= O(n^2) + T(2n) = O(n^2) + Θ((2n)^lg7) = Θ(n^lg7) using strassen's method

4.2-4
assume the meaning of matrices in "3 x 3 matrices" and "n x n matrices" are the same
the problem didn't state what will happen when n > 3
it just gives a potentially easier way to calculate the base case, which is Θ(1) anyway
it's not even clear what kind of multiplication it means by "k multiplications", maybe scalar
so this assumption will not affect the overall complexity, o(n^lg7) not achievable this way

4.2-5
same as above, only gives an easier way to calculate the base case, which is Θ(1) anyway
states nothing about the complexity when n > 72

4.2-6
let A = transpose([A1, A2 .. Ak]), B = [B1, B2 .. Bk], each of Ai, Bj is a n x n matrix
then A . B will be kn x kn
A . B = (Ai . Bj), k^2 multiplications of n x n matrices in total
assume matrix multiplication is O(g(n)), then T(n) = O(k^2 * g(n))
using strassen's algorithm g(n) = n^ln7, T(n) = O(k^2 * n^ln7)

4.2-7
let p1 = (a+b)(c+d) = ac + ad + bc + bd
    p2 = ac
    p3 = bd
then 
    real component = p2 - p3
    complex component = p1 - p2 - p3

4.3-1
assume T(n-1) <= (n-1)^2
T(n)    <= (n-1)^2 + n
        = n^2 - 2n + 1 + n 
        <= n^2
implicitly c = 1, T(n) = O(n^2)

4.3-2
assume T([n/2]) <= lg(n/2)
T(n)    <= lg(n/2) + 1
        = lgn - 1 + 1
        <= lgn
implicitly c = 1, T(n) = O(lgn)

4.3-3
assume T([n/2]) >= c(n/2)lg(n/2)
T(n)    >= 2 * c(n/2)lg(n/2) + n
        = cnlg(n/2) + n
        = cnlgn - cn + n
take 0 < c < 1, then T(n) >= cnlgn, T(n) = Ω(nlgn)

4.3-4
assume T([n/2]) <= c(n/2)lg(n/2) + 1
T(n)    <= 2 * c(n/2)lg(n/2) + 1 + n
        = cnlgn - cn + n + 1
        <= cnlgn when c > 2
T(1)    <= clg1 + 1 = 1

4.3-5
assume T([n/2]) <= c(n/2)lg(n/2) + 1
for any c1n = Θ(n)
T(n)    <= 2 * c(n/2)lg(n/2) + 1 + c1n
        = cnlgn - cn + 1 + c1n
        <= cnlgn + 1 when c <= c1
thus T(n) = O(nlgn)
assume T([n/2]) >= c(n/2)lg(n/2) + 1
for any c1n = Θ(n)
T(n)    >= 2 * c(n/2)lg(n/2) + 1 + c1n
        = cnlgn - cn + 1 + c1n
        >= cnlgn + 1 when c <= c1
thus T(n) = Ω(nlgn)
T(1)    = clg1 + 1 = 1 = Θ(1)
therefore T(n) = Θ(nlgn)

4.3-6
assume T(n) <= nlgn
T(n)    <= 2 * (n/2 + 17)lg(n/2 + 17) + n
        <= 2 * (n/2)lg(n/2) + n
        = nlgn - n + n = nlgn
T(n) = O(nlgn)

4.3-7
assume T(n) <= cn^log(3, 4)
T(n)    <= 4 * c(n/3)^log(3,4) + n
        = 4 * n^log(3,4)/4 + n
        = n^log(3,4) + n
        stuck
assume T(n) <= c1n^log(3,4) - c2n
T(n)    <= 4 * c1(n/3)^log(3,4) - 4 * c2(n/3) + n
        = c1n^log(3,4) - (4/3) * c2n + n
        <= c1n^log(3,4) - c2n when 4/3 * c2 - 1 >= c2
T(n) = O(n^log(3,4))
assume T(n) >= c1n^log(3,4) + c2n
T(n)    >= 4 * (c1(n/3)^log(3,4) - c2(n/3)) + n
        = c1n^log(3,4) - (4/3)*c2n + n
        >= c1n^log(3,4) - c2n when (4/3) * c2 - 1 <= c2
T(n) = Ω(n^log(3,4))
therefore T(n) = Θ(n^log(3,4))

4.3-8
the conclusion is incorrect, the recurrence
    T(n) = 4T(n/2) + n^2
is case 2 of master theorem, which means T(n) = Θ(n^2lgn)

4.3-9
T(n) = 3T(n^(1/2)) + lgn
let m = lgn, then
T(2^m)  = 3T((2^m)^(1/2)) + m
        = 3T(2^(m/2)) + m
let S(m) = T(2^m)
S(m) = 3S(m/2) + m
assume S(m) <= c1m^lg3 - c2m
S(m)    <= 3c1(m/2)^lg3  - (3/2)c2m + m
        = c1m^lg3 - (3/2)c2m + m
        <= c1m^lg3 - c2m when (3/2)c1 - 1 < c2
S(m) = O(m^lg3)
similarly, S(m) = Ω(m^lg3), S(m) = Θ(m^lg3)
T(n) = T(2^m) = S(m) = Θ(m^lg3) = Θ(lgn^lg3)

4.4-1
T(n) = 3T(n/2) + n
n/2 every level, lgn levels in total
for i = 0, 1 .. lgn, ith level has 3^i nodes, each node is n/2^i
the base level has 3^lgn = n^lg3 nodes, each is Θ(1)
Σ(3/2)^i will not converge
T(n)    = Σ(i = {0..lgn - 1})(3/2)^i * n + Θ(n^lg3)
        = ((3/2)^lgn - 1) / (3/2 - 1) * n + Θ(n^lg3)
        = 2(n^(lg3 - 1)/(3/2) - 1) * n + Θ(n^lg3)
        = O(n^lg3) + Θ(n^lg3)
        = O(n^lg3)
assume T(n) <= c1n^lg3 - c2n
T(n)    <= 3c1(n/2)^lg3 - 3c2(n/2) + n
        = cn^lg3 - (3/2)c2n + n
        <= c1n^lg3 - c2n when (3/2)c2 - 1 >= c2

4.4-2
T(n) = T(n/2) + n^2
n/2 every level, lgn levels in total 
for i = 0, 1 .. lgn, ith level has a single node, each node is (n/2^i)^2 = n^2/4^i
the base level has 1 node, Θ(1) in total
T(n)    = Σ(i = {0 .. lgn - 1})(1/4)^i * n^2 + Θ(1)
        <= 1/(1 - 1/4) * n^2 + Θ(1)
        = O(n^2)
without induction, this must be a tight bound, as T(n) >= n^2

4.4-3
T(n) = 4T(n/2 + 2) + n, assume it's asymptotically equivalent to T(n) = 4T(n/2) + n
n/2 every level, lgn levels in total
for i = 0, 1 .. lgn, ith level has 4^i nodes, each node is n/2^i
the base level has 4^lgn nodes, Θ(n^2) in total
T(n)    = Σ(i = {0 .. lgn - 1})2^i * n + Θ(n^2)
        = (2^lgn - 1) / (2 - 1) * n + Θ(n^2)
        = (n/2 - 1) * n + Θ(n^2)
        = O(n^2)
assume T(n) <= c1n^2 - c2n
T(n)    <= 4 * (c1(n/2 + 2)^2 - c2(n/2 + 2)) + n
        = c1n^2 + 4c1n + 4c1 - 2c2n + 8 + n
        = c1n^2 + (4c1 - 2c2 + 1)n + 8
        <= c1n^2 - c2n when 2c2 - 4c1 >= c2 and n is big enough
therefore T(n) = O(n^2)

4.4-4
n - 1 every level, n levels in total
for i = 0, 1, .. n, ith level has 2^i nodes, each node is 1
the base level has 2^n nodes, Θ(2^n) in total
T(n)    = Σ(i = {0 .. n - 1})2^i + Θ(2^n)
        = O(2^n)
assume T(n) <= c2^n - 1
T(n)    <= 2 * c2^(n-1) - 2 + 1
        = c2^n - 1
therefore T(n) = O(2^n)

4.4-5
T(n) = T(n-1) + T(n/2) + n
the longest simple path is of length n
as n - 1 + n/2 < (3/2)n, the sum of ith level is smaller than (3/2)^i * n
the base level have at most 2^n nodes, Θ(2^n) in total
T(n)    <= Σ(i = {0 .. n - 1})(3/2)^i * n + Θ(2^n)
        = Θ((3/2)^n * n) + Θ(2^n)
        = O(2^n)
assume T(n) <= c2^n
T(n)    <= c2^(n-1) + c2^(n/2) + n
        <= c2^n for large enough n

4.4-6
thanks https://ita.skanev.com/04/04/06.html
the shortest simple path from top to root is log(3, n)
each level still cn, this time the binary tree up to level log(3, n) is guarenteed to be complete
therefore T(n) >= log(3, n) * cn = Θ(nlgn), T(n) = Ω(nlgn)

4.4-7
cn
4c(n/2)
16c(n/4)
...
lgn levels in total, ith level has 4^i nodes, each is cn/2^i
the base level has 4^lgn = n^2 nodes, Θ(n^2) in total
T(n)    = Σ(i = {0 .. lgn - 1})2^i * cn + Θ(n^2)
        = Θ(2^lgn * cn) + Θ(n^2)
        = Θ(n^2)
assume T(n) <= c1n^2 - c2n
T(n)    <= 4 * (c1(n/2)^2 - c2(n/2)) + cn
        = c1n^2 - 2c2n + cn
        <= c1n^2 - c2n when c2 >= c
T(n) = O(n^2), similarly T(n) = Ω(n^2)

4.4-8
as a is a constant, T(a) = Θ(1)
T(n) = T(n-a) + Θ(1) + cn
n/a levels in total, ith level has 1 node, each node c(n - ia) + Θ(1)
base level a single node of Θ(1)
T(n)    = Σ(i = {0 .. n/a - 1})(c(n - ia) + Θ(1)) + Θ(1)
        = Θ(n/a) + cn^2/a - Θ((n/a)^2) * a
        = Θ(n^2)

4.4-9
reuse the solution to T(n) = T(n/3) + T(2n/3) + O(n) in text and problem 4.4-6
T(n) = Ω(nlgn) and T(n) = O(nlgn), T(n) = Θ(nlgn)

4.5-1
a.  a = 2, b = 4, log(b, a) = 1/2, f(n) = 1 = n^0, 1/2 > 0
    T(n) = Θ(n^(1/2))
b.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n^(1/2) = n^log(b, a)
    T(n) = Θ(n^(1/2)lgn)
c.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n, 1/2 < 1
    T(n) = Θ(n)
d.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n^2, 1/2 < 2
    T(n) = Θ(n^2)

4.5-2
log(4, a) < log(2, 7) = log(4, 49)
a < 49

4.5-3
a = 1, b = 2, log(b, a) = 0, f(n) = 1 = n^0 = n^log(b, a)
T(n) = Θ(lgn)

4.5-4
a = 4, b = 2, log(b, a) = 2, f(n) = n^2lgn
f(n) = Ω(n^log(b, a)) but not polynomially larger, master method doesn't apply

4.5-5
f(n) = n^(ε + |sin(n)|), a = 1, b = 2 for some small positive constant very close to 0
log(b, a) = 0, ε + |sin(n)| >= ε > 0
for any n/2 = 2kπ + π/2, sin(n/2) = 1, sin(n) = 0
af(n/b) = (n/2)^(ε + |sin(n/2)|) >= n/2
when n is large
cf(n) <= n^ε < n/2 for infinitely many k and n

4.6-1
n0 = n, n1 = [n/b], n2 = [[n/b]/b]
if b is an integer
n = kb + r, 0 <= r < b, [n/b] = k if r = 0, k + 1 if r > 0
as b is an integer, b > 1 => b >= 2, k = k'b + r', 0 <= r' < b, then
[[n/b]/b]   = k' if r = r' = 0
            = k' + 1 otherwise
n = kb + r = (k'b + r')b + r = k'b^2 + r'b + r
where r < b, r'b <= b(b-1), r'b + r < b^2
[n/b^2] = k' if r = r' = 0
        = k' + 1 otherwise
therefore [[n/b]/b] = [n/b^2] for all integer n
n2 = [[n/b]/b] = [n/b^2]
n3 = [n2/b] = [n/b^3]
...
nj = [n/b^j]

4.6-2
f(n) = Θ(n^log(b, a)lg^kn), then
g(n)    = Σ(j{0 .. log(b, n) - 1})(a^j * f(n/b^j))
        = {0 .. log(b, n) - 1})(a^j * Θ((n/b^j)^log(b, a) * lg^k(n/b^j)))
where
    Θ((n/b^j)^log(b, a) * lg^k(n/b^j)))
    = Θ((n^log(b, a) / a^j) * lg^k(n/b^j)
g(n)    <= Σ{0 .. log(b, n) - 1})(a^j * c1(n^log(b, a) / a^j) * lg^k(n/b^j))
        = Σ(c1 * n^log(b, a) * lg^k(n/b^j))
        = Σ(c1 * n^log(b, a) * lg(n / b^j)^k)
        = Σ(c1 * n^log(b, a) * (lgn - j * lgb)^k)
        <= c1 * n^log(b, a) * Σ(lg^k(n))
        = c1n^log(b, a) * log(b, n) * lg^k(n)
        = c1n^log(b, a) * O(lgn) * O(lg^k(n))
        = O(n^log(b, a) * lg^k+1(n))
similarly
g(n)    >= c2 * n^log(b, a) * Σ(j = {0 .. log(b, n) - 1})(lg(n/b^j)^k)
take only half of the summation terms,
g(n)    >= c2 * n^log(b, a) * Σ(j = {0 .. log(b, n) / 2})(lg(n/b^j)^k)
take the minimum over all summation terms
        >= c2 * n^log(b, a) * Σ(j = {0 .. log(b, n) / 2})(lg(n/b^(log(b, n) / 2))^k)
        = c2 * n^log(b, a) * (log(b, n) / 2)(lg(n^(1/2))^k)
        = c2 * n^log(b, a) * (log(b, n) / 2)(1/2 * lgn)^k
        = c2 * n^log(b, a) * Θ(lgn) * Θ(lg^kn)
        = Ω(n^log(b, a) * lg^(k+1)(n))
therefore T(n) = Θ(n^log(b, a)) + Θ(n^log(b, a) * lg^(k+1)(n))

4.6-3
thanks https://ita.skanev.com/04/06/03.html
af(n/b) <= cf(n)
=>  f(n)    >= (a/c)f(n/b)
            >= (a/c)^2f(n/b^2)
            ..
            >= (a/c)^if(n/b^i)
take i = log(b, n)
    f(n)    >= (a/c)^log(b, n)f(1)
            = Θ((a/c)^log(b, n))
            = Θ(n^log(b, a/c))
            = Θ(n^(log(b, a) - log(b, c)))
as c < 1, log(b, c) < 0, -log(b, c) > 0, let ε = -log(b, c)
    f(n)    >= Θ(n^(log(b, a) + ε))
    f(n)    = Ω(n^log(b, a) + ε)

4-1
a.  a = 2, b = 2, log(b, a) = 1, f(n) = n^4, 4 > 1
    2f(n/2) = n^4 / 8, c = 1/8 < 1
    T(n) = Θ(n^4)
b.  a = 1, b = 10/7, log(b, a) = 0, f(n) = n^1, 1 > 0
    c = 7/10 < 1
    T(n) = Θ(n)
c.  a = 16, b = 4, log(b, a) = 2, f(n) = n^2
    T(n) = Θ(n^2lgn)
d.  a = 7, b = 3, log(b, a) = log(3, 7) < 2, f(n) = n^2
    7f(n/3) = 7n/9, c = 7/9
    T(n) = Θ(n^2)
e.  a = 7, b = 2, log(b, a) = lg7 > 2, f(n) = n^2
    T(n) = Θ(n^lg7)
f.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n^(1/2)
    T(n) = Θ(n^(1/2)lgn)
g.  n - 2 every level, n/2 levels in total
    every level has a single node, each contributes (n - 2i)^2
    the base level has a single node of Θ(1)
    T(n)    = Σ(i = {0 .. n/2 - 1})(n - 2i)^2 + Θ(1)
            <= n/2 * n^2 + Θ(1) = O(n^3)
    similarly, take only half of the summation terms
    T(n)    >= Σ(i = {0 .. n/4})(n - 2i)^2 + Θ(1)
            >= Σ(i = {0 .. n/4})(n - n/2)^2 + Θ(1)
            = (n/4) * (n/2)^2 + Θ(1) = Ω(n^3)
    therefore T(n) = Θ(n^3)

4-2
a.  1.  Θ(lgN) as usual
    2.  now each call have to copy the whole array
        the function at most recursively call itself lgn times
        total cost NlgN
    3.  T(n) = T(n/2) + n
        T(N) = Θ(N) by master method
b.  1.  Θ(NlgN) as usual
    2.  all recursive call to the function itself forms a complete binary tree of height lgN
        2^lgN = N nodes in total, the whole array is copied at each node, contributes N * N = N^2 in total
        the number of merge is half the number of nodes, again Θ(N) and contributes Θ(N^2) in total
        overall running time Θ(N^2)
    3.  T(n) = 2T(n/2) + 2 * (n/2) + n = 2T(n/2) + Θ(n) = Θ(nlgn)

4-3
a.  a = 4, b = 3, log(b, a) = log(3, 4) > 1, f(n) = nlgn = O(n^(1 + ε))
    T(n) = Θ(n^log(3, 4))
b.  T(n) = 3T(n/3) + n/lgn = 3T(n/3) + cn/log(3, n)
    3/n every level, log(3, n) levels in total
    ith level has 3^i nodes, contributes 3^i * c(n/3^i) / log(3, (n/3^i)) = cn / (log(3, n) - i)
    the base level has 3^log(3, n) = n nodes, Θ(n) in total
    T(n)    = Σ(i = {0 .. log(3, n) - 1})(n / (log(3, n) - i)) + Θ(n)
            = n * Σ(i = {0 .. log(3, n) - 1}(1 / (log(3, n) - i))) + Θ(n)
    the harmonic series Σ(i = {0 .. n - 1})(1 / (n - i)) is asymptotically close to ln(n)
    T(n)    = n * Θ(ln(log(3, n))) + Θ(n)
            = Θ(nlglgn)
c.  a = 4, b = 2, log(b, a) = 2, f(n) = n^2.5, 2.5 > 2
    T(n) = Θ(n^2.5)
d.  guess with master method, T(n) = Θ(nlgn)
    assume T(n) <= nlgn - cn
    T(n)    <= 3((n/3 - 2) * lg(n/3 - 2)) - 3c(n/3 - 2) + n/2
            <= nlg(n/3 - 2) - 3c(n/3 - 2) + n/2
            <= nlg(n/3) - cn + 6c + n/2
            = nlgn - (c + lg3 - 1/2)n + 6c
            <= nlgn - cn when n is large enough
    simlarly T(n) >= nlgn + cn for some c, T(n) = Θ(nlgn)
e.  similar to b, T(n) = Θ(nlglgn)
f.  the shortest path from bottom to root passes log(8, n) = lgn/3 nodes
    immediate children of a node of value p has combined value p/2 + p/4 + p/8 = 7p/8
    therefore ith level contributes (7/8)^i * n
    there are 3^(lgn/3) = n^(lg3 / 3) nodes at level lgn/3, each Θ(1), Θ(n^(lg3 / 3)) in total
    T(n)    >= Σ(i = {0 .. lgn/3 - 1})((7/8)^i * n) + Θ(n^(lg3 / 3))
            = n * ((7/8)^(lgn/3) - 1) / (7/8 - 1) + Θ(n^(lg3 / 3))
            = 8n * (1 - n^(lg7 / 3) / n) + Θ(n^(lg3 / 3))
            = Θ(n) - Θ(n^(lg7 / 3)) + Θ(n^(lg3 / 3))
            = Ω(n)
    assume T(n) <= cn
    T(n)    <= c(n/2 + n/4 + n/8) + n
            = (7/8)cn + n
            <= cn when c >= 8
    therefore T(n) = Θ(n)
g.  n - 1 every level, n levels in total, a single node every level, contributes 1/(n-i)
    the base level is a single node of Θ(1)
    T(n)    = Σ(i = {0 .. n-1})(1/(n-i)) + Θ(1), the harmonic series
            = Θ(ln(n)) + Θ(1)
            = Θ(lgn)
h.  n - 1 every level, n levels in total, ith level sums to lg(n - i)
    T(n)    = Σ(i = {0 .. n-1})(lg(n-i)) + Θ(1)
            = Θ(nlgn) as int(lgx) = x(lgx - 1)
i.  n - 2 every level, n/2 levels in total, ith level sums to 1/lg(n - 2i)
    T(n)    = Σ(i = {0 .. n/2 - 1})(1/lg(n - 2i)) + Θ(1)
            = Θ(li(n)), where li(n) is the logarithmic integral function
j.  let n = 2^m, m = lgn, T(n) = T(2^m) = 2^(m/2)T(2^(m/2)) + 2^m
    let S(m) = T(2^m), S(m) = 2^(m/2)S(m/2) + 2^m
    m/2 each level, lgm levels in total
    first level has 2^(m/2) nodes, the next 2^(m/2) * 2^(m/4), ...
    ith level will have 2^(m - m/2^i) nodes, each contributes 2^(m/2^i), 2^m in total
    the base level has 2^(m - m/2^(lgm)) = 2^(m-1) nodes, Θ(2^(m-1)) in total
    S(m)    = lgm * 2^m + Θ(2^(m-1)) = Θ(lgm * 2^m)
    T(n) = T(2^m) = S(m) = Θ(2^mlgm) = Θ(nlglgn)

4-4
a.  F(z)    = Σ(i = {0 ..})(Fiz^i)
    zF(z)   = Σ(i = {0 ..})(Fiz^(i+1))
            = Σ(i = {1 ..})(F(i-1)z^i)
            = Σ(i = {2 ..})(F(i-1)z^i)  // as F(0) = 0
    z^2F(z) = Σ(i = {0 ..})(Fiz^(i+2))
            = Σ(i = {2 ..})(F(i-2)z^i)
    z + zF(z) + z^2F(z) = 0 + z + Σ(i = {2 ..})(F(i-1)z^i + F(i-2)^zi)
                        = 0 + z + Σ(i = {2 ..})(Fiz^i)
                        = F(z)
b.  F(z) = z + zF(z) + z^2F(z)
    F(z) - zF(z) - z^2F(z) = z
    F(z)(1 - z - z^2) = z
    F(z) = z / (1 - z - z^2)
    let g = (1 + r5) / 2, g' = (1 - r5) / 2, r5 = 5^(1/2)
    (1 - gz)(1 - g'z)   = (1 - (1 + r5)z / 2)(1 - (1 - r5)z / 2)
                        = 1 - (1 - r5)z / 2 - (1 + r5)z / 2 + (1 + r5)(1 - r5)z^2 / 4
                        = 1 - z + (-4)z^2 / 4
                        = 1 - z - z^2
    (1/r5) * (1 / (1 - gz) - 1 / (1 - g'z))
    = (1/r5) * (- g'z + gz) / ((1 - gz) * (1 - g'z))
    = (1/r5) * (r5z) / ((1 - gz)(1 - g'z))
    = z / ((1 - gz)(1 - g'z))

4-5
a.  divide the chips into three groups
    first group of size n1 are good chips, which will always give the correct answer
    second group of size n2 are bad chips imitating good chips
    third group of size n3 are other bad chips
    as the number of bad chips n2 + n3 > n/2
    it's always possible to take n2 = n1 and n3 = n - n1 - n2 will be non-negative, then
    1.  chips in the first group will always answer "good" for chips in that group, "bad" for chips in group 2 and 3
    2.  let chips in group 2 imitate group 1
        always answer "good" for chips in the same group, "bad" for chips in group 1 and 3
    3.  let chips in group 3 answer "good" for any chips
    then chips in group 3 can be easily detected: they answer "good" for all chips, but only n2 < n/2 chips are good
    however group 1 and 2 are totally symmetric:
        they both have size n1 = n2 < n/2
        they only answer "good" for chips in the same group, which also answers "good" in the pairwise test
        they answer "bad" for chips not in that group
    no matter how many test is performed, it's impossible to tell if a chip is in group 1 or group 2
b.  assume n1 in n chips are good, where n1 > n/2
    starting from a set S containing all n chips, and an empty set S'
    repeat the following procedure [n/2] times:
        randomly pop two chips c1 and c2 from S, perform the test
        if any of the two reports "bad", drop both chips
        if both chips answer "good", drop c2 and add c1 to S'
    let n1 be the number of good chips tested, n2 be the number of bad chips tested
    let n11 be the number of good chips tested against a good chip, n12 otherwise (tested against a bad chip)
    similarly define n21 and n22, obviously n12 = n21, both n11 and n22 have to be even
    when n is even, n1 = n11 + n12, n2 = n21 + n22, n1 > n2
    the number of good chips in S' at the end will be n11 / 2, bad chips n22 / 2
    n11 = n1 - n12, n22 = n2 - n21 = n2 - n12, so n11 > n22, still more good chips than bad chips in S'
    when n is odd, one chip cannot be tested
    if that chip is bad, n1 > n2 + 1, n1 >= n2 + 2, n11 >= n22 + 2, n11/2 >= n22/2 + 1
    the bad chip must be dropped or number of good and bad chips may equal when n11/2 = n22/2 + 1
    if that chip is good, n1 >= n2, n1 >= n2, n11/2 >= n22/2
    the good chip must be included or number of good and bad chips may equal when n11/2 = n22/2
    thanks http://cseweb.ucsd.edu/classes/su99/cse101/sample1.pdf
    (n11 + n12) / 2 can be observed as the number of tests answered "good, good"
    if (n11 + n12) / 2 is even, add the untested chip to S'
    if (n11 + n12) / 2 is odd, drop the untested chip
    then the algorithm will act correctly on boundary conditions
c.  the single good chip can be find in time T(n), where
        T(n) = T(n/2) + [n/2]
        T(n) = Θ(n) by master method
    once a single good chip is found, it can be used to find all other good chips in Θ(n) tests
    overall time cost Θ(n)

4-6
a.  given
        A[i, j] + A[i+1, j+1] <= A[i, j+1] + A[i+1, j]
    for some r that 1 <= r < m - i, assume
        A[i, j] + A[i+r, j+1] <= A[i, j+1] + A[i+r, j]
    combine the assumption with
        A[i+r, j] + A[i+r+1, j+1] <= A[i+r, j+1] + A[i+r+1, j]
        A[i, j] + A[i+r+1, j+1] <= A[i, j+1] + A[i+r+1, j]
    therefore for any 1 <= r < m,
        A[i, j] + A[i+r, j+1] <= A[i, j+1] + A[i+r, j]
    similarly, apply the induction on columns
        A[i, j] + A[i+r, j+c] <= A[i, j+c] + A[i+r, j]
    for 1 <= r < m - i and 1 <= c < n - j
    take r = k - i and c = l - j, this is exactly A[i, j] + A[k, l] <= A[i, l] + A[k, j]
    conversely, take k = i+1, l = j+1, 
        A[i, j] + A[k, l] <= A[i, l] + A[k, j] => A[i, j] + A[i+1, j+1] <= A[i, j+1] + A[i+1, j]
b.  change 22 on first line to 24
c.  if f(i) > f(i+1) instead, by definition of Monge,
        A[i, f(i+1)] + A[i+1, f(i)] <= A[i+1, f(i+1)] + A[i, f(i)]
    by definition of f(i), A[i, f(i)] and A[i+1, f(i+1)] is the leftmost minimum of their rows, therefore
        A[i, f(i+1)] >= A[i, f(i)], A[i+1, f(i)] >= A[i+1, f(i+1)]
        A[i, f(i+1)] = A[i, f(i)], A[i+1, f(i)] = A[i+1, f(i+1)]
    A[i, f(i+1)] is also the minimum of row i, but then f(i+1) < f(i) and A[i, f(i)] is not the leftmost minimum
d.  the leftmost minimum of even rows are
        f(2), f(4), f(6) ..
    as proven in part c, f(1) <= f(2) <= f(3) <= ..
    therefore it's sufficient to search f(1) as the minimum among A[1, 1] .. A[1, f(2)]
    f(3) among A[3, f(2)] .. A[3, f(4)] and so on
    at most 2m + n elements have to be compared, Θ(m + n) in total
e.  the base case of finding the minimum of a row is O(n)
    T(m, n) = O(n) when m = 1
            = T(m/2, n) + O(m + n) otherwise
    the recurrence tree has lgm levels
    ith level has a single node contributes O(m/2^i + n)
    the base level is a single node of O(n)
    T(n)    = Σ(i = {0 .. lgm})O(m/2^i + n) + O(n)
            <= Σ(i = {0 .. lgm})c1(m/2^i + n) + c2n
            <= c1m + c1nlgm + c2n
            = O(m + nlgm)

Chapter 5
5.1-1
it's always possible to decide the best between any two candidates
which means either a > b or b > a for two candidates a and b
for a fixed set of candidates C and any candidate c in it,
    its rank, as the number of candidates better than it - 1, is fixed
    no candidate can have rank lower than |C|
    no two candidates can have the same rank, assume c has rank k,
        exactly (k-1) candidates are better than it, none of which can have rank equal or higher than k
        exactly (|C| - k) candidates are worse than it, none of which can have rank equal or lower than k
    therefore the rank of the candidates are a bijection C => {1 .. |C|}, where {1 .. |C|} has a total order

5.1-2
let 2^p be the next power of two of b - a
take p results of RANDOM(0, 1), interpret the result array as a binary number n
than n is uniformly distributed between 0 and 2^p - 1
if n <= b - a, return n + a, otherwise run the procedure again
let x <- S denote that n follows discrete uniform distribution over set S
as n <- {0 .. 2^p - 1}, if n <= b - a, n <- {0 .. a - b}, n + a <- {a .. b}
as 2^p is the next power of two of b - a, 2^(p-1) <= b - a < 2^p
the number of calls to the procedure follows geometric distribution with success probability (b - a) / 2^p >= 1/2
therefore the expected number of calls is 2 and E[T(a, b)] = Θ(p) = Θ(lg(b - a))

5.1-3
let A and B denote two independent runs of procedure BIASED-RANDOM
p((A, B) = (1, 1)) = p^2
p((A, B) = (0, 0)) = (1-p)^2
p((A, B) = (1, 0)) = p((A, B) = (0, 1)) = p(1-p)
(A, B) = (1, 0) and (A, B) = (0, 1) happen with equal probability
define RANDOM as:
    run (a, b) = (BIASED-RANDOM(), BIASED-RANDOM());
    if (a, b) == (1, 0):
        return 1;
    else if (a, b) == (0, 1):
        return 0;
    else:
        recursively call RANDOM itself
the number of calls to RANDOM follows geometric distribution with success probability 2p(1-p) = 2p - 2p^2
the expected number of calls is 1/(2p - 2p^2), E[T(n)] = Θ(1/(p - p^2))

5.2-1
one time:   the first candidate is hired and is the best among all the candidates
            p = 1/n
n times:    every candidate is hired, implying the candidates are presented in strictly increasing order
            p = 1/n!

5.2-2
the first candidate is hired anyway
assume the first candidate has rank r1, the second hired candidate has rank r2
r1 != 1, or the second hire cannot happen
the best candidate definitely will be hired, thereby r2 = 1
candidate with rank 1 is the first candidate with rank higher than r1 after the first among all r1 - 1 such candidates
p = 1 / (r1 - 1)
as r1 <- {2 .. n}, p = (1 / n-1) * Σ(1 / (r1 - 1))

5.2-3
E[ΣXi] = Σ(E[Xi]) = 3.5n

5.2-4
define Xi as
Xi  = 1 when ith customer get back own hat
    = 0 otherwise 
for any Xi, E[Xi] = P(ith customer get back own hat) = 1/n
expected number of customers who get back their own hat = E[Σ(Xi)] = n * 1/n = 1

5.2-5
when A[1 .. n] is a uniform random permutation, A[i] <- {1 .. n} for all i when viewed independently
for a pair i < j, A[i] <- {1 .. n}, A[j] <- {1 .. n} - A[i]
p(A[i] < A[j])  = (1 / n) * Σ(A[i] = {1 .. n})((A[i] - 1) / (n - 1))
                = 1/2 asymptotically
p(inversion) = p(A[i] > A[j]) = 1 - p(A[i] < A[j]) = 1/2
define Ii,j as
Ii,j    = 1 when A[i] > A[j]
        = 0 otherwise
then the expected number of inversions is E[ΣIi,j] = C(n, 2) * E[Ii,j] = n(n - 1) / 4

5.3-1
swap A[1] with A[RANDOM(1, n)] before the loop, start the loop with i = 2
the same proof still implies but now starting with 1-permutations

5.3-2
after the procedure, A[1] is guarenteed not to be in it's original place
at least n!/n = (n-1)! permutations are excluded, not only the identity

5.3-3
for an array of size 3, A = [a1, a2, a3]
let A' denote the array after calling PERMUTE-WITH-ALL(A)
every swap may have 3 possible consequences, the procedure overall may have 3^3 = 27 consequences
there are 3! = 6 different permutations of A, there's no way to divide 27 comsequences evenly among 6 permutations
there must be some permutations Ap of A that p(A' = Ap) != 1/6

5.3-4
p(A[i] ends up in B[j]) = p(offset = (i - j) mod n) = 1/n
this procedure can only produce cyclic shifting of an array
for an array of size n, there are only n different shiftings
other n! - n permutations may never be produced

5.3-5
let Ei denote the event ith element in the array is different to all the previous elements
P[E1 ∩ E2 ∩ E3 .. En]   = P[E1] * P[E2 | E1] * P[E3 | E1 ∩ E2] .. 
                        = 1 * (n^3 - 1) / n^3 * .. * (n^3 - n) / n^3
                        >= ((n^3 - n) / n^3)^n
                        = ((n^2 - 1) / n^2)^n
                        = (1 - 1/n^2)^n
thanks https://ita.skanev.com/05/03/05.html
                        >= 1 - 1/n

5.3-6
the algorithm should also produce a uniform permutation when all n priorities are identical
the sorting algorithm is not specified, it can be stable or instable
when it's stable it will not change the array in that case, when unstable the result is UB
therefore the procedure either have to shuffle the array in other meanings (using no sorting)
or have to regenerate the priorities until no priorities are identical

5.3-7
when m = 0, ∅ is the only set with no member, returning ∅ is thus returning a uniformly random 0-subset
assume RANDOM-SAMPLE(m-1, m-1) returns uniformly random (m-1)-subset S' of U = {1 .. n-1}
each with probability 1/C(n-1, m-1)
i = RANDOM(1, n), p(i ∈ S) = (m-1) / n
for a particular m-subset S of U = {1 .. n}, S = {s1, s2 .. sm}
when n ∉ S, RANDOM-SAMPLE(m, n) will only return S when
    S'  = S - si for some i, p = 1/C(n-1, m-1)
        there are C(m, m-1) = m such (m-1)-subsets
    si  = RANDOM(1, n), p = 1/n, as si != n, si is included in S
p(S)    = 1/C(n-1, m-1) * m/n
        = (m-1)!(n-m)! / (n-1)! * m/n
        = m!(n-m)! / n! = C(n, m)
when n ∈ S, S' may not contain n, as S' is a subset of {1 .. n-1}
the only way RANDOM-SAMPLE(m, n) may return S is 
    S'  = S - n, only one (m-1)-subset satisfies, p = 1/C(n-1, m-1)
    i   = RANDOM(1, n) ∈ S' or i = n, p = (m-1)/n + 1/n = m/n
p(S)    = 1/C(n-1, m-1) * m/n = C(n, m)
therefore RANDOM-SAMPLE(m, n) returns any m-subset of {1 .. n} with probability C(n, m)

5.4-1
let n = 365 denote the days in a year
p(there is at least one in all k people whose birthday is a specific day i)
= 1 - ((n-1)/n)^k >= 1/2
1/2 >= ((n-1)/n)^k
-1 >= k(lg(n-1) - lgn)
k >= 1 / (lgn - lg(n-1))
k >= 253

5.4-2
// wasted several hours here by misintepreting the problem as "until all bins contain at least 2 balls"
the same as the birthday problem

5.4-3
the exact analysis depends on mutual independence of variables
the analysis with indicator variables will still apply if the variables are just pairwise independent
    for any i and j, E[Xij] still is 1/n

5.4-4
assume their birthday are mutually independent, n = 365, m people in total
define Xijk as
    Xijk    = 1 when i, j, k have the same birthday
            = 0 otherwise
p(i, j, k have the same birthday)
= p(j, k have the same birthday as i)
= 1/n^2
E[X]    = ΣΣΣ(Xijk)
        = C(m, 3) * 1/n^2
        = m!/6(m-3)! * 1/n^2
        = m(m-1)(m-2)/6n^2 >= 1
m(m-1)(m-2) > (m-2)^3 >= 6n^2
m >= 95

5.4-5
it's the complement of birthday paradox
a k-string is a k-permutation only if it doesn't repeat any set element
p   = 1 * 1/(n-1) * 1/(n-2) * .. * 1/(n-k+1)
    = 1/(n!/(n-k)!)
    = (n-k)!/n!

5.4-6
define indicator Xi as
Xi  = 1 when bin i is empty after n tosses
    = 0 otherwise
E[Xi]   = p(bin i is empty after n tosses)
        = ((n-1)/n)^n
        = (n-1)^n / n^n
E[X]    = E[ΣXi]
        = ΣE[Xi]
        = n * (n-1)^n / n^n
        = (n-1)^n / n^(n-1)
define indicator Yi as
Yi  = 1 when bin i has 1 ball after n tosses
    = 0 otherwise
E[Yi]   = p(bin i has 1 ball after n tosses)
        = C(n, 1) * (1/n) * ((n-1)/n)^(n-1)
        = (n-1)^(n-1) / n^(n-1)
E[Y]    = E[ΣYi]
        = Σ(E[Yi])
        = n * (n-1)^(n-1) / n^(n-1)
        = (n-1)^(n-1) / n^(n-2)

5.4-7
divide all coin tosses to slices of length lgn - 2lglgn, all slices are mutually independent
p(a single slice is all head)   = 1/2^(lgn - 2lglgn)
                                = 1/(2^lgn/2^2lglgn)
                                = lg^2n / n
p(no slice is all head) = (1 - lg^2n / n)^(n / (lgn - 2lglgn))
                        <= e^(-lg^2n / n * (n/(lgn - 2lglgn)))
                        = e^(-lg^2n / (lgn - 2lglgn))
                        = n^(-lgn / (lgn - 2lglgn))
                        <= n^(-lgn / lgn)
                        = 1/n
"no slice is all head" is the union of two events:
    1. the longest head streak is no longer than lgn - 2lglgn - 1
    2. the longest head streak is longer than lgn - 2lglgn - 1, but is divided between two slices
therefore 
p(longest head streak is no longer than lgn - 2lglgn - 1) <= 1/n
inject this into the original prove,
E[L] = Ω(lgn - 2lglgn)

5-1
a.  let Xk be the number represented by i after k increments
    X0 = 0 = n0
    assume E[Xk] = n
    X(k+1)  = Xk + (ni+1 - ni), p = 1/(ni+1 - ni)
            = Xk, p = 1 - (1 / (ni+1 - ni))
    E[X(k+1)]   = 1/(ni+1 - ni) * (E[Xk] + (ni+1 - ni)) + (1 - 1/(ni+1 - ni)) * E[Xk]
                = E[Xk] + 1/(ni+1 - ni) * (ni+1 - ni)
                = n + 1
    therefore E[Xn] = n
b.  i now follows binomial distribution with success rate 1/100
    Var(i) = np(1-p) = 99n/10000
    Var(Xn) = Var(100i) = 100^2Var(i) = 99n

5-2
a.  ./CLRS/start/index.ts#randomSearch
b.  the number of trials X follows geometric distribution with success rate 1/n
    E[X] = 1/(1/n) = n
c.  the same with part b but with success rate k/n
    E[X] = 1/(k/n) = n/k
d.  the same as balls and bins 
    E[X] = n(ln(n) + O(1))
e.  i <- {1 .. n}, the number of comparsions X = i
    E[X] = E[i] = (1/n) * n(1+n)/2 = (1+n)/2
f.  thanks http://sites.math.rutgers.edu/~ajl213/CLRS/Ch5.pdf
    define indicator variables {Xi} such that
    Xi  = 1 when A[i] is compared to x in a search
        = 0 otherwise
    when A[i] = x
        p(Xi = 1)   = p(i is the minimum among all indices that A[i] = x)
                    = 1/k
    when A[i] != x
        p(Xi = 1)   = 1/k+1 // why? what's the distribution of the minimum of a k-subset of {1 .. n}?
    which gives E[ΣXi] = ΣE[Xi] = k * 1/k + (n-k)/k+1 = (n+1)/(k+1)
    in worst case, the k matches are the last k elements of the array
    the algorithm has to perform n-k+1 comparsions
g.  both average and worst case running time Θ(n)
h.  exactly the same as part e, f and g
i.  DETERMINISTIC-SEARCH
    shuffling an array will cost Θ(n) time, which is similar to deterministic linear searching
    it's not worth the effort to do the shuffling unless comparsion is expensive and inputs are malicious
    and the first algorithm is O(nlgn), worse than deterministic linear searching

Chapter 6
6.1-1
a complete binary tree of height h has nodes 2^(h+1) - 1
of height h-1 has nodes 2^h - 1
therefore a heap with height h will have nodes between 2^h and 2^(h+1) - 1

6.1-2
as 6.1-1 shows, 2^h <= n <= 2^(h+1) - 1, where h is the height of the heap
therefore 
    lg(2^h) <= lgn <= lg(2^(h+1) - 1)
    h <= [lgn] <= h, h = [lgn]

6.1-3
for a subtree of a heap, there is a simple path from root to any child node
for an arbitrary node n, PARENT^(i)(n) = root for some i
as A[PARENT(n)] >= A[n] for all n, A[n] <= A[PARENT(n)] <= .. <= A[root]
root is the largest value occuring anywhere in that subtree

6.1-4
anywhere at the bottom level
it can not be anywhere else than the bottom level, or it will have children, and A[PARENT(i)] >= A[i] for all i
as the problem assumed that all values are distinct, A[i] won't be the minimum

6.1-5
yes, for a sorted array, for any i <= j, A[i] <= A[j]
as PARENT(i) < i for all i except the root, A[PARENT(i)] <= A[i] for all i, A is a min-heap

6.1-6
23
17  14
6   13  10  1
5   7   2
no, 6 has two children 5 and 7, where 7 > 6

6.1-7
for i >= [n/2] + 1, the index of its left child will be greater than 2 * ([n/2] + 1) = 2 * [n/2] + 2
if n is even, 2 * [n/2] + 2 = n + 2
if n is odd, 2 * [n/2] + 2 = n + 1 
both cases the index of its left child will be greater than n, thereby A[i] must be a leaf

6.2-1
                27
            17      3
        16  13      10  1
       5 7 12  4   8  9   0
3 = A[3] swapped with 10 = A[6], then with 9 = A[13]

6.2-2
./CLRS/sort/heap.ts#heapify
the code is abstract, subclasses can instantiate the abstract cmp function to anything they want

6.2-3
after comparing A[i], A[LEFT(i)] and A[RIGHT(i)]
the procedure terminates without modifying the underlying array or recursively call itself

6.2-4
both LEFT(i) and RIGHT(i) will be out-of-bound, defined by <= A.heap-size
the procedure terminates without modifying the underlying array or recursively call itself

6.2-5
./CLRS/sort/heap.ts#heapify

6.3-1
swapped A[3] = 10 and A[7] = 22
swapped A[2] = 17 and A[5] = 19
swapped A[1] = 3 and A[4] = 84
swapped A[0] = 5 and A[1] = 84
swapped A[1] = 5 and A[3] = 22
swapped A[3] = 5 and A[7] = 10

6.3-2
starting from 1, the non-leaf nodes may not meet the condition of MAX-HEAPIFY
which requires the left and right subtrees of A[i] be a max heap

6.3-3
for level i, there can only be 2^(h - i - 1) nodes if the level is complete
2^h <= n <= 2^(h+1) - 1
[n / 2^(i+1)] >= 2^(h-i-1) 

6.4-1
swapped A[2] = 2 and A[6] = 20
swapped A[1] = 13 and A[3] = 25
swapped A[0] = 5 and A[1] = 25
swapped A[1] = 5 and A[3] = 13
swapped A[3] = 5 and A[7] = 8
swapped A[0] = 4 and A[2] = 20
swapped A[2] = 4 and A[5] = 17
swapped A[0] = 5 and A[2] = 17
swapped A[0] = 2 and A[1] = 13
swapped A[1] = 2 and A[3] = 8
swapped A[0] = 4 and A[1] = 8
swapped A[1] = 4 and A[4] = 7
swapped A[0] = 4 and A[1] = 7
swapped A[0] = 2 and A[2] = 5
swapped A[0] = 2 and A[1] = 4
[ 2, 4, 5, 7, 8, 13, 17, 20, 25 ]

6.4-2
initialization:
    at the start of the first iteration, i = A.length, A[1 .. i] = A is a max heap
    A[i+1 ..] is empty
maintenance:
    assume at the start of ith iteration, A[1 .. i] is a max heap, A[i+1 .. n] contains n-i largest elements sorted
    the root of the max heap is largest among A[1 .. i] and smaller than any element in A[i+1 .. n]
    thereby A[1] is the n-i-1th largest element
    swapping A[1] with A[i], now A[i .. n] contains n-i+1 largest elemented in sorted order
    decrement the size of the heap and call MAX-HEAPIFY(1), elements in A[1 .. i-1] is now a max heap
termination:
    after the end of the last iteration, i = 1, A[2 .. n] contains n-1 largest elementes in sorted order
    A[1] thus must be the smallest element in A[1 .. n], and A is in sorted order

6.4-3
assuming all elements are distinct
increasing order:
    since all leaves are greater than internal nodes, at least half of the elements in the array have to be moved
    which takes Ω(n), increasing order is asymptotically the worst case of BUILD-MAX-HEAP
    every iteration will swap an element at bottom level to the root of the heap
    MAX-HEAPIFY have to pass it all the way down to the bottom level, taking time Θ(h) = Θ(lgi)
    Θ(Σlgi) = Θ(lg(n!)) = Θ(nlgn)
decreasing order:
    array in decreasing order is already a max heap, BUILD-MAX-HEAP takes constant time
    every iteration will swap the minimum element to the root of the heap
    MAX-HEAPIFY have to pass it all the way down to the bottom level, taking time Θ(h) = Θ(lgn)
    Θ(Σlgi) = Θ(lg(n!)) = Θ(nlgn)
if all elements are identical, both BUILD-MAX-HEAP and MAX-HEAPIFY will take constant time
overall running time Θ(n)    

6.4-4
the same to decreasing order in 6.4-3

6.4-5
The Analysis of Heapsort
Schaffer R., Sedgewick R. 
Journal of Algorithms Volume 15, Issue 1, July 1993, Pages 76-100

6.5-1
swapped A[0] = 15 and A[11] = 1
swapped A[0] = 1 and A[1] = 13
swapped A[1] = 1 and A[4] = 12
swapped A[4] = 1 and A[9] = 6
Maximum key is 15

6.5-2
Inserting 10...
swapped A[12] = 10 and A[5] = 8
swapped A[5] = 10 and A[2] = 9

6.5-3
again the code is abstract, subclasses can override cmp to anything they like (as long as it's a total order)
./CLRS/sort/heap.ts#MinPriorityQueue

6.5-4
because it's bad code
the function HEAP-INCREASE-KEY is multi-purpose: it both increases the key and fixes the position of the key in heap
the fix part should be seperated from HEAP-INCREASE-KEY, then HEAP-INSERT no longer has to call it or set -∞
the guard condition is only necessary when A[i] may have children, in this case it hasn't
the heap should work with all types which has a total order over it
many of them do not have a global minimum like -∞

6.5-5
initialization:
    A[1 .. A.heap-size] is a max heap, key is inserted to A[A.heap-size + 1], i = A.heap-size + 1
    then A[i] is guarenteed to have no left or right child
    A[1 .. A.heap-size+1] is a max heap except A[i] may be larger than A[PARENT(i)]
    then A.heap-size is incremented before the first iteration
maintenance:
    assume A[1 .. A.heap-size] is a max heap except A[i] may be larger than A[PARENT(i)]
    if A[PARENT(i)] >= A[i], then A[1 .. A.heap-size] is a max heap, loop terminated
    or A[PARENT(i)] is swapped with A[i], A[PARENT(i)] > A[i]
    but now A[PARENT(i)] may be larger than A[PARENT(PARENT(i))], otherwise A[1 .. A.heap-size] is a max heap
    by setting i = PARENT(i)
    A[1 .. heap-size] is a max heap except A[i] may be larger than A[PARENT(i)] at the start of next iteration
termination:
    i = 1 after the last iteration
    i has no parent, A[1 .. A.heap-size] is a max heap

6.5-6
./CLRS/sort/heap.ts#fix

6.5-7
FIFO queue: based on a min priority queue, data keyed with a auto incrementing counter
./CLRS/sort/heap.ts#FIFOQueue
Stack (or FILO queue): the same as FIFO queue, but based on a max priority queue
./CLRS/sort/heap.ts#Stack

6.5-8
swap A[i] with A[A.heap-size], decrement A.heap-size
the value of A[A.heap-size] is uncertain: it can be either greater than A[PARENT(i)] or not
depending on the value of A[A.heap-size], it may need to go up or down the heap to restore max heap property
if it's greater than its parent, call INCREASE-KEY(i, 0); otherwise call MAX-HEAPIFY(i)

6.5-9
if used a min heap instead of a max heap, elements have to be removed from the head of the array when extracting
there's no way to do this efficiently in javascript
(can be done in Haskell by tail and in Rust by slicing)
./CLRS/sort/heap.ts#mergeArrays
build the max heap with k arrays: O(k)
extract and heapify the heap: O(lnk), O(nlnk) in total

6-1
a.  Original input:   [ 1, 2, 3, 4, 5 ]
    BUILD-MAX-HEAP:   [ 5, 4, 3, 1, 2 ]
    Repeated insert:  [ 5, 4, 2, 1, 3 ]
b.  when inputs are in increasing order
    every element inserted at the bottom level have to be swapped all the way to the root
    T(n) = Σ(hi) = Σ(Θ(lgi)) = Θ(lg(n!)) = Θ(nlgn)

6-2
./CLRS/sort/d-heap.ts
a.  assume array index starts from 0
    the first element is the root, next d are the children of the root, next d^2 are their children, etc.
    let FIRST-CHILD(i) return the index of the first child of A[i]
    FIRST-CHILD(i+1) = FIRST-CHLID(i) + d
    FIRST-CHILD(0) = 1, FIRST-CHILD(i) = id + 1
    FIRST-CHILD(i) to FIRST-CHILD(i) + d - 1 (if in bound) will be the children of A[i]
    let PARENT(i) be the index of parent of i
    PARENT(i + d) = 1 + PARENT(i)
    PARENT(1) = PARENT(d) = 0, PARENT(i) = Math.floor((i - 1)/d)
b.  the complete d-ary tree of height h-1 has Σ(i = {0 .. h})(d^h) = (d^h - 1) / (d - 1) nodes
    the complete d-ary tree of height h has (d^(h+1) - 1) / (d - 1) nodes
    (d^h - 1) / (d - 1) + 1 <= n <= (d^(h+1) - 1) / (d - 1)
    d^h + d - 2  <= (d-1)n <= d^(h+1) - 1
    d^h <= (d-1)n - d + 2 <= d^(h+1) - d + 1
    Math.floor(log(d, (d-1)n - d + 2)) = h
    h = Θ(log(d, dn)) = Θ(log(d, n))
c.  has to find the maximum among d nodes at every level, Θ(log(d, n)) levels in total
    T(n) = Θ(dlog(d, n))
d.  the working part is function fix, which is the same to part e
    T(n) = Θ(log(d, n))
e.  at most have to swap all nodes on a simple path from root to bottom
    the length of the path is no longer than the height of the heap, which is Θ(log(d, n))
    T(n) = Θ(log(d, n))

6-3
a.  [[2, 3, 4, 5],
     [8, 9, 12, 14],
     [16, -, -, -],
     [-, -, -, -]]
b.  for any i <= l and j <= k
    A[i, j] <= A[i, k] <= A[l, k]
    therefore if Y[1, 1] = ∞, for all i, j, Y[i, j] >= ∞, all entries are empty
    if Y[m, n] < ∞, for all i, j, Y[i, j] <= Y[m, n] < ∞, all entries are filled
c.  ./CLRS/sort/young-tableau#extractMin
    correctness can be proved similarly as min heap
    the smallest of Y[i, j], Y[i+1, j] and Y[i, j+1] is swapped to Y[i, j]
    if Y[i+1, j] is the minimum, tableau property of ith row now restored, reduced to a problem one row smaller 
    if Y[i, j+1] is the minimum, tableau property of jth column now restored, reduced to a problem one column smaller
    p = m + n is incremented every iteration and every iteration takes constant time
    the loop ends either i = m and j = n (out of bound) or eariler
        T(p) = T(p-1) + Θ(1), T(p) = O(p) by stright-forward recurrence tree analysis
    T(m + n) = O(m + n) 
d.  ./CLRS/sort/young-tableau#insert
    correctness can be proved similarly as insert in priority queue
    p = m + n is decremented every iteration and every iteration takes constant time
        T(p) = T(p-1) + Θ(1), T(p) = O(p)
    T(m + n) = O(m + n)
e.  ./CLRS/sort/index.ts#problem_6_3
    insert n^2 numbers into the tableau, then call EXTRACT-MIN n^2 times
    T(p) = T(2n) = O(n) for both INSERT and EXTRACT-MIN, Θ(n^2) operations in total
    T(p) = O(n) * Θ(n^2) = O(n^3)
    which is not optimal since
        1. it's not in-place
        2. heapsort can be done in O(n^2lg(n^2)) = O(n^2lgn) time
f.  ./CLRS/sort/young-tableau#find
    starting from Y[i, j] = Y[0, n - 1]
    if Y[i, j] > key, the whole column j contains numbers greater than key 
    the problem size is reduced by 1 column
    if Y[i, j] == key, [i, j] is the answer
    if Y[i, j] < key, all numbers Y[0, j] to Y[i, j] < key
    the problem size is reduced by 1 row
    as p = i + j is decremented every iteration, overall running time is O(m + n)

Chapter 7
7.1-1
Swapped A[0] = 9, A[2] = 13
[ 9, 19, 13, 5, 12, 8, 7, 4, 21, 2, 6, 11 ]
Swapped A[1] = 5, A[3] = 19
[ 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 ]
Swapped A[2] = 8, A[5] = 13
[ 9, 5, 8, 19, 12, 13, 7, 4, 21, 2, 6, 11 ]
Swapped A[3] = 7, A[6] = 19
[ 9, 5, 8, 7, 12, 13, 19, 4, 21, 2, 6, 11 ]
Swapped A[4] = 4, A[7] = 12
[ 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 ]
Swapped A[5] = 2, A[9] = 13
[ 9, 5, 8, 7, 4, 2, 19, 12, 21, 13, 6, 11 ]
Swapped A[6] = 6, A[10] = 19
[ 9, 5, 8, 7, 4, 2, 6, 12, 21, 13, 19, 11 ]
Swapped A[7] = 11, A[11] = 12
[ 9, 5, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 ]
Swapped A[0] = 5, A[1] = 9
[ 5, 9, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 ]
Swapped A[1] = 4, A[4] = 9
[ 5, 4, 8, 7, 9, 2, 6, 11, 21, 13, 19, 12 ]
Swapped A[2] = 2, A[5] = 8
[ 5, 4, 2, 7, 9, 8, 6, 11, 21, 13, 19, 12 ]
Swapped A[3] = 6, A[6] = 7
[ 5, 4, 2, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[0] = 2, A[2] = 5
[ 2, 4, 5, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[1] = 4, A[1] = 4
[ 2, 4, 5, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[2] = 5, A[2] = 5
[ 2, 4, 5, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[4] = 7, A[6] = 9
[ 2, 4, 5, 6, 7, 8, 9, 11, 21, 13, 19, 12 ]
Swapped A[5] = 8, A[5] = 8
[ 2, 4, 5, 6, 7, 8, 9, 11, 21, 13, 19, 12 ]
Swapped A[6] = 9, A[6] = 9
[ 2, 4, 5, 6, 7, 8, 9, 11, 21, 13, 19, 12 ]
Swapped A[8] = 12, A[11] = 21
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[9] = 13, A[9] = 13
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[10] = 19, A[10] = 19
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[11] = 21, A[11] = 21
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[9] = 13, A[9] = 13
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[10] = 19, A[10] = 19
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]

7.1-2
every number in the array is smaller or equal to pivot
i = j = r - 1 after the loop
A[r] exchanged with A[i + 1] = A[r], return value is r
adding a specific check if all array elements are equal to the pivot, return Math.floor((p + r) / 2)
./CLRS/sort/quicksort#partition

7.1-3
the loop on lines 3-6 runs at most p - r + 1 times, bounded by j
each iteration takes constant time
other operations not in the loop takes constant times
T(n)    = Θ(p - r + 1) + Θ(1)
        = Θ(n)

7.1-4
change line 4 of PARTITION to
    if A[j] >= x
now the loop invariants are
    1.  if p <= k <= i, A[k] >= x
    2.  if i+1 <= k <= j-1, A[k] < x 
    3.  if k = r, A[k] = x

7.2-1
assume T(n) <= cn^2
T(n)    <= c(n-1)^2 + dn
        = cn^2 - 2cn + 1 + dn
        <= cn^2 by taking c > d/2
similarly, T(n) >= cn^2 
T(n) = Θ(n^2)

7.2-2
without the modification in problem 7.1-2, the problem is partitioned into two subproblems of size n-1 and 0
T(n) = T(n-1) + Θ(n) = Θ(n^2)
with the modification in problem 7.1-2, the problem is partitioned into two subproblem of size n/2
T(n) = 2T(n/2) + Θ(n) = Θ(nlgn)

7.2-3
when PARTITION is called with a sorted subarray A[p .. r], all elements in the slice is smaller or equal to r
return value will be r, the problem is partitioned into two sub-problems of size n-1 and 0
T(n) = Θ(n^2)

7.2-4
quiksort will take time Θ(n^2) for an already sorted array
insertion sort will take time Θ(n)

7.2-5
as α <= 1/2, 1 - α >= α
the shortest path from root to bottom is has length log(1/α, n) = lgn / lg(1/α) = -lgn/lgα
the longest path form root to bottom has length log(1/(1-α), n) = -lgn/lg(1-α)

7.2-6
thanks https://ita.skanev.com/07/02/06.html
as the input is uniformly random
the chance that the pivot falls within the αn smallest numbers or (1-α)n largest numbers is
    p = α + α = 2α
thereby with probability 1 - 2α, αn <= q <= (1-α)n, the partition is better than α to 1-α

7.3-1
the worst case of a randomized algorithm occurs rarely, also independent to the input

7.3-2
RANDOM is called once for each call to PARTITION, PARTITION is called for each call to QUICKSORT
for worst case, all partition are of size n - 1 and 0, PARTITION is called Θ(n) times
for best case, all partition are of size n/2
the number of calls to PARTITION is the same to the internal nodes in the recurrence tree
which is the number of nodes of a binary tree of height Θ(lgn), which is Θ(n)

7.4-1
assume T(n) >= cn^2
T(n)    >= max(cq^2 + c(n - q - 1)^2) + dn
        >= cn^2 - 2cn + 1 + dn
        >= cn^2 when c < d/2
T(n) = Ω(n^2)

7.4-2
T(n) = Θ(nlgn) => T(n) = Ω(nlgn)

7.4-3
the text already showed that the function is convex

7.4-4
E[X]    = Σ(i = {1 .. n-1})(Σ(k = {1 .. n-i})(2/(k+1)))
        = Σ(i = {1 .. n-1})(Σ(k = {1 .. n-i})(2/k) - 2)
        = Σ(i = {1 .. n-1})(Θ(lg(n-i)))
        = Θ(lg((n-1)!))
        = Ω(nlgn)

7.4-5
this time Xij = 1 only if i - j + 1 > k
E[X]    = Σ(i = {1 .. n-1})(Σ(j = {k+1 .. n-i})(2/(j+1)))
        <= Σ(i = {1 .. n-1})(Σ(k = {k+1 .. n})(2/j))
        = Σ(i = {1 .. n-1})(O(lgn) - O(lgk))
        = O(nlg(n/k))
the quicksort stops when the subarray has length <= k
thereby an element A[i] is guarenteed to be greater or equal to A[i - k]
when looking for a position to insert A[i], the insertion sort procedure will never have to search beyond A[i-k]
for all 0 <= i <= n-1, O(nk) in total
overall O(nk + nlg(n/k))

7.4-6
the median order of the three random variables falls within [αn, (1-α)n] only if at least two of them is in that range
it's a binomial distribution with success rate (1 - 2α)
P(X >= 2)   = C(3, 2)(1-2α)^2(2α) + C(3, 3)(1-2α)^3
            = 6α(1-2α)^2 + (1-2α)^3

7-1
a.  Swapped A[0] = 6, A[10] = 13
    [ 6, 19, 9, 5, 12, 8, 7, 4, 11, 2, 13, 21 ]
    Swapped A[1] = 2, A[9] = 19
    [ 6, 2, 9, 5, 12, 8, 7, 4, 11, 19, 13, 21 ]
    Return value:  8
b.  invariant: at the end of each iteration, p <= i, j <= r
    initialization:
        repeat clauses will be executed at least once, so i and j start with p and r
        as x = A[p], i will always be p after the first iteration
        if there's no element <= A[p] in the subarray, j will be p after the first iteration, !(i < j) and termination
        if there's some element <= A[p] in the subarray, the procedure swaps the element with A[p]
        denote this index as q
        after the first iteration:
            A[i] = A[p] <= x
            A[j] = A[q] = x
    maintenance:
        i and j is incremented at least once in each iteration
        the value they originally point to will not be changed
    termination:
        either j first reaches p or i first reaches q
        as A[p] <= x, j will stop at j = p
        i = p in the first iteration, thus i > p in this iteration, !(i < j) and the procedure is terminated 
        similarly, as A[q] = x, i will stop at i = q
        !(i < j) and termination
        therefore p <= i, j <= q <= r
c.  refine part 1 a little bit with assumption that r >= p + 1, subarray at least has length 2
    the procedure is only terminated at the first iteration when i = j = p, which means j < r
    otherwise j is decremented at least twice, j <= (r+1) - 2 = r - 1, j < r
d.  invariant:
        at the start of each iteration,
        A[p .. i-1] contains elements smaller or equal to the pivot
        A[j+1 .. r] contains elements greater or equal to the pivot
    initialization:
        A[p .. i-1] and A[j+1 .. r] are empty
    maintenance:
        after the two repeat clauses:
            A[p .. i-1] contains elements <= x
            A[j+1 .. r] contains elements >= x
            A[i] >= x
            A[j] <= x
        swap A[i] and A[j] then invariant maintained
    termination:
        !(i < j) => i >= j
        i = j only when A[i] >= x and A[i] <= x, A[i] = x
        every element in A[p .. i] = A[p .. j] <= x
        every element in A[j+1 .. r] >= x
    therefore every element in A[p .. i-1] <= x <= every element in A[j+1 .. r]
e.  ./CLRS/sort/quicksort.ts#hoareQuicksort

7-2
a.  any pivot will be equal to all array elements 
    the return of PARTITION will always be r, subarray partitioned to size n-1 and 0
    T(n) = Θ(n^2)
b.  ./CLRS/sort/quicksort.ts#partition2
    k incremented every iteration, starting from p, end condition is k > r - 1
    T(n) = Θ(p - r)
c.  ./CLRS/sort/quicksort.ts#quicksort2
d.  with QUICKSORT',
        if zi < x < zj, zi and zj will not be compared
        if zi or zj is first chosen as a pivot among {zi .. zj}, zi and zj will be compared
        if x = zi but not zi itself is first chosen as a pivot among {zi .. zj}
            zi will be in neither subproblems, zi will not be compared to zj
        simlarly if x = zj but not zj itself, zi will not be compared to zj
    the expected time analysis is still valid, E[X] = Θ(nlgn)

7-3
a.  pivot is chosen randomly from A[1] to A[n], 1/n for any particular element
    E[Xi] = 1/n
b.  if the qth smallest element is chosen as the pivot
    exactly q elements are smaller or equal
    the problem is partitioned into two subproblems of size q-1 and n-q
    T(n)    = ΣXq(T(q-1) + T(n-q) + Θ(n))
    as for a particular run, only one Xq can be 1, all others 0
    E[T(n)] = E[ΣXq(T(q-1) + T(n-q) + Θ(n))]
c.  the subproblems are independent to the value of Xq
    E[T(n)] = Σ(E[Xq](E[T(q-1)] + E[T(n-q)] + Θ(n)))
            = 1/n * Σ(E[T(q-1)] + E[T(n-q)] + Θ(n))
            = 1/n * (ΣE[T(q-1)] + ΣE[T(n-q)] + nΘ(n))
            = 1/n * (2ΣE[T(q)] + nΘ(n))
            = 2/n * ΣE[T(q)] + Θ(n)
d.  Σ(k = {2 .. n-1})(klgk)
    = Σ(k = {2 .. n/2 - 1})(klgk) + Σ(k = {n/2 .. n-1})(klgk)
    <= Σ(k = {1 .. n/2 - 1})(klg(n/2)) + Σ(k = {n/2 .. n-1})(klgn)
    <= (lgn - 1)(n/2)n/4 + lgn(3n/2 - 1)n/4
    = n^2lgn/8 - n^2/8 + 3n^2lgn/8 - Θ(nlgn)
    <= n^2lgn/2 - n^2/8
e.  assume E[T(n)] <= anlgn
    E[T(n)] <= 2/n * Σ(q = {2 .. n-1})(aqlgq) + dn
            <= 2/n * a(n^2lgn/2 - n^2/8) + dn
            = anlgn - an/4 + dn
            <= anlgn when a/4 > d
    E[T(n)] = O(nlgn)
    the best case is Ω(nlgn), so E[T(n)] = Ω(nlgn), E[T(n)] = Θ(nlgn)

7-4
a.  induction on problem size
    when p == r, the slice of one element is trivially sorted
    when p < r, after calling PARTITION(A, p, r),
        every element in A[p .. q-1] <= A[q]
        every element in A[q+1 .. r] > A[q]
    by induction assumption, TAIL-RECURSIVE-QUICKSORT(A, p, q-1) correctly sorts A[p, q-1]
    p set to q+1, equivalent to calling TAIL-RECURSIVE-QUICKSORT(A, q+1, r)
    by induction assumption, starting from the second iteration the procedure correctly sorts A[q+1, r]
    every element in A[p .. q-1] <= A[q] <= every element in A[q+1 .. r], therefore A[p .. r] is sorted
b.  the worst case of partition, subproblems of size n-1 and 0
    q = r - 1, TAIL-RECURSIVE-QUICKSORT(A, p, r) calls TAIL-RECURSIVE(A, p, r-1) recursively
    the stack keeps growing until p = r
    stack depth r - p + 1 = Θ(n)
c.  recursively solve the smaller subproblem first
    as the smaller of the subproblem has size <= n/2, stack depth <= lgn

7-5
a.  the three element chosen can be any 3-subset {Ap, Aq, Ar} of {A'1 .. A'n}
    for the median to be a particular i, 
        one of them should be Ai
        one of them should be smaller than Ai
        one of them should be greater than Ai
    pi = 1/n * (i-1)/n * (n-i)/n * A(3, 3) = 6(i-1)(n-i)/n^3
b.  with ordinary implementation, A'[(n+1)/2] is chosen with probability 1/n, the same with any other element
    with median-of-3 implementation, p = 6(n/2)(n/2)/n^3 = 3/2 * 1/n
    limit = 3/2
c.  for random pivot: p = Σ(i = {n/3 .. 2n/3})(1/n) = 1/3
    for median-of-3 pivot:
        p   = Σ(i = {n/3 .. 2n/3})(6(i-1)(n-i)/n^3)
            ≒ int(n/3 <= i <= 2n/3, 6(i-1)(n-i)/n^3)
            = 13/27 - 1/n
    as n -> ∞, p -> 13/27, better than 1/3 = 9/27
d.  as best possible split still gives Ω(nlgn) running time
    median-of-3 implementation can only be a constant factor better than the ordinary implementation on average

7-6
a.  the same idea as problem 7-2 can be used here 
    by comparing with a pivot interval [ap, bp], an array of interval [a, b] can be divided into three groups:
        1. strictly smaller than the pivot, b < ap
        2. strictly greater than the pivot, a > bp
        3. overlaps with the pivot, b >= ap && a <= bp
    the problem is the relation of overlapping is not transitive:
        [1, 2] overlaps with [2, 3]
        [2, 3] overlaps with [3, 4]
        [1, 2] doesn't overlap with [3, 4]
    thus group 3 may not overlap with each other
    the procedure must then maintain an interval that's the intersection of all overlapping intervals
    an interval must not be added to group 3 if it will make the intersection interval empty
    the intersection also must be computed beforehand, the left bound of the pivot must be constant during partition    
    ./CLRS/sort/quicksort.ts#fuzzysort
b.  the procedure is equivalent to quicksort in 7-2 on running time 
    which is Θ(nlgn) on average, Θ(n) when all elements are equivalent (i.e. contains the intersection)

Chapter 8
8.1-1
let array elements be vertices of a undirected graph
edge exists between vertices a and b iff a and b are compared
if the graph is not connected, i.e. array elements can be divided into two non-empty sets A1 and A2 that,
comparsions are only performed within these two sets but not between them
then any comparsion result between an elements from A1 and A2 are uncertain, the sort may not be accurate
thus for the comparsion sort algorithm to be correct, the graph must be connected
connected graph with n vertices has at least n-1 edges, which is the lower bound of comparsions
this lower bound is achievable
assume the input array is in sorted order, insertion sort only performs n-1 comparsions

8.1-2
Σ(k = {1 .. n})(lgk)    = lg(n!)
Σ(k = {1 .. n})(lgk)    <= Σ(k = {1 .. n})(lgn)
                        = nlgn = O(nlgn)
Σ(k = {1 .. n})(lgk)    = Σ(k = {1 .. n/2})(lgk) + Σ(k = {n/2 + 1 .. n})(lgk)
                        >= n/2 * lg(n/2)
                        = n/2 * (lgn - 1)
                        = Ω(nlgn)
lg(n!) = Θ(nlgn)

8.1-3
the unique simple path to half of n! leaves of a comparsion tree have length O(n)
a complete binary tree of height O(n) may only have 2^O(n) leaves
if it's possible, n!/2 = O(2^O(n)), n! = O(2^O(n)), which is not true
change 1/2 to any constant fraction and the argument still holds
n!/2^n = (1/2) * (2/2) * (3/2) ... >= (n/2)! = ω(2^O(n/2)) = ω(2^O(n))
it's not possible even for a 2^n fraction

8.1-4
each permutation of each subsequence is a possible sub-result, there are k! permutations for a length k subsequence
overall there are (k!)^(n/k) results, height of the comparsion tree at least 
    lg((k!)^(n/k))  = n/k * lg(k!)
                    = n / k * Ω(klgk)
                    = Ω(nlgk)
which is a lower bound of the worst case performance

8.2-1
[ 2, 4, 6, 8, 9, 9, 11 ]
[ null, null, null, null, null, 2, null, null, null, null, null ]
[ 2, 4, 5, 8, 9, 9, 11 ]
[ null, null, null, null, null, 2, null, 3, null, null, null ]
[ 2, 4, 5, 7, 9, 9, 11 ]
[ null, null, null, 1, null, 2, null, 3, null, null, null ]
[ 2, 3, 5, 7, 9, 9, 11 ]
[ null, null, null, 1, null, 2, null, 3, null, null, 6 ]
[ 2, 3, 5, 7, 9, 9, 10 ]
[ null, null, null, 1, null, 2, null, 3, 4, null, 6 ]
[ 2, 3, 5, 7, 8, 9, 10 ]
[ null, null, null, 1, null, 2, 3, 3, 4, null, 6 ]
[ 2, 3, 5, 6, 8, 9, 10 ]
[ null, null, 1, 1, null, 2, 3, 3, 4, null, 6 ]
[ 2, 2, 5, 6, 8, 9, 10 ]
[ null, 0, 1, 1, null, 2, 3, 3, 4, null, 6 ]
[ 1, 2, 5, 6, 8, 9, 10 ]
[ null, 0, 1, 1, 2, 2, 3, 3, 4, null, 6 ]
[ 1, 2, 4, 6, 8, 9, 10 ]
[ 0, 0, 1, 1, 2, 2, 3, 3, 4, null, 6 ]
[ 0, 2, 4, 6, 8, 9, 10 ]
[ 0, 0, 1, 1, 2, 2, 3, 3, 4, 6, 6 ]

8.2-2
assume Al = Ak, l < k
as j iters from A.length down to 1
Ak will first be placed into B at B[C[A[k]]]
C[A[j]] is then decremented by 1
as l < k, when j = l, C[A[l]] = C[A[k]] is smaller than when it was in iteration j = k
therefore if Al = Ak, l < k, they are placed at Bi, Bj that i < j
for n elements that are mutually equivalent
as the relative order of any two of them are preserved, the overall order must also be preserved

8.2-3
the argument in the text didn't specify the order of A[j]
reversing the order of A[j] and the argument still holds
but this time any two Al = Ak, l < k, are placed in Bi, Bj, i > j
the sorting algorithm is no longer stable

8.2-4
the array C can be used to solve this problem:
the number of elements falls into the range [a .. b]
can be computed as the number of elements equal or less than b - the number of elements equal or less than a - 1
which in turn is C[b] - C[a-1]
./CLRS/sort/linear-sort.ts#countingRange

8.3-1
[ 'SEA', 'TEA', 'MOB', 'TAB', 'DOG', 'RUG', 'DIG', 'BIG', 'BAR', 'EAR', 'TAR', 'COW', 'ROW', 'NOW', 'BOX', 'FOX' ]
[ 'TAB', 'BAR', 'EAR', 'TAR', 'SEA', 'TEA', 'DIG', 'BIG', 'MOB', 'DOG', 'COW', 'ROW', 'NOW', 'BOX', 'FOX', 'RUG' ]
[ 'BAR', 'BIG', 'BOX', 'COW', 'DIG', 'DOG', 'EAR', 'FOX', 'MOB', 'NOW', 'ROW', 'RUG', 'SEA', 'TAB', 'TAR', 'TEA' ]

8.3-2
insertion sort:
    stable, two element will be swapped only if the later is smaller than the former
merge sort:
    stable, in the merge procedure
    when element from left subarray is equal to the right one, the left one is always copied to A first
    within left or right subarray, elements are copied to A in the original order
heap sort:
    not stable
    e.g. [1a, 1b, 3]
        after BUILD-MAX-HEAP, will be [3, 1b, 1a]
        after first swap, will be [1a, 1b, 3]
        after second swap, will be [1b, 1a, 3]
quick sort:
    not stable     
    an element can be swapped to arbitrary place to its right if
        it is currently the first element greater than the pivot
        the element examined in the current iteration is smaller or equal to the pivot
    the original order of equivalent elements may not hold
zip the array with its index
comparsion now compares the index in the original array whenever two elements are equal
as array indices are distinct, the sort must be stable

8.3-3
invariant:
    after ith iteration, the array is sorted according to the i least significant digits of elements
inititialization:
    considering 0 digits (nothing), all elements are equal, trivially satisfied
maintenance:
    at the beginning of ith iteration, elements are sorted according to i-1 least significant digits 
    they are then sorted according to ith least significant digit
    at the end of ith iteration
        1.  they are sorted according to the ith least significant digit
        2.  for elements that have same ith least significant digit, the original order is preserved (stable)
            i.e. they are sorted according to i-1 least significant digits
    therefore the array is sorted according to i least significant digits
termination:
    i = d, the array is sorted

8.3-4
b = lg(n^3) = 3lgn >= lgn
choose r = lgn yields Θ(bn/lgn) = Θ(3nlgn/lgn) = Θ(3n) = O(n)

8.3-5
assume in n elements, Xi starts with digit i, Σ(i = {0 .. r-1})Xi = n, r is the radix
T(n, d) = Σ(T(Xi, d-1)) + Θ(n)
the height of the recurrence tree solely depends on d
in the worst case, n is evenly divided, the recurrence tree is a complete d-ary tree
number of sorting pass equals the number of interal nodes = r^d
a single call to the procedure creates r bins
when called recursively, the procedure keeps track of bins at most r * number of nodes in longest path = d

8.4-1
bucket 0:  []
bucket 1:  [ 0.13, 0.16 ]
bucket 2:  [ 0.2 ]
bucket 3:  [ 0.39 ]
bucket 4:  [ 0.42 ]
bucket 5:  [ 0.53 ]
bucket 6:  [ 0.64 ]
bucket 7:  [ 0.79, 0.71 ]
bucket 8:  [ 0.89 ]
bucket 9:  []
bucket 0 (sorted):  []
bucket 1 (sorted):  [ 0.13, 0.16 ]
bucket 2 (sorted):  [ 0.2 ]
bucket 3 (sorted):  [ 0.39 ]
bucket 4 (sorted):  [ 0.42 ]
bucket 5 (sorted):  [ 0.53 ]
bucket 6 (sorted):  [ 0.64 ]
bucket 7 (sorted):  [ 0.71, 0.79 ]
bucket 8 (sorted):  [ 0.89 ]
bucket 9 (sorted):  []
[ 0.13, 0.16, 0.2, 0.39, 0.42, 0.53, 0.64, 0.71, 0.79, 0.89 ]

8.4-2
when every element falls within the same bucket, insertion sort have to sort n elements in Θ(n^2) time
use better in-place algorithm than insertion sort when sorting the buckets

8.4-3
X ~ Bin(2, 1/2)
E[X]^2 = 1
E[X^2] = Var[X] + E[X]^2 = 3/2

8.4-4
the area is proportional to d^2
kth bucket should contain elements that 
    k/n <= di^2 < (k+1)/n
    k <= di^2 * n < k+1
    k = Math.floor(di^2 * n)

8.4-5
same as 8.4-4,
kth bucket should contains elements that
    k/n <= P(x) <= (k+1)/n

8-1
a.  each one of n! permutations of orders happens equally likely, with probaility 1/n!
    each one corresponds to a leaf in the comparsion tree as that's the only way to distinguish between them
    also an order of input can only corresponds to a single leaf
    if two leaves have the same value, as they start from the same root, they must have branched somewhere
    but for a fixed input, no comparsion can give different result on two runs
    the comparsion tree thus has n! leaves with 1/n! probability, other leaves unreachable
b.  assume LT has lk leaves, RT has rk leaves
    lk + rk = k, also when considered as the leaves of T their depth is one higher than in LT or RT
    D(T)    = D(LT) + lk + D(RT) + rk
            = D(LT) + D(RT) + k
c.  basically dynamic programming 
    the minial tree may have 0 <= i <= k leaves in its left subtree and k-i leaves in its right subtree
    if the left subtree didn't achieve d(i), swap one that does will reduce D(T)
    similarly the right subtree must have achieved d(k-i)
    d(k) = min(0 <= i <= k)(d(i) + d(k-i) + k)
    also when i = 0 or k, d(i) + d(k-1) + k = d(k) + k, which cannot be the minimal
    d(k) = min(1 <= i <= k-1)(d(i) + d(k-i) + k)
d.  d/di(ilgi + (k-1)lg(k-1))   = (lni - ln(k - x))/ln(2)
    has root i = k/2
    for 1 <= i < k/2, the first derivative is negative
    for k/2 < i <= k-1, the first derivative is positive
    therefore i = k/2 is a local maximum in range [1, k-1]
    assume d(k) >= cklgk
    d(k)    >= min{cilgi + c(k-i)lg(k-i) + k}
            = c(k/2)lg(k/2) + c(k/2)lg(k/2) + k
            = ck(lgk - 1) + k
            = cklgk - ck + k
            >= cklgk when c < 1
    d(k) = Ω(klgk)
e.  as d(k) is the minimum of D(k), D(k) = Ω(klgk), the average depth of leaves is D(k)/k = Ω(klgk) / k = Ω(lgk)
    D(T) = D(n!) = Ω(n!lgn!)
    average-case time is Ω(lgn!) = Ω(nlgn)
f.  as a randomization node didn't compare any two elements of the array, it gains no information of the order
    thus a randomization node didn't eliminate any possbility of the order of an array
    all children of a randomization node therefore must have the same set of leaves
    replace all randomization node with its child with minimal D(T)
    the result is still a valid comparsion tree which contains all possible n! permutations as leaves
    but the resulting tree will be deterministic
    as the child is chosen as having minimal D(T), the resulting tree has no worse average-case performance

8-2
a.  counting sort
b.  one pass of quicksort
c.  heap sort
d.  with counting sort, each bit has maximum 2, counting sort takes Θ(n + 2) = Θ(n)
    radix sort in turn takes Θ(bΘ(n)) = Θ(bn)
e.  ./CLRS/sort/linear-sort.ts#inplaceCountingSort
    not stable, whatever in C[A[i]] is swapped with A[i], it may not be the last element that equals to A[i]

8-3
a.  1.  divide numbers of different order of magnitudes into different sets
        the order of magnitude of a number k can be find in time d = log10(k) by repeatedly dividing 10
        the order of all numbers thereby can be computed in time Σd = n
    2.  radix sort each set of size ni, contains number with di digits
        each set takes time Θ(di * ni), Σdini = n, Θ(n) in total
    3.  concat the results in the correct order
        a number with more digits is always greater than a number with less digits
        as Σni <= n, this step takes O(n) time
    overall Θ(n)
b.  1.  first divide strings of different length into different list L[i]
        the length of all strings can be computed in time O(n)
    2.  initialize i as length of the longeset string, S an empty list, repeatedly:
            S = concat L[i] and S
            counting sort S with ith character (by definition of L, all strings in S now has i characters at least)
            decrement i
        when i > 0
        let ni = number of strings with at least i characters 
        each iteration takes O(ni) and Σni = n
    3.  on termination, S contains all strings of length at least 1 in sorted order
        return concatenation of all empty strings in L[0] with S
    overall Θ(n)
    at the start of each iteration, S contains strings sorted according to i+1th and later characters
    strings in L[i] has length i, can be treated as empty strings (i.e. least) considering i+1th and later characters
    thus prepending L[i] to S, the resulting list still sorted according to i+1th and later characters
    counting sort with ith character and the invariant carried to the next iteration
    ./CLRS/sort/linear-sort.ts#stringSort

8-4
a.  pick one red jug, test it with all blue jugs, repeat for all red jugs
    T(n) = n * n = Θ(n^2)
b.  for a fixed permutation of red jugs, any permutation of blue jugs may be the correct grouping
    n! possible results and the comparsion tree has height at least nlgn
    the worst case performance of the algorithm is Ω(nlgn) 
c.  by simultaneously quicksort red and blue jugs, repeatedly:
        1.  pick a random red jug r, test with all blue jugs
            divide blue jugs into groups that 
                R1: hold more water than r
                R2: hold less water than r
                R3: hold the same amount water as r (denote this jug as b)
        2.  test all red jugs with b, similarly divide red jugs into three groups B1, B2 and B3
        3.  recursively solve (R1, B1), (R2, B2)
    the set of red and blue jugs R and B can be treated as two equivalent set of numbers
    by picking r randomly, r works as a random pivot of B
    as b = a, R is partitioned in exactly the same way as B
    expected running time analysis of quicksort than applies

8-5
a.  just sorted normally
b.  1, 3, 2, 4, 5, 6, 7, 8, 9, 10 
c.  for any i,
        Σ(j = {i .. i+k-1})(A[j])/k <= Σ(j = {i+1 .. i+k})(A[j])/k
        A[i] <= A[i+k]  // other array elements cancelled each other
    conversely,
        A[i] <= A[i+k]
        //  add Σ(j = {i+1 .. i+k-1})(A[j]) to both sides
        Σ(j = {i .. i+k-1})(A[j]) <= Σ(j = {i+1 .. i+k})(A[j])
        Σ(j = {i .. i+k-1})(A[j])/k <= Σ(j = {i+1 .. i+k})(A[j])/k
d.  divides the array into k subarrays, each of length n/k
    sort each in time O(n/klg(n/k)), O(nlg(n/k)) in total
    merge the sorted subarrays to form A' that
        A'[i], A'[i+k], A'[i+2k] .. are from ith sorted subarray
    overall running time O(nlg(n/k))
e.  for 0 <= i <= k-1,
        A[i] <= A[i+k] <= A[i+2k] ..
    hence a k-sorted array can be divided into k sorted arrays
    by 6.5-9, k sorted arrays of length n/k can be merged into a single sorted array in time O(nlgk)
f.  when k is a constant, O(nlgk) = O(n)
    if k-sorting an array of size n can be done in time o(nlgn)
    o(nlgn) + O(n) = o(nlgn), the array can be completely sorted in time o(nlgn)
    in contradiction to Ω(nlgn) lower bound
    therefore k-sorting an array of size n must take Ω(nlgn)

8-6
a.  there are C(2n, n) ways to choose n elements from 2n ones
    C(2n, n) = (2n)! / (n!)^2
    the other n elements are automatically what not chosen
b.  all C(2n, n) leaves must be reachable from the root of the decision tree
    height of the tree at least lg(C(2n, n))
    by Stirling's approximation, 
        (2n)! / (n!)^2  = (4πn)^(1/2) * (2n/e)^2n * (1 + d/2n) / (2πn * (n/e)^2n * (1 + d/n)^2)
                        = (2^2n / (πn)^(1/2)) * (1 + d/2n) / (1 + d/n)^2
    where
        (1 + d/2n) / (1 + d/n)^2    = (1 + d/n - d/2n) / (1 + d/n)^2
                                    = 1/(1 + d/n) + (d/2n) / (1 + d/n)^2
                                    = 1 + O(1/n) as n -> ∞
        (2n)! / (n!)^2  = (2^2n)(1 + O(1/n)) / (πn)^(1/2)
    lg(C(2n, n))    = 2n + lg(1 + O(1/n)) - lg(πn)/2
                    = 2n - o(n) as n -> ∞
c.  if two elements a and b should be placed consecutively in sorted order, a <= b
    if a and b are not compared
        any element smaller than or equal to a will be smaller than or equal to b
        any element greatre than a will be greater than b
    no matter how much comparsions are made, a and b cannot be distinguished
    for the result to be correctly sorted, a and b must be compared
d.  there are 2n-1 consecutive pairs in an array of size 2n
    thus 2n-1 comparsions at least

8-7
-

Chapter 9
9.1-1
thanks https://ita.skanev.com/09/01/01.html
recursively:
    1.  divide the array into two
    2.  recursively find minimum of both subarrays, each with a list of elements compared to the minimum associated 
    3.  find minimum of the array as smaller of the two
    4.  append the greater to the list associated to the smaller
when the procedure terminates, it returns a list of elements been comapared to the minimum so far
number of comparsions performed equals to the number of internal nodes, which is n - 1
the list has size equal to the height of the recurrence tree, which is lgn
invariant:
    at each level, all elements in the subarray except the minimum is greater of equal to an element in the list
initialization:
    array size is 1, consists of a single element, which is the minimum, trivially satistied
maintenance:
    assume l, r are minimum of subarrays
    L, R are lists associated to them
    without loss of generality, if l <= r, L' = L ∪ {r} is the new list
    all elements in the left subarray except l >= one element in L
    all elements in the right subarray are >= r, thus >= one element in L'
termination:
    -
therefore the second minimum must be in the list associated to the minimum value
the minimum of them can be find with lgn - 1 comparsions, n - 1 + lgn - 1 = n + lgn - 2 in total

9.1-2
initially any element can be the maximum or the minimum
the number of potential maximum and minimum both equal to n, denote them as MIN and MAX
assume all elements are distinct, elements can be divided into 4 groups:
    1.  may be either maximum or minimum
    2.  may be maximum, may not be minimum
    3.  may be minimum, may not be maximum
    4.  may be neither maximum or minimum
comparing two elements a and b from group 1 will:
    if a < b, a now in group 3, b now in group 2, both MIN and MAX decremented by 1
comparsions between any combinations of elements from 2, 3 or 4, depends on the result, may be in vain
comparsions between a from group 1 and b from group 2, 3 or 4, depends on the result, may only decrement MIN+MAX by 1
thereby in worst case, it's optimal to perform comparsions between elements in group 1 first until group 1 is empty
such comparsions can only be performed n/2 times, eliminate n/2 potential maximum and minimum independent of the input
then the input array is divided into two disjoint groups, n/2 potential maximum and n/2 potential minimum
in worst case, maximum and minimum can be find among them in (n/2 - 1) * 2 = n - 2 comparsions
n/2 + n - 2 = 3n/2 - 2 in total

9.2-1
assume initially p < r, 1 <= i <= r - p + 1
if the left subproblem is empty, q = p, k = p - p + 1 = 1
i cannot be less than 1, so the procedure will not recursively solve the left subproblem
if the right subproblem is empty, q = r, k = r - p + 1
i cannot be greater than r - p + 1, so the procedure will not recursively solve the right subproblem

9.2-2
T(max(k-1, n-k)) is the same whether Xk = 0 or Xk = 1

9.2-3
./CLRS/sort/order.ts#randomizedSelect2

9.2-4
when pivot is always the maximum value

9.3-1
when divided into groups of 7, 
number of elements greater than x is at least
    4(1/2 * n/7 - 2) >= 2n/7 - 8
T(n)    <= c[n/7] + c(5n/7 + 8) + dn
        = 6cn/7 + 8c + dn
        = cn + (8c + dn - cn/7)    
        <= cn for c > 7d and large enough n
when divided into groups of 3,
number of elements greater than x is at least
    2(1/2 * n/3 - 2) >= n/3 - 4
T(n)    = T(n/3) + T(2n/3 + 4) + O(n)
        >= T(n/3) + T(2n/3) + O(n)
        = Θ(nlgn), no longer linear

9.3-2
3n/10 - 6 >= n/4
6n - 120 >= 5n
n >= 120

9.3-3
the lower median can be find in time O(n)
by finding the index of the lower median in O(n)
the array can be partitioned into subproblems of equal size in O(n)
a good split is then guarenteed, worst case running time thus O(nlgn)

9.3-4
assume all elements are distinct
let array elements be vertices of a graph, an edge a -> b exists iff some comparsion gives a > b
for vertices a and b
    if there is a path a -> b, a > b
    if there is a path b -> a, b > a
    otherwise it's uncertain whether a > b or b > a
also the graph is acyclic, otherwise for some element a on the cycle, a > a
for a procedure to correctly find the ith smallest element x, the comparsions it makes must ensure that:
    i-1 elements are smaller than x
    n-i elements are greater than x
therefore by additionally update the graph according to every comparsion
the set of i-1 elements smaller than x can be found by dfs from the vertex x
the set of i-1 elements greater than x can be found by dfs the reverse graph from the vertex x

9.3-5
by simulating binary search
let mid be the order of lower median, to find the ith smallest element
    if i = mid, one run of black-box can solve the problem
    if i > mid:
        find the median in O(n)
        filter elements smaller than the median from the array, O(n)
        find (i - mid)th smallest element in the resulting array recursively
    if i < mid:
        similarly filter the array
        find ith smallest element in the resulting array recursively
T(n) = T(n/2) + O(n) = O(n)

9.3-6
using select, an array can be partitioned around the median in time O(n)
therefore in time O(nlgk), the array can be divided into n/k chunks that
    1.  each chunk is of length k
    2.  elements in each chunk is greater than elements in previous chunks, smaller than elements in succeeding ones
then the maximum of each chunk forms the kth quantiles, which can be found in time O(n)
O(nlgk) + O(n) = O(nlgk) in total

9.3-7
find S in O(n)
substract S from every element, take abstract value in O(n)
find kth smallest element x of these abstract value in O(n)
filter the original input array for elements that has distance to S smaller or equal to x in O(n)

9.3-8
thanks https://ita.skanev.com/09/03/08.html
recursively:
    1.  find the medians ma, mb of the two array A, B
    2.  if the two medians equal, return either one
    3.  if ma > mb
            more than half of the elements in both arrays are greater than mb
            more than half of the elements in both arrays are smaller than ma
            the median of both arrays must be between ma and mb
            by dropping same amount of elements > ma and < mb, the overall median will not change
        drop elements > ma in A, elements < mb in B, get a subproblem of half size

9.3-9
denote the y coordinate of the main pipeline as y0, then
    y0 = min{Σ(|yi - y|)}
where (xi, yi) is the coordiante of ith well
when y is greater than the y coordinate of k wells and smaller than n-k,
    increasing y by an amount of ε will increase cost by εk, while decrease cost by ε(n-k) at the same time
therefore before k = n/2, increasing y will improve the solution
starting from = n/2 + 1, increasing y will increase the total cost
the problem is equivalent to finding the median of all y coordinates
the optimal solution is either the median, or any value between lower and higher medians

9-1
a.  O(nlgn)
b.  O(n) + O(nlgi)
c.  O(n) + O(ilgi)

9-2
a.  the median satisfies that, exactly [n/2] - 1 numbers are smaller than it
    each one has weight 1/n
        Σ(i = {1 .. [n/2] - 1})(1/n)
        = ([n/2] - 1)(1/n) < 1/2
        Σ(i = {[n/2] + 1 .. n})(1/n)
        = ([n/2] - 1)(1/n) < 1/2
b.  sort the elements
    find the greatest k that Σwi < 1/2 in sorted order
    then
        Σ(i = {1 .. k})(wi) < 1/2
        Σ{i = {1 .. k+1}}(wi) >= 1/2
        Σ(i = {k+2 .. n})(wi) >= 1/2
    and xk+1 is the median
c.  by modifying SELECT
    still partition the array around median of medians
    then compute the weight sum of elements smaller and greater than the pivot wl and wg
    if wl < 1/2 and wg <= 1/2, pivot is the weighted median
    if wl >= 1/2, the weighted median is among elements smaller than the pivot
        recursively find the weighted median in the subproblem
    if wg > 1/2, the weighted median is among elements greater than the pivot
        recursively find the element that's greater than elements sum to (1/2 - wl) in the subproblem
    running time O(n) as in text
d.  the same argument as 9.3-9
    increasing p a small amount ε will increase the sum by Σ(pi < p)(wiε) and decrease the sum by Σ(pi > p)(wiε)
    thus when Σ(pi > p)(wi) > 1/2, increasing p will reduce the total sum of weights
    when Σ(pi > p) < 1/2, inceasing p will increase the total sum of weights
    the minimum therefore is at greatest p where Σ(pi > p) >= 1/2
d.  min(p){Σd(a, b)} = min(x, y){Σ|xi - x| + Σ|yi - y|} = min(x){Σ|xi - x|} + min(y){Σ|yi - y|}
    when the x or y coordiate of the solution is changed, the other half of the sum won't change
    by finding the weighted median of x and y seperately, both part are minimized

9-3
a.  ?
b.  assuming Ui(n) <= n + cT(2i)lg(n/i)
    Ui(n)   <= n/2 + (n/2 + cT(2i)lg(n/2i)) + T(2i)
            = n + cT(2i)(lg(n/i) - 1) + T(2i)
            = n + cT(2i)lg(n/i) - cT(2i) + T(2i)
            <= n + cT(2i)lg(n/i) when c >= 1
c.  when i is constant,
    Ui(n)   = n + O(T(2i)lg(n/i))
            = n + O(lgn)
d.  when when i = n/k,
    Ui(n)   = n + O(T(2i)lg(n/i))
            = n + O(T(2n/k)lg(n/(n/k)))
            = n + O(T(2n/k)lgk)

9-4
a.  same as quicksort, zi and zj will be compared only if either zi or zj is first chosen as the pivot
    but this time, if i < j < k and an element between zj and zk is first chosen as pivot,
    the procedure will not recurse into the subproblem containing zi and zj
    Xijk = 1 only when zi or zj are the first pivot among {zi .. zk}
    similarly when k < i < j and an element between zk and zi is first chosen as pivot, zi and zj will not be compared
    Xijk = 1 only when zi or zj are the first pivot among {zk .. zj}
    namely
        E[Xijk] = 2/(k - i + 1) if i < j < k
                = 2/(j - k + 1) if k < i < j
                = 2/(j - i + 1) if i <= k <= j, i < j
                = 0 if i = j
b.  E[Xk]   = E[Σ(i = {1 .. n})(Σ(j = {i .. n})(Xijk))]
            = E[Σ(i = {1 .. k})(Σ(j = {k .. n})(Xijk))]
            + E[Σ(i = {k+1 .. n})(Σ(j = {i .. n})(Xijk))]
            + E[Σ(i = {1 .. k})(Σ(j = {i .. k-1})(Xijk))]
            = Σ(i = {1 .. k})(Σ(j = {k .. n})(2/(j - i + 1)))
            + Σ(i = {k+1 .. n})(Σ(j = {i .. n})(2/(j - k + 1)))
            + Σ(i = {1 .. k-2})(Σ(j = {i .. k-1})(2/(k - i + 1)))
            = 2Σ(i = {1 .. k})(Σ(j = {k .. n})(1/(j - i + 1)))
            + 2Σ(j = {k+1 .. n})(Σ(i = {k+1 .. j})(1/(j - k + 1)))
            + 2Σ(i = {1 .. k-2}(k-i-1)/(k-i+1))
            <= 2Σ(i = {1 .. k})(Σ(j = {k .. n})(1/(j - i + 1)))
            + 2Σ(j = {k+1 .. n})(j - k - 1)/(j - k + 1))
            + 2Σ(i = {1 .. k-2}(k-i-1)/(k-i+1))
c.  (j - k - 1)/(j - k + 1) < 1
    (k - i - 1)/(k - i + 1) < 1
    2Σ(j = {k+1 .. n})(j - k - 1)/(j - k + 1)) + 2Σ(i = {1 .. k-2}(k-i-1)/(k-i+1))
    <= Σ(i = {1 .. n}1) = n
    2Σ(i = {1 .. k})(Σ(j = {k .. n})(1/(j - i + 1)))
    = 1/k + 1/(k+1) + .. + 1/n
    + 1/(k-1) + 1/k + .. + 1/(n-1)
    + ...
    + 1/1 + 1/2 + .. + 1/(n-k+1)
    starting from 1/1, the term 1/i appears at most on i lines
    therefore the sum <= Σi/i = n
    E[Xk] <= 2 * 2n = 4n
d.  as E[Xk] <= 4n = O(n)

Chapter 10 
10.1-1
[ 4 ]
[ 4, 1 ]
[ 4, 1, 3 ]
Pop:  3
[ 4, 1 ]
[ 4, 1, 8 ]
Pop:  8
[ 4, 1 ]

10.1-2
two stacks, one starts from index 0 and grows upward, one starts from index n - 1 and grows downward

10.1-3
head: 0, tail: 1
[ 4, <5 empty items> ]
head: 0, tail: 2
[ 4, 1, <4 empty items> ]
head: 0, tail: 3
[ 4, 1, 3, <3 empty items> ]
Dequeue:  4
head: 1, tail: 3
[ 4, 1, 3, <3 empty items> ]
head: 1, tail: 4
[ 4, 1, 3, 8, <2 empty items> ]
Dequeue:  1
head: 2, tail: 4
[ 4, 1, 3, 8, <2 empty items> ]

10.1-4
./CLRS/collection/queue.ts#Queue
by maintaining the size of the queue explicitly

10.1-5
./CLRS/collection/queue.ts#Deque

10.1-6
two stacks A and B
enqueue push elements into A
dequeue pops elements from B
when B is empty, pop all elements from A and push into B in turn
enqueue: constant time
dequeue: amortized constant time

10.1-7
keep track of the number of elements in the queue A
push enqueues an element into A
when currently there are n elements in the queue, repeat n - 1 times:
    dequeue an element
    enqueue it immediately
then the next element dequeued is the last element pushed 
one queue is sufficient 
push: constant time
pop: linear time

10.2-1
insert still O(1), inserted before the head of the list
delete is now O(n), the previous node of can no longer be find in O(1), must traverse the list

10.2-2
PUSH:   insert a new node at the head of the list
POP:    delete the head, make head.next the new head, return the value contained by the head
        if head = NIL, throw underflow

10.2-3
additionally maintain the tail of the list
ENQUEUE:    append a new node to the tail of the list, update the tail to the new node
DEQUEUE:    delete the head, make head.next the new head, return the value contained by the head
            if head = NIL, throw underflow

10.2-4
set L.nil.key = k before the loop
when the loop terminates, either x = L.nil or another node x.key = k is found

10.2-5
./CLRS/collection/slist.ts#SList
INSERT: O(1)
DELETE: O(n)
SEARCH: O(n)

10.2-6
by explicitly maintaining the tail of the list as a field
./CLRS/collection/dlist.ts#concat

10.2-7
traverse the list, repeatedly delete and insert nodes
./CLRS/collection/slist.ts#reverse

10.2-8
assume the pointer to NIL is NULL = 0x0
this.head.np = this.head.next ^ NULL = this.head.next
for any two conservative nodes x and y,
    x.np = x.prev ^ x.next = x.prev ^ &y
    y.np = y.prev ^ y.next = &x ^ y.next
starting from any two concervative nodes, the dlist can be traversed in any direction
hence search can be performed in a similar manner
insert updates this.head.np to this.head.np ^ &new_head
delete cannot be performed in constant time, must traverse the list to get two conservative nodes
by swapping the value of head and tail, the list is reversed in constant time

10.3-1
double list
        1   2   3   4   5   6
next    2   3   4   5   6   -
key     13  4   8   19  5   11
prev    -   1   2   3   4   5
single list
        1   2   3   4   5   6
next    2   3   4   5   6   -
key     13  4   8   19  5   11

10.3-2
the same procedure
x.next now points to the word at x+1

10.3-3
the procedure never need to traverse the free list, only insert and delete from head
insertion only have to set the next of the new head
deletion only have to follow the next of the current head

10.3-4
will have to copy and move nodes as long as a node can be deleted from the middle of the list

10.3-5
assume a move operation that moves the content of node x to y
overwrites array entry y, while maintains the correctness of prev and next pointers
traverse the list L, for ith list node x
    inspect node in ith entry of the array
    if ith entry is free, move x to ith entry, free x
    if ith entry is another list node, allocate a new object y, move i to y, move x to i, free x
after n iterations, ith list node is stored at ith entry of the list

10.4-1
-

10.4-2
./CLRS/collection/tree.ts#printTree

10.4-3
./CLRS/collection/tree.ts#printTreeStack

10.4-4
./CLRS/collection/tree.ts#printSiblingTree

10.4-5
./CLRS/collection/tree.ts#printTreeConstant

10.4-6
use a boolean is_last_child to indicate whether this child is the last one of its parent
the first pointer is still the left-child
the second pointer, when is_last_child == false, is still the right-sibling
when is_last_child == true, the second pointer points to the parent

10-1
search      O(n)    O(n)    O(n)    O(n)
    search is always linear
    there's no way in linked list to reach a particular node without traversing all nodes before it
insert      O(1)    O(n)    O(1)    O(n)
    insert into unsorted lists can be done in constant time by inserting to the head
    while insertion into sorted lists have to maintain the order
delete      O(n)    O(1)    O(n)    O(1)
    deletion always has to modify the previous node
    in singly linked list, there's no efficient way to find the previous node, have to traverse from the head
successor   O(n)    O(1)    O(n)    O(1)
    in unsorted lists, successor have to search the whole list
    in sorted lists, successor only have to call next on the input node
predecessor O(n)    O(n)    O(n)    O(1)
    in unsorted lists, predecessor have to search the whole list
    in sorted lists, successor only have to call prev on the input node
    but singly linked list have no pointer points to the previous node, have to traverse the whole list
minimum     O(n)    O(1)    O(n)    O(1)
    have to search the whole lists when unsorted
    in sorted list minimum is just the head
maximum     O(n)    O(1)    O(n)    O(1)
    assumes the tail of the list is explicitly maintained
    otherwise if the lists are circular, O(1) for doubly linked sorted list, O(n) for singly linked
    both O(n) if the lists are not circular

10-2
a.  insert:         have to find the right position of the key, O(n)
    minimum:        at the head, O(1)
    extract-min:    delete and return the head, O(1)
    union:          merging two sorted list, O(n)
b.  insert:         insert at the head, O(1)
    minimum:        by searching the whole list, O(n)
    extract-min:    by searching the whole list then delete, O(n)
    union:          must dedup, O(n^2) by naive double loop
                    can be improved to O(nlgn) by first sort both lists then merge
c.  same to b except
    union:          dedup not necessary, simple concatenation will suffice, O(1)

10-3
a.  both algorithm is correct, returning the first node i in the list that key[i] = k or NIL if no such node exists
    if the while loop of COMPACT-LIST-SEARCH takes t iterations
    then none of the first t-1 random j can have key[j] == k
    otherwise by the loop condition, key[i] < k, i will be updated to j, the procedure will return at line 7
    therefore the for loop of COMPACT-LIST-SEARCH' will run for at least t iterations
b.  the for loop runs for t iterations, each O(1)
    after t iterations of the for loop, the distance from i to key k will be Xt
    the while loop sets i = next[i], thus will run for Xt iterations
    T(n) = O(n + Xt)
    E[T(n)] = O(n + E[Xt])
c.  for a particular r, there are
        1.  nodes with distance to the key greater or equal than r
        2.  nodes with distance to the key smaller than r
        3.  nodes after the key
    there are exactly r nodes in group 2
    if no random node j falls within group 2, Xt >= r
    E[Xt]   <= ΣP[Xt >= r]
            = Σ(1 - r/n)^t
d.  bounding by integral
    Σ(r = {0 .. n-1})(r^t)  <= int(0 <= r <= n, r^t)
                            = n^(t+1)/(t+1) - 0^(t+1)/(t+1)
                            <= n^(t+1)/(t+1)
e.  E[Xt]   <= Σ(1 - r/n)^t
            = Σ((n-r)/n)^t
            = Σ(r = {1 .. n})((n-r)^t)/n^t
            = Σ(r = {0 .. n-1})(r^t)/n^t
            <= n^(t+1)/(t+1)n^t
            = n/(t+1)
f.  combine b. and e.
g.  ?
h.  otherwise even node j is further down the list than i, i will not be updated to j as key[i] = key[j]
    when all but the last node contains the same key k
    searching any key smaller or equal to will be O(1)
    searching any key greater than k will be O(n) with or without random skip

Chapter 11
11.1-1
examine every slot, compare with the current maximum and update if not empty 
O(m), m the number of slots in the array

11.1-2
make a bijection between elements and a finite number set {0 .. n - 1} of size n
A[i] = 1 if i ∈ S, A[i] = 0 otherwise
INSERT(i) sets A[i] to 1, DELETE(i) set A[i] = 0
SEARCH(i) returns A[i] == 1

11.1-3
each slot stores a pointer to a doubly linked list
SEARCH(k) inspects whether the linked list at slot k is empty
INSERT(k) inserts at the head of the list
DELETE(x) removes the nodes from the list

11.1-4
thanks Instructor's Manual
by initializing two huge arrays which validate each other
array A stores index of array S, array S stores index of array A
additionally array S has a field S.top
if array A contains key k, A[k] < S.top && S[A[k]] == k, both insert and delete maintains this property
if array A does not contain key k, A[k] is not initialized thus can be any number
if A[k] >= S.top, A[k] is trivially invalid
if A[k] < S.top, it must be set by INSERT(k') with a different k', so S[A[k]] !== k

11.2-1
define indicator variable Xij as
    Xij = 1 when h(ki) = h(kj)
        = 0 otherwise
by simple uniform hashing, Pr{Xij = 1} = 1/m
the expected total number of collisions is
    E[Σ(i = {1 .. n})(Σ(j = {i+1 .. n})(Xij))]
    = C(n, 2)(1/m)
    = n(n-1)/m

11.2-2
A = [5, 28, 19, 15, 20, 33, 12, 17, 10]
B = A.map(n => n % 9);
B = [5, 1, 1, 6, 2, 6, 3, 8, 1]
three collisions in slot 1
two collisions in slot 6

11.2-3
deletion still O(1)
insertion now has to insert the key to the correct position, equivalent to unsuccessful search in average performance
unsuccessful search now stops when the node examined has a key larger than the search key
assume all inserted elements are drawn from a uniform distribution
about half of the α keys that collide with the search key will be smaller or greater than it
expected running time O(1 + α/2), which is O(1 + α)
performance of successful search now depends not on the keys inserted after k, but the keys smaller than k
    Pr{ Xij = 1 } = Pr{ h(ki) = h(kj) ∧ hi > hj } = 1/2m
expected number of nodes examined when searching ki is then
    E[1 + Σ(j = {1 .. n} - i)(Xij)]
    = 1 + (n-1)/2m
on average over all i:
    1/n * n(1 + (n-1)/2m)
    = 1 + (n-1)/2m
    = 1 + α/2 - 1/2m
    = Θ(1 + α)
so search and delete are asymptotically the same to unsorted lists
insertion performance is reduced from O(1) to (1 + α)

11.2-4
set a slot in the hash table not in the range of the hash function
that is, for a h_free, no k will give h(k) = h_free
can add 1 to any existing hash function, so the range changes from {1 .. m} to {2 .. m+1}
slot 1 then is the free slot now
search does not involve allocation and deallocation
insert operation now has to acquire a free node from the free list
    simply delete the head from list in slot h_free, O(1)
delete operation now has to return the node to the free node
    simply prepend the node to list in slot h_free, O(1)
a simply linked list will suffice

11.2-5
pigeonhole principle
if all slots contains list of length smaller than n
the total number of keys stored in the hash table is smaller than nm = |U|
thereby at least one slot contains a list of length greater or equal to n

11.2-6
thanks https://stackoverflow.com/questions/8629447/
repeatedly:
    1.  select a slot <- {1 .. m}
    2.  select a number i <- {1 .. L}
        if i <= length of the list in this slot, take the key in ith node
        otherwise go back to 1.
the procedure succeeds when i <= length of the list
as the expected length of list is α, success rate is α/L
expected number of trials is then L/α by geometric distribution
then ith node is retrieved in time O(L), O(L + L/α) = O(L(1 + 1/α)) in total
consider the hash table as a m x L 2d-array with n entries non-empty
this procedure than uniformly chooses a slot from all m x L entries, both empty and non-empty
as the procedure is redone when an empty entry is chosen, it's a uniform sampling of non-empty entry

11.3-1
compare h(k) to the stored hash value first, only compare the stored strings when hash values equal
comparing two strings will take time proportional to the length of the shorter string
comparing two hash values usually will be constant time

11.3-2
the number s represented by the string is Σ(i = {1 .. r})128^(r-i)ci
assume rk is the number represented by the first k characters of the string modulo m
    rk      = Σ(i = {0 .. k})(128^(r-i)ci) mod m
    rk+1    = Σ(i = {0 .. k+1})(128^(r-i)ci) mod m
            = (Σ(i = {0 .. k})(128^(r-i)ci) + ck+1) mod m
            = 128rk mod m + ck+1 mod m
therefore s mod m can be computed iteratively

11.3-3
let x = c1 .. cn
h(x)    = Σ(i = {1 .. n})((2^p)^(n-i)ci) mod 2^p - 1
        = Σ(i = {1 .. n})(2^p(n-i)ci mod 2^p - 1)
        = Σ(i = {1 .. n})(ci)
therefore any permutation y of x has h(y) = h(x)

11.3-4
[ 700, 318, 936, 554, 172 ]

11.3-5
assume there are |B| slots containing b1 .. b|B| keys
    Σ(i = {1 .. |B|})bi = |U|
then the number of collisions is
    Σ(i = {1 .. |B|})(bi(bi - 1))
    = Σbi^2 - Σbi
by Cauchy-Schwarz inequality,
    (Σbi)^2 <= |B|Σbi^2
    Σbi^2 >= |U|^2 / |B|
    Σ(bi(bi - 1)) >= |U|^2/|B| - |U|
assume Pr{h(x) = h(y)} < 1/|B| - 1/|U|
then the total number of collisions is less than
    C(|U|, 2) * (1/|B| - 1/|U|)
    = |U|(|U| - 1) * (1/|B| - 1/|U|)
    = (|U| - 1)(|U|/|B| - 1)
    < |U|^2/|B| - |U|
therefore it must has Pr{h(x) = h(y)} >= 1/|B| - 1/|U|

11.3-6
-

11.4-1
Linear probing:
[ 22, 88, null, null, 4, 15, 28, 17, 59, 31, 10 ]
Quadratic probing:
[ 22, null, 88, 17, 4, null, 28, 59, 15, 31, 10 ]
Double hashing:
[ 22, null, 59, 17, 4, 15, 28, 88, null, 31, 10 ]

11.4-2
./CLRS/collection/hashtable.ts#OpenAddressing

11.4-3
α = 3/4:
    unsuccessful:   1/(1 - α) = 4
    successful:     (1/α)ln(1/(1-α)) < 2
α = 7/8:
    unsuccessful:   1/(1 - α) = 8
    successful:     (1/α)ln(1/(1-α)) < 3

11.4-4
when gcd(h2(k), m) = d, the subgroup of Zm generated by h2(k) has m/d elements
which means c = m/d is the smallest positive c such that ch2(k) ≡ 0 mod m
hash value is once again h1(k) when i = c = m/d, therefore 1/dth of m entries of the hashtable

11.4-5
1/(1-α) = (2/α)ln(1/(1-α))
α/(1-α) = 2ln(1/(1-α))
e^(α/(1-α)) = 1/(1-α)^2
α = 0.71533 by wolframalpha

11.5-1
"no collisions occur" means insert operation never increments i
for the set of key K inserted, all {h(k): k ∈ K} are distinct
assuming uniform hashing,
    Pr{h(k) = h(k')} <= 1/m for all distinct k and k'
    p(n, m) = (1 - 1/m)^C(n, 2)
            <= e^(-1/m * C(n, 2))
            = e^(-n(n-1)/2m)
therefore when n >= m^(1/2) + 1, p(n, m) <= e^(-1/2) decreases exponentially

11-1
a.  the hashtable is at most half-full when i = n
    by uniform hashing
    all n = 2m keys inserted before, and all k probes of the current key points to uniformly random position in the array
    for any probe, Pr{ith probe hashes the key to an occupied slot} <= 1/2
    the insertion operation will do more than k probes if all the first k probes hashes the key to an occupied slot
    which happens with probability (1/2)^k = 2^-k
b.  2^-2lgn = n^-2 = O(1/n^2)
c.  Pr{X > 2lgn}    = Pr{X1 > 2lgn ∨ X2 <= 2lgn ∨ .. ∨ Xn <= 2lgn}
                    <= ΣPr{Xi > 2lgn}
                    = nO(1/n^2)
                    = O(1/n)
d.  X is capped by n, the total number of keys inserted
    therefore
        E[X]    <= 2lgn * (1 - O(1/n)) + nO(1/n)
                = O(lgn) + O(1)
                = O(lgn)

11-2
a.  the probability that a certain key is hashed to a certain slot is 1/n
    so the number of keys in a slot after n insertion follows binomial distribution with success rate 1/n
        Qk  = P(X = k) 
            = C(n, k)p^k(1-p)^(n-k)
            = C(n, k)(1/n)^k(1 - 1/n)^(n-k)
b.  M = k means at least a slot contains exactly k keys
    let Mi be the number of keys in slot i
    Pr{M = k}   = Pr{∃i, Mi = k ∧ ∀j != i, Mi <= k}
                <= Pr{∃i, Mi = k}
                = Pr{M1 = k ∨ M2 = k .. Mn = k}
                <= ΣPr{Mi = k}
                = nQk
c.  as n -> ∞, (1 - 1/n) -> 1
    (1 - 1/n)^(n-k) < 1
    C(n, k) = n!/k!(n-k)! <= n^k/k!
    Qk  = C(n, k)(1/n)^k(1 - 1/n)^(n-k)
        <= n^k/k! * (1/n)^k
        = 1/k!
        = 1/((2πn)^(1/2) * (k/e)^k * (1 + Θ(1/k)))
        <= 1/(k/e)^k
        = e^k/k^k
d.  when k0 = clgn/lglgn,
        lg(e^k0 / k0^k0)    = k0lge - k0lgk0
                            = clgnlge/lglgn - clgn/lglgn * (lgc + lglgn - lglglgn)
                            = c(lgelgn / lglgn - lgn/lglgn * (lgc + lglgn - lglglgn))
                            = clgn(lge / lglgn - 1/lglgn * (lgc + lglgn - lglglgn))
                            = clgn(lge / lglgn - lgc/lglgn - 1 + lglglgn/lglgn)
        lg(1/n^3) = -3lgn > lgQk0
        -3 > c(lge / lglgn - lgc/lglgn - 1 + lglglgn/lglgn)
    as n -> ∞, lge/lglgn -> 0, lgc/lglgn -> 0, lglglgn/lglgn -> 0
    c(lge / lglgn - lgc/lglgn - 1 + lglglgn/lglgn) -> -c
    so for c > 3, Qk0 < 1/n^3 for large enough n
    as c > 3 > e, e/k < 1, e^k/k^k monotonically decreasing
    Qk <= Qk0 when k <= k0
    Pk <= nQk <= nQk0 = 1/n^2
e.  M is capped by n
    E[M]    <= Pr{M > clgn/lglgn} * n + Pr{M <= clgn/lglgn} * clgn/lglgn
            <= n * 1/n^2 * n + 1 * clgn/lglgn for large enough n
            = 1 + O(lgn/lglgn)
            = O(lgn/lglgn)

11-3
a.  the ith probe examines position 
        (j + 1 + 2 + .. + i) mod m
        = (j + i(i+1)/2) mod m
        = (j + i/2 + i^2/2) mod m
    c1 = 1/2, c2 = 1/2
b.  assume (j + k(k+1)/2) ≡ (j + l(l+1)/2) mod m for some 0 <= k < l <= m-1
        k(k+1)/2 ≡ l(l+1)/2 mod m
        k(k+1)/2 - l(l+1)/2 ≡ 0 mod m
    thanks Instructor's Manual
        (j-i)(j+i+1)/2 ≡ 0 mod m
        (j-i)(j+i+1) = 2dm = 2d2^p = d2^(p+1)
    exactly one of j-i and j+i+1 must be even, the other must be odd
    which means 2^(p+1) must divide one of j-i and j+i+1
    as j-i < m, 2^(p+1) cannot divide j-i
    as j+i <= 2m-2, j+i+1 <= 2m-1 < 2^(p+1), 2^(p+1) cannot divide j+i+1
    therefore k(k+1)/2 !≡ l(l+1)/2 mod m for all distinct k and l
    for all i ∈ {0 .. m-1}, (j + i(i+1)/2) mod m is distinct

11-4
-

Chapter 12
12.1-1
Height 2:
  ┌-21
┌-17
| └-16
10
| ┌-5
└-4
  └-1
Height 3:
  ┌-21
┌-17
| └-16
10
└-5
  | ┌-4
  └-1
Height 4:
21
|   ┌-17
|   | └-16
| ┌-10
└-5
  └-4
    └-1
Height 5:
┌-21
17
└-16
  | ┌-10
  | | | ┌-5
  | | └-4
  └-1
Height 6:
  ┌-21
  | | ┌-17
  | | | | ┌-16
  | | | └-10
  | └-5
┌-4
1
12.1-2
binary search tree:
    left <= parent <= right
min heap:
    left >= parent, right >= parent
it cannot print elements in sorted order in O(n)
as building a min heap takes time O(n)
if it can traverse its element in sorted order in O(n), n elements can be sorted in time O(n)
which is lower than the lower bound of comparsion sort Ω(nlgn)

12.1-3
the same idea as 10.4-5
adjusted when a the key is yielded as 10.4-5 is preorder
./CLRS/collection/tree.ts#inorder

12.1-4
./CLRS/collection/tree.ts#preorder
./CLRS/collection/tree.ts#postorder

12.1-5
as inorder traverses the tree keys in sorted order in O(n)
if a binary search tree can be built from n elements in time o(nlgn), n elements can be sorted in time o(nlgn)
therefore building a binary search tree must take time Ω(nlgn)

12.2-1
by the nature of search tree structure, for any sequences of nodes examined and an index i,
    if A[i] >= A[i+1], A[i] >= A[j] for all j >= i+1
    if A[i] <= A[i+1], A[i] <= A[j] for all j >= i+1
c is not possible as 911 > 240 but 911 < 912

12.2-2
./CLRS/collection/tree.ts#treeMinimum
./CLRS/collection/tree.ts#treeMaximum

12.2-3
./CLRS/collection/tree.ts#treePredecessor

12.2-4
  ┌-21
┌-17
| └-16
10
| ┌-5
└-4
  └-1
search for 21, 10 on path, 16 to the left, 16 > 10
search for 1, 10 on path, 5 to the right, 5 < 10

12.2-5
assume all keys are distinct, current node x has key k
the successor s has key greater than k
it's either 
    1.  in the right subtree of x, or, 
    2.  at some level from the root, x is in the left subtree and s is the parent
then all nodes in case 1 have keys smaller than all nodes in case 2
as x has two children, it's successor must be case 1, in the right subtree of x
then if s has a left child, it has key smaller than s but greater than x
in contradiction to s being the successor of x
similarly the predecessor has no right child

12.2-6
any node s greater than x is either
    1.  in the right subtree of x, or, 
    2.  at some level from the root, x is in the left subtree and s is the parent / in the right subtree
in case 2, as all nodes in the right subtree are greater than the parent, case 2 can be refined to
    2.  at some level from the root, x is in the left subtree and s is the parent
when there's no nodes in case 1, the smallest node in case 2 is the successor of x
consider two nodes s1 and s2, where s2 is higher in the tree than s1
by definition s1 is in the left subtree of s2, s2 > s1
thereby the smallest node in case 2 is just the lowest node in case 2
which in turn is the lowest ancestor of x that has x in its left subtree / whose left child is also an ancestor of x

12.2-7
see 12.2-8, as h = O(n), k = O(n), k + h = O(2n) = O(n)

12.2-8
consider the subtree formed by nodes traversed by the k calls to SUCCESSOR
exactly k nodes is yielded during the traverse
the subtree may contain nodes smaller than the minimum of the k nodes on the path from root to the minimum
similary the subtree may contains nodes greater than the maximum
both path has at most h nodes
the number of nodes in the subtree is at most O(k + 2h)
for any node x in the subtree, x is traversed first from its parent p
either during call to SUCCESSOR(p) or SUCCESSOR(y) for some y
both case the left subtree of x will be traversed before x is visited again
then if x has a right child, its right subtree will be traversed before x is visited the third time
once m the maximum node in the right subtree of x is traversed, SUCCESSOR will go directly to p
if x = p.left, SUCCESSOR(m) = p, the left subtree of p will never be visited again
if x = p.right, SUCCESSOR(m) = p' for which x is in the left subtree of p', the same to above
therefore a node will be visited at most 3 times
T(k) = 3O(k + 2h) = O(k + h)

12.2-9
when x is a leaf node, it has no left nor right child
either SUCCESSOR(x) will return y or PREDECESSOR(x) will return y, depends on x being left or right child of y

12.3-1
./CLRS/collection/tree.ts#treeInsertRecur

12.3-2
TREE-INSERT and TREE-SEARCH are equivalent until a null leaf
search operation will traverse exactly the same path as insert
plus one final node which was NIL at the end of insertion

12.3-3
worst case:
    the element inserted is strictly increasing or decreasing
    before the ith insertion, the tree has height i-2, so ith insertion takes time Θ(i - 2) = Θ(i)
    T(n) = T(n-1) + Θ(n) = Θ(n^2)
best case:
    the tree is perfectly balanced
    T(n) = T(n-1) + Θ(lgn) = Θ(nlgn)

12.3-4
let x be a node with two children, y be its left child, y is a leaf
deleting x first will replace x with its successor without modifying its left child
then deleting y simply removes y
deleting y first will simply remove y
then x becomes case a in figure 12.4, the x is replaced by its right subtree
two results may be different if the right child of x has a left child

12.3-5
search doesn't reference parents
insert also doesn't reference parents
succ, the successor of the maximum node in the subtree rooted at x will be the first "right ancestor" of x
thereby if succ is NIL, x on the right-most branch of the tree
parent of x can be found by travering downward from the root and check if right child of the cursor is x
if succ is not NIL, x is on the right-most branch of the subtree rooted at the left child of succ
./CLRS/collection/tree.ts#treeParent
this procedure will only traverse from root to the bottom at most twice, running time is O(h)
both TRANSPLANT and TREE-DELETE only refers parent costant times, so TREE-DELETE takes time O(h)

12.3-6
the predecessor pred will be the maximum of the subtree rooted at the left child of z
then pred have to be first replaced by its left child before replacing z, if pred is not the left child of z
the two strategy can be defined as two seperate procedures, which are called with 1/2 probability each

12.4-1
when n = 1, ΣC(i+3, 3) = C(3, 3) = C(n+3, 4) = C(4, 4) = 1
assume Σ(i = {0 .. n-1})C(i+3, 3) = C(n+3, 4)
Σ(i = {0 .. n})C(i+3, 3)
= Σ(i = {0 .. n-1})C(i+3, 3) + C(n+3, 3)
= C(n+3, 4) + C(n+3, 3)
= C(n+4, 4)
so for all n >= 1, Σ(i = {0 .. n-1})C(i+3, 3) = C(n+3, 4)

12.4-2
let tree T' be a perfectly balanced tree with k nodes
then the height of T' will be O(lgk)
let tree T be a tree that:
    left subtree of T is T'
    right subtree of T is s single chain of length n - k - 1
then the average height in left subtree will be 1 + O(lgk) = O(lgk)
the average height in right subtree will be 1 + O(n-k) = O(n-k)
overall average will be
    (kO(lgk) + (n-k)O(n-k)) / n
    <= (nO(lgk) + (n-k)O(n-k)) / n
    = O(lgk) + O((n-k)^2 / n)
for the overall average be O(lgn), O((n-k)^2 / n) must be O(lgn)
    (n-k)^2 = O(nlgn)
    n-k = O((nlgn)^(1/2))
therefore the height of a tree of average height O(lgn) is O((nlgn)^(1/2))

12.4-3
let V(n) denote the variations of trees of n nodes
when the root has ith order, its left subtree has i-1 nodes, its right subtree has n-i nodes
    V(n) = Σ(i = {1 .. n})(V(i-1)V(n-i))
specifically:
    V(0) = 1
    V(1) = 1
    V(2) = 2
    V(3) = 5
when n = 3, a randomly chosen tree has 5 variations:
v1:
    3
    │ ┌─2
    └─1
v2:
    ┌─3
    2
    └─1
v3:
    3
    └─2
      └─1
v4:
    ┌─3
    │ └─2
    1
v5:
      ┌─3
    ┌─2
    1
when the tree is randomly chosen, each tree will be picked with probability 1/5
but 3! = 6, the 6 variations of inputs cannot be evenly divided into the 5 cases

12.4-4
f(x) = 2^x
(d/d^2x)f(x) = 2^x(ln2)^2 >= 0
2^x is convex

12.4-5
the randomized quicksort 
    1.  chooses a random pivot
    2.  compare all other elements with the pivot, partitions them into two subarray
    3.  repeat step 1. for both subarrays
this procedure is equivalent to
    1.  choose a random element as the root of the tree
    2.  compare all other elements with the root, decide whether they belongs to the left or right subtree
    3.  repeat step 1. for the two groups to build the two subtrees
which is in turn equivalent to build a random tree by insertion
the height of an element then is number of comparsions performed with its key as an operand
the total height bounds the total number of comparsions performed by randomized quicksort
as the expected height of the tree is O(lgn), the total height is O(nlgn)
thanks https://cs.stackexchange.com/questions/86149
the original bound O(lgn) in 12.4 can be strengthened 
as E[Yn] = E[2^Xn] <= C(n+3, 3)/4 = O(n^3), by Markov's inequality
    Pr{Xn >= lgt} = Pr{2^Xn >= t} <= E[2^Xn] / t = O(n^3/t)
choose t = n^(3+k),
    Pr{Xn >= (k+3)lgn} = O(n^-k)
therefore probability of running time of quicksort exceeds (k+3)nlgn = O(nlgn) is O(1/n^k) for any k > 0

12-1
a.  as k is inserted to the left of the current node when k <= node.key, the resulting tree will be a single chain
    ith insertion takes time Ω(i), total time Ω(n^2)
b.  for each node in the tree, the left subtree may have nodes at most one more than the right subtree
    let T(h) be the lower bound of number of nodes in such a tree of height h, then
        T(1)    = 1
        T(h)    = 1 + T(h-1) + (T(h-1) - 1) = 2T(h-1)
                = 2^(h-1) = Ω(2^h)
    therefore h = O(lgn), the tree is balanced
    expected time of n insertion is then nO(h) = O(nlgn)
c.  all nodes will be in the list associated with the root
    each insertion takes time O(1) of list insertion, O(n) in total
d.  in worst case the resulting tree is a single chain, Ω(n^2) in total
    on average it's roughly equivalent to building a tree by random sequences of insertion
    average height h = O(lgn), expected running time O(nlgn)

12-2
insertion is stright forward, takes time proportional to the length of the string, O(n) in total
nodes have to keep an additional boolean field indicating whether they are end of a string
then the total nodes in the tree is O(n)
instead of inorder, the tree has to be traversed in preorder to give sorted sequence
the path taken so far during the preorder traverse can be maintained in a stack-like structure
as both pop and push to the top of the stack takes O(1), this will not change the performance asymptotically
reconstructing the string from nodes takes time proportional to the length of the path
which in turn is the length of the originally inserted string, the combined length is O(n)
hence overall running time is O(n)
./CLRS/collection/radix-tree.ts#RadixTree

12-3
a.  there are n nodes in total
    each node x has depth d(x, T)
    average depth is Σd(x, T) / n = P(T) / n
b.  the root has depth 0
    total depth of nodes in left subtree is P(TL) + |TL|
    total depth of nodes in right subtree is P(TR) + |TR|
    |TL| + |TR| = n - 1
    P(T) = P(TL) + P(TR) + |TL| + |TR| = P(TL) + P(TR) + n - 1
c.  the root of the tree can have order from 1 to n uniformly
    if the root has order i, its left subtree has i-1 nodes, right subtree n-i nodes
    combine with part b.
    P(n)    = Σ(i = {1 .. n})(P(i-1) + P(n-i) + n - 1)
            = Σ(i = {0 .. n-1})(P(i) + P(n-i-1) + n - 1)
d.  each P(i) for i = {0 .. n-1} appears twice
    P(n)    = (2Σ(i = {0 .. n-1})P(i) + n(n-1)) / n
            = 2Σ(i = {0 .. n-1})P(i) / n + (n-1)
            = 2Σ(i = {0 .. n-1})P(i) / n + Θ(n)
e.  the same as 7-3
f.  randomized quicksort, as argued in 12.4-5

12-4
a.  see 12.4-3
b.  B(x)^2  = (Σ(n = {0..})bnx^n)^2
            = Σ(i = {0 ..})(Σ(j = {0 ..})(bibjx^(i+j)))
    which is the sum of a matrix where entry (i, j) is bibjx^(i+j)
    consider each diagonal line on which i+j = n >= 1, entries on that line sums to
    Σ(k = {0 .. n})bkbn-kx^n = bn+1x^n
    for n = 0, b0 = 1, b0b0x^0 = b1x^0
    B(x)^2  = Σ(n = {0 ..})(bn+1x^n)
    xB(x)^2 = Σ(n = {0 ..})(bn+1x^n+1)
            = Σ(n = {1 ..})(bnx^n)
    xB(x)^2 + 1 = B(x)
c.  routine
d.  Stirling's approximation

Chapter 13
13.1-1
bh(T.root) = 2:
    ┌─15r
  ┌─14b
  │ └─13r
┌─12r
│ │ ┌─11r
│ └─10b
│   └─9r
8b
│   ┌─7r
│ ┌─6b
│ │ └─5r
└─4r
  │ ┌─3r
  └─2b
    └─1r
bh(T.root) = 3:
    ┌─15b
  ┌─14r
  │ └─13b
┌─12b
│ │ ┌─11b
│ └─10r
│   └─9b
8b
│   ┌─7b
│ ┌─6r
│ │ └─5b
└─4b
  │ ┌─3b
  └─2r
    └─1b
bh(T.root) = 4:
    ┌─15b
  ┌─14b
  │ └─13b
┌─12b
│ │ ┌─11b
│ └─10b
│   └─9b
8b
│   ┌─7b
│ ┌─6b
│ │ └─5b
└─4b
  │ ┌─3b
  └─2b
    └─1b

13.1-2
36 will be inserted as the right child of 35
as 35 is red
    if 36 is inserted as a red node, property 4 is broken
    the red node 35 has a red child
    if 36 is inserted as a black node, property 5 is broken
    the path from root to 36 contains one more black node than other paths

13.1-3
property 1
    trivially satisfied
property 2
    satisfied
property 3
    satisfied
property 4
    only the root is colored black from red
    each red node in either the left or right subtree satisfies property 4
    the root is now black, so each red node satisfies property 4
property 5
    each node satisfies property 5 before the root is re-colored
    for each node in either left or right subtree, property 5 still holds after the re-coloring
    for the root, each path has one more black node after the re-coloring, property 5 still holds for all nodes

13.1-4
as red nodes must have black children, the parent of a red node must be black
then the red node itself also can only have black children
so after the "absorb", the parent of a red node may have at most 4 black children

13.1-5
both paths contain the same amount of black nodes bh(x)
as each red node must have black children
the longest such path may contain at most bh(x) red nodes, 2bh(x) combined
the shortest path has at least bh(x) nodes

13.1-6
by 13.1-5
T is at most a complete tree of height 2bh(T.root) = 2k, 2^(2k + 1) - 1 nodes in total
T is at least a complete tree of height bh(T.root)-1 = k-1, 2^k - 1 nodes in total

13.1-7
largest possible:
    root is black or red
    if one level contains red / black nodes only, the next level contains only the opposite color
    let 2^k - 1 <= n < 2^(k+1) - 1
    if k is odd, the first k levels contains 1/3 or 2/3 red nodes if the root is not counted
    the ratio increases / decreases towards 2/3 or 1/3 depends on whether root is red or black
    if k is even, the same to when k is odd, but this time the root can be counted
    the largest possible ratio then is between 2/3 and 1/2
smallest possible:
    if n = 2^k - 1, the tree is complete and all nodes can be black
    if 2^k - 1 < n < 2^(k+1) - 1, red nodes have to be inserted so property 5 holds
    if exactly half of the bottom level is filled and they are all decendents of the same subtree of the root
    by color the corresponding child of root to red, property 5 is maintained, red ratio is 1/n
    starting from a tree where all but the bottom nodes are black
    then for any black node one level above the bottom level which has two red children:
        color the children black and the parent red still maintains property 5
    thus the necessary number of red nodes is no more than lg(number of nodes at bottom level), which is <= lg(n/2)
    smallest possible red ratio is somewhere between 0 and (lgn - 1)/n

13.2-1
./CLRS/collection/redblack-tree.ts#rightRotate

13.2-2
there is a rotation for each edge between two internal nodes
for a tree with n nodes, there are exactly n-1 edges between internal nodes, thus n-1 possible rotations

13.2-3
depth of a increases by 1
depth of b won't change
depth of c decreases by 1

13.2-4
if the tree is not a right-going chain, some node on the right-most branch of the tree has a left child
then by right-rotating the node, its left child is now on the right-most branch
the right-most branch contains exactly one more node
so after at most n-1 right rotations, every tree can be transformed into a right-going chain
a right-going chain can be transformed into any tree by reversing the right rotations by left rotations
then every tree can be transformed into any other tree within 2n-2 rotations

13.2-5
let T1 be a right-going chain
no right rotation can be performed within T1, as no node in T1 has a left child
therefore T1 cannot be right-converted to any T2 that's not a right-going chain
thanks http://sites.math.rutgers.edu/~ajl213/CLRS/Ch13.pdf
consider nodes x, y and a, b, and c in subtrees α, β and γ, after a right rotation
the path from root to the node contains certain numbers of left-going edges, denote it by l(x)
    for a and x, l(a) and l(x) is decreased by 1
    for b, l(b) won't change (the order of one left-going and one right-going edge exchanged)
    for y and c, l(y) and l(c) won't change (the number of right-going edges increased by 1)
thus Σl(x) for all x in the tree is decreased by at least 1 after a right rotation
when Σl(x) = 0, the tree is a right-going chain, cannot be right-converted to any other trees
as l(x) can be at most n-1 for any node in a tree containing n keys
Σl(x) = O(n^2), any tree will become a right-going chain after at most O(n^2) right rotation
therefore if T1 can be right-converted to T2, it must happen within O(n^2) right rotaions

13.3-1
property 5 is violated then

13.3-2
┌─41b
38b
│ ┌─31b
└─19r
  └─12b
    └─8r

13.3-3
13.5:
    α:  the color of A and C swapped, bh still k
    β:  still only one black node among A, B and C, bh still k
    γ:  still only one black node among A, B and C, bh still k
    δ:  the color of C and D swapped, bh still k
    ε:  the color of C and D swapped, bh still k
13.6:
    as both A and B are red, the left rotation will not change bh of any node
    considering only the transformation from case 3 to a proper red black tree
    α:  C replaced by B, both black, bh still k
    β:  same to α
    γ:  before Cb -> Br -> γ, now Bb -> Cr -> γ, bh still k
    δ:  path from root to δ now contains one more red node, bh still k

13.3-4
by loop invariation, z is red at the start of each iteration, so z cannot be T.nil
then immediately the loop tests whether z.p.color == RED, terminates if z.p.color != RED
by loop invariation, if z and z.p both red, T.root is black, which means neither z or z.p is the root
so z, z.p, z.p.p all exists, none of them is T.nil
the uncle of z, z.p.p.right, is only modified when its color is red, so it cannot be T.nil too
as these are the only nodes ever get modified in an iteration of RB-INSERT-FIXUP, T.nil will not be colored red

13.3-5
the node will first be inserted as a red node
during RB-INSERT-FIXUP, the node z is always a red node
z may only be colored black at the end of RB-INSERT-FIXUP if z is the new root
which only happens when the tree is previously empty, n = 1
so for n > 1, z will point to a red node when RB-INSERT-FIXUP terminates, the tree contains at least one red node

13.3-6
during the initial insertion, where the new node z is placed somewhere at the bottom level
the path traversed are all ancestors of the node z
by pushing all the ancestors into a stack, z.p can be poped from the stack in time O(1)

13.4-1
root can only be red if y was the root and its red child x becomes the new root
then in RB-DELETE-FIXUP, the loop immediately terminates, and x is set to black
so the root after deletion is guarenteed to be black

13.4-2
if both x and x.p are red, the extra black can be fixed by setting x to black
then both property 1 and property 4 are restored

13.4-3
┌─41b
38b
│ ┌─31b
└─19r
  └─12b
    └─8r
Deleting 8
┌─41b
38b
│ ┌─31b
└─19r
  └─12b
Deleting 12
┌─41b
38b
│ ┌─31r
└─19b
Deleting 19
┌─41b
38b
└─31b
Deleting 31
┌─41r
38b
Deleting 38
41b
Deleting 41
Empty tree

13.4-4
anywhere examines or modifies x, as x may be T.nil

13.4-5
case 1:
    α:  A + B = 2 + 1 = 3 => A + B + D = 2 + 0 + 1 = 3
    β:  same to α
    γ:  B + D + C = 1 + 0 + 1 = 2 => D + B + C = 1 + 0 + 1 = 2
    δ:  same to γ
    ε:  B + D + E = 1 + 0 + 1 = 2 => D + E = 1 + 1 = 2
    ζ:  same to ε
case 2:
    α:  B + A = count(c) + 2 => B + A = count(c) + 1 + 1 = count(c) + 2
    β:  same to α
    γ:  B + D + C = count(c) + 1 + 1 = count(c) + 2 => B + D + C = count(c) + 1 + 0 + 1 = count(c) + 2
    δ:  same to γ
    ε:  B + D + E = count(c) + 1 + 1 = count(c) + 2 => B + D + E = count(c) + 1 + 0 + 1 = count(c) + 2
    ζ:  same to ε
case 3:
    α:  not modified
    β:  same to α
    γ:  B + D + C = count(c) + 1 + 0 = count(c) + 1 => B + C = count(c) + 1
    δ:  B + D + C = count(c) + 1 + 0 = count(c) + 1 => B + C + D = count(c) + 1 + 0 = count(c) + 1
    ε:  B + D + E = count(c) + 1 + 1 = count(c) + 2 => B + C + D + E = count(c) + 1 + 0 + 1 = count(c) + 2
    ζ:  same to ε
case 4:
    α:  B + A = count(c) + 2 => D + B + A = count(c) + 1 + 1 = count(c) + 2
    β:  same to α
    γ:  B + D + C = count(c) + 1 + count(c') => D + B + C = count(c) + 1 + count(c')
    δ:  same to γ
    ε:  B + D + E = count(c) + 1 + 0 = count(c) + 1 => D + E = count(c) + 1
    ζ:  same to ε

13.4-6
before calling RB-DELETE-FIXUP, the tree up to x is correct in property 4
as the sibling of x is red, x.p must be black

13.4-7
consider case (a) in figure 13.5
after insertion of node B, A and D is colored black, then C the root is colored red
RB-INSERT-FIXUP then fixes the color of C to black
deletion of node B will be simple, without calling RB-DELETE-FIXUP
but now the color of A and D is modified

13-1
a.  insertion:
        the new node will be inserted at the bottom level of the tree
        its parent must be copied or the new node may not be accessible from the root
        similarly its grandparent must be copied or the parent of the new node may not be accessible
        all O(h) ancestors of the new node must be copied
    deletion:
        if the deleted node y has no or one child, only its parent have to be copied and modified
        its parent now has NIL or y's sole child as a new child
        no modification to the subtree rooted at y's sole child is necessary
        if y has two children, then the successor of y is moved to its place
        every node on the path then has to be modified and copied, O(h) nodes at most
b.  ./CLRS/collection/persistent-tree.ts#PTree
c.  shallow copying the current node is O(1)
    if z.key <= copy.key and copy.left == null, the loop recursion terminates, similarly when z.key > copy.key
    otherwise the procedure recursively calls itself with copy.left or copy.right, which has depth one more than copy
    time complexity is T(h) = T(h-1) + O(1) = O(h)
    each recursive call shallow copies the current node, which consistents of a key and two pointers, takes O(1) space
    space complexity is T(h) = T(h-1) + O(1) = O(h) 
d.  PERSISTENT-TREE-INSERT must modify and copy the root to make a new tree
    then the left and right child of the root must be modified too
    otherwise their parent pointer points to the old root, the tree is no longer persistent
    so the children of the root also have to be copied
    recursively all nodes in the tree have to be copied, the procedure will take O(n) time and space
e.  RB-INSERT-FIXUP and RB-DELETE-FIXUP only ever modifies nodes as far as children of siblings
    when copying all the ancestors, also copy all the siblings and chlidren of siblings
    at most 4 times more nodes are copied, time and space complexity still O(h)
    then apply reb black fixup to the tree after insertion and deleteion, the height of the tree is O(lgn)

13-2
a.  as insertion and deletion in reb black tree maintains property 5
    the black height after insertion or deletion can be determined by traversing s simple path from root to any leaf
    length of that path will be O(lgn) as the property of red black tree
    so the complexity of insertion or deletion still O(lgn)
    by keeping a counter of black nodes, starting from T.bh, decrements every time a black node is visited
    T(n) = T(l) + T(r) + O(1), where l + r + 1 = n
    therefore T(n) = O(n) and T(n)/n = O(1)
b.  as each simple path from root to leaf contains the same number of black nodes
    also descending one level in the tree at most decreases the black height by 1
    thereby each simple path from root to leaf contains node with black height k for each k in {1 .. T.bh}
    starting from x = T.root and always go from x to x.right
    the last node has black height T2.bh is the maximum among all such nodes
c.  as node in Ty <= x <= node in T2, make a new tree that:
        x is the root
        left subtree is Ty
        right subtree is T2
    then as binary search tree property holds within Ty and T2, it also holds in the new tree
    y is chosen as the maximum node among all nodes in T1 with bh = T2.bh, each node in Ty is greater than y.p
    therefore each node in the new tree is greater than y.p, the new tree is in the right position within T1
d.  Ty has black height T2.bh, same to T2
    T2.root is black, the color of y uncertain
    x must be red or any simple path through x contains one more black node
    but then
        x may be the root after join
        x and T2.root may both be red
    if x is the new root, coloring it black will fix both proerty 2 and 4
    if x is not root and both T2.root and x are red
    the tree now satisfies the invariants in RB-INSERT-FIXUP with z = T2.root, or z = x.right
    so calling RB-INSERT-FIXUP(x.right) will fix the red black tree properties
e.  then T1 is inserted to the maximum node in T2 which has bh = T1.bh
f.  finding y: O(lgn)
    constructing the new subtree: O(1)
    RB-INSERT-FIXUP: O(lgn)
    overall runtime O(lgn)

13-3
a.  let L(h) denote the least number of nodes a height-h AVL tree may contain
        L(0) = 1
    x.h = 1 + max(x.left.h, x.right.h), so the height of subtrees are at least x.h - 1 and x.h - 2
        L(h)    = L(h-1) + L(h-2) + 1
                > L(h-1) + L(h-2)
                = Fh
    as Fibonacci number grows exponentially, h = O(lgn)
b.  when |x.right.h - x.left.h| <= 1, the subtree is balanced
    when x.right.h = x.left.h + 2, both subtree are balanced but the height of right subtree is 2 higher than left
    denote:
        h = x.h, the current height of x
        y = x.right
        α = left subtree of x
        β = left subtree of y
        γ = right subtree of y
    then
        h(x) = h 
        h(y) = h - 1
        h(α) = h - 3
        h - 3 <= h(β), h(γ) <= h - 2
    after a left rotation at x,
        h(x.left) = h(α) = h - 3
        h - 3 <= h(x.right) = h(β) <= h - 2
    x is balanced
        h - 2 <= h(y.left) = 1 + max(h(α), h(β)) <= h - 1
        h - 3 <= h(y.right) = h(γ) <= h - 2
    y may not be balanced if h(γ) = h - 3 and h(β) = h - 2
    a right rotation at y can prevent that situation, but also possibly introduces unbalance to y
    // stuck
    thanks http://courses.csail.mit.edu/6.046/spring04/handouts/ps5-sol.pdf
    the possible unbalance introduced by right rotation at y will be eliminated by a left rotation
    denote
        h = x.h
        z = y.left
        α = left subtree of x
        β = left subtree of z
        γ = right subtree of z
        δ = right subtree of y
    then
        h(α) = h - 3
        h(z) = h - 2
        h(δ) = h - 3
        h - 4 <= h(β), h(γ) <= h - 3
    after right rotation at y,
        h - 4 <= h(y.left) = h(γ) <= h - 3
        h(y.right) = h(δ) = h - 3
    y is balanced, and h(y) = h - 2
    after left rotation at z,
        h(x.left) = h(α) = h - 3
        h - 4 <= h(x.right) = h(β) <= h - 3
    x is balanced, and h(x) = h - 2
    z now is balanced with h(z) = h - 1
    ./CLRS/collection/avl-tree.ts#balance
c.  ./CLRS/collection/avl-tree.ts#insert
d.  depends on whether it's necessary to fix the height of all the ancestors after calling BALANCE
    BALANCE(x) decreases height of the subtree rooted at x by 1
    this decrement may be propagated all the way to the top, change the height of all the ancestors of x
    but during insertion, this propagation can be compensated by fix the height of roots recursively
    so BALANCE takes O(1), INSERT takes O(h) = O(lgn)
    as BALANCE(x) decrements the height of subtree rooted at x by 1
    once BALANCE(x) is called for some node x, none of the ancestors of x will be unbalanced
    (they are balanced before the insertion
    and the possible height increase caused by insertion solved at x)
    so BALANCE(x) may only be fully executed once and O(1) rotations are performed

13-4
a.  given distinct priorities, all permutations of x1 .. xn has a same sorted order 
    by the property of treap, the resulting treap is the tree formed if x1 .. xn is inserted in sorted order
    therefore for a set of elements x1 .. xn with fixed keys and priorities, the resulting treap is the same
    if the keys are different, the resulting tree or treap is certainly different
    if the priorities are different up to ordering, the resulting treap may not be different
    both insertion sequence [2, 1, 3] and [2, 3, 1] results in the same tree
b.  as elements are given random priorities
    sorted form of x1 .. xn can be any of n! permutations of x1 .. xn with equal probability
    when n = 1:
        trivial
    assume the sorted form of x1 .. xn can be any of n! permutations of x1 .. xn with probability 1/n!:
        next element xn+1, given random priority, may appear anywhere in the sorted form with equal probability
        there are n+1 places xn+1 may end up at in any permutations, n! * (n+1) = (n+1)! new permutations in total
        each with probability 1/(n! * (n+1)) = 1/(n+1)!
    as the sorted form of x1 .. xn is a uniformly random permutation of x1 .. xn
    theoriem 12.4 applies, the expected height of the treap is O(lgn)
c.  first the normal insertion of z is performed according to binary search tree property
    but then the priority of z.p may be greater than z, and this is the only possible violation of treap properties
    which means the other child of z.p and its descendants have priority greater than z.p
    the descendants of z (if any) has priority greater than z
    if z == z.p.let, perform RIGHT-ROTATE(z.p), otherwise LEFT-ROTATE(z.p)
    then z becomes the new parent of z.p, the treap property violation between z and z.p is fixed
    the possible violation is now between z and its new parent, one level higher in the treap
    ./CLRS/collection/treap.ts#insert
d.  the initial insertion takes O(h)
    each iteration of INSERT-FIXUP moves z one level up the treap, O(h) iterations at most
    each iteration performs constant number of rotations, which takes O(1)
    O(h) + O(h) = O(h) in total
    as the expected height of a treap is O(lgn), insertion takes time O(lgn) on average
    if insertion takes time o(lgn), n keys can be sorted in time o(nlgn), thereby insertion is Ω(lgn) and Θ(lgn)
e.  x is initially a leaf to be inserted into the treap, C + D = 0
    after a right rotation at x.p when x = x.p.right
        x.p is added to the left spine of x's right subtree
        left subtree of x and its right spine unchanged
        C + D incremented by 1
    similary, a right rotation at x.p when x = x.p.left increments C + D by 1
    so C + D equals to the number of rotations performed
f.  =>: as y is in the left subtree of x, x.priority < y.priority, x.key > y.key
        as y is in the right spine of the subtree
        each z that y.key < z.key < x.key is in right subtree of y
        otherwise it will be traversed inorder before y or after x
        therefore x.priority > y.priority
    <=: as y.priority > x.priority and y.key < x.key,
        either y is in the left subtree of x or y is in the left subtree of some left ancestor of x
        then this left ancestor satisfies y.key < z.key < x.key, y.priority < z.priority
        y cannot be in the left subtree of z, so y must be in the left subtree of x
        if y is not in the right spine of the left subtree of x
        there must be some ancestor z of y that y.key < z.key < x.key
        which implies y.priority < z.priority, in contradiction to z being y's ancestor
        therefore y must be in the right spine of the left subtree of x
g.  the problem implicitly assumes k > i or (k - i - 1)! is not well defined
    as priority is randomly chosen, independent from the key
    considering only the order of these priorities, mapping from set of random numbers to {1 .. n}
    for each z that y.key < z.key < x.key, y.priority_order < z.priority_order
    there are k-i-1 such z, k-i+1 with x and y
    there are (k-i+1)! possible permutations of their priority orders
    only those in which x.priority is the minimum and y.priority is the second smallest is valid
    so only (k-i-1)! permutations satisfies the conditions in part f
    Pr{Xik = 1} = (k-i-1)! / (k-i+1)! = 1/(k-i+1)(k-i)
h.  for x and k = x.key,
        E[C]    = E[Σ(i = {1 .. k-1})Xik]
                = Σ(i = {1 .. k-1})E[Xik]
                = Σ(i = {1 .. k-1})(1 / (k-i+1)(k-i))
                = Σ(i = {1 .. k-1})(1 / i(i+1))
                = Σ(i = {1 .. k-1})(1/i - 1/(i+1))
                = 1 - 1/k
i.  for y != x, k = x.key, i = y.key, define
        Yik = I{y is in the left spine of the right subtree of x}
    then Yik = 1 iff y.priority > x.priority, y.key > x.key and 
    for every z such that x.key < z.key < y.key, y.priority > z.priority
        Yik = 0 if i <= k, otherwise
        Pr{Yik = 1} = 1/(k-i+1)(k-i)
        E[D]    = E[Σ(i = {k+1 .. n})E[Yik]]
                = 1 - 1/(n - k + 1)
j.  E[C + D]    = E[C] + E[D]
                = 2 - O(1) <= 2

Chapter 14
14.1-1
26, 12 + 1 = 13 > 10, go left
17, 7 + 1 = 8 < 10, go right, 10 - 8 = 2
21, 2 + 1 = 3 > 2, go left
19, 0 + 1 = 1 < 2, go right, 2 - 1 = 1
20, 0 + 1 = 1, selected node 20

14.1-2
35, r = 1, not root, go up
38, not root, go up
30, 1 + 1 = 2, r = 1 + 2 = 3, not root, go up
41, not root, go up
26, 12 + 1 = 13, r = 3 + 13 = 16, root, terminate

14.1-3
./CLRS/collection/augmented-redblack-tree.ts#select

14.1-4
traverse the tree from root to find where a node with key k is or will be inserted into the tree
while counting the number of elements smaller than k in sibling subtree and up during the course
if followed a right edge, add x.left.size + 1 to the counter
terminate when x = T.nil
./CLRS/collection/augmented-redblack-tree.ts#keyRank

14.1-5
find the order k of node x, find the node with order k+i
overall running time O(lgn) + O(lgn) = O(lgn)

14.1-6
insertion:
    if z is inserted into the left subtree of x, x.rank += 1
    otherwise x.rank won't change, x.rank is the local rank in the subtree rooted at x, not global rank
deletion:
    traverse from the deleted node p = y to the root
    if followed a left edge, decrement p.rank by 1
rotation:
    in left rotation, x.rank is unchanged
    y.rank changes from size of β to the size of α, β and x combined
    which is x.rank + y.rank + 1
    right rotation is symmetric

14.1-7
for an array [x1 .. xn], assume x1 .. xn-1 is already inserted into the OS tree
after insertion of xn, the rank of xn is 1 + the number of elements smaller than xn in x1 .. xn-1
each element in x1 .. xn-1 that is greater than xn causes an inversion
therefore the total number of inversions can be counted by inserting x1 .. xn that:
    after insertion of xi, get the current rank ri of xi, add (i - ri - 1) to the counter
then after insertion of xn, the counter is the number of inversions in the array
each iteration performs one insertion, one search and one OS-RANK, all of them O(lgn), O(lgn) in total
overall running time O(nlgn)

14.1-8
each chord has two end points, give them keys as their angular coordinate in polar system
for a chord c let c.x be the smaller between the two end points, c.y be the greater
two chords c and d, assuming d.y > c.y, can be one of three cases:
    1.  d.x > c.y > c.x, no intersection, both c.y and c.x smaller than d.x
    2.  c.y > d.x > c.x, intersection, c.x smaller than d.x but not c.y
    3.  c.y > c.x > d.x, no intersection, both c.x and c.y greater than d.x
thereby if two OS trees for both .x and .y are maintained
the number of intersections will be the difference between number of keys smaller than d.x in the two trees
sort the chords according to c.y, for each chord c and two OS trees T1 and T2:
    insert c.x into T1, get rx the rank of c.x in T1
    use OS-KEY-RANK(T2, c.x) from problem 14.1-4 to find ry the rank of c.x in T2
    insert c.y into T2
    add rx - ry to a global counter cnt
at the end the global counter cnt holds the number of intersections
initial sorting O(nlgn)
constant number of insertion, rank operation and search are performed each iteration, O(lgn) in total
overall running time O(nlgn)

14.2-1
minimum in subtree rooted at x can be calculated as the minimum of left subtree or x itself
maximum in subtree rooted at x can be calculated as the maximum of right subtree or x itself
both easily maintained as described in the text
insertion or deletion at most changes the predecessor and successor of two nodes
during first stage of insertion of a node z:
    if z end up as the left child of a node p
        set p.predecessor.successor = z (if p.predecessor exists)
        set z.predecessor = p.predecessor (same condition as above)
        set z.successor = p
        set p.predecessor = z
    if z end up as the right child of a node p
        set p.successor.predecessor = z (same condition)
        set z.successor = p.successor
        set z.predecessor = p;
        set p.successor = z;
during first stage of deletion, assume a node z is removed:
    z.predecessor.successor = z.successor
    z.successor.predecessor = z.predecessor
rotation will not change the nodes stored in a tree, thereby will not change predecessor or successor of any node
all operations take constant time, fixing predecessors and successors will not propagate to ancestors

14.2-2
since second stage of insertion and deleteion changes color of some nodes
it cannot be simply maintained by method described in the text
but the black height still can be maintained without changing the asymptotic performance of set operations
the first stage of insertion will not change black height of any node, as the new node z is colored red
in case 1 of RB-INESRT-FIXUP:
    bh of A and D incremented by 1
in case 2 of RB-INSERT-FIXUP:
    no bh change
in case 3 of RB-INSERT-FIXUP:
    bh of C decremented by 1
    bh of B incremented by 1
the first stage will not change bh of any node if x has an extra black
in case 1 of RB-DELETE-FIXUP:
    decrement bh(B)
in case 2 of RB-DELETE-FIXUP:
    decrement bh(A)
    decrement bh(D)
in case 3 of RB-DELETE-FIXUP:
    increment bh(C)
    decrement bh(D)
in case 4 of RB-DELETE-FiXUP:
    decrement bh(A)
    increment bh(E)
when x = T.root, the extra black is removed, bh(T.root) stays intact

14.2-3
the inorder listing of subtree rooted at x is
    xl1.a * .. * xml.a * x.a * x xr1.a  * .. * xrm.a
where 
    xl1.a .. xml.a is the inorder listing of its left subtree
    xr1.a .. xrm.a is the inorder listing of its right subtree
as the operation is associative, x.left.f * x.a * x.right.f = x.f
set x.a = 1 for all node x, * = +, this scheme calculates the size of the subtree

14.2-4
use 14.2-1, add pointers to successor and predecessor to nodes, set operations still O(lgn)
search for the initial key a is O(lgn)
traversing m successors between b and a is O(m)
O(m + lgn) in total

14.3-1
abstract over all augmented red black trees
./CLRS/collection/augmented-redblack-tree.ts#leftRotate

14.3-2
the meaning of overlapping changes a little bit, from
    !(a.high < b.low || b.high < a.low)
to
    !(a.high <= b.low || b.high <= a.low)
then change line 3 of INTERVAL-SEARCH to
    if x.left !== T.nil and x.left.max > i.low
theorem 14.2 still holds since:
    if line 5 is executed, x.left.max <= i.low    
    by definition of overlapping of open intervals, no intervals in left subtree of x will overlap with i
    if line 4 is executed, x.left.max > i.low
    if no interval in left subtree overlaps with i, there must be some i' that
        i'.high = x.left.max > i.low
        i'.low >= i.high
    then for every interval i'' in right subtree of x,
        i''.low >= i'.low >= i.high
    so no intervals in right subtree of x overlaps with i

14.3-3
basically INTERVAL-SEARCH, but not terminate on finding an overlapping intervals
but keeps track of the interval that overlaps with i which has minimal low end point
only terminates when x == T.nil
by theorem 14.2, either 
    1.  there are no overlapping intervals in left subtree (line 5), or
    2.  there are overlapping intervals in left subtree and possibly right subtree too (line 4)
in case 2, to find the overlapping interval with minimal low end point, x should be x.left in next iteration
./CLRS/collection/interval-tree.ts#minimalOverlappingInterval

14.3-4
thanks http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/tut2.pdf
do INTERVAL-SEARCH(i), delete the node returned, do next INTERVAL-SEARCH(i)
re-insert all the nodes back to the tree when INTERVAL-SEARCH returns null
do this at most n/lgn times, then re-insert removed nodes and traverse all the nodes instead
running time = O(klgn) when k < n/lgn, O(n) when k >= n/lgn, O(min(n, klgn)) overall

14.3-5
when comparing two intervals, compare their high end points if their low end points equal
by searching in this new ordering, the exact key (interval) can be find in time O(lgn)
namely
    le(a: Interval, b: Interval): boolean {
        if (a.low === b.low) {
            return a.high <= b.high;
        } else {
            return a.low <= b.low;
        }
    }
    eq(a: Interval, b: Interval): boolean {
        return a.low === b.low && a.high === b.high;
    }
the normal search operation then works like the definition of INTERVAL-SEARCH-EXACTLY

14.3-6
store:
    minimum in the subtree rooted at x
    maximum in the subtree rooted at x
    min gap in the subtree rooted at x
in the node x alongside the key
by search tree property, left subtree of x contains number smaller than x, right subtree of x contains number greater
the min gap is one of:
    the min gap of the left subtree
    the min gap of the right subtree
    x.key - x.left.maximum 
    x.right.minimum - x.key
all the three additional fields can be calculated from information in current node, left and right children in O(1)

14.3-7
assume a rectangle is a structure like
    interface Rect {
        vert: Interval;
        horiz: Interval;
    }
    interface Interval {
        low: number;
        high: number;
    }
define a new structure RectNode that 
    interface RectNode {
        rect: Rect;
        node: IntNode;
    }
where IntNode is node in an interval tree
make two copies of points to all RectNodes rn, name them L and H
sort L according to rn.rect.horiz.low, sort H according to rn.horiz.vert.high
create an initially empty interval tree T that stores vertical intervals of rectangles
until both L and H is empty:
    compare the top of L and H
    if rn the top (minimum) of L is smaller, pop it from the set L
        search intervals overlaps rn.rect.vert in T, if exists terminate and return true
        insert rn.rect.vert into T, assign the inserted node to rn.node
    if rn the top of H is smaller of L is empty, pop it from the set H
        as rn.rect.horiz.low <= rn.rect.horiz.high, rn.node always points to an existing node
        delete rn.node from T
any time during the procedure, T contains vertical intervals of rectangles that overlap with a certain horizontal line
as they already overlaps horizontally, if they overlaps vertically, the two rectangles overlaps

14-1
a.  assume the point of maximum overlap overlaps with a set of intervals {ik}
    let p = max{ik.low}, it must have:
        p <= ik.high for all k
    otherwise the interval with low end point p will not overlap with ik, hence do not contain a common point
    then p overlaps with all intervals in {ik}, p is a point of maximum overlap
b.  if an interval i does not overlap with a point p, then
        i.low > p or i.high < p
    assuming distinct end points, additionally store:
        size_low, number of low end points in subtree
        size_high, number of high end points in subtree
        pom, point of maximum overlap in subtree
        non_overlap, number of intervals that may not overlap with point of maximum in subtree
    calculating size_low and size_high from children and current node is strightforward
    for a node x, compare x.left.size_high + x.right.non_overlap and x.right.size_low + x.left.non_overlap
    pom of subtree rooted at x may be one of:
        x.left.pom, not overlapping with x.left.non_overlap + x.right.low_size intervals
        x.right.pom, not overlapping with x.right.non_overlap + x.left.high_size intervals
        x.key, not overlapping with x.left.high_size + x.right.low_size intervals
    update x.pom and x.non_overlap accordingly, T.root then contains pom of the whole tree

14-2
a.  put all numbers in a circular doubly linked list in sorted order
    starting from x.key = 1, repeat the procedure below until no node remains in the linked list:
        1.  follow x = x.next m times
        2.  delete node pointed by x.prev and yield its key
    since m is constant, step 1 takes O(1)
    step 2 also O(1) as deletion of a node in doubly linked list is O(1)
    the procedure runs for at most n iterations until the linked list is empty, hence O(n) total running time
b.  rank starting from 0 for convinence here
    if the previously removed node has rank i, there are k numbers not removed yet
    node with rank i + m - 1 mod k should be removed next
    [1, 2, 3, 4, 5, 6, 7]
    remove 3 with rank 2
    [1, 2, 4, 5, 6, 7]
    2 + 3 - 1 mod 6 = 4
    remove 6 with rank 4
    [1, 2, 4, 5, 7]
    4 + 3 - 1 mod 5 = 1
    remove 2 with rank 1
    [1, 4, 5, 7]
    1 + 3 - 1 mod 4 = 3
    remove 7 with rank 3
    [1, 4, 5]
    3 + 3 - 1 mod 3 = 2
    remove 5 with rank 2
    [1, 4]
    2 + 3 - 1 mod 2 = 0
    remove 1 with rank 0
    [4]
    0 + 3 - 1 mod 1 = 0
    remove 4 with rank 0
    using an order statistic tree, deletion and searching by rank both O(lgn)
    O(nlgn) in total
    ./CLRS/collection/augmented-rebblack-tree.ts#josephus

Chapter 15
15.1-1
T(0) = 1
assume T(k) = 2^k for all k <= n
T(n+1)  = 1 + Σ(j = {0 .. n})T(j)
        = 1 + Σ(j = {0 .. n})2^j
        = 1 + 2^(n+1) - 1
        = 2^(n+1)

15.1-2
let price table be [0, 0, 4, 5] for rod length 0 ~ 3
let n = 3
greedy method will first cut a piece of length 2, which has the highest price density 4/2 = 2
but then the remaining rod with length 1 is valueless, total price 4
the optimal solution will not cut the rod, total price 5

15.1-3
./CLRS/technique/rod.ts#cutRodCC

15.1-4
./CLRS/technique/rod.ts#extendedMemoizedCutRod

15.1-5
problem n depends on problem n-1 and problem n-2
./CLRS/technique/rod.ts#fibonacci

15.2-1
2010 '((A0A1)((A2A3)(A4A5)))'

15.2-2
./CLRS/technique/matrix-chain.ts#matrixChainMultiply

15.2-3
assume P(k) >= 2^n for k < n
    P(n)    >= Σ(k = {1 .. n-1})c2^(n-k)
            = Σ(k = {1 .. n-1})2^n
            = (n-1)2^n
            >= 2^n
therefore P(n) = Ω(2^n)

15.2-4
the subproblem (i, j) refers solutions to subproblems (i, k) and (k+1, j) for all i <= k < j
there are j - i such k and vertex (i, j) has degree 2(j-i)
vertices on the same diagonal has the same j - i, which ranges from 0 to n-1
each diagonal contains n - (j - i) vertices
    Σ(k = {0 .. n-1})2k(n-k)
    = 2nΣ(k = {0 .. n-1})k - 2Σ(k = {0 .. n-1})k^2
    = 2n * n(n-1) / 2 - 2(n-1)n(2n-1) / 6
    = n^2(n-1) - n(n-1)(2n-1)/3
    = n(n-1)(3n - 2n + 1)/3
    = n(n-1)(n+1)/3
    = n(n^2 - 1)/3
    = (n^3 - n)/3
edges in total
C(n, 2) + n = n(n-1) + n = n^2 vertices in total

15.2-5
equals to the number of edges in the subproblem graph

15.2-6
the number of pairs of parentheses as a function of the number of matrices is
    P(1) = 0
    P(n) = P(k) + P(n-k) + 1
P(n) = n - 1 easily proved by induction

15.3-1
RECURSIVE-MATRIX-CHAIN enumerates fewer results than a complete enumeration
instead all subproblem of Ai..k and Ak+1..j, only the optimal subproblem is returned
but a complete enumeration performs one comparsion for each possible parenthesization
while RECURSIVE-MATRIX-CHAIN performs many comparsion for each subproblem
enumerating all possibilities is at least Ω(4^n / n^(3/2)), the Catalan numbers
redo 15.8 with different setting
line 1-2, line 5 and line 6-7 takes at most constant time, so
    T(1)    <= c
    T(n)    <= c + Σ(k = {1 .. n-1})(T(k) + T(n-k) + c)
            = c + c(n-1) + 2Σ(k = {1 .. n-1})T(k)
            = cn + 2Σ(k = {1 .. n-1})T(k)
thanks Instructor's Manual
assume T(n) <= cn3^(n-1)
    T(n)    <= cn + 2Σ(k = {1 .. n-1})(ck3^(k-1))
            = c(n + 2Σ(ck3^(k-1)))
            = c(2(n3^(n-1) / (3 - 1) + (1 - 3^n) / (3 - 1)^n) + n)
            = cn3^(n-1) + c(2n + 1 - 3^n)/2
            <= cn3^(n-1)
T(n) = O(n3^(n-1)), RECURSIVE-MATRIX-CHAIN is more efficient than complete enumeration

15.3-2
subproblems in merge sort do not overlap
each subproblem is solved only once, hence memoization won't improve the performance

15.3-3
this problem do has optimal substructure
the subproblems are independent and overlaps

15.3-4
when i and j fixed, minimizing pi-1pkpj equivalent to minimizing pk
for p = [30, 20, 10, 1]
    greedy method returns (A0A1)A2, 6300 multiplications
    optimal solution is A0(A1A2), 800 multiplications

15.3-5
the subproblems no longer overlap as much as in 15.1
a subproblem now is described by the length of the rod and the the remaining allowance of length i rods
for example the two subproblem:
    1.  a length 3 rod after cutting length 3 from a length 6 rod
    2.  a length 3 rod after cutting length 1 from a length 4 rod
are no longer the same subproblem

15.3-6
consider currencies as vertices in a graph and rij be directed edges
the problem only makes sence when:
    for any cycle in the graph, the production of rij along the cycle equals to 1
    (especially rij = 1/rji for all i and j)
otherwise looping in the cycle will result in infinite profit 
thereby for each optimal solution i ~> j, there must be a equally optimal solution i ~> j that is a simple path
the problem space then is bounded and has size O(n^2)
problem can be solved by 
    for each pair (i, j)
    finding the optimal exchange rate with at most k trades for k ranges from 0 to n-1
for at most k+1 trades, search each vertex v that
    has an optimal exchange rate riv[k] with at most k trades from i to v 
    there is an edge rvj from v to j
then the maximum of riv[k] * rvj is the solution for at most k+1 trades from i to j
only one subproblem each iteration, trivially independent
subproblems overlap a lot with each other as optimal solutions for all i and j is computed
when ck != 0, the argument above no longer holds
optimal solution of size k+1 does not necessary come from optimal solutions of size k
when the gap between ck and ck+1 is huge
for k+1 trades it may need to choose a less optimal solution of size k which makes fewer trades
instead of an optimal k-trade solution that uses exactly k trades

15.4-1
100110

15.4-2
./CLRS/technique/substring.ts#constrSubstring

15.4-3
./CLRS/technique/substring.ts#memoizedLcs

15.4-4
only the case with min(m, n) entries and O(1) extra space
./CLRS/technique/substring.ts#linearSpaceLcs

15.4-5
let Y be X in sorted order, run LCS with X and Y
total running time O(nlgn) + O(n^2) = O(n^2)

15.4-6
thanks https://www.csie.ntu.edu.tw/~kmchao/seq06fall/dp.pdf
maintain a list best_end while traversing the elements in input array A
right before examining element A[i]
best_end[k] contains the minimum among last element of increasing subsequences of length k in A[1 .. i-1]
then best_end must be in sorted order:
    if increasing subsequences of length k in A[1 .. j] all end with an element >= p
    then no increasing subsequences of length k+1 can have last element <= p
    since the first k elements in the k+1-element sequence is, a k-element increasing subsequence
    so kth p <= kth element <= k+1th element as the increasing subsequence is in increasing order
then A[i] may update best_end in two ways:
    if A[i] >= any element in best_end, for all best_end[k] currently defined
        appending A[i] to any of the subsequences of length k will not change best_end[k+1], as best_end[k+1] <= A[i]
        for max_k the maximum such k, best_end[max_k + 1] is not defined so far
        which means no increasing subsequence of length max_k + 1 exists in A[1 .. i-1]
        then appending A[i] to any of the max_k-element subsequences will update best_end[max_k + 1] to A[i]
    if A[i] < best_end[k] for some k
        let min_k be the minimal such k that best_end[k] > A[i]
        for all k < min_k, as the same argument above, best_end[k] will not be updated by A[i]
        by appending A[i] to one of the increasing sequence of length min_k - 1
        (possible since best_end[min_k - 1] <= A[i])
        best_end[min_k] can be updated to A[i]
        for sequences of length k >= min_k, A[i] can be appended to no sequences, hence updates no best_end[k]
        as best_end[k] > A[i] and best_end[k] stores the minimal last element among all sequences
as max_k is just the current length of best_end[k], min_k can be find in O(lgn) time by binary search
each iteration above runs in time O(lgn), O(nlgn) in total
as best_end[k] records the minimum among last element of increasing subsequences of length k
the greatest k defined then is the length of the longest increasing subsequence of A
the last element of which is stored in corresponding best_end[k]
maintain another array prev that starts with all entries = null
    when A[i] updates best_end[k], set prev[A[i]] = best_end[k-1]
    (best_end[0] set to some null value)
invariant:
    for i = best_end[k]
    traversing prev[i] -> prev[prev[i]] -> .. before null value yields a k-element decreasing subsequence of A
initialization:
    all entries of prev = null, best_end has length 0, invariant trivially satisfied
maintenance:
    prev[best_end[k-1]] has been set already, as the only way best_end grows is appending to the end
    then prev[best_end[k-1]] points to a k-1 element decreasing subsequence of A
    as A[i] replaces best_end[k] by appending to the increasing sequence indicated by best_end[k-1]
    prepending the decreasing sequence with A[i] yields a decreasing sequence of length k
termination:
    let best_end[k] be the content of the last entry of best_end
    traversing prev[best_end[k]] then yields a length k decreasing subsequence of A
    reverse of this sequence then is the longest increasing subsequence of A
even if the numbers are large, prev can be a efficient dynamic set and operations on it O(lgn)
overall running time still O(nlgn) 

15.5-1
./CLRS/technique/optimal-bst.ts#constructOptimalBST

15.5-2
3.12
  ┌─d7
┌─k7
│ │ ┌─d6
│ └─k6
│   └─d5
k5
│     ┌─d4
│   ┌─k4
│   │ └─d3
│ ┌─k3
│ │ └─d2
└─k2
  │ ┌─d1
  └─k1
    └─d0

15.5-3
computation of w[i, j] takes time j - i + 1 = O(n)
it's computed for at most O(n^2) times, O(n^3) in total
the asymptotic running time will not change

15.5-4
initially root[i][i] = i for all 1 <= i <= n
starting with l = 1, j = i + l for all i
    root[i][i] <= root[i][i+1] <= root[i+1][i+1]
    i <= root[i][i+1] <= i+1
if for l >= 1, root[i][i+l] is in sorted order, root[i][i+l] <= root[i+1][i+l+1] for all i
    root[i][i+l] <= root[i][i+l+1] <= root[i+1][l+l+1]
    root[i+1][i+l+1] <= root[i+1][i+l+2] <= root[i+2][i+l+2]
the next diagonal in the table (as shown in figure 15.10) is also in sorted order
sets of possible candidates for root[i][i+l+1] and root[i+1][i+l+2] only overlaps at end points (at most one)
fix l, the number of candidates for each root[i][i+l] combined is at most n + entries on the last diagonal = O(n)
as l = 1 to n, O(n^2) in total
as least Ω(n^2) entries are filled in three tables, overall running time Θ(n^2)

15-1
topologically sort the vertices
remove all the vertices before s (won't appear in a path s ~> t) and after t (similar) 
if an edge (s, u) exists, solve subproblem u ~> t, add w(s, u) to the solution, take the maxmimum among all such u
a bottom-up solution traverses the sorted list reversely from t to s, solve subproblems of smallest size first
topological sort is O(|V| + |E|), removing nodes O(|V|)
the graph is naturally a subproblem graph, subproblems are referred to at most O(|E|) times
ovarall time complexity O(|V| + |E|)
one subproblem at a time is examined, tivially independent
O(|E|) possible references to O(|V|) subproblems, naturally overlaps


15-2
longest common subsequence of a string and its reverse
O(n^2)

15-3
thanks Instructor's Manual
for a sorted list of points [p1 .. pn]
define a bitonic path Pij as a path that:
    visits all points in {p1 .. pj}
    starts from pi and go strictly left to p1
    then go srtictly right from p1 to pj
let |pipj| be the distance between pi and pj
let b[i, j] be the length of the shortest among all bitonic path Pij
for i < j - 1:
    pj-1 must be on the right going subpath of Pij, precedes pj
    if Pij is a shortest bitonic path, then its subpath Pij-1 must also be a shortest bitonic path
    otherwise by cut and paste an optimal Pij-1, b[i, j] can be reduced
    b[i, j] = b[i, j-1] + |pj-1pj|
for i = j - 1:
    pi is the first point on the left going subpath of Pij
    the point precedes pj on the right subpath can be pk for any 1 <= k < j - 1
    again Pkj-1 must be the optimal bitonic path or cut and paste can reduce b[i, j]
    b[j-1, j] = min{b[k, j-1] + |pkpj|} over all k
for i = j:
    the bitonic path starts and ends with pi
    its subpath Pj-1j, for the same reason above, must be a optimal bitonic path
    b[j, j] = b[j-1, j] + |pj-1, pj|
at the end b[n, n] is the length of the optimal bitonic path pn ~> p1 ~> pn, which is also a bitonic tour
by recording each minimal choice of k during the computation, the bitonic tour can be reconstructed
r[i][j] stores k that pk precedes pj on an optimal bitonic path i ~> 1 ~> j, where i < j
if i > j, the optimal bitonic path is Pji, r[j][i] stores k that precedes pi 
as initially i = n-1, j = n, if i > j then pk is on the left going subpath of Pn-1n, should be printed later
./CLRS/technique/bitonic-tour.ts

15-4
denote the length of ith word by li for 1 <= i <= n
define ws[i, j] as the number of white spaces at the end of of line with word i to j
    ws[i, j]    = M - (Σ(k = {i .. j})li + j - i)
                = ws[i, j-1] - (lj + 1)
so ws[i, j] can be computed in time O(n^2)
if words from i to j do not fit in a line, ws[i, j] will be negative
define the whitespace cost wc[i, j] for a line containing words i to j as:
    wc[i, j]    = +∞ if ws[i, j] < 0, words i to j will not fit in a line
                = 0 if ws[i, j] >= 0 and j = n, this is the last line, contributes nothing to the cost
                = ws[i, j]^3 otherwise
define the total cost cost[j] as the optimal cost of words 1 to j
if the last line starts with kth word
then the paragraph without the last line (which contains words k to j) has optimal cost[k-1]
otherwise cut and paste will reduce cost[j]
the optimal solution then is min{cost[k-1] + wc[k, j]} over all k < j
assume each word no longer than M, there is always an finite solution
if wc[k, j] = ∞ the solution will never be chosen
by recording optimal choice of k, the paragraph can be reconstructed
both O(n^2) in time and space
./CLRS/technique/print-neatly.ts

15-5
a.  a recursive solution is to work backwards from z = y
    as the operations never backtracks or decrements i or j
    once i > i* and j > j*, x[1 .. i*] can not be examined, z[1 .. j*] cannot be edited
    whenever the operations increments j, it must have z[j] == y[j]
    hence only a few operations are legal for each step
        only when x[i] == y[j], a copy operation can be performed
        delete operation only possible when i <= x.length
        replace operation may only replace z[j] = y[j]
        insert operation may only insert z[j] = y[j]
        twiddle operation only possible when x[i] == y[j+1] && x[i+1] == y[j]
        kill operation only possible when z = y
    thereby subproblems are defined solely by i and j, it's unnecessary to maintain z in subproblems or anywhere
    by determining the possible last operation and work backwards
        only when x[i-1] = y[j-1], i-1 >= 1, j-1 >= 1, the last operation may be copy
        only when i >= 2, the last operation may be delete
        only when i >= 2 and j >= 2, the last operation may be replace
        only when j >= 2, the last operation may be insert
        only when x[i-1] = y[j-2], x[i-2] = y[j-1] i >= 3, j >= 3, the last operation may be twiddle
        only when j = y.length + 1 and i = x.length + 1, the last operation may be kill
    each operation at least increments one of i or j
    current problem can then be solved by optimal solutions of subproblems
    ./CLRS/technique/edit-distance.ts
b.  a space in y is equivalent to delete operation (j incremented but i not)
    a space in x is equivalent to insert operation (i incremented but j not)
    then the problem can be casted to edit distance problems with operations:
        insert:     cost +2
        delete:     cost +2
        replace:    cost +1
        copy:       cost -1
    the inverse of the cost is then the score of the two sequence

15-6
define two subproblem for each subtree rooted at x for some node x
    1.  maximizes the sum of conviviality with the node x
    2.  maximizes the sum of conviviality without the node x
then for a node x and its children y1 .. yk,
    the solution to problem 1 is the sum of subproblem 2 of its children
    the solution to problem 2 is the sum of the greater between solution to subproblem 1 and 2 of its chilren
whether a guest should join the party can be solely defined top-down by 
    whether the immediate supervisor joins the party
    a comparsion between solutions to subproblem 1 and 2 
complexity proportional to the number of edges, O(n) in total

15-7
a.  bfs doesn't apply here as the path may not be simple
    define subproblem (v, j) as if there is a path labeled <σj, σj+1 .. σk> starting from vertex v
    starting from vertex s, for each edge (s, u) with label σi = σ1:
        solve the subproblem (u, i+1)
        choose randomly from all vertices u which (u, 2) has a solution, record the choice
    total running time O(n|E|) as each subproblem traverses at most |E| edges
b.  store not only a boolean, but also the probability of the subproblem
    record the u which (u, i+1) yields maximal probability instead of a random one
    still O(n|E|) as real number operations take unit time

15-8
a.  let n = 2
    no matter which pixel is chosen on the last row
    the pixel removed can be either the first or the second pixel on each row
    2^m possibilities in total
    for n > 2, each possibility in the case n = 2 still valid, O(2^m) possibilities
b.  define subproblem (i, j) as the minimal disruption seam starting from row i and column j
    when i = m, optimal solution to subproblem (i, j) is simply d(i, j)
    when i < m, the optimal solution is d(i, j) plus the minimum among optimal solutions to
        (i+1, j-1), (i+1, j) and (i+1, j+1)
    if j-1 or j+1 falls within the bound
    the problem then easily solved by a bottom up algorithm starting with i = m and goes upwards
    mn subproblems, each refers to at most 3 subproblems, O(mn) in total

15-9
complexity of this problem only depends on the length of the list of break points, not n the length of string S
add the two end points (1 and n) to the list of break point bp, sort the list
assume bp has length m, define subproblem (i, j) as optimally break the substring S[bp[i], bp[j]] according to bp
the original problem is then (1, m) and for all 1 < k < m:
    the solution to (1, m) is the minimal of solutions to (1, k) + (k, m) over all k plus bp[m] - bp[1] + 1
and all subproblems can be solved similarly
problems (i, i+1) has no break point between, solution is 0
the optimal way to break the string then can be reconstructed from choices of k
total number of solution is O(m^2), each refers m subproblems at most, O(m^3) in total
./CLRS/technique/break-string.ts

15-10
it not clear if the fee is deducted at the end of the 10 year period or deducted each year
if it's deducted each year, it must be be deducted from the money in one or more investment
which does no match quite well with the description "leave the money made in the previous year in the same investments"
so assume the fee is deducted from the profits only at the end of the 10 year period
a.  assume in a span from year i to year j, money is not moved among investments
    assume two investments s and t in the portfolio and a total amount of q dollars invested in them, qs and qt each
    the combined return equals to
        qsΠ(1 + rks) + qtΠ(1 + rkt)
    all rates fixed, it must have
        1.  Π(k = {i .. j})(1 + rks) >= Π(k = {i .. j})(1 + rkt), or
        2.  Π(k = {i .. j})(1 + rks) <= Π(k = {i .. j})(1 + rkt)
    in case 1, invest all the q dollars in investment s will not decrease the profit
    in case 2, invest all the q dollars in investment t will not decrease the profit
    in an optimal solution, the 10 years can be divided into spans in which money is not moved
    therefore there is also an optimal solution that only invests in a single investment each year
b.  define the subproblem (i, s) as 
        the return of investment strategy from year i if the investment in year i-1 is s
    then optimal solution to (i, s) can be derived from
        let k range over all investments
        get solutions to all subproblem (i+1, k)
        if k = s, deduct f1 from the solution; otherwise deduct f2
        take the maximum among all choice of k, record the choice
    //  the problem does not seem to have optimal substructure
    //  the optimal choice of k also depends on the total return of previous investments
    //  for example, if f2 - f1 = 10000 and the total amount of money at the start of year i is q = 20000 dollars
    //  no return rate <= 50% worths the additional fee charged
    //  but if q = 200000, a return rate of 5% will suffice
    //  the subproblems is defined also by the total amount of money can be invested in this year
    //  then the space of problem is enormous
c.  as the number of years is a constant 10
    it's possible to do a complete enumeration of arrangements about move investments or not at the end of each year
    2^9 = 512 different arrangements in total, which is a constant
    then the fee paid for each arrangement is a constant and can be ignored
    each arrangement can be solved in time O(n^3), where n is the total amount of investments, with scheme in part b
d.  at some point, the total aount of dollars at the start of year i exceeds 15000
    then the same argument in part a no longer applies

15-11
define subproblem (i, j) as
    the cost to satisfy demand from month i with j machines in the inventory
obviously at any point during the optimal strategy, the inventory should never exceed D
or the total demand can be satisfied solely with inventories and it's a waste to produce and store even one more unit
then optimal solution to (i, j) can be derived from
    calculate cost ck to produce k machines so k + j - di ranges from 0 to D, di - j <= k <= D + di - j
    get optimal solution to (i+1, k + j - di), plus ck
    minimize the total cost over all choices of k
there are at most nD subproblems, each refers to at most D subproblems
O(nD^2) in total

15-12
define subproblem (n, x) as
    maximize VORP from position n with x dollars
similar to any other dynamic problemming problem in this chapter
NX/1000000 subproblems, each refers to P subproblems at most, O(NXP) overall
why a greedy algorithm doesn't work:
let N = 2 with three players:
    1.  in position 1, VORP 10, sign $10
    2.  in position 1, VORP 6, sign $9
    3.  in position 2, VORP 6, sign $9
with an allowance $18, the optimal solution signs player 2 and 3
greedy solution signs player 1 first, as $10 / 10 is the highest VORP density

Chapter 16
16.1-1
./CLRS/technique/activity-selection.ts#activitySelection

16.1-2
optimal substructure still applies
let subproblem Sk = {ai ∈ S : fi <= sk}, Ak ⊆ Sk be an optimal solution to Sk
let am be the activity with greatest start time in Sk
let aj be the activity with greatest start time in Ak
as activities in Ak is mutually compatible, for all ai ∈ Ak, i != j
    f[ai] <= s[aj] <= s[am]
then Ak - {aj} ∪ {am} is still mutually compatible
as |Ak - {aj} ∪ {am}| = |Ak|, this is also an optimal solution to Sk
so a greedy strategy that always select the compatible activity with last start time also yields optimal solution

16.1-3
least duration:
        1   2   3
    s   1   9   10
    f   10  11  20
    if the activity of least duration is selected, activity 2 will be selected first
    then any other activity is not compatible with it, solution has size 1
    the optimal solution of size 2 contains activities 1 and 2
fewest overlaps:
        1   2   3   4   5   6   7   8   9   10  11
    s   1   2   2   2   3   4   5   6   6   6   7
    f   3   4   4   4   5   6   7   8   8   8   9
    solution by fewest overlaps: {a2, a6, a8}
    optimal solution: {a1, a5, a7, a11}
earliest start time:
        1   2   3
    s   1   2   3
    f   4   3   4
    solution by earliest start time: {a1}
    optimal solution: {a2, a3}

16.1-4
basically the same to half the problem 14.3-7, scan the start and finish times altogether in increasing order
assume an activity is represented by an object:
interface Act {
    s: number;
    f: number;
    room?: number;
}
make two copies of pointers to object representing activities S and F
sort S according to start time, F according to finish time
maintain a counter k and two index i, j all starting from 0
if S[i].s > F[j].f
    increment k
    assign k to S[i].room
    increment i
if F[j].f >= S[i].s
    decrement k
virtually equivalent to moving a point from past to future on the timeline
k counts the number of activities overlap the point
it's greedy in the sense that the activity is always assigned to the first (i.e. with smallest index) empty room

16.1-5
solution to 16.1-1 can be modified so instead of
    c[i, j] = c[i, k] + c[k, j] + 1
now it compute and maximize
    c[i, j] = c[i, k] + c[k, j] + vk

16.2-1
assume in an optimal solution, w of wm pounds of the item with highest value density is not in the knapsack
replace w pounds of anything in the knapsack with it will yield a equal or better solution
if the capacity W is higher than the total weight of the item with highest value density
all of it should be put into the knapsack and it is now a subproblem with W - wm capacity and all other items

16.2-2
define a subproblem (i, j) as
    maximize the total value from items i to n with capacity j
then the solution to (i, j) is the best among:
    1.  when wi <= j, take item i, add vi to the optimal solution of subproblem (i+1, j - wi)
    2.  do not take item i, exactly the optimal solution to subproblem (i+1, j)
at most nW subproblem that each refers to 2 subproblems at most, O(nW) in total
./CLRS/technique/knapsack.ts#knapsack

16.2-3
let I be the list of items sorted according to their weight
consider a solution which contains I[j] but not I[i], i < j
then it's always safe to replace I[j] by I[i] as the I is sorted by weight, I[i].weight <= I[j].weight
the replacement also improves the solution since I[i].value >= I[j].value as stated in the problem
therefore one of the optimal solutions must be defined by the largest i that Σ(k = {1 .. i})I[k].weight <= W

16.2-4
as there's no cost for carrying more water, assume the professor always refills 2 liters of water at water stop
define subproblem i as
    minimize the number of water stops starting from water stop k with 2 liters of water
then for i < j, the optimal solution to subproblem i is no better than the optimal solution to subproblem j
either the solution to i didn't refill at water stop j and starts from there with less than 2 liters of water
or it refills water at water stop j thus refills at least one more time than the solution to j
let di be the distance between water stop i and water stop i+1
a greedy choice that refills water at water stop j that's the greatest j such that Σ(k = {i .. j - 1})dk <= m
all other choices yields a subproblem j with smaller or equal j, hence is no better than the greedy choice
all di examined at most once, total running time O(n)

16.2-5
first, there must be an optimal solution in which all left end points of intervals is a point in the set 
assume some interval vi in an optimal solution does not start at a point in the set
if vi covers a set of points {xi .. xj}, it must have
    max{xi .. xj} - min{xi .. xj} <= 1
a unit interval starting at min{xi .. xj} then covers the same set of points or more
thereby it's safe to assume all intervals start at a point in the set 
let X be a sorted array of the points in the set, define subproblem i as:
    minimized the number of unit intervals to cover all points from X[i] to X[n]
the leftmost point X[i] must be the start point of an interval, otherwise it will not be covered by any interval
let j be the first point that's not covered by a unit interval starting at X[i]
the optimal solution is then 1 + an optimal solution to subproblem j
all points in the sorted array is examined at most once, O(n) in total, sorting takes O(nlgn)
overall running time O(nlgn) + O(n) = O(nlgn)

16.2-6
thanks Instructor's Manual
basically binary search on unsorted array with linear time median algorithm
first find the median m of value density (value / weight)
calculate the sum of weights w of items which has value density >= m
if w > W
    no item with value density lower than m should be chosen in an optimal solution
    the solution is then the optimal solution to a subproblem with weight limit W and half the items
if w <= W
    all items with value density higher than m should be included in an optimal solution
    solution is the sum of values of these items + an optimal solution with weight limit W - w and half the items
median, sum, filter and combining results of subproblems all take O(n)
T(n) = T(n/2) + O(n), T(n) = O(n) by master theorem
./CLRS/technique/knapsack.ts#linearKnapsack

16.2-7
take logarithm of the result, since ai > 0 for all i,
    lg(Πai^bi) = Σbilg(ai)
as logarithm function is monotonically increasing
maximizing the product is equivalent to maximizing the sum of logs
assume for two indices i and j, ai <= aj but bi >= bj
    bilg(ai) + bjlg(aj) - bilg(aj) - bjlg(ai)
    = bilg(ai / aj) + bjlg(aj / ai)
    = bilg(ai / aj) - bjlg(ai / aj)
    = (bi - bj)lg(ai / aj)
ai <= aj, lg(ai / aj) <= 0, (bi - bj)lg(ai / aj) <= 0
exchange the position of bi and bj then will improve or yield a equally optimal solution
if {ai} is in sorted order, then arrange {bi} also in sorted order yields an optimal solution

16.3-1
x is the character with least frequency, y is the character with second least frequency
which means x.freq <= y.freq <= a.freq <= b.freq
if x.freq = b.freq, all four characters have the same frequency

16.3-2
if the binary tree is not full, there must be some interal node n which has only one child
without loss of generality, assume the sole child is a left child
then replace n with its left subtree will reduce depth of leaves in the left subtree by 1
the resulting binary tree is still a valid encoding of the same alphabet since no leaf is removed

16.3-3
            ┌─b:1
          ┌─2
          │ └─a:1
        ┌─4
        │ └─c:2
      ┌─7
      │ └─d:3
    ┌─12
    │ └─e:5
  ┌─20
  │ └─f:8
┌─33
│ └─g:13
54
└─h:21
prove Σ(k = {1 .. i})fi < fi+2 by induction
    initially f1 + f2 = 2 < f4 = 3
    assume Σ(k = {1 .. i})fi < fi+2
    Σ(k = {1 .. i+1})   = Σ(k = {1 .. i}) + fi+1
                        < fi+2 + fi+1
                        = fi+3
assume in an alphabet C, character ci has frequency fi, where fi is the ith fibonacci number
initially c1:1 and c2:1 is extracted from the priority queue, an internal node with frequency 2 is inserted
assume in iteration i, the new internal node inserted into the queue has frequency Σ(k = {1 .. i+1})fi
which indicates that it has leaves c1 .. ci+1
as Σ(k = {1 .. i+1})fi < fi+3, the next character merged with the internal node must be ci+2
the resulting binary tree has d(ci) = |C| - i + 1

16.3-4
a leaf x with depth k contributes x.freq to all of its ancestors, and there are exactly k of them
by summing all frequences of internal nodes frequency of each leaf x.freq is added exactly d(x) times
formally define d(x, y) as the depth of x in subtree rooted at y
prove for all leaves in a subtree rooted at y, the sum of frequencies of internal nodes equals Σx.freq * d(x, y)
base case is d(x, x) = 0, the leaf has no internal nodes
inductively, for a node y
    Σx.freq * d(x, y.left) = sum of frequencies of internal nodes in left subtree 
    similarly the right subtree
    the tree rooted at y has one more internal nodes which is y
    y.freq = y.left.freq + y.right.freq, which is the sum of frequencies of all the leaves
    d(x, y) = d(x, y.left) + 1 if x is in the left subtree of y, symmetrically y.right
    Σ(x in left subtree)x.freq * d(x, y.left) + Σ(x in right subtree)x.freq * d(x, y.right) + Σ(x in both subtree)x.freq
    = Σ(x in both subtree)x.freq * d(x, y)
at the root d(x, root) = d(x)

16.3-5
if for ci and cj, i < j, d(ci) >= d(cj)
as frequencies of characters in C is in decreasing order, ci.freq >= cj.freq
by switching the prefix code of ci and cj, the total cost is changed by 
    ci.freq * d(cj) + cj.freq * d(ci) - ci.freq * d(ci) - cj.freq * d(cj)
    = ci.freq(d(cj) - d(ci)) + cj.freq(d(ci) - d(cj))
    = (ci.freq - cj.freq)(d(cj) - d(ci))
    <= 0
so there must be an optimal solution in which code length of C is in increasing order

16.3-6
a huffman tree has n leaves and n - 1 internal nodes, 2n - 1 nodes in total
use the heap-like notation to encode internal nodes, the tree can be represented in 2(n-1) + 1 = 2n - 1 bits
(which enumerates nodse level by level, output 1 for internal nodes and 0 for leaves)
leaves can be added back unambiguously since a huffman tree is full
then list all keys in preorder, each takes at most [lgn] bits
the keys can be plugged into the nodes unambiguously since preorder walk of a fixed tree structure is unique

16.3-7
an optimal ternay code tree may not be full
but only one internal node with maximal depth may have 2 children, all other internal nodes must have three
if any internal node has 1 or no children, by problem 16.3-2 the node can be eliminated
if two internal nodes have only 2 children, denote them by x and y, assume d(x) <= d(y)
move a child c of y to x will not increase the total cost of the tree as c.freq * (d(x)+1) <= c.freq * (d(y)+1)
then x is full and y has only 1 child, y can be eliminated
if the node with 2 children has less than maximal depth
move one leaf with maximal depth to it will reduce the total cost, hence the node must have maximal depth
replace a leaf with a tree with three children adds 2 leaves to the tree, thus full ternay tree has 3 + 2k leaves
if the number of leaves n = 3 + 2k for k >= 0, the tree is full
if otherwise n = 4 + 2k, the tree is full except one internal node have only two children
a ternay equivalence to Lemma 16.2 then have two cases
    when n = 4 + 2k, swap x, y with least frequency with a, b as siblings with maximal depth
    when n = 3 + 2k, swap x, y, z with least frequency with a, b, c as siblings with maximal depth (must exist)
by almost the same argument as the proof of Lemma 16.2, the tree is still optimal
a ternay equivalence to Lemma 16.3 is strightforward
the algorithm should merge two least frequent nodes in the first iteration if n = 4 + 2k
otherwise it should merge three least frequent nodes

16.3-8
by the HUFFMAN algorithm in this chapter
as 2min{ci.freq} > max{ci.freq}
once two leaves are merged into an internal node
the node will not be extracted from the queue until all leaves are extracted
all the 256 leaves are merged before any leaf is merged with an internal node
after 128 iterations, the queue contains 128 internal nodes, each has size 2
let {xi} be the set of internal nodes, min{xi.freq} >= 2min{ci.freq} > max{ci.freq} >= 1/2max{xi.freq}
again the maximum node frequency is less than twice the minimum node frequency
once two internal nodes are merged to a size-4 node, it will not be extracted until all size-2 nodes are extracted
recursively the huffman tree built will be a complete tree of height 8
each leaf has depth 8, the encoding is no better than 8-bit fix-sized encoding

16.3-9
an n-bit random file has 2^n variations
if it's compressed to a k-bit file that k < n
2^k < 2^n, there may not be a bijection between original files and compressed files
thus decoding cannot always succeed, the compression method is not valid

16.4-1
let A ∈ Ik, if B ⊆ A, |B| <= |A| <= k, B ∈ Ik, A is independent, Ik is hereditary
if A, B ∈ Ik, |A| < |B|, then |A| < k, A ∪ {x} ∈ Ik for any x ∈ S, including some x ∈ B - A

16.4-2
|S| = n, S is finite
if A ∈ I, B ⊆ A, by definition of linear independence, B ∈ I
let A, B be two set of linear independent columns, |A| < |B|
if there isn't any x ∈ B - A that A ∪ {x} ∈ I
all columns in B can be expressed as a linear combination of columns in A, A spans B
by Steinitz exchange lemma, |B| <= |A|, in contradiction to the assumptions
therefore there must be some x ∈ B that A ∪ {x} ∈ I

16.4-3
let A' ∈ I', B' ⊆ A', as A ⊆ S - A' ⊆ S - B', A ⊆ S - B', B' ∈ I'
let A', B' ∈ I', |A'| < |B'|
both S - A' and S - B' contain some maximal A, B ∈ I
if there exists some x ∈ B' - A' that x ∉ A, A ⊆ S - (A' ∪ {x}), A' ∪ {x} ∈ I
otherwise B' - A' ⊆ A
all maximal A ∈ I have the same size, |A| = |B|
thanks Instructor's Manual
B' - A' ⊆ A ⊆ S - A', B' ∩ A = B' - A', |B' ∩ A| = |B' - A'| = |B'| - |B' ∩ A'|
B ⊆ S - B', |B ∩ A'| <= |A' - B'|
    |A'| < |B'|
    |A'| - |B' ∩ A'| < |B'| - |B' ∩ A'|
    |B ∩ A'| < |B'| - |B' ∩ A'|
    |B ∩ A'| < |B' ∩ A|
    |B| - |B ∩ A'| > |A| - |B' ∩ A|
    |B - A'| > |A - B'|
as B - A' ⊆ B ∈ I, A - B' ⊆ A ∈ I, B - A', A - B' are independent
by exchange property, there must be some y ∈ B - A' that A - B' ∪ {y} ∈ I
if y ∈ A, as y ∉ A - B', y ∈ A ∩ B', but y ∈ B => y ∈ S - B' => y ∉ B', thereby y ∉ A
//  stuck
//  adding elements in B - A' to A - B' cannot always result in a set C, |C| = |A|
//  if |B - A'| = |A| = |B|, it must have B ∩ A' = ∅

16.4-4
let A ∈ I, obviously removing elements from A will not increase |A ∩ Si| for any i, so B ⊆ A => B ∈ I
let A, B ∈ I, |A| < |B|
both A and B has at most 1 element from each disjoint subset Si
let AS = {Si | |A ∩ Si| = 1}, similarly BS
ovbiously |A| = |AS|, |B| = |BS|
|AS| < |BS| implies |BS| contains more subset Si than AS, BS - AS != ∅
take any Sk ∈ BS - AS, add the only x ∈ B ∩ Sk to A, then A ∪ {x} ∈ I

16.4-5
let w0 be a constant > max{w(xi)}, define w'(x) = w0 - w(x)
let A be the greedy solution to a weighted matroid with weight function w'
    w'(A)   = Σ(w0 - w(x))
            = w0|A| - w(A)
each maximal independent has the same size, thus w0|A| is a constant
A maximizes -w(A) among all independent subset of S, hence minimizes w(A) among all A ∈ I

16.5-1
[ { index: 5, deadline: 1, weight: 50, original_weight: 30 },
  { index: 4, deadline: 3, weight: 40, original_weight: 40 },
  { index: 6, deadline: 4, weight: 60, original_weight: 20 },
  { index: 3, deadline: 4, weight: 30, original_weight: 50 },
  { index: 7, deadline: 6, weight: 70, original_weight: 10 } ]

16.5-2
assume declaring an array D of size n takes O(1) time (true in some languages but not all)
for each task t in A:
    increment D[t.d]
    if D[t.d] > t.d, return false
return true

16-1
a.  quarter:    25 cent
    dime:       10 cent
    nickel:     5 cent
    penny:      1 cent
    always choose the coin with most value <= n
    let subproblem n be the optimal solution to n cents
    when n >= 5, let C be an optimal solution using only nickels and pennies
        C may not contain more than 5 pennies, otherwise that 5 pennies can be replaced with a nickel
        if k < [n/5], the solution have more than 5 pennies
        if k > [n/5], k > n/5, 5k > n, the solution is not valid
        thereby k = [n/5], p = n % 5
    when n >= 10, let C be an optimal solution using no quarters
        let d be the number of dimes in C, C - {d dimes} is an optimal solution to n - 10d using no dime and above
        p = (n - 10d) % 5 = n % 5
        k = [(n - 10d) / 5]
        furthermore, if k >= 2, two nickels can be replaced by a dime, so k <= 1
        if d > [n / 10], 10d > n, the solution is not valid
        if d < [n / 10], n - 10d > 10, [(n - 10d) / 5] >= 2
        thereby d = [n / 10]
    when n >= 25, let C be an optimal solution
        let q be the number of quarters in C
        C - {q quarters} must be an optimal solution to n - 25q using no quarters
        p = (n - 25q) % 5 = n % 5
        d = [(n - 25q) / 10]
        k = [(n - 25q - 10d) / 5]
        furthermore, if d >= 3, 3 dimes can be replaced by a quarter and a nickel
        if d >= 2 and k >= 1, 2 dimes and 1 nickel can be replaced by a quarter
        if q > [n/25], 25q > n, the solution is not valid
        if q < [n/25], n - 25q > 25
            for 25 < n - 25q < 30, d = 2, k = 1
            for n - 25q >= 30, d >= 3
        thereby q = [n / 25]
    taking q = [n / 25], d = [(n - 25q) / 10], k = [(n - 25q - 10d) / 5]
    is equivalent to choose the most valuable coin <= n and solve the subproblem with the same strategy
b.  an optimal solution with coins {c^0, c^1} will have n1 = [n / c] coins of value c^1
    assume an optimal solution with coins {c^0 .. c^i}, ni = [n / c^i]
    an optimal solution with coins {c^0 .. c^i+1} then must have ni+1 = [n / c^i+1]
    if ni+1 > [n / c^i+1], the solution is invalid
    if ni+1 < [n / c^i+1]
    the remaining coins must be an optimal solution to n - ni+1 * c^i+1 with coins {c^0 .. c^i}
    ni+1 < [n / c^i+1] implies n - ni+1 * c^i+1 >= c^i+1
    ni = [(n - ni+1 * c^i+1) / c^i] >= c
    but c coins of value c^i can be replaced by a coin of value c^i+1
    therefore ni+1 = [n / c^i+1], equivalent to a greedy solution
c.  coins have face value (1, 7, 10)
    for n = 14, greedy solution is (10, 1, 1, 1, 1), optimal solution is (7, 7)
d.  with dynamic programming
    define subproblem i naturally as
        minimize the number of coins that add up to i cents
    optimal solution to problem i can be derived from
        for all k diffrent coins:
        choice one of them, compute j = i - its face value, get an optimal solution to subproblem j, plus 1
        minimize the sum over all k
    at most n subproblems, each refers to k subproblems, O(nk) in total

16-2
a.  always choose the task with least completion time
    n is fixed for each problem, minimizing average completion time is equivalent to minimizing total completion time
    assume A = (ai1, ai2 .. ain) is a solution that does not start with the task aik with least completion time
    consider A' that swap ai1 and aik in A
    since ths first k tasks still takes that much time
    the completion time for all aij that j >= k will not be affected
    but the completion time for all tasks ai1 .. aik-1 is improved by ai1 - aik > 0
    the subproblem with S' = S - {aik} can be solved similarly
    the optimal sum of completion time to S is the optimal time to S' + n * aik
    the optimal solution is S sorted according to completion time
    running time is O(nlgn)
b.  for each time unit, the task with least remaining completion time should be executed
    after rn, the problem is the same to part a
    preemption will not improve the optimal solution after rn:
        let S = (a1 .. an) sorted by completion time in increasing order
        assume in the first time unit, instead of a1, another task with completion time ak.c > a1.c is executed
        S reduced to a subproblem S' = (a1 .. ak' .. an) instead of S'' = (a1' .. an)
        where ak'.c = ak.c - 1, a1'.c = a1.c - 1
        let A' be an optimal solution to S'
        there are ak.c + a1.c - 1 time units in A' that executes a1 and ak'
        since ak.c > a1.c, the completion time of the earlier between a1 and ak' >= the a1.cth time unit
        the the later one completes at the last time unit among the ak.c + a1.c - 1
        let the first a1 - 1 time units execute a1' instead, A' changed into A'', a solution to A''
        the completion time of a1' is a1'.c = a1.c - 1th time unit
        the completion time of the later one unchanged
        thereby the optimal solution to S'' is better than the optimal solution to S'
        the task with least completion time should be executed at each time unit
    between rn-1 and rn, either
        all tasks except the last one can be fully completed before rn
        the optimal solution is the same to part 1 for tasks available between rn-1 and rn
        then execute the last one after rn
    or
        the tasks may not be completed in time rn-1 to rn
        the optimal solution should left a subproblem as small as possible to time after rn
        by the argument above
        //  guess, not prove
        execute rn - rn-1 time units of tasks with least remaining completion time leaves a smallest subproblem
        doing so also minimizes the completion time of the tasks completed between rn - rn-1
        //  guess, not prove
    both case the optimal strategy is to execute the task with least remaining completion time at each time unit
    recursively for each time span between ri ~ ri+1, the same strategy applies
    the scheduling can be computed with a min heap
    make a copy of tasks T sorted according to release time
    create a min heap Q initially empty according to remaining completion time
    set a variable t starting from 0, representing the current time
    if Q is empty, pop the next task a from T, insert it into Q, set t to a.r the release time of a
    if Q is not empty, extract the minimum a from Q, let b be the next task in T
    compare a.c the remaining completion time of a and b.r - t
    if a.c > b.r - t
        set a.c to a.c - (b.r - t)
        insert a back to Q
        pop b from T, insert b into Q
        set t to b.r
    if a.c <= b.r - t or T is empty
        record the completion time of a as t + a.c
        set t to t + a.c
    repeat until both Q and T empty
    each iteration O(lgn) by complexity of heap operations
    in each iteration, either a task popped from T or completed, 2n iterations at most
    overall running time O(nlgn)

16-3
a.  each column ci in M corresponds to an edge i
    ci[j] = 1 iff edge i and vertex j is incident
    obviously for each i, exactly two entries have value 1, others have value 0
    in the field of integers modulo 2, the only possible coefficient is 1 or 0
    the columns of M is linearly dependent iff for a column ci an a set of column C, ci ∉ C
        ci = Σ(cj ∈ C)cj
        ci + Σ(cj ∈ C)cj = 0    // over field of integers modulo 2
        Σ(cj ∈ C ∪ {ci})cj = 0
    in other words Σ(ci ∈ C)ci = 0 for some subset C of columns of M
    Σ(ci ∈ C)ci counts the number of edges in a subset of E incident on each vertex in V modulo 2
    if a set of edges forms a cycle, each vertex is incident to exactly two edges
    if a set of edges does not contain a cycle (i.e. is a forest)
    there must be some vertex v that only one edge is incident on v
    therefore Σ(ci ∈ C)ci = 0 iff the set of edges represented by C forms a cycle
    a set of columns of M is linearly independent iff the corresponding set of edges is acyclic
    by problem 16.4-2, define S be the set of columns, I = {A | A ⊆ S, A is linearly independent}
    (S, I) is a matroid
b.  sort E by weight, let E' = ∅, for each e in E:
        if E' ∪ {e} is acyclic, add e to E
    return E' after the traversal
c.  exchange property fails
    let E1 = {(v1, v2), (v2, v3), (v1, v3)}
    let E2 = {(v2, v1), (v3, v2)}
    E1, E2 ∈ I, but adding any edge in E1 to E2 forms a directed cycle
d.  if some set of edges forms a directed cycle
    they enter and leave each vertices in the cycle exactly once
    the sum of the corresponding columns = 0, the set of corresponding columns is not linearly independent
    if the set of columns is linearly independent, no set of edges forms a directed cycle in the graph
e.  the converse of d is not true
    in the field of integers, coefficient in linear combinations can be negative
    an edge in the sum can be interpreted as itself or its reverse

16-4
a.  case 1: such a slot t exists
        let A be an optimal solution to the problem that didn't assign a1 to the first empty slot before d1
        that slot t must be assigned to some ak that pk the penalty of ak <= p1
        if a1 is early in A, it must occupy a time slot earlier than t, as t is the latest such slot
        if ak is early in A, exchanging the position of a1 and ak will not make ak late
        if otherwise ak is late, ak may still be late after the exchange
        total penalty is not decreased
        if a1 is late in A, the exchange makes a1 early and possibly makes ak late
        total penalty changed by at most pk - p1 <= 0
        the resulting solution is at least as optimal as A
    case 2: such a slot doesn't exist
        let A be an optimal solution to the problem
        a1 must be late in A
        if the last task ak in A is early, exchanging a1 with ak will not make ak late
        if ak is late, ak may still be late after the exchange
        the total penalty is not increased
    the subproblem is defined by the set of remaining tasks and remaining time slots, optimal substructure applies
b.  wait for section 21.3

16-5
a.  preprocess:
        maintain a dynamic set S and an array N, scan the sequence of requests from tail to head
        upon a request ri:
            search key ri in S
            if ri is in S, retrieve the value and assign it to N[i]
            update key ri in S to i
        on termination, N[i] points to the next occurrence of ri in sequence of requests (if any)
    simulation:
        maintain a max heap H keyed by N[i] with value ri and a search tree T keyed by ri
        for each request ri in the sequence:
            if ri is in T, it is a cache hit
            if ri is not in T, if H.size < k, insert (N[i], ri) into H and ri into T
            otherwise extract the maximum from H, let it be rj
            if N[j] > N[i], delete rj from T, insert (N[i], ri) and ri into H and T, output "rj evicted"
            if N[j] < N[i], insert (N[j], rj) back to H, output "ri not cached"
    in preprocess, running time is dominated by O(n) search and insertion operation to S
    if S has O(1) search and insertion (e.g. hash table), running time is O(n)
    if S has O(lgn) search and insertion, running time is O(nlgn)
    in simulation, running time is dominated by O(n) heap and search tree operations, each O(lgn)
    O(nlgn) in total
b.  subproblem is defined by a tail of the requests and the set of items in the cache, a size-k subset of requests
    no matter how the current request is treated
    if the subproblem can be handled better, the solution can be improved
    assume the current request being processed is ri, |S| <= k is the cache items
    the possible treatment for the current request ri is:
        1.  a cache hit, subproblem is (i+1, S)
        2.  a cache miss and |S| < k, subproblem is (i+1, S ∪ {ri})
        3.  a cache miss and |S| = k, either ri or one rj ∈ S is evicted
            possible subproblems are (i+1, S) and (i+1, S - {rj} ∪ {ri}) for rj ∈ S
c.  thanks https://www.cs.princeton.edu/~wayne/kleinberg-tardos/pearson/04GreedyAlgorithms.pdf
    assume before the ith request, there is an optimal solution A that follows furtherest-in-future strategy up to i
    consider the ith request ri
    if A evicts an item rp in the cache that is not rk the furtherest in future (FF)
    let A' be a solution that evicts rk instead and imitates A as long as possible
    let AF be the solution derived from FF
    let j be the next time that A' can no longer imitate A (must involve rp or rk)
    case 1: rj = rk
        can't happen, rk is the furtherest in future, there must be a request to rp before rk
        in A' the cache won't contain rp, request to rp will be a cache hit in A but a cache miss in A'
    case 2: rj = rp, a cache miss
        rp is not in the cache of A, an element rq is evicted
        if rq = rk, A' accesses rp in the cache, A and A' has the same cache from now
        if rq != rk, let A' evicts rq and add rk into the cache, A and A' has the same cache from now
        let A' imitate A from now on, the total number of cache miss not increased
    case 3: rj != rp or rk, a cache miss
        A must evict rk or A' can imitate it
        let A' evict rk, A and A' has the same cache from now on
        the total number of cache miss is the same
    thereby A' is a equally optimal solution that follows FF before i+1th request
    inductively there is a solution that follows FF all the way to the end

Chapter 17
17.1-1
no, consider n MULTIPUSH(K) operation followed by n MULTIPOP(|K|) operations
the complexity is now Θ(n|K|), depending on |K|

17.1-2
increment from 1^k-1 to 10^k-1 changes k-1 bits
decrement from 10^k-1 to 1^k-1 changes k-1 bits
if the sequence of operations repeatedly increment and decrement around that two numbers
the total running time is Θ(nk)

17.1-3
the operation costs i when the binary representation of i is 10^k for some k
when incrementing i from 1 to n, i = 10^k happens exactly ||n|| = |b(n)| times for each k from 0 to ||n|| - 1
the sum of such i equals 2^||n|| - 1 < 2n
plus the O(1) cost for other i, the amortized running time is O(n) / n = O(1)

17.2-1
assign costs to operations:
    PUSH    3
    POP     1
    COPY    0
each POP is paid by previous PUSH just like in the text
each PUSH and POP also adds one extra credit, so after k operations there are k extra credits to pay for the copy

17.2-2
assign a contant cost of 3 to increment operation
between any two i = 2^k + 1 and j = 2^(k+1), the total number of credits accumulated is
    3 * (2^(k+1) - 2^k) = 3 * 2^k = 2^(k+1) + 2^k
deduct the costs of j - i - 1 = 2^k - 1 normal incremental operations
there are still 2^(k+1) + 1 credits, which is enough to pay for the cost incurred by jth increment operation

17.2-3
keep the index of the highest order 1 in the array, update the index at the end of INCREMENT
assign a constant cost of 3 to increment operation
so even after be set to 0 in INCREMENT, each bit that was once 1 has at least 1 credits on it
the RESET operation then set every bit from the highest order 1 to 0, which was once 1
the cost of RESET can be paid by previous INCREMENT operations

17.3-1
define Φ'(Di) = Φ(Di) - Φ(D0)
as Φ(Di) >= Φ(D0), Φ'(Di) >= 0 = Φ(D0) - Φ(D0) = Φ'(D0)
the amortized cost of ith operation is
    či  = ci + Φ'(Di) - Φ'(Di-1)
        = ci + Φ(Di) - Φ(D0) - Φ(Di-1) + Φ(D0)
        = ci + Φ(Di) - Φ(Di-1)

17.3-2
define Φ(Di) as
    Φ(Di) = Φ(Di) + 2 if i != 2^k
    Φ(Di) = 0 if i = 2^k or i = 0
as Φ(Di) either increases or be set to 0, Φ(Di) >= 0 = Φ(D0) for all i
    či  = ci + Φ(Di) - ΦD(Di-1)
        = 1 + 2 = 3 if i != 2^k
when i = 2^k, Φ(Di) = 0, Φ(Di-1) equals twice the number of operations between j = 2^k-1 and 2^k - 1
    Φ(Di-1) = 2(2^k - 2^k-1) = 2 * 2^k-1 = 2^k
    Φ(Di) - Φ(Di-1) = -2^k
    či  = ci + Φ(Di) - Φ(Di-1)
        = i - 2^k = 0
therefore Σci <= Σči <= Σ3 = O(n)

17.3-3
as the EXTRACT-MIN operations takes O(lgn) time, cost of the operation <= clgn for some c
let n be the size of the heap, define Φ(Di) = cnlgn when n >= 1, Φ(Di) = 0 when n = 0, Φ(Di) >= 0 = Φ(D0) for all i
for an insert operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = O(lgn) + cnlgn - c(n-1)lg(n-1)
        = O(lgn) + c(n-1)lg(n / (n-1)) + clgn
        = O(lgn) + c(n-1)lg(1 + 1/(n-1))
        <= O(lgn) + c(n-1)lg(e^(1/(n-1)))
        = O(lgn) + c(n-1)(1/(n-1))lge
        = O(lgn) + clge = O(lgn) if n >= 2
when n = 1, Φ(Di) = 0 = Φ(Di-1), či = ci = O(1)
for EXTRACT-MIN operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        <= clgn - (cnlgn - c(n-1)lg(n-1))
        = clgn - clgn + c(n-1)lg((n-1) / n)
        <= c(n-1)lg(n / n) = 0 = O(1) if n >= 2
when n = 1, či = ci = O(1)
the cost of EXTRACT-MIN is paid by previous insert operations

17.3-4
let n be the size of the stack, define Φ(Di) = n, Φ(Di) >= 0 = Φ(D0) for all i
for push operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = 1 + 1 = 2
for pop operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = 1 + -1 = 0
for MULTIPOP operation:
    či  = min(n, k) + Φ(Di) - Φ(Di-1)
        = min(n, k) + -min(s, k) = 0
the amortized cost for each operation <= 2
    Σci = Σči - Φ(Di) + Φ(D0)
        <= Σ2 - sn + s0
        = 2n - sn + s0

17.3-5
Σci <= 2n - bn + b, bn >= 0, if n = Ω(b) then 2n + b = O(n)

17.3-6
define two stack S1 and S2
ENQUEUE operation pushes items into S1
DEQUEUE operation pops items from S2, if S2 is empty, pop all items in S1 and push them into S2 in order
define Φ(Di) = 2|S1|, Φ(Di) >= 0 = Φ(D0)
for ENQUEUE operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = 1 + 1 = 2
for DEQUQUE operation, if |S2| > 0:
    či  = ci + Φ(Di) - Φ(Di-1) = ci = 1
if |S2| = 0, |S1| push and |S1| + 1 pop operations performed
    či  = ci + Φ(Di) - Φ(Di-1)
        = 2|S1| + 1 - 2|S1| = 1
    Σci <= Σči <= Σ2 = O(n)

17.3-7
use an unsorted array
insert simply puts the item to one beyond the current length and increment the length n
DELETE-LARGER-HALF operates as following:
    find the median of the array in O(n)
    filter the array by the median
    copy remaining items to a new array, set the length to n/2
each step O(n), O(n) in total, let cn be an upper bound of the cost of DELETE-LARGER-HALF
define Φ(Di) = 2cn, Φ(Di) >= 0
for insert operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = O(1) + 2c = O(1)
for DELETE-LARGER-HALF operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        <= cn + 2c(n/2) - 2cn
        = 0 = O(1)
therefore Σci <= Σči <= ΣO(1) = O(m)
may use the variable-size array introduced in the next section to support unlimited insertion

17.4-1
by Corollary 11.7, the expected probes of insertion in an open addressing hash table is 1 / (1 - α)
when α -> 1, 1 / (1 - α) -> ∞, the last few insertions before the table is full may be costy
by restricting α, the expected performance of insertion is guarenteed to be better than a constant
only allow insertion if α < α0, where α0 < 1 is a constant
then the expected number of probes in an insertion is at most 1/(1 - α) <= 1/(1 - α0) = O(1)
the underlying table can be extended easily as described in the text
the hash function however must be adjusted or the new slots will never be accessed
upon expansion, copy each item in slot i to slot 2i in the new array, for 0 <= i <= m - 1
originally the hash function h(k, i) produces a permutation of {0 .. m - 1}
let h'(k, i) be the new hash function that
    h'(k, i)    = 2h(k, i) if i <= m - 1
                = 2h(k, i) + 1 if i >= m
h'(k, i) thereby produces a permutation of {0, 2 .. 2m - 2} ∪ {1, 3 .. 2m - 1} = {0, 1, .. 2m - 1}
the slots containing items in the old table is probed in the same order as h first
only after that the new slots are accessed, so the validity of search operation is not damaged
table expansion is Ω(n), when α = α0, the table must be expanded before insertion
thus expected running time of insertion is not necessarily O(1)

17.4-2
when αi-1 >= 1/2, deletion will not cause contraction
if both αi and αi-1 >= 1/2
    či  = ci + Φi - Φi-1
        = 1 + (2numi - sizei) - (2numi-1 - sizei-1)
        = 1 + (2(numi-1 - 1) - sizei) - (2numi-1 - sizei)
        = -1
if αi < 1/2 and αi-1 >= 1/2
    či  = ci + Φi - Φi-1
        = 1 + (sizei / 2 - numi) - (2numi-1 - sizei-1)
        = 3sizei-1 / 2 - 3numi-1
        = 3sizei-1 / 2 - 3αi-1sizei-1
        <= 3sizei-1 / 2 - 3sizei-1 / 2
        = 0

17.4-3
if the deletion didn't cause a contraction,
    či  = ci + Φi - Φi-1
        = 1 + |2numi - sizei| - |2numi-1 - sizei-1|
        = 1 + |2numi-1 - 1 - sizei| - |2numi-1 - sizei|
        <= 1 + 1 = 2
if the deletion causes a contraction
    numi-1 / sizei-1 >= 1/3, (numi-1 - 1) / sizei-1 < 1/3
    for all except a few small sizei-1, 2numi-1 - sizei-1 < 0
    sizei = 2sizei-1 / 3
    numi-1 / sizei >= 1/2, (numi-1 - 1) / sizei < 1/2, 2numi / sizei < 0
    ci = numi + 1
    či  = ci + Φi - Φi-1
        = numi + |2numi - sizei| - |2numi-1 - sizei-1|
        = numi-1 - 2numi-1 + 2 + 2sizei-1 / 3 + 2numi-1 - sizei-1
        = numi-1 - 2 - sizei-1/3 
        = numi-1 - 1 - sizei-1/3 - 1
        < -1
the amortized running time of deletion is O(1)

17-1
a.  ./CLRS/technique/bit-reverse.ts#rev
    ./CLRS/technique/bit-reverse.ts#bitReversalPermutation
b.  ./CLRS/technique/bit-reverse.ts#BitReversedCounter
c.  shift by more than 1 place is only used when computing the initial mask 1 << (1 - k)
    which can be computed once and stored as a field of the counter
    amortized running time still O(1) assuming k = O(n)

17-2
a.  do binary search on each array
    worst case is unsuccessful search where all bits in b(n) is 1
    T(n)    = O(Σ(k = {0 .. lgn - 1})lg2^k)
            = O(Σ(k = {0 .. lgn - 1})k)
            = O(lg^2n)
b.  insert to A0, |A0| <= 1, the insertion will take constant time
    if |A0| = 2, starting from i = 1
        if Ai is empty, replace it will Ai-1, set Ai-1 = ∅ and terminate
        otherwise set Ai to the merge of Ai-1 and Ai, set Ai-1 = ∅, set i = i + 1, go to the next iteration
    after ith iteration
        the correspondence between up to ni-1 and Ai-1 is restored, |Ak| = 0 for k <= i-1
        the correspondence between ni and Ai restored (termination) or |Ai| = 2^(i+1)
    the iteration terminates only when Ai is empty, Ai replaced by Ai-1, |Ai-1| = 2^(i-1+1) = 2^i
    in worst case every array is merged, total cost Σ(k = {1 .. lgn - 1})O(2^k) = O(n)
    each merge up to Ai occurs once each 2^i insertions, takes O(2^i)
    for k = 2^j insertions, the cost of merge is no greater than
        Σ(O(2^i) * (2^j / 2^i))
        = Σ(O(2^i) * 2^(j-i))
        = Σ(O(2^j))
        = O(klgk)
    amortized cost for each insertion is then O(klgk) / k = O(lgk)
c.  if the deleted item is in array Aj, find the first nonempty array Ai 
    take the last element of Ai, insert it to the correct position in Aj in O(2^j) = O(n)
    now |Ai| = 2^i - 1
    cut Ai to i slices of length 2^0, 2^1 .. 2^i-1, set A0 .. Ai-1 to these slices in O(2^i) = O(n)
    as Ai is sorted, each A0 .. Ai-1 is automatically sorted
    ni = 0, n0 .. ni-1 = 1 now, the correspondence restored in O(n) total

17-3
a.  if x.size is even, x.left.size and x.right.size may differ by at most 1
    otherwise the greater between x.left.size and x.right.size > 1/2 * x.size
    if x.size is odd, x.left.size must equal x.right.size
    walk the tree in inorder, enumerate nodes in an array, set .parent, .left and .right of each node to NIL
    recursively:
        find the median x, the median than divide the array into two subarrays
        the left subarray has key < x.key, the right subarray has key > x.key
        construct the 1/2-balanced tree l and r of both subarray
        set x.left = l, x.right = r
        as x is the median, if x.size is odd, l.size = r.size, otherwise l.size and r.size differ by at most 1
        the tree rooted at x is a 1/2-balanced tree
    T(n) = 2T(n/2) + O(1) = O(n)
b.  a tree with height h = 0 has a single node
    if an α-balanced tree of height h has at least n nodes, for an α-balanced tree of height h+1
    one of the subtree of x the root has height h, let it be the left subtree
        α * x.size >= x.left.size >= n
        x.size >= n/α
    inductively, an α-balanced tree of height h has at least (1/α)^h nodes, where 1/α > 1
    n >= (1/α)^h, lgn >= hlg(-α), h <= lg(-α)lgn = O(lgn)
    a search at most examines h = O(lgn) nodes
c.  Δ(x) is a absolute value, Δ(x) >= 0 for all x, ΣΔ(x) >= 0
    consider a 2-node tree with root and its single left child
    the tree is 1/2-balanced, but Δ(root) = 1, Φ(T) = c
    assume x.left.size = x.right.size for each node in 1/2-balanced tree in next two parts
d.  in worst case, both left and right subtree is 1/2-balanced
    the cost have to be paid solely with potential of the root x
        Φ(T) = cΔ(x) = c|x.left.size - x.right.size|
    α * x.size = αm < max(x.left.size, x.right.size)
    assume x.left.size is greater, denote ml = x.left.size, mr = x.right.size
        m > ml > αm, mr = m - ml - 1, c|ml - mr| = c(2ml - m + 1) > c(2α - 1)m
    for c(2α - 1)m >= m, c(2α - 1) >= 1, c >= 1/(2α - 1)
e.  for insertion, if rebalance unnecessary:
        či  = ci + Φ(Ti) - Φ(Ti-1)
            = O(lgn) + O(lgn) (at most O(lgn) nodes has Δ(x) changed by 1)
            = O(lgn)
    if insertion caused a rebalance at level x, x.size = m:
        Φ(Ti)   = 0
        Φ(Ti-1) >= c|x.left.size - x.right.size| >= c|αm - (1-α)m| = c(2α - 1)m >= m
        či  = ci + Φ(Ti) - Φ(Ti-1)
            <= O(lgn) + m - c(2α - 1)m = O(lgn)
    for deletion, if rebalance unncessary:
        či  = ci + Φ(Ti) - Φ(Ti-1)
            = O(lgn) + O(lgn) = O(lgn)
    if deletion caused a rebalance at level x, x.size = m:
        Φ(Ti)   = 0
        Φ(Ti-1) = c(2α - 1)m
        či  = ci + Φ(Ti) - Φ(Ti-1)
            = O(lgn) + m - c(2α - 1)m
            = O(lgn)
    therefore amortized cost of insertion and deletion is O(lgn)

17-4
a.  for a newly inserted node:
        its parent is red and has a red sibling
        each black ancestor of it has a red parent and a red uncle
    then case 1 in RB-INSERT-FIXUP will propagate all the way to the top, make O(h) = O(lgn) color changes
    assume after deletion, node x is double black:
        all ancestors of x is black
    then the while loop in RB-DELETE-FIXUP will not terminate until x = T.root, run for O(h) = O(lgn) iterations
    in each iteration some nodes change color, O(lgn) color changes in total
b.  for RB-INSERT-FIXUP, case 2 and 3 are terminating
    for RB-DELETE-FIXUP, case 1, 3 and 4 are terminating, case 2 is terminating if new x is red
c.  in case 1 of RB-INSERT-FIXUP, two red nodes colored black, one black node colored red
    total number of red nodes decremented by 1, Φ(T') = Φ(T) - 1
d.  in case 1 (line 5 - 8):
        two red nodes colored black
        one black node colored red
        potential decreased by 1
    in case 2 (line 10 - 11):
        a left rotation
        no potential change
    in case 3 (line 12 - 14):
        one red node colored black
        one black node colored red
        a right rotation
        no potential change
e.  in case 1 of RB-INESRT-FIXUP, each iteration decreases potential by 1
    the cost is paid by the potential, či = O(1)
    case 2 and 3 takes O(1) and terminates
    amortized cost of RB-INESRT-FIXUP is then O(1)
f.  as w(x) = 0 if x is red, potential will only change if a red node is colored black
    in case 1 the only nonterminating case (referring nodes by their name in figure 13.5 - 13.7):
        w(C) changed from 2 to 0
        as A is originally red, its left child must be black
        w(A) 0 -> 0
        as D is originally red, both its children must be black
        w(D) 0 -> 1
        Φ(T') = Φ(T) - 1
    each iteration of case 1 is paid by the potential
    case 2 and 3 changes potential by at most a constant
        ci <= či = O(1)
g.  in case 2 the only nonterminating case:
        w(D) 1 -> 0
        B must be black or case 2 terminates
        w(B) 1 -> 0
        Φ(T') = Φ(T) - 2
    each iteration of case 2 is paid by the potential
    case 1, 3 and 4 change potential by at most a constant
        ci <= či = O(1)
h.  Σci <= Σči = ΣO(1) = O(m)

17-5
a.  when H does not know σ in advance
    the operation of H is totally defined by the current ordering and the search key σi
    after such an operation, H must choose some x as the last node of the list
    let a sequence σ that always searches key in x after H performed its reordering
    each search takes Ω(n), Ω(mn) in total
b.  searching x examines rank(x) nodes
    since each transposition decreases rank(L, x) by 1
    moving x to the head of the list uses at least rank(x) - 1 transposition
    by always transpose x with its prev, the lower bound can be achieved
        ci = rank(L, x) + rank(L, x) - 1 = 2rank(x) - 1
c.  obvious
d.  assume (x, y) is transposed
    if (x, y) was not an inversion, it becomes one after the transposition, Φ(Li) increased by 2
    if (x, y) was an inversion, after the transposition y precedes x, (y, x) is not an inversion
    Φ(Li) decreased by 2
e.  a node precedes x in Li-1 either precedes x in L*i-1 or not
    the number of nodes precede x thereby is |A| + |B|, rank(Li-1, x) = |A| + |B| + 1
    similarly rank(L*i-1, x) = |A| + |C| + 1
f.  after access σi, x is moved to the head of Li
    each nodes in set A now causes a new inversion with x as they follow x in Li but precede x in L*i-1
    each nodes in set B eliminates an inversion with x as they now follow x in both Li and L*i-1
    relative position between elements in C and D with x in Li not changed
    they cause inversions between Li and L*i-1 iff they cause inversions between Li-1 and L*i-1
    the relative position between any other (y, z) in Li that y != x and z != x is not changed from Li-1
    then i* transpositions in L*i-1 at most increases the potential by 2i*
        Φ(Li) - Φ(Li-1) <= 2(|A| - |B| + i*)
g.  ci + Φ(Li) - Φ(Li-1)    <= 2(|A| + |B| + 1) - 1 + 2(|A| - |B| + i*)
                            = 4|A| + 1 + i*
                            <= 4|A| + 4|C| + 4 + 4i*
                            = 4rank(L*i-1, x) + 4i*
                            = 4c*i
h.  summation

Chapter 18
18.1-1
then t-1 = 0 and some node will store no keys and only one child
the height of the tree is no longer O(lgn)

18.1-2
node stores 2 or 3 keys, t-1 <= 2, 2t - 1 >= 3
node has 3 or 4 children, t <= 3, 2t >= 4
t may be 2 or 3

18.1-3
the root contains 1-3 keys
1 key:
    2 children, each with 1-3 keys
    if any of them is not a leaf, it has at least 2 children with at least 1 key each
    the other child of root then may not have any child, leaves do not have the same height
    thus both children of the root must be a leaf
    ((1), 2, (3, 4, 5)), ((1, 2), 3, (4, 5)), ((1, 2, 3), 4, (5))
2 keys:
    3 children, each with 1 key
    ((1), 2, (3), 4, (5))
3 keys:
    impossible, must have 4 children with 1 key each, 7 > 5 keys in total

18.1-4
each internal node has 2t - 1 keys and 2t children
a complete B-tree of height h has (2t)^d nodes at depth d
    n   = 1 + (2t - 1)Σ(d = {1 .. h})((2t)^d)
        = 1 + (2t - 1)((2t)^(h+1) - 1) / (2t - 1)
        = (2t)^(h+1)
    
18.1-5
the children of red nodes must be black
a black node after absorbing red children has 4 children and 3 keys, 3 children and 2 keys, or 2 children and a key
c1 originally root of left subtree of left child, c2, c3 and c4 accordingly, property 3 of B-tree follows
the black height is universal among all leaves, property 4 of B-tree follows
therefore the resulting tree is a 2-3-4 tree

18.2-1
About to insert K
Empty BTree
About to insert L
┌──S
├Q
│ ┌K
└─├F
  └C
About to insert W
  ┌V
┌─├T
│ └S
├Q
│ ┌L
├─├K
│ └H
├F
└──C
About to insert M
    ┌W
  ┌─└V
┌─├T
│ └──S
├Q
│   ┌L
│ ┌─├K
└─│ └H
  ├F
  └──C
About to insert P
    ┌W
  ┌─└V
┌─├T
│ │ ┌S
│ └─└R
├Q
│   ┌N
│ ┌─├M
│ │ └L
└─├K
  ├──H
  ├F
  └──C
About to insert A
    ┌W
  ┌─└V
┌─├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
│ ├M
│ ├──L
└─├K
  ├──H
  ├F
  └──C
About to insert Y
    ┌X
  ┌─├W
  │ └V
┌─├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
├─├M
│ └──L
├K
│ ┌──H
│ ├F
└─│ ┌C
  └─├B
    └A
About to insert D
    ┌Y
  ┌─└X
  ├W
┌─├──V
│ ├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
├─├M
│ └──L
├K
│ ┌──H
│ ├F
└─│ ┌C
  └─├B
    └A
Final configuration
    ┌Z
  ┌─├Y
  │ └X
  ├W
┌─├──V
│ ├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
├─├M
│ └──L
├K
│ ┌──H
│ ├F
│ │ ┌E
└─├─├D
  │ └C
  ├B
  └──A

18.2-2
no DISK-READ or DISK-WRITE in the body of B-TREE-INSERT
B-TREE-SPLIT-CHILD modifies each of x, y and z, no DISK-WRITE is redundant
in B-TREE-INSERT-NONFULL:
    the only DISK-WRITE is after inserting the new key into a leaf
    the only DISK-READ(x.ci) reads a child of x in the current recursion
    as insertion is performed by a single pass down the tree, each x is visited only once
    there's no way to refer to the parent of a node, the only way to visit x.ci is through x
    therefore the read of x.ci must be the first time and the last time it is read
there doesn't seem to be any redundant read or write in insertion

18.2-3
minimum:
    traverse x.c1 until x is a leaf, then select x.key1
predecessor:
    if x is an internal node, predecessor of x.keyi is the minimum of x.ci+1
    if x is a leaf, predecessor of x.keyi is x.keyi+1
    when x.keyi+1 doesn't exist, predecessor is the deepest y.keyj+1 for which y.ci contains x

18.2-4
in worst case, the keys are in sorted order 
at least every 2 insertions cause a leaf to split
at least two splits of the same leaf cause a node with height 1 to split
at least two splits of the same node of height 1 causes a node with height 2 to split
the total number of splits caused by insertion is n/2 + n/4 + n/8 ... ≒ n
the tree after n insertion may have n nodes at most
in best case, each internal node contains 2 keys and each leaf 3
an internal node with 3 keys after B-TREE-SPLIT-CHILD has a child with only 1 key
any inserion into that child will cause the internal node to split
let h be the height of the tree
there are 3^d nodes at depth d
    2Σ(d = {0 .. h-1})3^d + 3 * 3^h = n
    (3^h - 1) + 3 * 3^h = n
    4 * 3^h - 1 = n
    3^h = (n + 1) / 4
so 3^h = (n + 1) / 4 leaves and (3^h - 1) / 2 = (n - 3) / 8 internal nodes
(3n - 1) / 8 nodes at least

18.2-5
modify B-TREE-SPLIT-CHILD so it now takes t as a parameter
./CLRS/structure/b-tree.ts#splitChild
and define different t for leaves and internal nodes, give different parameter to B-TREE-SPLIT-CHILD accordingly
instead of manually checking x.n == 2t - 1, make a method isFull(x) in the tree that chooses t according to x.leaf

18.2-6
searching the key in each iteration now takes time O(lgt)
O(lgt * log(t, n)) = O(lgt * lgn / lgt) = O(lgn)

18.2-7
a search operation at worst case performs O(h) DISK-READ
as the CPU time is just O(lgn), for reasonable n < 2^64, CPU time is negligible compared to disk operations
height of the tree is approximately log(t, n), optimal t then minimizes 
    (a + tb)log(t, n) = (a + tb)ln(n) / ln(t)
    d/dt((a + tb) / lnt) = 0
for a = 5, b = 10, the solution is approximately 3

18.3-1
  ┌Z
┌─└Y
├X
│ ┌V
├─└U
├T
│ ┌S
├─├R
│ └Q
├P
│ ┌O
├─└N
├L
│ ┌K
├─└J
├E
│ ┌C
└─└A
Deleting C
  ┌Z
┌─└Y
├X
│ ┌V
├─└U
├T
│ ┌S
├─├R
│ └Q
├P
│ ┌O
├─└N
├L
│ ┌K
│ ├J
└─├E
  └A
Deleting P
  ┌Z
┌─└Y
├X
│ ┌V
├─└U
├T
│ ┌S
├─└R
├Q
│ ┌O
├─└N
├L
│ ┌K
│ ├J
└─├E
  └A
Deleting V
  ┌Z
┌─└Y
├X
│ ┌U
│ ├T
├─├S
│ └R
├Q
│ ┌O
├─└N
├L
│ ┌K
│ ├J
└─├E
  └A

18.3-2
./CLRS/structure/b-tree.ts#delete

18-1
a.  Θ(n) disk operations, Θ(nm) CPU time
b.  each p push operations requires a disk read
    Θ(n/m) disk operations in total, Θ(n) CPU time
c.  a series of
        PUSH PUSH POP POP PUSH PUSH POP POP ...
    operations at the boundary between two pages
    every two operations incurs a disk access, Θ(n) in total, Θ(nm) CPU times
d.  only read a new page when the pointer is about to cross the boundary and enter a page not in memory
    the page at the opposite end is written to disk and freed from memory
    after each time this pair of read/write disk operations occur
    the pointer is at the boundary between two pages, both of them in memory
    at least m stack operations (m pop or m push) before the next time a disk operation is necessary
    2/m = O(1/m) amortized disk operations, (O(m) + 2O(m)) / m = O(1) amortized CPU time

18-2
a.  search will not change the structure
    insertion may change the height of nodes when:
        a child is splited, the new child z has the same height of y in B-TREE-SPLIT-CHILD
            z.height can be assigned in O(1)
        the tree grows upward, the new root s has height one more than the old root
    deletion may change the height of nodes when:
        the root is removed, no height in subtree is changed
b.  if either tree is empty, simply insert k into another, running time O(h) = O(|h - h'|) = O(1 + |h - h'|)
    insert the lower tree as a subtree of the higher tree
    if h' > h'', insert T'' as a child of the right-most node with height h'' + 1 in T', k as the last key
    if h' < h'', insert T' as a child of the left-most node with height h' + 1 in T'', k as the first key
    if h' = h'', allocate a new node x with sole key k, set x.c = [T1.root, T2.root]
    split child when necessary on the path
    at most max(1, |h' - h''|) nodes visited, spliting child takes O(t) = O(1), O(1 + |h' - h''|) in total
    ./CLRS/structure/b-tree.ts#join
c.  assume at some subtree rooted at x, p descends into a child x.c[i]
    define a new tree T' rooted at x' which has keys x.key[1] to x.key[i-2] and children x.c[1] to x.c[i-1]
    define another tree T'' rooted at x'' which has keys x.key[i+1] to x.key[x.n] and children x.c[i+1] to x.c[x.n+1]
    if any of T' and T'' has no key not a single child, replace root with that child
    either T' or T'' may be empty but not both of them as x has at least two children
    let k' = x.key[i-1], k'' = x.key[i]
    obviously each y' ∈ T' < k' since all keys are distinct, similarly y'' ∈ T'' then y'' > k''
    repeat the process at lower levels, for each key z in x.c[i], y' < k' < z, k < k'' < y''
    either k' or k'' may not exist but no both of them as x has at least one key
    if T' is empty, k' doesn't exist; if T'' is empty, k'' doesn't exist
    at the node x in which k is found, assume x.key[i] = k, split x into two subtree T' and T'' that
    T' contains x.key[1] to x.key[i-1] and x.c[1] to x.c[i]
    T'' contains x.key[i+1] to x.key[x.n] and x.c[i+1] to x.c[x.n+1]
    let T'i and T''i be subtrees splited at ith node x on the path p, similarly k'i and k''i
    the process splits the entirety of a subtree rooted at x into five parts: T', T'', k', k'' and x.c[i]
    a key in T then is
        1.  in T'i for some i
        2.  in T''i for some i
        3.  is k'i for some i
        4.  is k''i for some i 
        5.  is k
    therefore S' = 1 + 2, S'' = 3 + 4
    each subtree T'i or T''i, if not empty, has height equals to or one less than the corresponding x
    the difference in height between T'i and T'i+1 is bounded from above by 2
    consider only the set of non-empty T'i, the sum of differences in height between T'i and T'i+1 <= 2h = O(lgn)
d.  maintain two 2-3-4 tree S' and S'' initially empty
    for each node x on p from k to root,
        split x into T', T'', k', k'' as described in part c
        join (T', k', S') and (S'', k'', T''), assign result to S' and S''
    join operation takes O(1 + |h' - h''|)
    sum of differences between height of T'i and T'i+1 is O(lgn), therefore running time of split operation is O(lgn)
    ./CLRS/structure/b-tree.ts#split

Chapter 19
19.2-1
46
 ┌─23
 │   ┌─39
7┤ 18┴─38──41
 └─21──52
  ┌─30
17┴─26──35

19.3-1
x was once a root
then it's linked to another node y, later one of its children is removed
after zero or more EXTRACT-MIN, y became H.min
the next EXTRACT-MIN added x to the root list
no node in the root list has the same degree as x
CASCADING-CUT will not recurse into a root

19.3-2
each recursive call to CASCADING-CUT moves one child to the root list
at most n-1 recursive calls to CASCADING-CUT may happen before all nodes are roots in the heap
no further calls to FIB-HEAP-DECREASE-KEY will call CASCADING-CUT
m calls to FIB-HEAP-DECREASE-KEY then takes O(m + n) / m amortized time
if m = Ω(n), O(m + n) / m = O(m) / m = O(1)

19.4-1
from a heap H that is a single chain of length n
make H', an exact copy of H, by repeat the construction of H
union H and H', assign the result to H
insert -∞
now H contains three root: the x root of original H, x' the root of H' and -∞
call EXTRACT-MIN(H), -∞ is extracted, x and x' is linked
assume x' became a child of x, denote the other child of x by z
call FIB-HEAP-DELETE(H, z), the sub-heap rooted at z is cutted from x
the heap rooted at x now is a single chain of length n+1
later z removed and the child of z added to the rooted list, which soon linked to x as both have degree 1
repeat this procedure and the heap rooted at x will became a single chain of length n+1
for a fixed n, starting from a singleton heap, repeat the above procedure n-1 times

19.4-2
now yi.degree >= i - k
define z and sd(originally sk) similarly
    size(x) >= sd
            >= 1 + Σ(i = {1 .. d})(syi.degree)
            >= 1 + Σ(i = {1 .. d})(si-k)
            >= k + Σ(i = {k .. d})(si-k)
            = k + Σ(i = {0 .. d - k})(si)
compared to the old series of {s'i} where k = 2:
    s0 = 1 = s'0
    s1 = 2 = s'1
    s[k+1]  >= k + s0 + s1 >= s'3
    s[2k+1] >= k + s0 + s1 + s[k+1] >= s'4
inductively, if s[dk+1] >= s'[d+2]
    s[(d+1)k+1] >= k + s0 + s1 + ... + s[dk+1] >= s'[d+3]
therefore 
    skn = O(φ^(n))
    sn = O(φ^(n/k))
as long as k is a constant, D(n) = O(lgn)

19-1
a.  all parent pointers in the children of x have to be fixed to NIL
    the actual complexity is O(x.degree)
b.  PISANO-DELETE is the same to FIB-HEAP-DELETE except it will not consolidate the root list
    the first stage equivalent to call FIB-HEAP-DECREASE-KEY and decrease x.key to -∞, takes O(c)
    the second stage adds each child of x to root list, takes O(x.degree)
    O(c + x.degree) in total
c.  after the first stage, the change in potential is at most 4 - c
    the second stage adds x.degree and removes one node from root list without changing any mark
    total change in potential at most 4 - c + (x.degree - 1) = 3 - c + x.degree
d.  the potential change will not cover x.degree
    since x.degree can be as large as D(n), amortized complexity of PISANO-DELETE is O(D(n)) = O(lgn)

19-2
a.  1.  |B0| = 1 = 2^0, |Bk| = 2|Bk-1| = 2^k
    2.  height of B0 is 0
        let height of Bk be k constantly, Bk+1 consists of two Bk trees
        one is rooted at the root of Bk+1, another rooted at depth 1
        the height of Bk+1 thereby is k+1
    3.  for B0, C(0, 0) = 0
        assume at depth i there are C(k, i) nodes in Bk
        for Bk+1, there are a single root at depth 0, C(k, k) = 1 node at depth k+1
        every depth i between have C(k, i) + C(k, i-1) = C(k+1, i) nodes
    4.  each subtree in a binomial tree is also a binomial tree
            for B0, the only node is the root
            for Bk+1, each subtree in two Bk is a binomial tree by induction, the new root is the root of Bk+1
        the root of Bk has degree k since:
            B0 has degree 0
            Bk+1 is a Bk tree with a new child, degree of Bk+1 is k+1
        each subtree of Bk is a binomial tree Bk' strictly smaller than Bk, their root has degree k' < k
        assume children of root of Bk are Bi for i = k-1, k-2 .. 0
        Bk+1 adds another Bk as the left-most child of Bk
        children of root of Bk+1 are Bi for i = k, k-1 .. 0
b.  each binomial tree in H has distinct degree k, which means it is Bk with size 2^k
    n = Σ2^ki for distinct ki
    a binary number that has (ki+1)-th least significant bit 1 for all i and other bits 0 equals is b(n)
    since b(n) is unique to n, the existance of Bk can be determined by (k+1)th bit of b(n)
    i <= |b(n)| = [lgn] + 1
c.  most operations on binomial heaps has much resemblance to operations on binary numbers
    ./CLRS/structure/binomial-heap.ts
    most functions are based on an auxiliary function add that combines two root lists
    insert:
        equivalent to incrementing a binary number
        add the root list of the heap with [x]
    union:
        add the root list of the two heaps
    minimum:
        enumerate all roots in the root list, return the one with minimum key
    extractMin:
        find the minimum root
        remove it from the root list
        add its child list with the root list
    decreaseKey:
        the same to binary heap, move the node upward until the heap property is restored
    delete:
        decrease key to -∞ then extractMin
d.  EXTRACT-MIN only removes a root in the root list
    the children of a node is never removed from the structure
    let y1, ... yk denote the children of z in the order in which they were linked to z
        yi.degree = i-1
    let sk denote the number of nodes in a tree of degree k
    s0 = 1 = 2^0 is a constant
    if sk = 2^k is a constant for all k <= d, for k = d+1,
        sk  = 1 + Σ(i = {1 .. k})(syi.degree)
            = 1 + Σ(i = {1 .. k})(si-1)
            = 1 + Σ(i = {1 .. k})(2^(i-1))
            = 2^k
    therefore k = lgn, trees in fibonacci heap is just binomial trees
    the difference is, since CONSOLIDATE is delayed as far as possible
    there may be several trees with the same degree in the root list of a fibonacci tree
e.  McGee heap is exactly binomial heap without DECREASE-KEY and DELETE operations

19-3
a.  delete the node, change the key and insert it back
    one deletion and one insertion, O(lgn) amortized time
    if k <= x.key, may call DECREASE-KEY instead, O(1) amortized time
b.  maintain another pair of left / right pointers in nodes
    use that pair of pointers to maintain a leaf list, which contains leaves in the heap
    H.leaf points to one of the leaves in the leaf list, or NIL if the heap is empty
    a few operations have to be adjusted to maintain the leaf list
    (LINK):
        if a leaf had a child through linking, remove it from the leaf list
    INSERT:
        the new node x is appended to H.leaf
    UNION:
        the leaf lists of the two heap is concatenated
    EXTRACT-MIN:
        CONSOLIDATE using the adjusted LINK instead
    DECREASE-KEY:
        if x is the last child of y, CUT also adds y to leaf list
    DELETE: DECREASE-KEY + EXTRACT-MIN
    add the size of the heap to the potential function
    it's only increased by a constant in INSERT and decreased by a constant in DELETE
    the amortized cost of existing operations will not be affected
    then for x = H.leaf and y = x.parent
    PRUNE repeat
        CUT(x)
        CASCADING-CUT(x, y)
        remove x from the root list
    min(r, H.n) times
    like in DECREASE-KEY, CASCADING-CUT(x, y) will be paid by the potential function
    each iteration of PRUNE takes constant amortized time and decrements n
    all min(r, H.n) iterations then is also paid by the potential function
    PRUNE takes amortized O(1) time in total

19-4
by Theorem 18.1, 2-3-4 heaps have height h = O(lgn)
assume each node has a pointer x.parent points to its parent
a.  compare x.small for all children x of the current node y
    descend into the node with smallest x.small
    height of y decreases each iteration, O(h) = O(lgn) in total
b.  set x.small and x.key to k
    update y.small for all ancestors of x, O(h) = O(lgn) in total
c.  similar to inserting a new minimum in 2-3-4 trees, split full child on the way down to a leaf
    update y.small for y on the path if necessary, O(h) = O(lgn)
d.  update .small for all its ancestors in O(lgn) time
    deletion in 2-3-4 heaps is the bottom-up version of case 3 in B-tree deletion
    delete the leaf, denote y = x.parent, z = y.parent
    if y now has only one child, move one child from its sibling to it
    if all its siblings has only 2 child, merge y with one of them
    now z may have only 1 child, repeat the process up to the root
    if the root has only one child, set the sole child as the new root, delete the old root
    O(h) = O(lgn) in total
e.  MINIMUM -> DELETE
f.  same to 2-3-4 tree join, update x.small
    O(1 + |h - h'|) = O(lgn)

Chapter 20
20.1-1
store linked lists in the underlying array instead of bare integers
deletion will not cause update to summary if the linked list is not empty

20.1-2
store pointers to objects with field key and value in the underlying array instead of bare integers

20.1-3
from root, at each node x:
    if x.key == k, terminate
    if x.key > k, descend into x.left or terminate if x.left == NIL
    if x.key < k, descend into x.right or terminate if x.right == NIL
the procedure will find an x with greatest x.key <= k
assume there is a y that x.key < y.key <= k and the procedure selects x
if x is an ancestor of y, x.right exists and y belongs to the right subtree of x
the procedure will descends into x.right as x.key < k
if y is an ancestor of x, x belongs to the left subtree of y
the procedure must visit y to reach x, but the procedure will not descend into y.left as y.key <= k
otherwise let z be the lowest common ancestor of x and y, x.key <= z.key <= y.key
x and y must be in different subtrees of z or it will not be the lowest common ancestor
x belongs to left subtree, y belongs to right subtree of z
to reach x the procedure must visit z, but then it will not descend into z.left as z.key <= y.key <= k
its successor is the successor of k in the tree

20.1-4
there are u^(1/k) nodes at depth 1, (u^(1/k))^2 = u^(2/k) at depth 2, u^(2^(i-1)/k) at depth i
the bottom level contains u^(2^(i-1)/k) = u nodes, 2^(i-1)/k = 1, 2^(i-1) = k, i = lgk + 1
depth of the bottom level equals height of the top level, h = lgk + 1
MINIMUM:
    search the u^(1/k) nodes of the current node, find the first with nonzero key
    descend that the child and repeat
    u^(1/k) * (lgk + 1) = O(lgk * u^(1/k))
MAXIMUM: symmetric to MINIMUM
SUCCESSOR:
    starting from a leaf x, search y the first right siblings with nonzero key of x
    if no such y exists, go up a level by setting x = x.parent
    otherwise find the minimum in the subtree rooted at y
    O(lgk * u^(1/k))
PREDECESSOR: symmetric to SUCCESSOR

20.2-1
./CLRS/structure/proto-veb-tree.ts#maximum
./CLRS/structure/proto-veb-tree.ts#predecessor

20.2-2
let DELETE procedure report whether after the deletion the cluster become empty
for the base case (u = 2) it sets A[x] = 0 and checks both A[0] and A[1]
for u = 2^(2^k), if deleting low(x) from cluster high(x) reports true, delete high(x) from the summary
if the deletion from summary also reports true, return true as the result
otherwise report false
at most two recursive calls, running time is the same to insertion
    T(u) = 2T(u^(1/2)) + O(1) = O(lgu)

20.2-3
./CLRS/structure/proto-veb-tree.ts#delete
two recursive calls in worst case, running time is the same to insertion
    T(u) = 2T(u^(1/2)) + O(1) = O(lgu)
INSERT should be adjusted so now it reports whether a key is truely inserted or an existing key is overwritten
in the first case .n is incremented
asymptotic running time still O(lgu) since both report and incrementing n takes constant time

20.2-4
each slot is incremented / decremented in insertion and deleteion instead of being set to 1 and 0

20.2-5
./CLRS/structure/proto-veb-tree.ts#ProtoVEBTree
summary now have different structure to the main tree since it stores no satellite data
MEMBER is replaced by SEARCH that return NIL or value stored under the input key

20.2-6
./CLRS/structure/proto-veb-tree.ts#factory
for u = 2 it creates a base case tree ProtoVEBBase
otherwise it creates an internal node ProtoVEBNode
only a super type of the two is exposed

20.2-7
when u = 2, PROTO-vEB-MINIMUM returns NIL iff A is empty, the tree contains no key
for V.u = 2^(2^k), assume proto vEB tree with universe < u, PROTO-vEB-MINIMUM return NIL iff the tree is empty
V.summary is a proto vEB tree with smaller universe
line 9 is only executed when PROTO-vEB-MINIMUM(V.summary) return NIL, which means V.summary is empty
if any of its cluster contains at least one key, the summary will not be empty

20.2-8
there are now u^(3/4) clusters in each tree, and the summary has universe u^(3/4)
high(x)     = [x / u^(1/4)]
low(x)      = x % u^(1/4)
index(h, l) = h * u^(1/4) + l
MEMBER:
    search cluster high(x) for low(x)
    T(n)    = T(n^(1/4)) + O(1)
    T(2^m)  = T(2^(m/4)) + O(1)
    S(m)    = S(m/4) + O(1)
            = Θ(lgm) = Θ(lglgn)
MINIMUM and MAXIMUM:
    at most two recursive calls, one to the summary and one to a cluster
    T(n)    = T(n^(1/4)) + T(n^(3/4)) + O(1)
    S(m)    = S(m/4) + S(3m/4) + O(1)
    by Akra-Bazzi method, (1/4)^p + (3/4)^p = 1, p = 1
    S(m)    = Θ(m(1 + int(1, m, 1/u^2)))
            = Θ(m(1 + 1 - 1/m))
            = Θ(m - 1) = Θ(m)
    T(n)    = S(m) = Θ(lgn)
SUCCESSOR and PREDECESSOR:
    at most two recursive calls and a call to MINIMUM
    T(n)    = T(n^(3/4)) + T(n^(1/4)) + Θ(lgn)
    S(m)    = S(3m/4) + S(m/4) + Θ(m)
    by Akra-Bazzi method, p = 1, g(m) = Θ(m) >= cm
    S(m)    = Ω(m(1 + int(1, m, cu / u^2)))
            = Ω(m(1 + ln(m) - 0)
            = Ω(mlgm)
    similarly S(m) = O(mlgm), S(m) = Θ(mlgm)
    T(n)    = S(m) = Θ(lgnlglgn)
INSERT:
    two recursive calls, similar to MINIMUM, Θ(lgn)
DELETE:
    follow implementation of 20.2-3, two recursive calls, similar to MINIMUM, Θ(lgn)

20.3-1
same to 20.2-4

20.3-2
./CLRS/structure/veb-tree.ts

20.3-3
./CLRS/structure/veb-tree.ts#factory

20.3-4
insertion with duplicate keys will cause exceptions
at line 1-2
    the current tree is empty, no duplicate keys
at line 3-4
    x < V.min, no duplicate keys
at line 6-8
    the cluster that x will go into is empty, no duplicate keys
at line 9
    insertion into a tree with strictly smaller universe
    the only line in the procedure that may be executed when inserting a duplicate key
    the procedure will recursively call line 9 until V.cluster doesn't exist (i.e. the base case)
line 10-11 executes only when x > V.max, does not handle duplicate keys
deletion of non-exist keys will cause trouble when V has a single key V.min and x !== V.min
at line 1-3 the only key in the tree will be deleted, without checking whether it is x
at line 4-8
    V.min != V.max, both 0 and 1 exist in the base case, x may only be 0 or 1
line 9-12 will not execute as x cannot be equal to V.min
at line 13
    deletion from a tree with strictly smaller universe, by induction only cause inconsistency at line 1-3
at line 14-15
    deletion from summary with strictly smaller universe, by induction only cause inconsistency at line 1-3
line 16-22 will not execute as x cannot be equal to V.max
by maintaining another array A of size u, set the bit A[x] when inserting x, reset A[x] when deleting x
A[x] indicates if x is in the tree and can be checked in constant time

20.3-5
MEMBER:
    T(u)    = T(u^(1/k)) + O(1)
    T(2^m)  = T(2^(m/k)) + O(1)
    S(m)    = S(m/k) + O(1) = O(lgm) as k > 1
    T(u)    = S(m) = O(lglgu)
MINIMUM and MAXIMUM:
    still O(1)
SUCCESSOR and PREDECESSOR:
    T(u)    <= max{T(u^(1/k)), T(u^(1 - 1/k))} + O(1)
    for k < 2
    T(u)    <= T(u^(1/k)) + O(1)
            = O(lglgu)
    for k >= 2
    T(u)    <= T(u^(1 - 1/k)) + O(1)
            = T(u^((k-1)/k)) + O(1)
    S(m)    = S(m(k-1)/k) + O(1)
            = O(lgm)
    T(u)    = O(lglgu)
INSERTION:
    similar to SUCCESSOR
    T(u)    <= max{T(u^(1/k)), T(u^(1 - 1/k))} + O(1)
            = O(lglgu)
DELETION:
    similar to INSERTION

20.3-6
(O(u) + nO(lglgu)) / n = O(lglgu)
O(u) / n = O(lglgu)
n = Ω(u/lglgu)

20-1
a.  each internal node allocates an array of size u^(1/2) has u^(1/2) clusters and a summary
    both clusters and summary are a vEB tree with universe u^(1/2)
    thus P(u) = (u^(1/2) + 1)P(u^(1/2)) + Θ(u^(1/2))
b.  assume P(u) <= cu - d
        P(u)    <= (u^(1/2) + 1)(c(u^(1/2)) - d) + Θ(u^(1/2))
                = cu - (d-1)(u^(1/2)) - d + Θ(u^(1/2))
                <= cu - d by ajusting d-1 so it covers Θ(u^(1/2))
        P(u)    = O(u)
c.  ./CLRS/structure/veb-tree.ts#insert
d.  ./CLRS/structure/veb-tree.ts#successor
e.  with universal hashing, accessing a dynamic hash table takes O(1) expected time
    the same analysis in text still applies, but now probabilistic
f.  for u = 2
        insertion into the base case will take O(1)
    for u = 2^k
        insertion either creates a new cluster or not
        if it creates a new cluster, the key will become min of the new cluster
        the new cluster contains no child clusters or summary, takes O(1) space
        the hash table may need to be extended, takes O(1) amortized space
        the summary may need to be updated
        as summary has strictly smaller universe, insertion into summray takes O(1) amortized time
        if it didn't create a new cluster, the insertion consumes no space at all
    inductively insertion will take O(1) amortized space, n insertion will take O(n) worst case space
g.  O(1), each line in CREATE-NEW-RS-vEB-TREE takes constant time (including creation of a dynamic hash table)

20-2
a.  each number and all its prefixes have 1 + 2 + .. lgu = O(lg^2u) characters in total
    each number also allocates a single node in the linked list
    if n numbers are stored, the structure takes at most nO(lg^2u + 1) = O(nlg^2u) space
    O(nlgu) if each string can be stored in constant space, this seems to be the supposed answer
b.  the doubly linked list should only contain actual numbers, not their prefixes
    or there's no way to distinguish a prefix and a number in the hash table
    MINIMUM and MAXIMUM:
        take the first or last node in the circular doubly linked list
    MEMBER:
        assume hashing x takes constant time, check if x is in the hash table takes O(1)
    SUCCESSOR:
        binary search p the longest prefix of x in the hash table that satisfies:
            1.  x = p||0||w
            2.  p||1 exists
        then p is the longest common prefix of x and its successor y since:
            any other number with a different prefix will be < x or > y
            assume there exists some z that x < z < y
            if z = p||1||w', p is still the longest common prefix of x and z
            if z = p||0||w', it must be different at some bit with x
            the first such difference must be z = p'||1||w and x = p'||0||w' since z > x
            then the binary search will find p' instead of p
        //  the structure introduced in this part is actually x-fast trie
        //  but instead of descendant pointer and leaf list this structure maintains a sorted linked list of all nodes
        //  no matter how the nodes are sorted, traversing down to the nearest child may take O(lgu)
        //  if they are sorted numerically, the next node is not even guarenteed to be in the same subtree
        //  if they are sorted lexicographically, traversing down to a leaf may take time O(lgu)
    INSERT:
        find y the predecessor of x in O(lglgu)
        allocate a node for x, insert it after the node for y, O(1)
        insert x and the pointer to its node into the hash table, O(1)
        insert each prefix of x into the hash table, O(lgu) prefixes of x must be inserted
        O(lgu) in total
    DELETE:
        delete the node from the linked list, delete the key from the hash table
        for each prefix p of x from the longest to shortest:
            if x = p||1, search p||0 in the hash table
            if p||0 doesn't exist, delete p as well
        each iteration takes O(1), O(lgu) prefixes may be deleted, O(lgu) in total
c.  there are only O(n/lgu) elements in the x-fast trie which occupies O((n/lgu)lgu) = O(n) space
    each representative corresponds to a O(lgu)-node red black tree which occupies O(lgu) space
    O(n) + (n/lgu)O(lgu) = O(n) space in total
d.  MINIMUM:
        find x the minimum in the x-fast trie
        the actual minimum must be in the binary tree repesented by x, since all other trees has keys greater than x
        as red black tree is balanced, h = O(lglgu), find the minimum in the tree takes O(h) = O(lglgu) time
        O(1) + O(lglgu) = O(lglgu) in total
    MAXIMUM: symmetric
e.  for input x, find its successor s in x-fast trie, or s = x if x is itself a representative
    x must be in the tree represented by s or nowhere in the structure, since
        all other keys in the x-fast trie either < x or > s
        if a representative y < x, all nodes in the tree represented by y contains key <= y < x
        if a representative y > s, all nodes in the tree represented by y > s >= x
    search the successor of x in x-fast trie takes time O(lglgu), search for x takes O(1)
    search x in the tree takes time O(lglgu), O(lglgu) in total
f.  SUCCESSOR:
        again for key x, find its successor s in x-fast trie, find successor ss of s
        the successor of x is either
            the successor of x in tree s, or
            the minimum of tree ss
        all four operations can be done in O(lglgu), O(lglgu) in total
    PREDECESSOR: symmetric
g.  they must update the red black tree of height Ω(lglgu)
    in which a node is always inserted to the bottom level
h.  use 2-3-4-trees instead of red black trees
    split and join trees if their sizes are too big or too small
    by definition and 18-2, a tree, its representative and the next tree can be joined in O(lglgu)
    if a tree contains more than 2lgu nodes, split it around the median results in two trees of size lgu
    (the size of each subtree can be easily maintained in each node to support order statistics operations)
    if a tree contains less than lgu/4 nodes, join it with a neighbour in O(lglgu)
    the resulting tree contains lgu/2 to 9lgu/4 nodes, split it if it contains more than 3lgu/2 nodes
    there will be at most 2n/lgu representatives, x-fast trie operations still O(lglgu)
    each tree has lgu/2 to 2lgu nodes, tree operations still O(lglgu)
    a join or split is only caused by at least lgu/4 insertion or deletion, O(lglgu) / Ω(lgu) = O(1)
    the join and split operation takes O(1) amortized time

Chapter 21
21.1-1
Unioned d and i
{a}
{b}
{c}
{i, d}
{e}
{f}
{g}
{h}
{j}
{k}
Unioned f and k
{a}
{b}
{c}
{i, d}
{e}
{k, f}
{g}
{h}
{j}
Unioned g and i
{a}
{b}
{c}
{i, d, g}
{e}
{k, f}
{h}
{j}
Unioned b and g
{a}
{i, d, g, b}
{c}
{e}
{k, f}
{h}
{j}
Unioned a and h
{h, a}
{i, d, g, b}
{c}
{e}
{k, f}
{j}
Unioned i and j
{h, a}
{i, d, g, b, j}
{c}
{e}
{k, f}
Unioned d and k
{h, a}
{i, d, g, b, j, k, f}
{c}
{e}
Unioned a and e
{h, a, e}
{i, d, g, b, j, k, f}
{c}

21.1-2
if two vertices v and u are in the same connected component
there is a simple path v ~> u in the graph, inductively,
if v ~> u has length 1, there is an edge (v, u)
    line 3-5 of CONNECTED-COMPONENTS calls UNION(v, u) if v and u is not already in the same set
    v and u must be in the same set after CONNECTED-COMPONENTS
if v ~> u has length k, assume it is v ~> w -> u where (w, u) is an edge
    by induction v ~> w has length k-1, v and w will be in the same set after CONNECTED-COMPONENTS
    as (w, u) is an edge, w and u will be in the same set after CONNECTED-COMPONENTS
therefore v and u will be in the same set after CONNECTED-COMPONENTS
for a vertex v, after k iteration of line 3-5 in CONNECTED-COMPONENTS
for each vertices u in the same set of v, there is a path u ~> v of length at most k
for k = 1,
    u may be in the same set with v iff edge (u, v) exists and is chosen in the first iteration
    u ~> v has length 1
inductively,
    each u in the same set with v has a path u ~> v of length at most k after k iterations
    if this iteration adds another vertex w to the set, there is an edge (u, w)
    then w -> u ~> v is a path of length at most k + 1
therefore after |E| iterations, u and v will be in the same set only if there is a path u ~> v
if u and v are not in the same connected component, there is no path u ~> v, u and v are not in the same set
A => B and ~A => ~B, A <=> B

21.1-3
FIND-SET is called twice for each edge, 2|E| in total
within each connected component of size n,
    at beginning there are n disjoint sets in the component
    each call to UNION decreases the number of disjoint sets by 1
    on termination there is only one disjoint set for each connected component
there are exactly n - 1 calls to UNION within each connected component
and no UNION may happen between between components
Σ(i = {1 .. k})(ni - 1) = |V| - k calls to UNION in total

21.2-1
./CLRS/structure/disjoint-set-list.ts

21.2-2
./CLRS/structure/index.ts#problem_21_2_2
set of x2
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}
set of x9
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}

21.2-3
O(nlgn) / n = O(lgn)

21.2-4
n MAKE-SET operations take Θ(n)
each UNION operation is performed with one set having size 1, O(1) for each, O(n) in total
Θ(n) overall running time

21.2-5
each list contains a single pointer to its tail
each list element contains two pointers, one to the set object, one to the previous element or NIL
union(g, e) updates pointers to set object in e by traversing prev pointers from tail to head
then set prev of head to tail of g, set tail of g to tail of e
union is still linear in the size of disjoint set containing e, Theorem 21.1 still applies

21.2-6
union(g, e) update pointers to set object in e by traversing next pointers from head of e to tail
on termination it has a pointer to the tail node in set e
set next of tail to head of g, set head of g to head of e
the list of e is prepended to the list of g
running time still linear in the size of e, Theorem 21.1 still applies

21.3-1
./CLRS/structure/index.ts#problem_21_3_1
set of x2
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}
set of x9
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}

21.3-2
store all ancestors traversed in a stack
./CLRS/structure/disjoint-set-forest.ts#findSet

21.3-3
first n-1 UNION operations form a single binomial tree of degree lgn
the deepest leaf in a binomial tree of degree k has depth k
obey union by rank by always union disjoint trees with same height from height 0
then n FIND-SET operations to the deepest leaf nodes, each traverses a path from leaf to root, Ω(h) = Ω(lgn) each
m = 2n - 1 = Θ(n), total running time is Ω(nlgn) = Ω(mlgn)

21.3-4
thanks https://stackoverflow.com/questions/22945058/
each node stores a new pointer next that originally points to itself after MAKE-SET
next links form a circular linked list of elements in a disjoint set
when linking two root x and y, exchange their next pointers
following x.next now traverses all nodes in disjoint set represented by y first
the last node traversed has its next pointer points to y
following y.next then traverses all nodes in the disjoint set represented by x, go back to x at the end

21.3-5
define r(x) as
    x.rank before the first call to FIND-SET(x)
    0 after the first call to FIND-SET(x)
define the potential function Φ(T) = Σr(x) for all node x in the disjoint set
each call to MAKE-SET(x) takes constant time and does not change the potential, O(1) amortized time
each call to LINK at most increases Φ(T) by 1, takes O(1) amortized time combined with the O(1) actual time
each call to FIND-SET(x) will recurse more than once only if it's the first call to FIND-SET(x)
otherwise x.p will be set to the representative, x.p.p = x.p (assuming x is not linked to other nodes later)
FIND-SET(x) recursively calls FIND-SET at most twice
the number of recursive calls in FIND-SET(x) <= 2 + k
where k is the number of nodes y on the path from x to the representative that FIND-SET(y) is never called before
for each recursive call, denote y = x.p, y.rank >= 1 since rank is an upper limit of height
calling FIND-SET(y) reduces r(x) to 0, Φ(T) decreased by at least 1
by adjusting the constant in Φ(T), each recursive call to FIND-SET is paid by the potential function
all three operations take O(1) amortized time, m operations take O(m) worst case time

21.4-1
if x.p == x, x.p.rank == x.rank
x.p will change only when
    1.  x is a root and UNION(x, y) is called, x.p is set to z the root of y by LINK
        as x is linked as a child of z, x.rank <= z.rank
        if x.rank == z.rank, z.rank is incremented at the end of LINK
        after the call to UNION(x, y), x.p.rank > x.rank
    2.  x.p is not the root, FIND-SET(x) sets x.p to z the root of the tree
        assume it is the first call to FIND-SET(x)
        z.p is set to z, z.rank > x.p.rank > x.rank, the first call to FIND-SET maintains the inequality
        therefore the following call to FIND-SET will also maintain the inequality

21.4-2
a tree with rank 0 has exactly 1 node
assume a tree with rank r has at least 2^r nodes
the rank of the tree grows only if a tree of same rank is linked to it
the resulting tree of rank r+1 has at least 2 * 2^r = 2^(r+1) nodes
by induction, a tree with rank r has at least 2^r nodes
maximum rank in an n-node disjoint set forest thereby is lgn

21.4-3
x.rank <= lgn, ||x.rank|| <= lg(x.rank) + 1 <= lglgn + 1

21.4-4
MAKE-SET:   O(1)
FIND-SET:   traverses a path from a node x to its root z
            as rank is an upper limit of height, the path has length at most z.rank = O(lgn)
UNION:      two calls to FIND-SET and a call to LINK
            2O(lgn) + O(1) = O(lgn)
each operation takes O(lgn) time, O(mlgn) for m operations

21.4-5
a sequence of MAKE-SET and UNION may form a binomial tree of any degree
x.p.rank = x.rank + 1 for any node x in the tree
let x be a leaf, x.rank = 0, x.p.rank = 1
    A2(0) = 2^(0 + 1) * (0 + 1) - 1 = 1 <= x.p.rank
    level(x) >= 2
let y = x.p, y.rank = 1, y.p.rank = 2
    A1(1) = 3 > 2
    level(y) < 1

21.4-6
α'(n) >= 4 only if lg(n+1) > A3(1) = 2047, n+1 > 2^2047, n > 2^2047 - 1
define 
    level'(x)   = max{k | x.p.rank >= Ak(x.rank)}
    iter'(x)    = max{i | x.p.rank >= A(i)(level'(x), x.rank)} 
two bounds in the text can be proved similarly
bound 21.1:
    assume x.rank >= 1, x is not a root
    x.p.rank >= x.rank + 1 = A0(x.rank)
    A(α'(n), x.rank) = >= A(α'(n), 1) >= lg(n + 1) > lg(n) >= x.p.rank
    0 <= level'(x) < α'(n)
bound 21.2:
    x.p.rank >= A(level'(x), x.rank) = A(1)(level'(x), x.rank)
    A(x.rank+1)(level'(x), x.rank) = A(level'(x)+1, x.rank) > x.p.rank
    1 <= iter'(x) <= x.rank
x.rank will no longer change once x.p != x, x.p.rank monotonically increases over time
thereby level'(x) monotonically increases over time
iter'(x) only decreases when level'(x) increases
as long as level'(x) remains unchanged, iter(x) must either increase or remain unchanged
define potential function of node x after q operations:
    φ'q(x)  = α'(n) * x.rank if x is a root or x.rank = 0
            = (α'(n) - level'(x)) * x.rank - iter'(x) otherwise
    Φq(T)   = Σφ'q(x) for all x in the disjoint set
Lemma 21.8, Corollary 21.9 and Lemma 21.10 can be derived from 21.1 and 21.2
Lemma 21.11 still holds since MAKE-SET incurs no change in potential
Lemma 21.12 can be derived from Lemma 21.10 and Corollary 21.9
Lemma 21.13:
    by Lemma 21.10, no node's potential increases
    let x be a node on the find path such that x.rank > 0 and an ancestor y of x has y != x and level'(y) = level'(x)
    at least max(0, s - (α'(n) + 2)) nodes satisfies the condition, where s is the number of nodes on the search path
    i.e. all but the first node (may has rank 0), the root and α'(n) nodes which are the last node with level'(x) = k
    for such a node x, let k = level'(x) = level'(y)
        x.p.rank >= Ak(iter'(x))(x.rank) by definition of iter'(x)
        y.p.rank >= Ak(x.rank) by definition of level'(x)
        y.rank >= x.p.rank
        y.p.rank    >= Ak(y.rank)
                    >= Ak(x.p.rank)
                    >= Ak(Ak(iter'(x))(x.rank))
                    = Ak(iter'(x) + 1)(x.rank)
    let i be the current iter'(x)
    the root has rank >= y.p.rank, x.p.rank >= Ak(i+1)(x.rank) after path compression
    if x.p.rank >= A(k+1, x.rank), level'(x) will increase
    if Ak(i+1)(x.rank) <= x.p.rank < Ak(x.rank), iter'(x) will increase
    either case φq(x) will decrease by at least 1 according to Lemma 21.10
    amortized cost is O(s) - (s - (α'(n) + 2)) = O(α'(n))
Theorem 21.14 follows Lemma 21.7, 21.11, 21.12 and 21.13
overall running time of m operations is O(mα'(n))

21-1
a.  4, 3, 2, 6, 8, 1
b.  invariant:
        at the beginning of iteration i
        Kj doesn't exist iff extracted[j] is already assigned correctly
        Kj contains elements that can only be assigned to kth extraction, k >= l
        and i is the smallest element still in the set before jth extraction if i ∈ Kj
    initialization:
        Kj = Ij, the sequence of insertions between j-1th extraction and jth extraction
        obviously elements in Ij can be assigned to jth minimum or later but not earlier
        if i = 1 ∈ Kj = Ij, a subset of K1 ∪ .. ∪ Kj is in the set before jth extraction, 1 is the smallest
    maintenance:
        if i ∈ Kj, i is the smallest element in the set before jth extraction
        if j = m + 1, i cannot be extracted as a minimum for all m extractions
        if j <= m, i can be assigned to extracted[j] but not any earlier slots and is the smallest still in the set
        assigning extracted[j] = 1 is thereby correct
        let Kl be the next set after Kj that exist
        elements in Kj can be assigned to j and later slots, including l and later slots
        each slot from j to l-1 is already assigned correctly
        elements in Kl cannot be assigned to slots earlier than j and between j and l-1, earlier than l combined
        Kl ∪ Kj therefore contains elements that can only be assigned to kth extractions, k >= l
        each element in {1 .. i} is either assigned to an extraction or ∈ Km+1
        i+1 therefore is the smallest element still can be assigned to an extraction
    termination:
        each assignmenet to extracted was correct
        if the sequence of operations is valid, i.e. there are more insertions than extractions before each extraction
        let extracted[j] be the first slot that's not assigned, by invariant Kj exists on termination
        |I1 ∪ .. ∪ Ij| >= j, if Ij is not empty, in iteration i ∈ Kj extracted[j] will be assigned
        if Ij is empty, each iteration i ∈ I1 ∪ .. ∪ Ij destroys a set among K1 .. Kj
        after j such iterations, Kj must also be destroyed, in contradiction to the assumption
        thus all slots of extracted is assigned and assigned correctly
c.  roots of disjoint sets are maintained in doubly linked list, so next set can be found in O(1), as well as deletion
    disjoint set forest contains list nodes as keys
    after union, delete the root that's linked as a child to the other root from the linked list
    set the key of the node in the new root to be the maximum of the key between two original nodes
    at most O(m + n) UNION and O(m + n) FIND-SET are performed in a disjoint set forest with n nodes
    O(α(n)(m + n)) in total, since m <= n O(α(n) * n) in total
    ./CLRS/structure/offline-minimum.ts

21-2
a.  let n = m/3
    a sequence of m operations that
        starting with n MAKE-SET
        n GRAFT then links all the nodes into a single chain
        n FIND-DEPTH call to the node with maximum depth, which is n - 1
    total running time is Θ(n) + Θ(n) + Θ(n^2) = Θ(n^2) = Θ(m^2)
b.  root node has initial pseudodistance 0
    ./CLRS/structure/depth-determination.ts#constructor
c.  for each node u on the path from v to the root z where u != z
        set u.parent = z
        add the sum of pseudodistance for all nodes but u and z on u ~> z to u.d
    if initially Σ(v ∈ {u .. z})v.d is the depth of u, the new sum of pseudodistance equals to
        u.d + z.d + Σ(v ∈ {u .. z} - {u, z})v.d = Σ(v ∈ {u .. z})v.d
    if any w originally traverses a unique path w ~> u' -> u ~> z to the root
    where u is the deepest ancestor of w on v ~> z
    the sum of pseudodistance on w ~> u' is unchanged
    the path u ~> z is replaced by a single edge u -> z, which has the same sum of pseudodistance
    therefore path compression will not invalidate any sum of pseudodistance
    ./CLRS/structure/depth-determination.ts#findDepth
d.  each node is a tree T is in the same disjoint set S, and S contains no other node outside T since
        MAKE-TREE creates the same S and T with a single node
        FIND-DEPTH will not alter the content of a set
        GRAFT combines two trees as well as two sets, resulting in the same content
    do path compression on r and v, now r.p and v.p are roots in corresponding trees
    first all the nodes in tree r should have their depth increased by the depth of v plus 1
    set v.p.d += FIND-SET(v) will suffice and takes time O(1) since v.p is now the root
    then link two trees without changing any sum of pseudodistance
    if r.p is linked to v.p as a child
        sum of any node in tree v is not changed
        sum of any node in tree r is increased by v.p.d since the path to root includes a new node v.p
        set r.p.d -= v.p.d will restore all the changed sums
    if v.p is linked to r.p as a child, symmetric
    ./CLRS/structure/depth-determination.ts#graft
e.  MAKE-TREE has the same complexity with MAKE-SET
    FIND-DEPTH has the same complexity with FIND-SET
    GRAFT has the same complexity with UNION
    both path compression and union by rank is performed in the same way
    apply Theorem 21.14, m operations in which n are MAKE-TREEs takes time O(mα(n))

21-3
a.  from LCA(T.root), the procedure recursively calls LCA(x) for each node x and color x black in the tree once
    (must be a call to LCA(x) as there is a path from T.root to x; must be the only call as it is the only path)
    line 10 will be executed for a pair {u, v} only when
        u or v is just colored black, the current call is LCA(u) or LCA(v)
        another node is already black
    if u is colored black before v, line 10 will execute during LCA(v) but not LCA(u) and vice versa
    exactly one of the two calls to LCA execute line 10 for a particular pair {u, v}
b.  induct on tree height:
        after line 7 of LCA(u), the disjoint set of u contains exactly all descendants of u
    for h = 0:
        line 3-6 will not execute as u has no child
        set of u contains u itself, its only descendant
    for h = k:
        each recursive call to LCA(v) is on a subtree with height <= k-1
        union(u, v) will union u with all of v's descendants
        doing this for all children of u and the set of u now contains exactly all descendants of u
    since LCA(u) is called once for each u
    if LCA(u) is not executed yet, none of its descendants will be in any disjoint set
    similarly, when LCA(v) is called, all of its siblings and their descendants are either
        in the same disjoint set with v.p, or
        not in any disjoint set
    also when LCA(u) has not returned, LCA(u.p) is stuck on line 4, u and u.p cannot be in the same set
    therefore if LCA(u) is just about to execute line 4 and u has depth k
    execution of LCA(u.p) is on line 4
    all siblings and their descendants are either in the same set with u.p or not in any set
    u and u.p are not in the same set, no descendant of u is in any disjoint set
    the subtree rooted at u.p is divied into 3 groups, 2 of them are represented by a disjoint set
    similarly the execution of u.p.p is on line 4, the subtree rooted at u.p.p is 5 groups and 3 disjoint set
    accounting all the way to T.root, there are currently k+1 disjoint sets in the structure
    k disjoint sets if LCA(u) is just about to execute line 1
c.  if u is an ancestor of v,
        u will not be colored black before all its descendants are colored black
        on line 7 of LCA(u), u is colored black, FIND-SET(u).ancestor = u
        line 10 will print u as the LCA of u and v
    if v is an ancestor of u, symmetric
    otherwise,
        let w be the LCA of u and v, u and v are in different subtrees of w
        assume w has child x and y, x is an ancestor of u, y is an ancestor of v
        upon calling LCA(w) none of u and v are black, after return of LCA(w) both u and v are black
        without lose of generality assume LCA(x) is called before LCA(y) on line 4 of LCA(w)
        LCA(u) will not print LCA of {u, v} since v is still white
        when LCA(x) returned, descendants of x including u is unioned with set w, FIND-SET(u) = w
        during the execution of LCA(y), only descendants of y are joined with each other
        FIND-SET(u).ancestor is not modified
        therefore during LCA(v), v is colored black, line 10 prints FIND-SET(u) = w, the LCA of u and v
    LCA of each pair is printed once and correctly
d.  LCA(u) is called once for each node
    each call to LCA(x)
        performs one MAKE-SET, one UNION and two FIND-SET, O(α(n)) times in total
        traverses P in time O(|P|)
    overall running time is O(n(|P| + α(n)))

Chapter 22
22.1-1
the out degree of a vertex v can be computed in time proportional to its out degree by traversing Adj[v]
or if the adjacency lists explicitly maintains their own length, O(1)
the in degree of a vertex v however may only be computed by traversing all adjacency lists, O(E) in total

22.1-2
assume only directed edges from parent to children
adjacency list:
    1:  2 -> 3
    2:  4 -> 5
    3:  6 -> 7
    4, 5, 6, 7: no edge
adjacency matrix:
    0 1 1 0 0 0 0
    0 0 0 1 1 0 0
    0 0 0 0 0 1 1
    0 0 0 0 0 0 0
    0 0 0 0 0 0 0
    0 0 0 0 0 0 0
    0 0 0 0 0 0 0

22.1-3
adjacency list:
    an obvious lower bound is Ω(V + E) as a new graph must be created
    copy vertices to the new graph
    let Adj' be adjacency lists of the transpose graph
    traverse Adj[v] for each v, for each u in Adj[v], insert v to Adj'[u]
    Θ(V + E) in total
adjacency matrix:
    an obvious lower bound is Ω(V^2) as a new graph must be created
    copy vertices to the new graph
    allocate an |V| x |V| matrix A', assign each aij to a'ji
    Θ(V^2) in total

22.1-4
assume the original graph is undirected
allocate an array A of size |V|, indexed by vertices in the graph
for each u, for each v in Adj[u]
    set A[u] = u
    if A[v] != u, add (u, v) to the new graph, set A[v] = u
for each Adj[u], only the first (u, v) will be inserted into the new graph
all later (u, v) will find A[v] = u and is not inserted into the new graph
especially (u, u) will not be in the new graph since A[u] = u at the beginning
for the next list Adj[u'], all entries in A will not contain value u'

22.1-5
adjacency list:
    let Adj' be a new adjacency list of graph G^2
    for each u in G, for each v in Adj[u],
        add v to Adj'[u]
        concat Adj[v] to Adj'[u]
    dedup but keep self loops using procedure described in 22.1-4
    an edge added to Adj'[u] iff
        an edge in G
        for some u, v and w, v ∈ Adj[u] and w ∈ Adj[v], there is a path of length 2 u ~> w
    |E| concatenations at most, each copies |E| vertices at most, O(V + E^2) in total
    G^2 before dedup contains at most |E|^2 edges, as dedup takes time O(V + E), O(V + E^2) in total
adjacency matrix:
    basically discrete time Markov chain
    compute A^2, the square of the adjacency matrix, then set every nonzero entry to 1
    time complexity is O(f(V)), where f(V) is the complexity of some efficient matrix multiplication algorithm
    f(V) can be V^3 using plain triple loops, or V^lg7 using strassen's method

22.1-6
thanks Instructor's Manual
for an entry aij, i != j,
    if aij = 1, vertex i cannot be an universal sink as its out degree >= 1
    if aij = 0, vertex j cannot be an universal sink as its in degree < |V| - 1
also there may be only one universal sink in a graph:
    two universal sinks has no edges to each other, both of them are not universal sinks
thereby examining aij will always eliminate a potential universal sink if i != j
maintain a stack S initially contains all vertices in the graph by |V| insertions
repeat the following process:
if i is the last element in the stack:
    test if i is a universal sink by examining row i and column i
otherwise:
    pop two vertices i and j from the stack
    examine aij
    if aij = 1, discard vertex i and push j back to S
    if aij = 0, discard vertex j and push i back to S
each iteration O(1), O(V) in total

22.1-7
BB'(i, j)   = Σ(bik * b'kj)
            = Σ(bik * bjk)
if bik * bjk = 0, edge k does not incident with i or j
if bik * bjk = 1, edge k enters both i and j or leaves both i and j
    which is impossible unless i = j
if bik * bjk = -1, edge k is (i, j) or (j, i)
therefore for i != j, BB'(i, j) is the number of edges between i and j
for i = j, bik^2 = 0 or 1, BB'(i, i) is the number of edges incident to i

22.1-8
O(1), for an edge (u, v), search v in hash table Adj[u] takes O(1) expected time
the edge list may no longer be traversed in time O(E)
use balanced trees for each Adj[u], searching now O(lg(min(V, E))), slower than hash tables
the edges in a graph still can be traversed in time O(E)
insertion now takes time O(lg(min(V, E)))

22.2-1
1.d = Infinity, 1.π = NIL
2.d = 3, 2.π = 4
3.d = 0, 3.π = NIL
4.d = 2, 4.π = 5
5.d = 1, 5.π = 3
6.d = 1, 6.π = 3

22.2-2
r.d = 4, r.π = s
s.d = 3, s.π = w
t.d = 1, t.π = u
u.d = 0, u.π = NIL
v.d = 5, v.π = r
w.d = 2, w.π = t
x.d = 1, x.π = u
y.d = 1, y.π = u

22.2-3
the procedure never tests whether a vertex is GRAY or BLACK, only WHITE or not WHITE
therefore a single bit indicating whether the node is WHITE or not will suffice

22.2-4
initialization still takes O(V)
each vertex is enqueued once, all of its edges is examined, which is a row in the adjacency matrix
O(V^2) in total

22.2-5
by Theorem 22.5, after calling BFS(G, s), u.d = δ(s, d)
the proof of Theorem 22.5 is independent of the order in adjacent lists, so is u.d
if x precedes t in Adj[w], x will be enqueued before t in (c)
(e) will paint x BLACK first and set u.π = x
the resulting tree contains (x, u) instead of (t, u)

22.2-6
define a graph G that
(s, u), (s, v) are the only edges of s
there are a set of vertices W that for all w ∈ W, (u, w) and (v, w) exists
in bfs, when s is dequeued, both u and v are gray
either u or v a dequeued next, all the vertices in W will have w.π = u or w.π = v
any breath-first tree that set more than one w.π = u and more than one w.π = v cannot possibly be produced by bfs

22.2-7
different wording for bipartite graphs
do bfs on the graph, make each vertex with odd distance to s a heel, even distance a babyface, O(V + E) + O(V)
check if any edge (u, v) connects two vertices of the same characteristic, O(E)
if there is an edge, the sum of the distance δ(s, u) and δ(s, v) is even
s ~> u -> v ~> s is an odd cycle, the graph cannot be bipartite
otherwise the result is a valid solution
O(V + E) + O(V) + O(E) = O(V + E)

22.2-8
each vertex in a tree has a single parent
thereby a path in a tree, once descends into a child of some node s, can no longer go up
the path can only go up, make a turn at some vertex s, and go all the way down
let p be the longest shortest path in a tree, s be the highest vertex on that path
p starting from some vertex in a subtree of s, go all the way up to s and descends into another subtree of s
let h1, h2 be the height of two highest subtree of s, then
    |p| <= h1 + h2 + 2
obviously since p goes up in a subtree and goes down in another
there is a path of length h in a tree of height h, hence the upper bound h1 + h2 + 2 is achievable
the height of trees can be computed recursively in time O(E) = O(V)
the first and second highest can be selected by order statistics algorithms in time O(e) in a vertex with e subtrees
O(E) = O(V) in total

22.2-9
single-source dfs
use face / tail of coins as indicator of color of vertices in DFS
(GRAY and BLACK make no difference in the procedure, can be merged)

22.3-1
directed:
    WHITE -> WHITE: both i and j not visited yet, (i, j) can be any edge type
    WHITE -> GRAY:  any time between j.d and j.f
                    j.d > i.d, j.f < i.f, j is a descendant of i in dfs tree, (i, j) is back edge
    WHITE -> BLACK: after j.f, j.f < i.d, i and j may be in different subtree of an ancestor or in different tree
                    (i, j) is a cross edge
    GRAY -> WHITE:  at any time between i.d and i.f, by white path theorem, j is a descendant of i
                    depends on whether j is visited directly from i or from some descendant of i
                    (i, j) is a tree or forward edge
    GRAY -> GRAY:   after i.d and j.d, before i.f and j.f
                    either i is a descendant of j or j is a descendant of i, (i, j) tree, back or forward edge
    GRAY -> BLACK:  after i.d and j.f, before i.f, i.d < j.d or j.d > i.d
                    either j is a descendant of i or they are in different trees, (i, j) tree, forward or cross edge
    BLACK -> WHITE: cannot happen, i is only painted black after all vertices in Adj[i] are visited
    BLACK -> GRAY:  after i.f and j.d, j.d < i.d otherwise j is painted black before i
                    (i, j) a back edge
    BLACK -> BLACK: after i.f and j.f, any edge type
undirected:
    WHITE -> WHITE: before i.d and j.d, any edge type
    WHITE -> GRAY:  between j.d and j.f, i must be a descendant of j by white path theorem
                    if i is visited directly from j, (j, i) is a tree edge, otherwise (i, j) is a back edge
    WHITE -> BLACK: i must be visited before j, otherwise i will be painted black before j
                    but then j cannot be black when i is still white, impossible case
    GRAY -> GRAY:   after i.d and j.d, before i.f and j.f
                    if i is visited before j and j is visited directly from i, (i, j) is a tree edge
                    if j is not visited directly from i, (j, i) is a back edge
                    similarly when j is visited first
    GRAY -> BLACK:  after i.d and j.f, before i.f
                    j cannot be visited before i or i will be painted black before j
                    on time i.d, j is white, reduced to case GRAY -> WHITE, which is symmetric to WHITE -> GRAY
    BLACK -> BLACK: after i.f and j.f, any edge type
    other cases symmetric

22.3-2
q.color = BLACK, q.d = 1, q.f = 16, q.π = NIL
r.color = BLACK, r.d = 17, r.f = 20, r.π = NIL
s.color = BLACK, s.d = 2, s.f = 7, s.π = q
t.color = BLACK, t.d = 8, t.f = 15, t.π = q
u.color = BLACK, u.d = 18, u.f = 19, u.π = r
v.color = BLACK, v.d = 3, v.f = 6, v.π = s
w.color = BLACK, w.d = 4, w.f = 5, w.π = v
x.color = BLACK, x.d = 9, x.f = 12, x.π = t
y.color = BLACK, y.d = 13, y.f = 14, y.π = t
z.color = BLACK, z.d = 10, z.f = 11, z.π = x
(q, s) is a tree edge
(q, t) is a tree edge
(q, w) is a forward edge
(r, u) is a tree edge
(r, y) is a cross edge
(s, v) is a tree edge
(t, x) is a tree edge
(t, y) is a tree edge
(u, y) is a cross edge
(v, w) is a tree edge
(w, s) is a back edge
(x, z) is a tree edge
(y, q) is a back edge
(z, x) is a back edge

22.3-3
(u(v(y(xx)y)v)u)(w(zz)w)

22.3-4
same to 22.2-3
the procedure never tests whether a vertex is GRAY or BLACK, only WHITE or not WHITE
therefore a single bit indicating whether the node is WHITE or not will suffice

22.3-5
a.  if (u, v) is a tree or forward edge, v is a descendant of u
b.  if (u, v) is a back edge, u is a descendant of v
c.  if (u, v) is a cross edge, neither u nor v is the descendant of the other 
apply Theorem 22.7

22.3-6
consider a simple graph with vertices u, v and a single edge (u, v)
if u is visited first, (u, v) is a tree edge, (v, u) is a back edge
if v is visited first, (v, u) is a tree edge, (u, v) is a back edge
classify by ordering, both case (u, v) and (v, u) is a tree edge
classify by which is encountered first, (u, v) may be a back edge

22.3-7
./CLRS/graph/directed-graph.ts#stackVisit

22.3-8
define a graph with vertices u, v, w and edge (w, u), (u, w), (w, v)
there is a path u ~> v following (u, w) then (w, v)
if w is visited first in dfs, both u and v become descendant of w but not descendant of each other
if u appears before v in Adj[w], u.d < v.d

22.3-9
the same graph in 22.3-8
u.d = 2, u.f = 3, v.d = 4

22.3-10
./CLRS/graph/directed-graph.ts#visit or stackVisit
if G is undirected, an edge should be updated to back only if its type is not defined yet
hence an edge that's both tree and back will have resulting type tree

22.3-11
define a graph with vertices u, v, w and edge (v, u), (u, w)
u has both incoming and outgoing edges
assume dfs first visits w, the df tree of w contains only w
then dfs visits u, u cannot visit w since its already black, u cannot visit v since (u, v) doesn't exist
u will end up in a df tree with a single vertex u

22.3-12
if G1 is a connected component in G, all vertices are connected to each other
at the beginning of dfs, there is a white path from each vertex in G1 to any other
thereby calling DFS-VISIT(u) for any u in G1 will make every vertex in G1 a descendant of u
if v is in a different connected component G2, there is no path from u to v and no white path at the beginning
as vertices are never colored white after initialization, there's no white path from u to v at any time
v is not a descendant of u
df trees after a dfs is one-one corresponding to connected components in undirected graph
initialize a global variable cc = 0, increment it every time DFS-VISIT is called from DFS

22.3-13
thanks https://pdfs.semanticscholar.org/c85f/ddc73cf467827956078893f8edb5f183981c.pdf
if there are more than one simple path u ~> v in a graph G, consider the dfs starting at u and the resulting df tree T
by white path theorem there is a path p1 from u to v in T
let p2 be another simple path u ~> v in G
let w be the last common vertex on the two paths traversing backwards from v to u
there must be two edges (x, w) and (y, w) that
    x != y
    p1 = u ~> x -> w ~> v
    p2 = u ~> y -> w ~> v
    (x, w) not in p2 and (y, w) not in p1
T is a tree, (y, w) is not in T, (y, w) is a back edge, a forward edge or a cross edge
(y, w) may only be a back edge if y is a descendant of w and x, let y' be the the vertex precedes y on u ~> y
if y' is also on u ~> x, (y', y) is not in T, furthermore (y', y) must be a forward edge since y' is an ancestor of w
if y' is not on u ~> x, (y', y) may only be a tree or back edge if 
    (y', y) is on the path w ~> y, or
    y' is a descendant of y
both case y' is also a descendant of w, let y'' be the vertex precedes y' on u ~> y and repeat the argument
the propagation cannot be repeated forever since p1 and p2 starts both at u
so there must be a cross or forward edge in T if there are more than one simple path u ~> v in G
conversely, in a df tree rooted at u produced by a dfs on G
    if there is a forward edge (x, w), there are at least two paths u ~> w in the tree
        one follows tree edges, another skips from x to w
    if there is a cross edge (x, w) within the tree, similarly there are at least two paths u ~> w
therefore G is singly connected iff dfs starting at all vertices contain no forward or cross edge
O(V^2 + VE)
./CLRS/graph/directed-graph.ts#singlyConnected

22.4-1
p -> n -> o -> s -> m -> r -> y -> v -> x -> w -> z -> u -> q -> t

22.4-2
a simple path from a vertex u to t must start with an edge (u, v) for some vertex v in Adj[u]
then the remaining path must be a simple path from v to t
assume the number of paths from each vertex v that appears later than u when topologically sorted is already computed
let p[v] be the result, p[u] = Σ(v ∈ Adj[u])p[v], all p[v] is already computed as (u, v) exists
topologically sort the vertices, O(V + E)
assign 0 to p[u] if u appears before s or after t
assign 1 to p[t]
compute p[u] according to the equation above for all vertices between s and t, in reverse topological order
O(E) access to array p, O(V + E) in total
./CLRS/graph/directed-graph.ts#numberOfPaths

22.4-3
count the number of edges in the graph, immediately report cyclic when |E| execeeds 2|V| - 2
if |E| <= 2|V| - 2, run dfs and search for a back edge
an acyclic graph can have at most |V| - 1 edges, 2|V| - 2 including the reverse of each
dfs will finish in O(V + E) = O(V) time given |E| = O(V)

22.4-4
consider a graph with vertices u, v and 3 edges, two are (u, v) and one is (v, u)
dfs starting at u will produce topological sort [u, v], gives one bad edge
dfs starting at v will produce [v, u], gives two bad edges
therefore topological sort will not always minimizes number of bad edges

22.4-5
traverse each entry of Adj, count the in degree for each vertex in d[v], O(E)
find all vertices u with in degree 0 and push them onto a stack, O(V)
repeat |V| times:
    pop a vertex u from the stack, append it to the sorted list
    decrement d[v] for each v in Adj[u], O(E) aggregated
    if d[v] becomes 0, push v onto the stack, O(E) aggregated
if the graph contains a cycle, there may not be a vertex with in degree 0, the procedure will stuck
./CLRS/graph/directed-graph.ts#alterTopologicalSort

22.5-1
if the new edge connects two vertices in the same SCC, the number of SCC won't change
if the new edge connects two distinct SCCs C and C',
    if there is already a path from C to C', the new egde will not change the number of SCC
    if otherwise the edge forms the first path between C and C', it may reduce the number of SCCs arbitrarily
    if the SCC graph is still acyclic, the number of SCCs won't change
    if there is a path from C' to C, the number of SCCs is reduced by at least 1
    if the SCC graph becomes a simple cycle after adding the edge, the number of SCCs is reduced to 1

22.5-2
in G:
    q.f = 16
    r.f = 20
    s.f = 7
    t.f = 15
    u.f = 19
    v.f = 6
    w.f = 5
    x.f = 12
    y.f = 14
    z.f = 11
in transpose of G:
    vertex r: 1/2, π = NIL, cc = 0
    vertex u: 3/4, π = NIL, cc = 1
    vertex q: 5/10, π = NIL, cc = 2
    vertex t: 7/8, π = y, cc = 2
    vertex y: 6/9, π = q, cc = 2
    vertex x: 11/14, π = NIL, cc = 3
    vertex z: 12/13, π = x, cc = 3
    vertex s: 15/20, π = NIL, cc = 4
    vertex v: 17/18, π = w, cc = 4
    vertex w: 16/19, π = s, cc = 4

22.5-3
f(C) > f(C') for each SCCs where there is an edge (u, v) from C to C'
but f(C) only bounds the greatest finish time in C, not the just any finish time
a vertex in C may have earlier finish time than a vertex in C'
define a graph G with vertices u, v, w and edges (u, v), (v, u), (u, w)
G contains two SCCs, {u, v} and {w}
dfs may set v.f = 3, w.f = 5, u.f = 6
another dfs starting from v will produce a single df

22.5-4
SCCs in G and GT are the same, SCC(G) and SCC(T(G)) have the same set of vertices
for each edge (x, y) connecting two SCCs C and C' in G, there is an edge (y, x) in T(G)
therefore T(SCC(G)) = SCC(T(G)), SCC(G) = T(SCC(T(G)))

22.5-5
strongly connected components can be computed in time O(V + E)
compute the number of SCCs in the graph by counting maximum u.cc for all vertex 
initialize a list for each SCC, at most |V| lists since number of SCCs is bounded by |V|
preprocess vertices in each SCC into a list, O(V)
repeat for all such stacks:
    repeat for each u in the stack:
        repeat for all v in Adj[u]:
            if connected[v.cc] == true or u.cc == v.cc, skip this vertex
            otherwise create an edge between components indicated by u.cc and v.cc, set connected[v.cc] to true
            O(E) aggregated
    repeat for each u in the stack:
        repeat for all v in Adj[u]:
            set connected[v.cc] to false, effectively resets the array connected, O(E) aggregated
./CLRS/graph/directed-graph.ts#componentGraph
    
22.5-6
in a SCC of n vertices {u1 .. un}, there is a path u1 ~> u2, u2 ~> u3 .. un-1 ~> un, un ~> n1
which means there is a cycle connecting all vertices in the SCC 
the shortest such cycle is a simple cycle with n edges, SCC with n vertices has at least n edges
the lower bound can be achieved by a simple cycle
the algorithm is similar to 22.5-5
connect vertices in each list to a simple cycle
create edge (u, v) instead (u.cc, v.cc) in the loop

22.5-7
for each vertex u in the graph:
    do single source dfs from u in G and T(G)
    the vertices visited in the two dfs combined must be all the vertices in G

22-1
a.  1.  if (u, v) is a back edge, either v is the parent of u in bf tree, or v is a proper ancestor of u.π
        if v = u.π, (u, v) is a tree edge
        otherwise the path s ~> v -> u is shorter than following only tree edges, in contradiction to Theorem 22.5
        similarly forward edges (which is back edge at the same time)
    2.  v is visited the first time from (u, v), line 15 of BFS will set v.d = u.d + 1
    3.  assume without loss of generality u.d <= v.d
        if otherwise u.d <= v.d + 2, a path s ~> u -> v will have length u.d + 1 < v.d, in contradiction to 22.5
b.  1.  similar to a.1
    2.  same to a.2
    3.  if otherwise v.d >= u.d + 2, s ~> u -> v has length shorter than v.d, in contradiction to Theorem 22.5
    4.  since v is an ancestor of u, the path s ~> v ~> u is the shortest path from s to u, 0 <= v.d <= u.d

22-2
a.  if u is an articulation point, it connects two parts C and C' of G
    after removing u, there is no path from any vertex in C to any vertex in C'
    each path from C to C' must visit u
    a dfs starting from u
        if it visits a vertex in C next, there are no white path from C to C' since u is already gray
        vertex in C' cannot be descendants of any vertex in C, they must be in different subtree of u
        if it visits a vertex in C' next, similar
        if it visits a vertex v not in C or C', there cannot be white path from v to C and C' at the same time
        otherwise C and C' are connected
        the visit starting from v cannot visit all the vertices in both C and C'
        as u has a white path to each vertex in C and C' at u.d, u must have another child other than v
    conversely, if u has two or more children, let x be the first child visited from u, let y be another child
    there are no white path at x.d from x to y, otherwise y will be in the same tree with x
    the only vertex that isn't white at x.d is u, by removing u there will be no path x ~> y, G is disconnected
b.  by Theorem 22.10, there's no forward and cross edge in Gπ
    if a non-root vertex v in Gπ has a child s that there is no back edge from descendant of s to proper ancestor of v
    the only edges in the subtree rooted at s are tree edges and back edges within the subtree
    assume there is a path from a vertex in the subtree to a vertex outside the subtree that does not visit v
    there must be an edge (x, y) connecting x in the subtree and y != v not in the subtree
    such a y must exist since v and all descendants are non-root, the root must be some other vertex
    if y is an ancestor of v, (x, y) is a back edge to a proper ancestor of v
    if y is not an ancestor of v, (x, y) is a cross edge, in contradiction to Theorem 22.10
    if v is removed, there's no path from the subtree to any other vertices in the graph, v is an articulation point
    conversely, assume v is an non-root articulation point
    v connects at least two components C and C' in G that all paths from C to C' (or any other components) visits v
    if the root is not in C', it must visit v before visiting any vertices in C'
    all vertices in C' will be descendants of v in Gπ, let s be the first vertex in C' visited from v
    at s.d, there is a white path from s to all vertices in C', all vertices in C' will be descendants of s
    there cannot be cross edges from C' to other components connected to v by the assumption
    there cannot be back edges from C' to proper ancestors of v since the ancestor must be in another component
    when the root is in C', symmetric argument applies to C
c.  first compute v.back as min{w.d | (v, w) is a back edge}, v.back = +∞ if there's no back edge in Adj[v], O(E)
    by traversing Gπ from root to leaf, v.low can be computed recursively as 
        min{v.d, s.back, s.low}
    for all children s of v in Gπ, O(Eπ) = O(V)
    since the graph G is connected, |V| <= |E| - 1, V = O(E), O(E) in total
d.  there's no cross edge in undirected graph
    if s a descendant of u has an egde (s, w) and w.d < s.d, w must be a proper ancestor of u, u.low < u.d
    if no such s exists, all edges involving vertices in subtrees of u must be tree edges or back edges to up to u
    u.low >= u.d 
    so an articulation point in G is
        the root if it has more than one child, or
        any non-root vertex v with v.low < v.d
    a check for all vertices can be performed in time O(V + E) = O(E)
e.  if edge (u, v) is on a simple cycle, removing (u, v) and there is still a path from u ~> v
    any path originally passes (u, v) may traverse that path instead, (u, v) is not a bridge
    conversely if (u, v) is not a bridge, removing (u, v) will not disconnect the graph 
    which means there is another path u ~> v that does not pass (u, v), u ~> v -> u is a cycle
    therefore (u, v) is not a bridge iff (u, v) is on a simple cycle
    (u, v) is a bridge iff (u, v) is not on any simple cycle
f.  reuse the v.low and v.back attributes computed in part c
    back edges always form a simple cycle, back edges cannot be bridges
    for a tree edge (u, v), compare u.d with min{v,low, v.back}
    if min{v.low, v.back} <= u.d, there is a back edge from subtree rooted at v to an ancestor of u (including u)
    u is on a simple cycle
    if min{v.low, v.back} > u.d there is no back edge from subtree roote at v to an ancestor of u
    if (u, v) is on a simple cycle, there must be some edge (x, y) connecting y in the subtree and x not in the subtree
    if x is an ancestor of u, (x, y) cannot be a forward edge according to Theorem 22.10
    (x, y) must be a back edge (y, x), but then min{v.low, v.back} <= u.d
    if x is not an ancestor of u, (x, y) is a cross edge, in contradiction to Theorem 22.10
    therefore u is on a simple cycle iff min{v.low, v.back} <= u.d
g.  let C and C' be two different BCCs, f ∈ C, g ∈ C'
    if there is an edge e in both C and C', there is a cycle passing f and g
    have no idea how to reduce the cycle to a simple cycle
h.  articulation points seperates biconnected components
    each path connecting two BCCs must visit an articulation point
    assume two BCCs C and C' has a path that does not visit any articulation point
    removing any vertex on the path will not disconnect the graph
    there must be another path from C to C' that does not share any vertex with the last path
    there must be a simple cycle that passes an edge in C and another edge in C', C and C' are the same BCC
    do dfs on the graph with additional restrictions that:
        dfs cannot start at articulation points
        a search may visit a articulation point, but not any other vertices connected to it
        (i.e. articulation points are initially grey and never painted black)
    and assign bcc for each df tree

22-3
a.  the Euler tour will have to traverse each edge entering v and leaving v
    each time it enters v, it must leave v from an edge in Adj[v] that not yet traversed
    the problem is then reduced to a Euler tour problem with both in-degree(v) and out-degree(v) decremented
    thereby if in-degree(v) != out-degree(v), at some time either:
        there are edges from v but no edge to v, or 
        there are edges to v but no edge from v
    both case there's no possible Euler tour for such a graph
    conversely, if in-degree(v) = out-degree(v) for each v in G
    thanks http://sites.math.rutgers.edu/~ajl213/CLRS/Ch22.pdf
    for a fixed set of vertices V
    let in-degree(u) + 1 = out-degree(u), out-degree(v) + 1 = in-degree(v)
    all other vertices has the same in and out degrees
    for |E| = 1, there is a single edge (u, v), u -> v is an Euler path
    for |E| = k, assume all such graphs with |E| < k edges has an Euler path
    then for all |E| <= k, if in-degree(v) = out-degree(v) for all v in V, there is an Euler tour in G:
        by removing any edge (u, v), |E| < k, there is an Euler path v ~> u in the resulting graph
        adding (u, v) to the path forms an Euler tour
    let (u, w) be any edge leaving u, remove it from the graph, the resulting graph either
        if w = v, there is an Euler tour starting and ending at v
        if w != v, |E| < k, in-degree(w) + 1 = out-degree(w) now, there is an Euler path w ~> v
    both case adding (u, w) to the path or tour forms an Euler path
b.  follow the inductive steps described in part a
    if there is an Euler tour in the graph
    it can be produced from choosing any unvisited leaving edge from the current vertex
    starting from any vertex v in V, follow and remove the first edge in Adj[v] until there are no edges remaining
    O(E) in total

22-4
lemma 1: reachability of a dag can be computed in O(V + E)
    assume min(v) is computed for all v in Adj[u]
    each path from u ~> w must pass (u, v) for some v in Adj[u], R(u) = ∪R(v) ∪ {u}
    min(u) = min{min(v), L(u)} for all v in Adj[u]
    if G is a dag, by topological sorting V and compute min(u) from the last to the first in sorted order
    min(u) can be computed in O(V + E) time
lemma 2: vertices u and v in the same SCC has min(u) = min(v)
    since there is a path u ~> v, R(v) ⊆ R(u), similarly R(u) ⊆ R(v), so R(u) = R(v), min(u) = min(v)
therefore min(u) can be computed by:
    1.  decompose G to SCCs, O(V + E)
    2.  compute minimal L(u) within each SCC, assign it as L(c) for the component, O(V)
    3.  compute min(c) for each SCC c in the component graph, which is a dag, O(V + E)

Chapter 23
23.1-1
assume T is a minimum spanning tree and (u, v) is not in T
connecting (u, v) will form a cycle with the unique path v ~> u
choose any (x, y) != (u, v) on the cycle and remove (x, y), x and y still connected
each path originally passes (x, y) can now traverse the path formed by (u, v) instead, graph still connected 
a connected graph with |V| - 1 edges must be a tree, the resulting graph is a spanning tree T'
as w(u, v) <= w(x, y), w(T') <= w(T), T' is a minimum spanning tree

23.1-2
take Figure 23.1 for an example
define S = {d, e, f}, (S, V - S) is a cut respecting the single edge (d, e), which is in the minimum spanning tree
(c, d) is a safe edge, but (c, d) is not light edge to the cut

23.1-3
removing (u, v) must decompose the MST to two connected components
    if removing (u, v) doesn't disconnect the MST, the MST contains another path u ~> v and a cycle
    if removing (u, v) decomposes the MST to more than two connected components
    adding (u, v) back will connect at most two of them
thereby there is a cut (S, V - S) that respects all edges in the MST but (u, v)
also S and V - S are connected by edges in MST - (u, v)
if (u, v) is not a light edge crossing that cut, connecting a light edge results a MST with lower total weight
therefore (u, v) must be a light edge crossing (S, V - S)

23.1-4
define G with vertices u, v, w, and edges (u, v), (v, w), (w, u) where
    w(u, v) = w(v, w) = w(w, u) = 1
each edge is a light edge, {(u, v) | (u, v) is a light edge crossing some cut} contains three edges
MST of the graph can only have two edges

23.1-5
let T be a MST that contains e
there must be an edge on the cycle no in T, otherwise T is not a tree
removing e decomposes the MST to two connected components
let e = (u, v), u and v belongs to different connected components
the cycle must pass another edge (x, y) that connects the two components, since there is another path u ~> v on it
w(x, y) <= w(u, v) as (u, v) has maximum weight on the cycle
adding (x, y) instead of (u, v) results in a spanning tree T' that w(T) >= w(T')
by the description of GENERIC-MST, there is a MST for each possible graph G
therefore either all MST of G do not contain e, or there is also an MST that does not contain e

23.1-6
let T and T' be two MSTs of the graph G
let (u, v) ∈ T not not in T', (u, v) defines a cut and is the unique light edge crossing the cut by 23.1-3
there must be a path u ~> v in T'
there must be an edge (x, y) on the path in T' crossing the cut defined by (u, v), then w(x, y) > w(u, v)
replace (x, y) by (u, v) reduces the total weight of T', by Theorem 23.1 T' is still a spanning tree
(u, v) must be in any MST of G
apply the argument to all edges in T, each possible MST will be exactly T, T is unique
conversely, define a graph G with edge u, v, w and edge (u, v), (u, w) where
    w(u, v) = 1, w(u, w) = 1
then the cut({u}, {v, w}) has two light edges
but G has a single MST

23.1-7
if T connects all the vertices but is not a tree, T must contain a cycle
removing any edge on the cycle will reduce w(T) as w(e) > 0 for all e
and the resulting tree still connects all the vertices
therefore T must be a tree
define G with vertices u, v, w and edges (u, v), (v, w), (w, u) where
    w(u, v) = 0, w(v, w) = 0, w(w, u) = 0
then {(u, v), (v, w), (w, u)} has minimum total weight but is not a spanning tree

23.1-8
let T, T' be a MST of G
by 23.1-6, for each edge e in T - T', there is an edge T' - T that has the same weight with e
let e and f be two different edges in T - T', e and f defines two different cuts 
f cannot cross the cut defined by e, e and f decomposes G to three connected components
hence a single edge in T' cannot cross both the cuts
each e in T - T' has a unique f in T' - T that w(e) = w(f)
the sorted list of weights is the same in T and T'

23.1-9
optimal substructure
if otherwise T' is not a MST of G', replacing T' by a MST of G' results a better spanning tree for G

23.1-10
let T' be any spanning tree according to w'
if T' does not contain (x, y), w'(T') = w(T') >= w(T), T' has weight no lower than T
if T' contains (x, y), assume w'(T') < w'(T)
then w(T') < w(T), in contradiction to T being a MST
therefore w'(T') >= w'(T), T is a MST according to w'

23.1-11
let the edge be (u, v), it forms a simple cycle in T
remove the edge of maximum weight (s, t) on the cycle results a tree T'
let T'' be any spanning tree, w' defined similarly to 23.1-10
if T'' does not contain (u, v), w'(T'') = w(T'') >= w(T) >= w(T')
if T'' contains (u, v), (u, v) defines a cut in T''
there must be an edge (x, y) on u ~> v in T that crosses the cut
replace (u, v) by (x, y) results in a spanning tree T'''
as T''' does not contain (u, v), w'(T''') >= w'(T)
w(T'') = w(T''') + w(u, v) - w(x, y)
by definition of T', w(s, t) >= w(x, y), w(T') <= w(T'')
therefore T' is a MST according to w'
the cycle can be find by dfs in O(V), the maximum can be find and deleted in O(V)
O(V) in total

23.2-1
by 23.1-8, the list L of sorted weights in any MST is the same
for a particular MST T, sort the edges so that edges in T come first among all edges of the same weight
invariant:
    A contains all edges in T examined so far and nothing else
initialization:
    A = ∅    
maintenance:
    let e = (u, v) be the edge examined in the current iteration
    if w(e) ∉ L, there's no edge of weight w(e) in any MST, e cannot be added to A
    if w(e) ∈ L, let n be the number of w(e) in L
        if e is the first n edges of weight w(e), e ∈ T
        as A ⊆ T, e ∉ A, e must connect two trees in the forest, e is added to A
        if e is not the first n edges of weight w(e),
        by invariant all n edges of weight w(e) in T has already been added to A
        by definition of n, any MST may not contain more edges of weight w(e), e is not added to A
    thus e is added to A <=> e ∈ T
termination:
    all edges examined, A = T

23.2-2
the loop starting at line 8 will always take Θ(V) time in adjacent matrix
therefore finding the minimum in Q by traversing all the elements in O(V) will not affect the asymptotic running time 
Q can be a doubly linked list instead of a heap

23.2-3
when E = O(V), O(E + VlgV) = O(VlgV) = O(ElgV), fibonacci heap implementation is no better than binary heap
when E = Θ(V^2), O(E + VlgV) = O(V^2), O(ElgV) = O(V^2lgV), fibonacci heap implementation is asymptotically better
when E = ω(VlgV), fibonacci heap implementation is better

23.2-4
|V| = O(E), counting sort the edges takes O(E) time, O((V + E)α(V)) = O(Eα(V)) in total
for arbitrary constant W, the overall running time will be O(W + Eα(V)) by counting sort
or O(ElgW + Eα(V)) by radix sort

23.2-5
by using a vEB tree, minimum can be extracted in time lglgV
decrease key can be simulated by deletion and insertion in time lglgV
total running time will be O(ElglgV)
for arbitrary constant W, total running time will be O(ElglgW)

23.2-6
by bucket sort, the running time of Kruskal's method can be improved to O(Eα(V)) expected time
no dynamic set implementation particularly good for [0, 1) real numbers is introduced in the text

23.2-7
let G' be the resulting graph
if there is a single edge between the new vertex and G, adding it to T gives a MST in G'
adding one more edge is equivalent to decrease the weight of the edge from infinity to w(e)
the MST can be updated in O(V) by 23.1-11, O(V^2) for all |V| possible edges between the new vertex and G
actually worse than Prim's method if the graph is not dense
thanks Instructor's Manual
by 23.2-1, for T a MST of G, there exists an ordering L of E that produces T in Kruskal's algorithm
order E' the edges in G' so that:
    edges in E have the same order as L
name this ordering L', let T' be the MST produced by Kruskal's method with L'
claim: 
    upon evaluation of an edge (x, y) in E
    if two vertices u and v are in the same set during MST-KRUSKAL(G) with edge ordering L
    then they are in the same set during MST-KRUSKAL(G') with edge ordering L'
proof:
    u and v are in the same set iff the edges evaluated so far puts them in the same connected component
    by definition of L', upon evaluation of (x, y)
    the edges evaluated in MST-KRUSKAL(G') is a superset of edges evaluated in MST-KRUSKAL(G)
    adding an edge never disconnects a component
    u and v must be in the same connected component in MST-KRUSKAL(G') too
claim:
    edge in E - T cannot be in T'
proof:
    an edge (u, v) in E - T is included in T' iff
    upon evaluation of (u, v), u and v are in the same set during MST-KRUSKAL(G) but not MST-KRUSKAL(G')
    by the previous argument, such a case is impossible
therefore there is a MST T' only includes edges in E' - E and T
adding a vertex adds at most |V| edges, |E' - E| = O(V), |E' - E| + |T| = O(V)
running Kruskal's method on the graph (V', E' - E ∪ T) gives one possible T' in O(VlgV)

23.2-8
the initial cut has only one edge crossing it
define G with vertices w, x, y, z and edges (w, x), (x, y), (y, z), (z, w) where
    w(w, x) = 1, w(y, z) = 1, w(x, y) = 2, w(z, w) = 2
if the initial cut is ({w, y}, {x, z}), only one between (w, x) and (y, z) will be included in the result
while a MST must include both

23-1
a.  MST is unique by 23.2-1
    define G with vertices w, x, y, z and edges (w, x), (x, y), (y, z), (z, w) and (w, y) where
        1
     w+---+x
     +XX   +
    3| X2X |4
     +   XX+
     z+---+y
        5
    there are two second-best MST of weight 8
    T1 = {(w, x), (w, y), (z, y)} and
    T2 = {(w, x), (w, z), (x, y)}
b.  let (x, y) be some edge ∈ E - T
    adding (x, y) to T forms a cycle, let (u, v) != (x, y) be the edge with maximum weight on the path x ~> y in T
    T' = T - (u, v) ∪ {(x, y)} is a spanning tree
    claim: T' has minimum weight among all trees that contains (x, y)
    proof: 
        let T'' be any spanning tree with (x, y)
        induct on the size of T - T''
        the second-best MST has at least one edge different to T, |T - T''| >= 1
        initially |T - T''| = 1, the sole edge in T - T'' must be an edge on the path x ~> y in T
        by definition of T', w(T'') > w(T')
        inductively, |T - T''| >= 2, let (x', y') != (x, y) be another edge in T'' - T
        there must be an edge (u', v') ∈ T - T'' on the path x' ~> y' in T, otherwise T'' contains a cycle
        it must have w(u', v') < w(x', y'), otherwise T - (u', v') ∪ {(x', y')} is a spanning tree better than T
        replace (x', y') with (u', v') results a tree T''' with w(T''') < w(T'')
        also |T - T'''| = |T - T''| - 1, by induction w(T'') > w(T''') > w(T')
    as second-best MST must contain at least one (x, y) ∉ T
    the optimal among all possible T' gives a second-best MST
c.  dynamic programming
    assume max(w, v) is computed for all w ∈ Adj[u] (-∞ if no path from w to v)
    exactly one max(w, v) will have value != -∞ as there is a unique path between each pair of vertices in a tree, then
        max(u, v) = max{max(w, v), w(u, w)}
    since (u, w) is the only additional edge on the path
    the size of subproblems can be naturally defined as the length of the path
    max(u, v) has size one more than max(w, v), hence optimal substructure
    a bottom-up implementation starts from v, propagates max(w, v) as w(w, v) to all w in Adj[v]
    at each step, max(w, v) is propagated to w' lower in the tree by comparing w' to w's parent (a pointer)
    for a single v, max(u, v) for all u ∈ T.V can be computed in O(E) = O(V)
    O(V^2) in total
d.  follow argument in part b
    max(u, v) for each pair (u, v) can be computed in O(V^2)
    find an edge (x, y) in E - T that minimizes w(x, y) - max(x, y) takes at most O(E)
    looking for the exact edge (u, v) to replace traverses a cycle in the tree in O(V)
    O(V^2 + E) = O(V^2) in total

23-2
a.  in the loop between line 4-9:
        u is chosen iff u.mark == false
        as u.mark is set to true if an edge (u, v) is added to T, there's no edge in T that incidents u
        ({u}, V - {u}) then is a cut repects T
        by the way (u, v) is chosen, (u, v) is a light edge crossing the cut ({u}, V - {u})
        thereby (u, v) is safe to T
    after line 9, T contains the subset of some MST since only safe edges are added
    also vertices in each set is connected, each disjoint set represents a connected component in (V, T)
    MST-PRIM then operates with connected components but not vertices
    each edges produced by MST-PRIM still is safe respecting G since
        it is a light edge among (u, v) ∈ G'.E that crosses the cut (A, G'.V - A), minimizes (u, v).c'
        (u, v).c' is set to the minimum of edge weights between two components represented by u and v
        (u, v).orig' is set to the edge connecting the two components with weight (u, v).c'
        (u, v).orig' is a safe edge respecting G
    after the return of MST-PRIM, it produces edges that connects each connected component in (V, T)
    the resulting set of edges is a spanning tree of G
    since only safe edges are included, the resulting set of edges is a MST
b.  claim: if u.mark == true, it's in a disjoint set with at least two elements
        before set u.mark = true on line 9, the set of u is unioned with the set of v
        thus the set contains at least two elements
    claim: u.mark == true for all u ∈ G.V after line 9
        the loop starting at line 4 iterates over all vertices in G.V
        if u.mark == false, it's set to true on line 9
    therefore after line 9, each vertices in G.V is in a disjoint set with at least two elements
    there are at most |G.V| / 2 disjoint sets, each is made a vertex of G'
    |G'.V| <= |G.V| / 2
c.  loop between line 1-3 takes O(V) and performs O(V) MAKE-SET
    loop between line 4-9 takes O(E) and performs O(V) UNION
    line 10 performs O(V) FIND-SET and takes O(V) if vertices are deduped using an array with their integer key
    loop between line 12-21 takes O(E) and performs O(E) FIND-SET
    adjacency list can be computed in O(E) given O(E) edges
    using an linked list implementation of disjoint sets with weighted-union heuristic will suffice since
        MAKE-SET is O(1)
        FIND-SET is O(1)
        each time UNION is called, the set represented by u has only one element since u.mark = false
        which indicates there's no edge incidents u in T
        as T represents connected components, u is in a connected component of itself only, the set has one element
        union it with the set represented by v thereby takes O(1) with weight heuristic
d.  G' returned by MST-REDUCE cannot have more edges than G
    (u, v).orig' for each (u, v) in G'.E points to a distinct edge (x, y) in G.E
    the same (x, y) will give the same (FIND-SET(x), FIND-SET(y)), assume (u, v) and (v, u) are the same edge
    inductively G' in each iteration cannot have more than |E| edges, O(kE) in total
e.  let k = lglg|V|
    the nested call to MST-REDUCE will take O(kE) = O(ElglgV)
    the last iteration of G' will have |G.V| / 2^k = |V| / lg|V| vertices
    Prim's method thus takes O(E + V'lgV') = O(E + V/lgV * (lgV - lglgV)) = O(E + V - VlglgV / lgV) = O(E)
    generally the complexity will be O(kE) + O(E + V/2^k * (lgV - klg2)) = O(kE + VlgV/2^k - kV/2^k)
    when k = o(lglgV), O(kE) = o(ElglgV), O(VlgV / 2^k) = ω(V)
    the question is reduced to "is there any k = o(lglgV) that produces VlgV / 2^k = o(ElglgV)"
    not familiar to math enough to solve this exactly, but most elementary functions will not do
    any lg(lgV)^ε for ε < 1 is just Θ(lglgV)
    any (lglgV)^ε for ε < 1 will give VlgV / 2^k = VlgV / 2^(lglgV / (lglgV)^(1 - ε)) = V(lgV)^(1 - (lglgV)^(1 - ε))
    lglglgV will give VlgV / lglgV = ω(VlglgV)
f.  assume part e is correct
    ElglgV = o(VlgV), E = o(VlgV / lglgV)

23-3
a.  let T be a MST of the graph G
    assume the edge (u, v) of maximum weight in T
    if T is not a bottleneck spanning tree, there is another spanning tree T' that maximum edge weight in T' < w(u, v)
    there must be an edge (x, y) in T' crossing the cut defined by (u, v) and T 
    by assumption w(x, y) < w(u,  v), replacing (u, v) with (x, y) in T reduces the total weight
    in contradiction to T being MST
b.  in other words, "whether V can be connected with edges of weight <= b"
    define G' = (V, E'), where E' = {e | e ∈ E and w(e) <= b}, run single source dfs from any vertex
    G' is connected iff all vertices after the dfs is colored BLACK
    both the construction of G' and dfs takes O(V + E)
    by calculating |E'| first and return false when |E'| < |V| - 1, the running time can be reduced to O(E)
c.  first find the edge e with median weight with linear time median algorithm in O(E)
    if the value of a bottleneck tree is at most w(e)
        construct a new graph with only edges of weight <= w(e), O(E) by reusing the set of vertices
        the subproblem has half the edges in G
    if the value of a bottleneck tree is greater than w(e)
        contract all the edges of weight <= w(e), O(E) by 23-2
        the subproblem has half the edges in G
    T(n) = T(n/2) + O(n), T(n) = O(n), the maximum edge can be found in linear time
    then dfs on the set of edges with smaller or equal weight gives a possible bottleneck spanning tree

23-4
a.  T may not be a MST
    consider a graph G with two connected components C and C', assume there are two paths between C and C'
    one is a single edge (u, v), another is a very long sequence of edges (xi, yi)
    if (u, v) is the edge with maximum weight, it will be removed in the first iteration
    the path (xi, yi) hence must be included in the resulting tree T
    if Σw(xi, yi) > w(u, v), T cannot be MST
    whether T - {e} is a connected graph can be determined by dfs in O(V + E), O(VE + E^2) = O(E^2) in total
b.  produces a random spanning tree of G, not necessarily a MST
    this algorithm has complexity O(Eα(V))
    trees in forest T can be maintained as disjoint sets
    e = (x, y) forms a cycle in T iff x and y are in the same tree, or FIND-SET(x) == FIND-SET(y)
    otherwise x and y are in different trees merged by adding e to T, UNION(x, y) maintains the correspondence
c.  for an arbitrary ordering L of the set of edges E
    define Ei be the subset of E up to (not including) ith edge in L
    invariant:
        T at the start of iteration i is a forest of MSTs of SCCs in Gi = (V, Ei)
    initialization:
        SCCs in Gi are single vertices, each have an empty MST
    maintenance:
        let e = (u, v), if e does not form a cycle in T, e connects two SCCs C and C' in (V, Ei-1)
        by invariant (u, v) is the only edge between components C and C'
        (u, v) is the light edge crossing the cut (C, C')
        the resulting tree is a MST of C ∪ C'
        if otherwise e forms a cycle in a tree T' in forest T
        T' is a MST by invariant, adding (u, v) to the universe is equivalent to reduce its weight from ∞ to w(e)
        by 23.1-11, this algorithm computes a MST with the additional edge
    termination:
        Ei = E, if G = (V, E) is connected, T is a single tree, T is a MST
    the cycle and e' can be found by dfs in time O(V + E), O(E^2) in total

Chapter 24
24.1-1
w(z, x) = 7
Vertex s: d = 0, π = NIL
Vertex t: d = 2, π = x
Vertex x: d = 4, π = y
Vertex y: d = 7, π = s
Vertex z: d = -2, π = t
w(z, x) = 4
Warning: graph contains a negative cycle
Vertex s: d = -2, π = z
Vertex t: d = 0, π = x
Vertex x: d = 0, π = z
Vertex y: d = 7, π = s
Vertex z: d = -4, π = t

24.1-2
for each v, v.d = δ(s, v)
if there is no negative weighted cycles reachable from s in G, δ(s, v) is well defined
if there is a path p = s ~> v, there is a simple path p' = s ~> v, w(p') is finite, v.d = δ(v, s) <= w(p) is finite
if there is no path from s to v, v.d = +∞ by no-path property

24.1-3
set a boolean variable `updated` indicating whether RELAX updated v.d for any v in the current iteration
if v.d = δ(s, v) for all v, v.d will never change afterwards, updated == false
if otherwise v.d != δ(s, v) for some vertex, by upper-bound property, v.d > δ(s, v)
assume RELAX didn't update any v.d in this iteration
there must be a path s ~> v, otherwise v.d is initialized to +∞ = δ(s, v)
let u be the vertex precedes v on the shortest path s ~> v, δ(s, v) = δ(s, u) + w(u, v) by optimal substructure
if u.d = δ(s, u), v.d > δ(s, v) = δ(s, u) + w(u, v), v.d would be updated in this iteration
it must also have u.d > δ(u, d), and this property propagates all the way to the source
however δ(s, s) = 0 = s.d at the beginning and RELAX never increases s.d
therefore RELAX must update some v.d if there is some u.d != δ(s, u), updated == true
by the text, after m iterations, all vertices will have v.d = δ(s, v), updated == false starting from iteration m+1
by immediately terminate the loop when updated == false, the algorithm runs for at most m+1 iterations

24.1-4
run the check on line 5-7 |V| iterations, update both u.d and v.d to -∞ everytime v.d > u.d + w(u, v)
in the first iteration, if v.d > u.d + w(u, v), v and u are on a negative cycle reachable from s by the text
let V0 be the set of vertices on the negative cycle detected in the first iteration
at least one vertex on each negative cycle reachable from s is in V0 by Theorem 24.4
invariant:
    at the end of ith iteration, v.d = -∞ if there is a path from some v0 ∈ V0 ~> v of length at most i-1
initialization:
    at the end of first iteration, for each v ∈ V0 (i.e. has a path of length 0), v.d = -∞
maintenance:
    if there is a path v0 ~> v of length i-1, let u be the vertex precedes v on the path
    there is a path v0 ~> u of length i-2, by invariant u.d = -∞, u.d + w(u, v) = -∞
    v.d will be updated to -∞
termination:
    v.d = -∞ for all vertices v where there is a path of length at most |V| - 1 from v0 ∈ V0 to v
    if there is a negative cycle on some path s ~> v, let u be the last vertex on that negative cycle on the path
    by assumption there is a v0 ∈ V0 on the same cycle with u, there is a path v0 ~> u ~> v
    there is a simple path v0 ~> v, simple path has length at most |V| - 1, v.d = -∞
./CLRS/graph/shortest-path.ts#spBellmanFord

24.1-5
if there is no edge with negative weight, min{δ(u, v) | u ∈ V} = δ(v, v) = 0, otherwise:
let In[v] be the set of vertices u where (u, v) ∈ E
any path u ~> v of length >= 1 will have to visit a vertex in In[v] before end up at v
assume the shortest path weight with at most k edges to any vertices in In[v] is already calculated as u.d
the shortest path weight with at most k+1 edges can be derived from min{0, u.d + w(u, v) | u ∈ In[v]}
do that |V| - 1 iterations gives min{δ(u, v) | u ∈ V} since there's no negative cycle in the graph
calculating min{u.d + w(u, v) | u ∈ In[v]} for all v takes O(E)
O(VE) in total

24.1-6
thanks https://people.cs.nctu.edu.tw/~tjshen/doc/ne.pdf
first detect the existance of a negative cycle by BELLMAN-FORD method
each cycle in the shorest path graph will be a negative cycle, otherwise removing it reduces the path weight
if (u, v) still can be relaxed afer |V| - 1 passes of BELLMAN-FORD, the new shortest path must not be simple
following v.π in the shortest path graph thereby will end up in a negative cycle
by coloring vertices BLACK when following v.π, at some point v.π will be BLACK
which means v is an ancestor of v.π in the shortest path graph, (v.π, v.π.π .. v) is a negative cycle

24.2-1
Vertex r: d = Infinity, π = NIL
Vertex s: d = 0, π = NIL
Vertex t: d = 2, π = s
Vertex x: d = 6, π = s
Vertex y: d = 5, π = x
Vertex z: d = 3, π = y

24.2-2
the last vertex in a dag in topologically sorted order cannot have edges to any vertices
the loop on line 4-5 will not execute for that vertex

24.2-3
define a new graph G' = (V', E') that 
    V' = V ∪ {void}
    E' = E ∪ {(u, void) | u ∈ V}
define the weight function on E' as
    w'(u, v) = the weight of u
run DAG-SHORTEST-PATHS(G', w', s) with negated weight function w' then computes the critical path
if the critial path in G has weight w(p):
    let p = (v0, v1 .. vk)
    w(p) = Σw(vi)
    there is a path p' = (v0, v1 .. vk, void) in G' that
    w'(p')  = Σw'(vi, vi+1)
            = Σw(vi)    // void is exluded from the sum
            = w(p)
if the critical path in G' has weight w'(p'):
    let p' = (v0, v1 .. vk)
    vk must be void, otherwise add void to the end of the path increases w'(p') by w(vk)
    none of v0 .. vk-1 can be void since void has out-dgree 0
    all other edges are in E, therefore there is a path p = (v0, v1 .. vk-1) in G that
    w(p)    = Σw(vi)
            = Σw'(vi, vi+1)
            = w(p')
therefore the critical path in both graph has the same weight
solving G' gives a solution to G
the transformation takes O(V + E) + O(V) + O(1) = O(V + E) linear time
running DAG-SHORTEST-PATHS(G', w', s) also takes linear time

24.2-4
assume for all v in Adj[u] the number of paths starting from v is already calculated as v.n
each path starting from u have to take (u, v) for some v ∈ Adj[u], end right there or follow a path from v
u.n = Σ(v ∈ Adj[u])(v.n + 1)
by computing u.n in reversed topological order, subproblems in Adj[u] is already calculated for each u
O(V + E) in total

24.3-1
Starting from vertex s
{s}
{s, t}
{s, t, y}
{s, t, y, x}
{s, t, y, x, z}
Vertex s: d = 0, π = NIL
Vertex t: d = 3, π = s
Vertex x: d = 9, π = t
Vertex y: d = 5, π = s
Vertex z: d = 11, π = y
Starting from vertex z
{z}
{z, s}
{z, s, t}
{z, s, t, x}
{z, s, t, x, y}
Vertex s: d = 3, π = z
Vertex t: d = 6, π = s
Vertex x: d = 7, π = z
Vertex y: d = 8, π = s
Vertex z: d = 0, π = NIL

24.3-2
define a graph with vertices u, v, w, x and edges (u, v), (u, w), (v, w), (w, x) where
    w(u, w) = 1, w(u, v) = 2, w(v, w) = -2, w(w, x) = 0
DIJKSTRA(G, w, u) will extract u first, relax (u, v) and (u, w), update v.d = 2 and w.d = 1
the next EXTRACT-MIN will extract w next and update x.d = 1
x.d will no longer be updated as the only edge to it is (w, x)
but a shortest path u ~> x is (u, v, w, x) with weight 0
the assumption δ(s, y) <= δ(s, u) is no longer true in the proof

24.3-3
it is still correct
let u be the last element in the heap Q, at the beginning of the last iteration S = V - {u}
by invariant in Theorem 24.6, all vertices other than u already has v.d = δ(s, v)
by convergence property v.d will not be updated afterwards 
the last iteration also does not update u.d itself as u.d may only be updated through relaxation on line 7-8
which means it may only be updated by a self loop (u, u), with w(u, u) >= 0
since u.d <= u.d + w(u, u), relaxation will not update u.d
the last iteration will not do any meaningful work hence can be emitted

24.3-4
check that each vertex is reachable from s in predecessor graph (i.e. u.d finite) iff it's reachable from s in G by dfs
then run dfs on the predecessor graph from s, each vertex should be colored BLACK, also there can only be tree edges
check v.d = v.π.d + w(v.π, v) and s.d = 0
now it's safe to claim that 
    each finite u.d corresponds to the weight of a simple path s ~> u in the original graph
    u.d = +∞ iff u is not reachable from s in the original graph
if one pass of BELLMAN-FORD will not update any u.d, by 24.1-3 these paths are indeed shortest paths
otherwise the result is not optimal yet
two dfs takes O(V + E), each explicit check on edges and vertices takes O(V) or O(E)
one pass of BELLMAN-FORD takes O(E), O(V + E) in total

24.3-5
Dijkstra's algorithm will relax one shortest path for each vertex in order, not all of them
each edge in Dijkstra's algorithm is relaxed exactly once
let u be the last vertex before v on the resulting shortest path s ~> u -> v in the predecessor graph
on extracting u, s ~> u in the predecessor graph is already a shortest path
by optimal substructure, each w on s ~> u will not be updated again, w.π won't change afterwards
therefore upon relaxing (u, v), every edge on the path s ~> u is already relaxed
this property propagates all the way up to s, the edges on s ~> v is relaxed in order
define a graph with vertices s, x, y, z, t and edges (s, x), (s, y), (x, z), (y, z), (z, t) where
    w(u, v) = 0 for all u, v in the graph
Dijkstra's algorithm may visit the vertices in arbitrary order
if it visits vertices in order (s, y, z, t, x), (z, t) will be relaxed before (x, z)
(s, x, z, t) is also a shortest path of s ~> t, its edges are not relaxed in order

24.3-6
on a given path s ~> t, the reliability of s ~> t is given by Πr(u, v) over all edges (u, v) on s ~> t
by monotonicity of logarithm function, maximizing Πr(u, v) is equivalent to maximizing lg(Πr(u, v)) = Σlgr(u, v)
which in turn is equivalent to minimizing -Σlgr(u, v) = Σ-lgr(u, v)
as 0 <= r(u, v) <= 1, lgr(u, v) will be negative, -lgr(u, v) will be positive
for r(u, v) = 0, such an edge immediately sets the reliability of the path to 0
may remove such edges and check reachability later if t.d = +∞ to distinguish the case of no path and 0 reliability path
define w(u, v) = -lgr(u, v), the graph now contains only non-negative weight edges
Dijkstra's algorithm solves it in O(E + VlgV)

24.3-7
replace an edge of length i adds i-1 new vertices
Σ(e ∈ E)(w(e) - 1) vertices are added to G', |V| + Σ(w(e) - 1) in total
in a graph with unit edges, bfs finds a shortest path to each vertex in non-decreasing order of path length
as no two vertices have the same shortest-path weights from s in G, the order of coloring in bfs is fixed
assume in Dijkstra's algorithm a vertex u is extracted right before another vertex v
it must have v.d >= u.d = δ(s, u), or v will be extracted instead
v.d also cannot be updated to v.d < u.d in the current iteration, since it can at most be updated to u.d + w(u, v)
where w(u, v) is non-negative
so in the next iteration, when v is extracted, δ(s, v) = v.d >= u.d = δ(s, u)
vertices in Dijkstra's algorithm are also extracted in non-decreasing order of path weight
the two algorithms visits vertices in V in the same order

24.3-8
similar to 24.3-9 but with a different data structure
initial an array D that D[k] stores a list of vertices with estimate k, or nothing if no such vertex exist
by 24.3-9, there are at most W lists in the array
also the array can be initialized with length |V|W as a simple path in G has weight at most (|V| - 1)W
maintain a root list that doubly links all the lists in W
EXTRACT-MIN:
    traverse the root list, find the list with minimum estimate, O(W)
    extract the head of the list, O(1)
    if the list becomes empty, remove it from both the root list and D, O(1)
DECREASE-KEY:
    remove the vertex from its list, O(1)
    remove the list from root list and W if it becomes empty, O(1)
    insert the vertex to D[k] where k is the new estimate, create a new list if D[k] is empty, O(1)
overall running time will be O(WV + E)

24.3-9
invariant:
    at the start of each iteration, let S' = {v ∈ V - S | v.d is not +∞}
    max{v.d | v ∈ S'} - min{v.d | v ∈ S'} <= W
initialization:
    S' = {s}, max{v.d | v ∈ S'} = min{v.d | v ∈ S'} = 0
maintenance:
    u with estimate min{u.d | u ∈ S'} is extracted from the heap
    if the estimate of a vertex v is not updated, v.d <= max{v.d | v ∈ S'} <= u.d + W
    each v ∈ Adj[u] has their estimate updated to min(v.d, u.d + w(u, v))
    as 0 <= w(u, v) <= W, u.d + w(u, v) <= u.d + W
    as v is not extracted instead of u, v.d >= u.d
    therefore for all vertices in S', u.d <= v.d <= u.d + W
    max{v.d | v ∈ S'} - min{v.d | v ∈ S'} <= W if S' != ∅
termination:
    not important here
use a dynamic set to store doubly linked lists of vertices keyed by estimates
there can be at most W different keys in the dynamic set, set operations can be done in O(lgW)
at first all vertices are in the same list with key +∞
EXTRACT-MIN can be simulated by
    search the minimum of the set, O(lgW)
    extract the head of the list, O(1)
    delete the key if the list becames empty, O(lgW)
DECREASE-KEY can be simulated by
    removing the vertex from the original list, O(1)
    delete the list from the dynamic set if the list is empty, O(lgW)
    search a list with the new estimate, insert a new one if it doesn't exist, O(lgW)
    insert vertex into the list, O(1)
therefore the running time will be O((V + E)lgW)

24.3-10
δ(s, y) <= δ(s, u) still holds
only edges leaving s may have negative weight
if p2 = y ~> u ever visits s, it forms a cycle
since there are no negative cycle in the graph, there must be a simple shortest path s ~> u
use that path in the proof will give δ(s, y) <= δ(s, u) by the argument in the text

24.4-1
Vertex x0: d = 0, π = NIL
Vertex x1: d = -5, π = x4
Vertex x2: d = -3, π = x6
Vertex x3: d = 0, π = x0
Vertex x4: d = -1, π = x2
Vertex x5: d = -6, π = x1
Vertex x6: d = -8, π = x3
a possible solution is (-5, -3, 0, -1, -6, -8)

24.4-2
Warning: graph contains a negative cycle
Vertex x0: d = 0, π = NIL
Vertex x1: d = -Infinity, π = x5
Vertex x2: d = -Infinity, π = x4
Vertex x3: d = -Infinity, π = x2
Vertex x4: d = -Infinity, π = x1
Vertex x5: d = -Infinity, π = x3
no feasible solution

24.4-3
it cannot be positive 
there is an edge (v0, vi) to each vi of weight 0, a shortest path v0 ~> vi can only have non-positive weight

24.4-4
each edge (u, v) by triangle property can be translated to a constraint
    δ(s, v) - δ(s, u) <= w(u, v)
the objective function is δ(s, s) - δ(s, t)

24.4-5
m constraints at most connects 2m unknowns in the graph
at least n - 2m unknowns in the graph has no out edge and only one in edge from v0
thus the only path to them is the single edge (v0, vi), which must be the shortest path
by counting in degree of all the vertices, set δ(v0, vi) = 0 for these of in degree 1
these edges can be removed from the graph, only O(m + 2m) = O(m) edges have to be relaxed in each iteration
O(m + n + nm) = O(nm) in total

24.4-6
an equality constraint can be decomposed to two inequality constraints
    xi - xj <= bk and
    xi - xj >= bk, or xj - xi <= -bk
an equality constraint thus maps to two edges (i, j), (j, i) in the graph where
    w(i, j) = -bk, w(j, i) = bk

24.4-7
instead of calling INITIALIZE, set u.d to 0 for each u ∈ V, run |V| - 1 passes of BELLMAN-FORD
each shortest path v0 ~> vi starts with an edge (v0, vj)
setting vj.u = 0 effectively relaxes the edge (v0, vj)
as vj.d will not increase, (v0, vj) will not be relaxed again in later passes
|V| - 1 passes of BELLMAN-FORD thus correctly relaxes edges on the shortest path in order
on termination, vi.u = δ(v0, vi)

24.4-8
on termination of BELLMAN-FORD, vj.d <= vi.d + w(vi, vj), vi.d = vi.π.d + w(vi.π, vi), vi.d <= 0
assume Σxi = Σvi.d is not maximized, let Σx'i be an optimal solution, there must be some x'i > xi
the edge w(vi.π, vi) corresponds to a constraint
    vi.d - vi.π.d <= w(vi.π, vi)
so it must also have x'i.π > xi.π = vi.π.d, or xi - xi.π > w(vi.π, vi)
this propagates all the way to the second vertex on the shortest path v0 ~> vi, let it be v1
v1.π = v0, the shortest path v0 ~> v1 is (v0, v1) has weight 0, v1.d = 0
x'1 > x1 = v1.d = 0, the optimal solution (x'i) is not valid
therefore the solution given by BELLMAN-FORD must be the maximum

24.4-9
by 24.4-8, max{vi.d} is fixed to 0, let m = min{vi.d}, max{vi.d} - min{vi.d} = -m
if otherwise there is a solution to the system where max{xi} - min{xi} > -m
by adjusting max{xi} to 0 (always possible since x + d is also a solution for any d) and plugging {xi} to {vi.d}
each vi.d is a valid estimate as it satisfies triangle property and vi.d >= max{vi.d} >= 0
let vj be the vertex that achieves min{vi.d}, vj.d >= δ(v0, vj)
as max{xi} - min{xi} = -min{xi} > -m, min{xi} < m, vj.d < m = δ(v0, vj), contradiction
therefore BELLMAN-FORD must minimize max{xi} - min{xi}
it minimizes the span of events, allows a set of works to be done in shortest time period

24.4-10
for each constraint xi <= bk, set w(v0, vi) = bk
for each constraint -xi <= bk, add a new edge (vi, v0) with w(vi, v0) = bk
the solution given by BELLMAN-FORD then satisfies normal difference constraints as well as
    δ(v0, vi) <= v0.d + w(v0, vi) = bk for all vi
also if -δ(v0, vi) > w(vi, v0), δ(v0, vi) + w(vi, v0) < 0, BELLMAN-FORD will find a negative cycle and return false
thereby when BELLMAN-FORD returns true, {δ(v0, vi)} is a feasible solution to the system
when BELLMAN-FORD returns false, let x0 denote v0.d which is fixed to 0
    each constraint xi <= bk can be interpreted as xi - x0 <= bk
    each constraint -xi <= bk can be interpreted as x0 - xi <= bk
the argument in Theorem 24.9 still applies, there's no feasible solution to the system

24.4-11
modify each b to an integer
if xi and xj are both integer, xi - xj must also be an integer
let b = b' + f, where b' is an integer and 0 <= f < 1
if xi - xj = k <= b, k <= b' as b' + 1 > b
run BELLMAN-FORD on the modified constraints, correctness is guarenteed by Theorem 24.9

24.4-12
-

24.5-1
1.  remove (y, z) from (b), add (x, z)
2.  remove (t, x) from (b), add (y, x)

24.5-2
define G with vertices s, x, y and edges (s, x), (s, y), (x, y), (y, x) all of weight 0

24.5-3
assume otherwise there's no path s ~> v, δ(s, v) = +∞
if for an edge (u, v), δ(s, u) != +∞, s ~> u -> v will be a path s ~> v, contradiction
thus for all (u, v) ∈ E, δ(s, u) = +∞, δ(s, v) <= δ(s, u) + w(u, v)
if there is a negative cycle reachable from s, v is reachable from the negative cycle, δ(s, v) = -∞
δ(s, v) <= δ(s, u) + w(u, v) guarenteed no matter the value of δ(s, u)

24.5-4
right before s.π is updated for the first time, s.d = 0 > u.d + w(u, s) >= δ(s, u) + w(u, s)
a cycle c = s ~> u -> s have weight w(c) = δ(s, u) + w(u, s) < 0

24.5-5
the same graph defined in 24.5-2
(s, x, y) is a shortest path s ~> y, (s, y, x) is a shortest path s ~> x
an assignment may set x.π = y and y.π = x, forms a cycle

24.5-6
initialization:
    Vπ = {s}, Gπ = ∅, s is reachable from itself with no edge
maintenance:
    by invariant, each u ∈ Vπ has a path s ~> u with edges in Gπ
    if v.π is updated by relaxing (u, v) for u ∈ Vπ, (u, v) is added to Gπ, s ~> u -> v is a path from s to v in Gπ
    for u ∉ Vπ, u.π = NIL and u != s, u.d = +∞ as relaxation always updates u.d and u.π at the same time
    relaxing (u, v) will not set v.π since v.d <= u.d + w(u, v) is guarenteed by u.d = +∞
termination:
    not important

24.5-7
for each v ∈ V, if v is reachable from s, there is a shortest path s ~> v with length <= |V| - 1
relax edges on the path in order, by path-relaxation property, sets v.d = δ(s, v)

24.5-8
let v0 be the first vertex on the negative cycle reachable from s
relax edges on the cycle in order from v0, each relaxation will update an estimate
during the first traverse, each vi.d on the cycle is set to v0.d + Σw(vk, vk+1) from +∞
once the traverse went back to v0
since (v0, v1 .. v0) is a negative cycle, it updates v0.d to v0.d' = v0.d + Σw(vk, vk+1) < v0.d
starting from v0 again, each vi will be updated since v0.d' + Σw(vk, vk+1) < v0.d + Σw(vk, vk+1) = vi.d
v0.d' will be updated to a smaller value when v0 is visited each time, causes infinite updates of estimates

24-1
a.  for Gf, if (vi, vj) ∈ Gf.E, i < j, vi appears earlier in the sorted order
    Gb: similar
b.  assume the first edge in a shortest path s ~> v is a forward edge
    then the first k edges are forward edges, k >= 1
    all forward edges are in Gf, as Gf is acyclic, relaxing Gf.E in topological order relaxes the k edges in order
    the k+1th edge is a back edge, let the next k' edges be back edges, k' >= 1
    all back edges are in Gb, relaxing Gb.E in topological order relaxes the k' edges in order
    the problem is reduced to relaxing all but the first k + k' edges in order
    by assumption (k + k' + 1)th edge is a forward edge, the same argument still applies
    k + k' >= 2, each pass relaxes at least 2 edges in the shortest path in order
    assume otherwise the first edge in s ~> v is a back edge
    relaxing edges in Gf may not relax any edge in order in the first pass
    let the first k' edges be back edges, relaxing edges in Gb relaxes all k' edges in order
    at the start of the next pass, the first edge not relaxed in order is a forward edge
    the problem is reduced to the first case
    k' >= 1, the first pass relaxes at least 1 edge in order, all following passes relaxes at least 2
    as a shortest path in a graph without negative cycle contains at most |V| - 1 edges
    maximum passes <= 1 + [(|V| - 1 - 1) / 2] = 1 + [|V| / 2 - 1] = [|V| / 2]
c.  it does not improve asymptotically
    still all |E| edges have to be relaxed each pass
    [|V|/2]|E| = Ω(VE)

24-2
a.  if x = (x1, .., xd) nests within y = (y1 .. yd), y nests within (z1 .. zd)
    there exists a permutation π that π(x) < y, another permutation π' that π'(y) < z
    it must have π(π'(x)) < z since
        π(xi) < yi, π'(yi) < zi for each i
        let π'(xi) = xj, π'(yi) = yj
        π(π'(xi)) < π'(yi) < xi
b.  let x, y be in sorted order, x nests within y iff xi < yi for each i
    sufficiency:
        if xi < yi for each i in sorted order, let π permutes x into sorted order, let π' permutes y into sorted order
        π(x) < π'(y), apply the reverse of π' to both sides
        π'^-1(π(x)) < y, π'^-1 . π is a permutation
    necessity: 
        let i be the first index where xi > yi
        for all j >= i, xj > yi, there are at most (i - 1) x smaller than yi
        if a permutation makes π(x) < y, for all k <= i, π(xk) < yk <= yi
        that makes i x smaller than yi, one more than possible
    thereby it can be determined by sorting both x and y then compare each xi < yi
c.  first sort each d-dimensional vector in O(ndlgd)
    create a graph with vertices {B0, B1, B2 .. Bn}
    add edges (B0, Bi) for each 1 <= i <= n with weight 0
    add an edge for each (Bi, Bj) where Bi nests within Bj with weight -1
    which can be checked in time O(d), O(n^2d) in total
    by transitivity, if there is a path Bi ~> Bj, Bi can be nested within Bj
    hence the graph must be acyclic, otherwise there is a path Bi ~> Bi, but Bi cannot be nested within itself
    a path in the graph, B0 excluded, corresponds to a sequence (Bi1 .. Bik) and vice versa
    (B0 can be interpreted as a vector (-∞, -∞, .., -∞))
    find the shortest path (i.e. the path with most edges) from B0 by DAG-SHORTEST-PATHS
    running time O(V + E) = O(n + n^2) = O(n^2)
    O(n^2d + ndlgd) in total

24-3
a.  since all R[ix, iy] is positive, lgR[ix, iy] is well defined
    by monotonicity of logarithm function
    R[i1, i2] * .. * R[ik-1, ik] > 1 is equivalent to
    lg(R[i1, i2] * .. * R[ik-1, ik]) = lgR[i1, i2] + .. + lgR[ik-1, ik] > 0
    define a graph with vertices {c1 .. cn} and vertices (ci, cj) for each ci, cj
    define w(ci, cj) = -lgR[i, j]
    by the inequality above, there is an arbitrage iff there is a negative cycle in the graph
    a negative cycle can be found by BELLMAN-FORD in O(VE) = O(n^3)
b.  by 24.1-6, after constructing the predecessor graph in O(VE) = O(n^3)
    a negative cycle can be find in predecessor graph in time O(V) = O(n)

24-4
a.  thanks Instructor's Manual
    implement min heap by an array of lists Q, each slot Q[d] contains a list of vertices with v.d = d
    v.d is initialized to |E| + 1 for all v ∈ V - {s}
    DECREASE-KEY can be implemented as O(1):
        remove the vertex from the old list, O(1)
        if the list became empty, set Q[d] to NIL
        prepend it to the new list in Q[d], where d is the new estimate, O(1)
    EXTRACT-MIN can be implemented as O(E) aggregated time:
        since DECREASE-KEY only updates v.d when v.d > u.d + w(u, v), where u is the extracted minimum
        w(u, v) >= 0, the new minimum is not smaller than u.d the old minimum
        the new minimum can be found by scanning from Q[d] to a higher slot in O(E) aggregated time
    by Dijkstra's algorithm, δ(s, v) can be computed in O(E)
b.  as 0 <= w1(u, v) = [w(u, v) / 2^(k-1)] <= [W / (W + 1) / 2] <= 1 
    δ1(s, v) <= (|V| - 1) * max{w1(u, v)} <= |V| - 1
    as all vertices are reachable from the source, G is at least a tree, |E| >= |V| - 1
    δ1(s, v) <= |E| for all v ∈ V, by part a it can be computed in O(E) 
c.  wi(u, v) is given by the i most significant bits of w(u, v)
    if the ith most significant bit of w(u, v) = 0, wi(u, v) = 2wi-1(u, v)
    if the bit is 1, wi(u, v) = 2wi-1(u, v) + 1
    let p be a shortest path that achieves δi-1(s, v)
    with new weight function wi, wi(p) is increased to at most 2wi(p) + |p| <= 2wi(p) + |V| - 1 = 2δi-1(s, v) + |V| - 1
    let p' be any path s ~> v, it must have wi-1(p') >= wi-1(p) = δi-1(s, v)
    wi(p') >= 2wi-1(p') = 2δi-1(s, v)
    therefore 2δi-1(s, v) <= δi(s, v) <= 2δi-1(s, v) + |V| - 1
d.  by triangle property, δi-1(s, v) <= δi-1(s, u) + wi-1(u, v)
    δi-1(s, u) - δi-1(s, v) >= -wi-1(u, v)
    wi(u, v) + 2δi-1(s, u) - 2δi-1(s, v) >= wi(u, v) - 2wi-1(u, v) >= 0
e.  let p = (v0, v1 .. vk) be a shortest path v0 = s ~> v = vk according to weight function ^wi
    ^wi(p)  = Σ(wi(vj, vj+1) + 2δi-1(s, vj) - 2δi-1(s, vj+1))
            = Σ(wi(vj, vj+1)) - 2δi-1(s, v), as all other 2δi-1(s, vj) cancelled each other
    the last term 2δi-1(s, v) is independent to the path ^wi(p), p must minimize Σ(wi(vj, vj+1)) = wi(p)
    wi(p) = δi(s, v) = ^wi(p) + 2δi-1(s, v) = ^δi(s, v)
    by part d, ^δi(s, v) = δi(s, v) - 2δi-1(s, v) <= |V| - 1 <= |E|
f.  ^wi(u, v) can be computed from δi-1(s, u) and δi-1(s, v) in O(E)
    as ^δi(s, v) <= |E| for all v ∈ V, ^δi(s, v) can be computed in O(E)
    δi(s, v) can be computed from ^δi(s, v) + 2δi-1(s, v) in O(V)
    O(V + E) = O(E) in total
    therefore δk(s, v) can be computed in O(ElgW) for k = lg(W + 1)
    ./CLRS/graph/shortest-path.ts#spGabow

24-5
a.  if G contains a negative cycle c, μ* <= μ(c) = w(c) / k < 0
    if G contains no negative, cycle, each shortest path is a simple path
    let p be a shortest path s ~> v, w(p) = δ(s, v) = δ|p|(s, v), |p| <= |V| - 1
    δk(s, v) >= δ(s, v), w(p) is the minimum among all δk(s, v)
    δ(s, v) = min(0 <= k <= n-1){δk(s, v)} = w(p)
b.  as 0 <= k <= n-1, n - k > 0
    as μ* = 0, let p be a shortest path s ~> v, |p| <= |V| - 1, δ|p|(s, v) = w(p) <= δk(s, v) for all k
    thus δn(s, v) - δ|p|(s, v) >= 0
    max(0 <= k <= n-1)((δn(s, v) - δk(s, v)) / (n - k))
    >= (δn(s, v) - δ|p|(s, v)) / (n - k) >= 0
c.  by transitivity and triangle property, δ(s, v) <= δ(s, u) + x
    similarly, δ(s, u) <= δ(s, v) - x, δ(s, v) >= δ(s, u) + x
    δ(s, v) = δ(s, u) + x combined
d.  the minimum mean weight cycle has w(c) = kμ(c) = kμ* = 0
    let p = s ~> v0 be the shortest path to the first vertex on c
    (always possible by optimal substructure, if u on p is also on c, s ~> u is a shortest path to u)
    let v1 be the next vertex on the cycle, then p' = p -> vi is a shortest path s ~> v1 by part c
    repeat the process until |p'| = n, p' is the shortest path s ~> vk for some vk on c
    however |p'| > |V| - 1, since μ* = 0, there is another shortest path p'' of length <= |V| - 1 to vk
    δn(s, vk) = δ(s, vk) = δ|p''|(s, vk), δn(s, vk) <= δk(s, vk) for all k
    (δn(s, vk) - δ|p''|(s, vk)) / (n - |p''|) = 0
    max{(δn(s, vk) - δk(s, vk)) / (n - k)} = 0
e.  combine part b and part d
f.  let c be the minimum mean weight cycle before the increase
    if after the increase another cycle c' becomes the minimum mean weight
        w'(c') / |c'| < w'(c) / |c|
        (w(c') + |c'|t) / |c'| < (w(c) + |c|t) / |c|
        w(c') / |c'| < w(c) / |c|
    c' should also be the minimum mean weight cycle before the increase
    thus c is also the minimum mean weight cycle after the increase
    w(c) is increased by |c|t, μ* increased to (w(c) + |c|t) / |c| = w(c) / |c| + t = μ* + t
    similarly a shortest path p that achieves δ|p|(s, v) still achieves δ|p|(s, v) after the increase
    let μ* be the minimum mean weight of a graph G
    decrease each w(u, v) by μ(c) results a graph G' where μ* = 0 in G'
    let δ' be shortest paths in G'
    by part e, min(v ∈ G'.V){max(0 <= k <= n-1){(δ'n(s, v) - δ'k(s, v)) / (n - k)}} = 0
    δk(s, v) = δ'k(s, v) + kμ*
    min(v ∈ G.V){max(0 <= k <= n-1){(δn(s, v) - δk(s, v)) / (n - k)}}
    = min(v ∈ G.V){max(0 <= k <= n-1){(δ'n(s, v) - δ'k(s, v) + (n-k)μ*) / (n - k)}}
    = min(v ∈ G.V){max(0 <= k <= n-1){(δ'n(s, v) - δ'k(s, v)) / (n - k)}} + μ*
    = μ*
g.  run a procedure similar to BELLMAN-FORD
    but instead of update v.d in each pass, store v.d computed in each pass in a new slot in an (|V| + 1) x |V| array
    the array is initialized with +∞ in each slot other than A[0, s], A[0, s] = 0
    let s ~> u -> v be a shortest path with exactly k edges from s to v
    s ~> u then must be a shortest path with exactly k - 1 edges from s to u
    or by cut and paste δk(s, v) can be improved
    δk(s, v) thereby can be computed as min((u, v) ∈ E){δk-1(s, u) + w(u, v)}
    row k in A can be computed from row k-1 in time O(E), O(VE) in total
    once A is fully computed, 
        μ* = (v ∈ G.V){max(0 <= k <= n-1){(δn(s, v) - δk(s, v)) / (n - k)}}
    is the row minimum of column maximums, μ* can be computed in O(VE)
    a proper source s can be found by dfs from all vertices, all v ∈ V reachable from s iff dfs colored all v BLACK
    V dfs takes O(V(V + E)) = O(VE)
    O(VE) in total
    ./CLRS/graph/shortest-path.ts#minimumMeanWeightCycle

24-6
sort the edges by weight, relax each edge twice, once in sorted order, once in reverse sorted order
since all edges have unique weight, the first pass will relax each edge before the turning point in order
the second pass will relax each edge after the turning point in order
O(V + ElgE) in total

Chapter 25
25.1-1
Slow
[ [ 0, 6, ∞, ∞, -1, ∞ ],
  [ -2, 0, ∞, 2, 0, ∞ ],
  [ 3, -3, 0, 4, ∞, -8 ],
  [ -4, 10, ∞, 0, -5, ∞ ],
  [ 8, 7, ∞, 9, 0, ∞ ],
  [ 6, 5, 10, 7, ∞, 0 ] ]
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ -2, -3, 0, -1, 2, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 5, 0 ] ]
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ -5, -3, 0, -1, -3, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ -5, -3, 0, -1, -6, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]
Fast
[ [ 0, 6, ∞, ∞, -1, ∞ ],
  [ -2, 0, ∞, 2, 0, ∞ ],
  [ 3, -3, 0, 4, ∞, -8 ],
  [ -4, 10, ∞, 0, -5, ∞ ],
  [ 8, 7, ∞, 9, 0, ∞ ],
  [ 6, 5, 10, 7, ∞, 0 ] ]
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ -5, -3, 0, -1, -3, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ -5, -3, 0, -1, -6, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]

25.1-2
because bad code
W should strictly reflect the weight of edges, if there's no self loop on i then W[i, i] should be +∞
all calculation should start from a unit matrix L(0) described in 25.1-3
otherwise the definition of W will cause trouble in 25.1-7
L(1) in the text can be computed from L(0) x W
equation 25.2 still holds, W[i, i] = 0 or +∞ won't make any change as L[i, i] + W[i, i] >= L[i, i] >= L'[i, i]
starts from L(0) and run one more iteration
./CLRS/graph/all-pair-shortest-path.ts#slowAllPairShortestPaths
starts from L(0) x W
./CLRS/graph/all-pair-shortest-path.ts#fasterAllPairShortestPaths

25.1-3
in EXTEND-SHORTEST-PATHS(L, L0),
    L'[i, j]    = min{L[i, k] + L0[k, j]}
                = L[i, j] + L0[j, j] = L[i, j]
in EXTEND-SHORTEST-PATHS(L0, L),
    L'[i, j]    = min{L0[i, k] + L[k, j]}
                = L0[i, i] + L[i, j]
                = L[i, j]
therefore EXTEND-SHORTEST-PATHS(L, L(0)) or EXTEND-SHORTEST-PATHS(L(0), L) always results L
L0 is equivalent to unit matrix in matrix multiplication

25.1-4
let L = (L0 x L1) x L2
L[i, j] = minp{mink{L0[i, k] + L1[k, p]} + L2[p, j]}
        = minp,k{L0[i, k] + L1[k, p] + L2[p, j]} as L2[p, j] independent to k
let L' = L0 x (L1 x L2)
L'[i, j]    = mink{L0[i, k] + minp{L1[k, p] + L2[p, j]}}
            = mink,p{L0[i, k] + L1[k, p] + L2[p, j]} as L0[i, k] independent to p
therefore L = L', EXTEND-SHORTEST-PATHS is associative

25.1-5
let L be an n-dimensional vector initialized with L[s] = 0 and L[v] = +∞ for all v ∈ V - {s}
L represents the weight of shortest path s ~> v for all v ∈ V with at most 0 edges
assume L(m) is the vector of weight of shortest path s ~> v with at most m edges
L(m+1)[j] can be computed as min{L(m)[i] + W[i, j]}, which is similar to multiplication between matrix and vector
considering only finite W[i, j], updating L(m+1)[j] to L(m)[i] + W[i, j] is equivalent to relaxing the edge (i, j)

25.1-6
any L[i, k] that L[i, k] + W[k, j] = L[i, j] may be a predecessor of j on the shortest path i ~> j
a problem is when there is a 0-weight cycle in the graph
predecessor graph constructed in this way may also contain a cycle
this case can be avoided by an explicit check that j is not an ancestor of Π[i, k]
doing the check for each k may cost O(n^2), O(n^4) in total
when traversing Π[i, k], if j is an ancestor of k in Gπ,i, all the ancestors of k up to i cannot be predecessor of i
thest vertices can be cached in an auxiliary binary array A of size n
a check refers the array first, traverse predecessors only when A[k] is not set
set all predecessors of k up to i after the traverse
each vertex is thus checked at most once, all n checks can be done in O(n) amortized time, O(n^3) in total
./CLRS/graph/all-pair-shortest-path.ts#predecessorMatrix

25.1-7
./CLRS/graph/all-pair-shortest-path.ts#extendShortestPaths
./CLRS/graph/all-pair-shortest-path.ts#slowAllPairShortestPaths

25.1-8
allocate two matrices L and L'
compute each EXTEND-SHORTEST-PATHS(L, L) inplace in L'
next iteration compute EXTEND-SHORTEST-PATHS(L', L') inplace in L

25.1-9
perform another iteration of EXTEND-SHORTEST-PATHS, check if any entry changed
no entry should change if the graph contains no negative cycle by argument in the text
if there is a negative cycle in the graph, the shortest path weight may be decreased indifinitely
however if EXTEND-SHORTEST-PATHS(L, L) = L, it will never change in any iteration afterwards
therefore EXTEND-SHORTEST-PATHS(L, L) = T must be different to L

25.1-10
by 24.1-6, negative cycle will be a cycle in the predecessor graph
traverse Gπ,i to find negative cycle, assign the cycle length to all vertices traversed
O(V) aggregated time for each Gπ,i, O(n^2) for all rows
O(n^3lgn) when combined with the time calculating Gπ,i at the first place

25.2-1
Before first iteration, D:
[ [ 0, ∞, ∞, ∞, -1, ∞ ],
  [ 1, 0, ∞, 2, ∞, ∞ ],
  [ ∞, 2, 0, ∞, ∞, -8 ],
  [ -4, ∞, ∞, 0, 3, ∞ ],
  [ ∞, 7, ∞, ∞, 0, ∞ ],
  [ ∞, 5, 10, ∞, ∞, 0 ] ]
Iteration 0, D0:
[ [ 0, ∞, ∞, ∞, -1, ∞ ],
  [ 1, 0, ∞, 2, 0, ∞ ],
  [ ∞, 2, 0, ∞, ∞, -8 ],
  [ -4, ∞, ∞, 0, -5, ∞ ],
  [ ∞, 7, ∞, ∞, 0, ∞ ],
  [ ∞, 5, 10, ∞, ∞, 0 ] ]
Iteration 1, D1:
[ [ 0, ∞, ∞, ∞, -1, ∞ ],
  [ 1, 0, ∞, 2, 0, ∞ ],
  [ 3, 2, 0, 4, 2, -8 ],
  [ -4, ∞, ∞, 0, -5, ∞ ],
  [ 8, 7, ∞, 9, 0, ∞ ],
  [ 6, 5, 10, 7, 5, 0 ] ]
Iteration 2, D2:
[ [ 0, ∞, ∞, ∞, -1, ∞ ],
  [ 1, 0, ∞, 2, 0, ∞ ],
  [ 3, 2, 0, 4, 2, -8 ],
  [ -4, ∞, ∞, 0, -5, ∞ ],
  [ 8, 7, ∞, 9, 0, ∞ ],
  [ 6, 5, 10, 7, 5, 0 ] ]
Iteration 3, D3:
[ [ 0, ∞, ∞, ∞, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ 0, 2, 0, 4, -1, -8 ],
  [ -4, ∞, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]
Iteration 4, D4:
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ 0, 2, 0, 4, -1, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]
Iteration 5, D5:
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ -5, -3, 0, -1, -6, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]

25.2-2
assign weight 1 to each edge
compute shortest paths by FASTER-ALL-PAIR-SHORTEST-PATHS
set finite entries to 1, infinite entries to 0

25.2-3
./CLRS/graph/all-pair-shortest-path.ts#floydWarshall
Π[i, j] is only updated together with D[i, j]
if Π[i, j] = l, D[i, j] must be updated to D[i, l] + W[l, j]
later D[i, l] may be updated to a smaller value but never greater, thus D[i, j] >= D[i, l] + W[l, j]
if some update forms a cycle, let c = (v0 .. vk), v0 = vk
let D[i, vk] = D[i, vk-1] + W[vk-1, vk] be the update that forms the cycle
Π(i, vj) = vj-1 for j = 0 .. k-1
by the above inequality, D[i, vj] >= D[i, vj-1] + W[vj-1, vj] for j = 0 .. k-1
as D[i, vk] is updated, before the update it must have D[i, vk] > D[i, vk-1] + W[vk-1, vk]
add both sides of all the inequalities gives
    0 > ΣW[vj-1, vj] for j = 0 .. k
which means there is a negative cycle on the graph
thereby if the graph contains no negative cycle, the predecessor graph Gπ,i must be acyclic for each i
furthermore since each vertex has at most 1 parent, the acyclic graph is actually a tree

25.2-4
D[i, k] and D[k, j] for each i, j will not be updated in iteration k since
    D[i, k] = min(D[i, k], D[i, k] + D[k, k]) = min(D[i, k], D[i, k]) = D[i, k]
    D[k, j] = min(D[k, j], D[k, k] + D[k, j]) = D[k, j]
therefore each other update that refers D[i, k] and D[k, j] will still refer those values from the last iteration
exactly the same to FLOYD-WARSHALL defined in the text

25.2-5
it's no longer correct
the behavior only changes when D[i, j] == D[i, k] + D[k, j]
if both sides are finite, i ~> k ~> j is an equally optimal path compared to i ~> j computed in iteration k-1
both the original D[i, j] or the path composed from D[i, k] and D[k, j] may be the solution to iteration k
however if both sides are infinite, Π[i, j] must be NIL since j is not yet reachable from i in current iteration
when D[i, k] = +∞ but D[k, j] is finite, Π[i, j] may be incorrectly updated to a non-NIL value Π[k, j]

25.2-6
if the graph contains no negative cycle
run another pass of EXTEND-SHORTEST-PATHS(D, W) should not update any entry of D returned by FLOYD-WARSHALL
if the graph contains negative cycle, EXTEND-SHORTEST-PATHS(D, W) must update some entry of D
after computing D in O(n^3), presence of a negative cycle can be detected in O(n^2)

25.2-7
Φ(k)[i, j]  = Φ(k-1)[i, j] if D(k-1)[i, j] <= D(k-1)[i, k] + D(k-1)[k, j]
            = k if D(k-1)[i, j] > D(k-1)[i, k] + D(k-1)[k, j]
every entry in Φ(0) is NIL since no paths have intermediate vertices yet
shortest path can be printed recursively, assume D[i, j] != +∞
by optimal substructure, if Φ[i, j] = k, i ~> k ~> j is a shortest path from i to k
recursively search the shortest path i ~> k and k ~> j
the two path combined (cycle deducted) is the shortest path i ~> j
the base case is Φ[i, j] = NIL and the shortest path i ~> j is a single edge (i, j)
like most other dynamic programming algorithm introduced in the text, Φ records choices made at each step
a shortest path can be reconstructed from Φ, similar to s in matrix chain multiplication

25.2-8
first detect and exclude all the vertices with in degree = out degree = 0 in O(V + E)
transitive closure rows of these vertices will simply be a 1 in slot (i, i) for some i and all 0 otherwise
the resulting graph must have |E| >= |V| / 2 since edges covers the vertices, |V| = O(E)
run single source dfs from all the vertices, set the entries according to the colors of vertices after dfs
O(V(V + E) + V + E) = O(V^2 + VE) = O(VE) in total

25.2-9
errata: 
    https://www.cs.dartmouth.edu/~thc/clrs-bugs/bugs-3e.php 
    change the time bound f(|V|, |E|) + O(V + E*) to O(f(|V|, |E|) + V + E*)
compute SCCs and SCC graph of G in O(V + E)
the SCC graph is acyclic with edges fewer than G, the transitive closure of it can be computed in f(|V|, |E|)
group vertices with their SCC index u.cc, for each v ∈ V,
    add (u, v) for each v in SCC reachable from u.cc by the transitive closure of SCC graph
E* edges are added in total, running time O(f(|V|, |E|) + V + E*) combined

25.3-1
h: [ -5, -3, 0, -1, -6, -8 ]
w: [ 0, 3, 0, 5, 0, 0, 8, 4, 0, 2 ]
[ [ 0, 6, ∞, 8, -1, ∞ ],
  [ -2, 0, ∞, 2, -3, ∞ ],
  [ -5, -3, 0, -1, -6, -8 ],
  [ -4, 2, ∞, 0, -5, ∞ ],
  [ 5, 7, ∞, 9, 0, ∞ ],
  [ 3, 5, 10, 7, 2, 0 ] ]

25.3-2
there may be no vertex u in G that all v ∈ V is reachable from u, δ(u, v) may be infinity
with s and w(s, v) = 0 for all v ∈ V, δ(s, v) is well-defined and finite

25.3-3
δ(s, v) = 0 for all v ∈ V since w(u, v) >= 0 and w(s, v) = 0
^w(u, v) = w(u, v) + δ(s, u) - δ(s, v) = w(u, v)

25.3-4
a shortest path according to ^w is not necessarily a shortest path according to w
if w(u, v) = W a constant for all (u, v) ∈ E, w* = W
^w(u, v) = w(u, v) - w* = 0 for all (u, v) ∈ E
a shortest path according to w is a path u ~> v with fewest edges
a shortest path according to ^w is just any path u ~> v

25.3-5
let c = (v0, .., vk) where v0 = vk
edges on the cycle thus are (v0, v1) .. (vk-1, vk)
by equation 25.10
    Σ^w(vj-1, vj) = ^w(c) = w(c) = 0
by definition, ^w(vj-1, vj) >= 0, hence ^w(vj-1, vj) = 0 for all 0 <= j <= k

25.3-6
define G with vertices u, v and edge (u, v), w(u, v) = c
when v is chosen as the start vertex, h(v) = 0, h(u) = +∞, ^w(u, v) = +∞, c is lost
if G is strongly connected, from any u δ(u, v) is finite for all v ∈ V
h(v) computed will be finite, the w(u, v) can be recovered from  ^w(u, v)

25-1
a.  assume an edge (u, v) is added to G, let T be the transitive matrix
    if T[u, v] == true, the new edge will not change G*
    otherwise if u is reachable from some vertex s, all t ∈ Adj*[v] is now reachable from s
    similar to u, if T[s, v] == true, adding (u, v) will not update Adj*[s]
    all such s can be find by scanning a column in O(V), then row s and v is zipped with || operator in O(V)
    overall running time is O(V^2)
b.  define a graph in which:
        let s, t be two distinct vertices
        E = {(s, v) | v ∈ V} ∪ {(v, t) | v ∈ V}
    hence each vertex is reachable from s, t is reachable from all vertices
    Adj*[v] = {v, t} for v ∈ V - {s, t}, Adj*[s] = V, Adj*[t] = {t}
    |E*| = 2(|V|-2) + 1 + |V| = 3|V| - 3
    add the edge (t, s) to the graph makes each vertex reachable from any other vertices
    |E*| = |V|^2, |V|^2 - 3|V| + 3 = Ω(V^2) entries have to be set from 0 to 1
c.  algorithm described in part a has already achieved the upper bound
    if no zip operation is triggered (T[s, v] == true for all s), the algorithm scans a column and terminate in O(V)
    if otherwise some zip operation is triggered, it sets T[s, v] to true, which is originally false
    there are |V|^2 entries in the matrix, after |V|^2 zip operations, no further zip operations may happen
    the aggregated running time thereby is O(nV + V^3)
    since there are at most |V|^2 edges in G, n = O(V^2), overall running time is O(V^3)

25-2
a.  by 6-2:
        (n^α)^(1/α) = n, thus log(n^α, n) = 1/α
        INSERT:         O(log(d, n)) = O(1/α) = O(1)
        EXTRACT-MIN:    O(dlog(d, n)) = O(n^α * 1/α) = O(n^α)
        DECREASE-KEY:   O(log(d, n)) = O(1)
    fibonacci heap:
        INSERT:         O(1)
        EXTRACT-MIN:    O(lgn)
        DECREASE-KEY:   O(1)
b.  using Dijkstra's algorithm
    shortest paths can be computed with O(V) INSERT, O(V) EXTRACT-MIN and O(E) DECREASE-KEY
    let α = ε, O(V) EXTRACT-MIN can be done in O(V * V^α) = O(V^(1 + ε)) = O(E)
    O(V + E + E) = O(E) combined
c.  run the algorithm described in part b from each vertex
    O(VE) in total
d.  reweight edges in the same way to Johnson's algorithm in O(VE) before running algorithm in part c, O(VE) in total

Chapter 26
26.1-1
let f be a maximum flow of G
f(u, v) <= c(u, v), the flow can go through u ~> x ~> v instead since
    c'(u, x) = c'(x, v) = c(u, v) >= f(u, v)
    x doesn't exist in D, no flow through x in f
let f' be the flow where f'(u, v) = 0 and f'(u, x) = f'(x, v) = f(u, v)
f(u, v) amount of flow still leaves u and enters v
f' is a valid flow defined on G' with same amount of flow leaving s to f
let f' be a maximum flow of G'
since (u, x) is the only edge entering x, (x, v) is the only edge leaving x
by flow conservation, f'(u, x) = f'(x, v) <= c'(x, v) = c(u, v)
the flow can go through (u, v) instead, still f(u, v) = f'(u, x) = f'(x, v) enters v and leaves u
f is a valid flow with same amount of flow leaving s to f
therefore the maximum flow of G is no greater than the maximum of G' and vice versa
G and G' have the same maximum flow

26.1-2
it's not equivalent
let |fi| be the flow of a source si, |gi| be the flow of a sink ti
when |f| < 0, there's no trivial way to redirect the additional flow into some source to restore flow conservation
assume |f| >= 0, the flow into each source si is less or equal to the flow leaving each source
add a supersource and set f(s, si) = |fi| for each source si (always possible since c(s, si) = +∞)
symmetrically add a supersink and set f(ti, t) = -|gi|
flow conservation is restored for all sources and sinks, the resulting flow is still valid
conversely the supersink and supersource can be simply removed with incident edges
as supersource and supersink have edges only to sinks and sources, removing them will not violate flow conservation

26.1-3
thanks http://sites.math.rutgers.edu/~ajl213/CLRS/Ch26.pdf
let d(u) = Σf(u, v) - Σf(v, u)
by flow conservation, d(u) = 0 for all u ∈ V - {s, t}
each flow leaving a vertex must enter another, Σd(u) = 0, thus |f| = d(s) = -d(t)
if for some v0, f(v0, u) > 0
by flow conservation, there must be the same amount of flow leaving u
let w0 be a vertex that f(u, w0) > 0, w0 must exist
if there is a path s ~> u, there's no path u ~> t
by flow conservation, until reaching t
the path u ~> wi can always be extended with an edge (wi, wi+1) which has f(wi, wi+1) > 0
but there's no path u ~> t and |G.V| is finite, u ~> wi must form a cycle somewhere
decrease all f(wi, wi+1) by the minimum flow on the cycle then sets at least one positive f(wi, wi+1) to 0
the decrease will not affect |f| since t is not on the cycle, d(t) will not be changed as well as d(s)
symmetrically if there is a path u ~> t, one positive f(wi, wi+1) may be decreased to 0 without affecting |f|
as long as Σf(u, v) > 0, positive flow on some edge can be decreased to zero
since |G.E| is finite, there is a maximum flow where Σf(u, v) = Σf(v, u) = 0

26.1-4
for any u ∈ V - {s, t}
if f1 and f2 are valid flows, for each flow (v, u) entering u,
    f1(v, u) <= c(v, u), αf1(v, u) <= αc(v, u)
    f2(v, u) <= c(v, u), (1 - α)f2(v, u) <= (1-α)c(v, u)
    αf1(v, u) + (1-α)f2(v, u) <= αc(v, u) + (1-α)c(v, u) = c(v, u)
symmetrically for each flow (u, v) leaving u, αf1(u, v) + (1-α)f2(u, v) <= c(u, v)
the flow f' = αf1 + (1-α)f2 satisfies capacity constraint
also for any u ∈ V - {s, t}
    Σf'(v, u)   = αΣf1(v, u) + (1-α)Σf2(v, u)
                = αΣf1(u, v) + (1-α)Σf2(u, v)
                = Σf'(u, v)
f' also satisfies flow conservation, f' is a valid flow defind on the graph

26.1-5
define an unknown for each edge
flow conservation can be translated to
    (u, v) <= c(u, v)
flow conservation around a vertex u can be translated to
    Σ(v ∈ V)(v, u) - Σ(v ∈ V)(u, v) = 0
the objective function is
    Σ(v ∈ V)(s, v) - Σ(v ∈ V)(v, s)

26.1-6
give each road a capacity 1
determine the maximum flow |f| from the house to the school
iff |f| >= 2, it's possible to send both children to school

26.1-7
define G' by divide each vertex u to two vertices ui, uo and an edge (ui, uo)
c'(ui, u0) = c(u), the vertex capacity of u
each edge enters u now enters ui, each edge leaves u now leaves uo, their capacity unchanged
a valid flow f in G is a valid flow in G' since:
    capacity constraint of each edge in G still satisfied
    Σf(v, u) <= c(u) = c'(ui, u0) = Σf(u, v), capacity constraint of each (ui, u0) is satisfied
    Σf'(v, ui) = Σf(v, u) = Σf(u, v) = Σf'(uo, v)
    by setting f(ui, u0) = Σf(u, v) = Σf(v, u), flow conservation of each new vertex is restored
conversely:
    since (ui, uo) is the only edge entering uo and the only edge leaving ui
    Σf'(v, ui) = Σf'(uo, v) = f'(ui, uo) <= c(ui, uo) = c(u)
    merging ui and uo satisfies vertex capacities
|V'| = 2(|V| - 2) + 2, |E'| = (|V| - 2) + |E|

26.2-1
since V1 = {v | (s, v) ∈ E},
    Σ(v ∈ V - V1)f(s, v) = 0
    Σ(v ∈ V)f(s, v) = Σ(v ∈ V1)f(s, v)
similarly since V2 = {v | (v, s) ∈ E},
    Σ(v ∈ V - V2)f(v, s) = 0
    Σ(v ∈ V)f(v, s) = Σ(v ∈ V2)f(v, s)
by the definition of residual graph, f'(s, v) > 0 iff f(s, v) > 0 or f(v, s) > 0
which means (s, v) ∈ E or (v, s) ∈ E, v ∈ V1 ∪ V2
    Σ(v ∈ V - (V1 ∪ V2))f'(s, v) = 0
    Σ(v ∈ V)f'(s, v) = Σ(v ∈ V1 ∪ V2)f'(s, v)
similarly,
    Σ(v ∈ V - (V1 ∪ V2))f'(v, s) = 0
    Σ(v ∈ V)f'(v, s) = Σ(v ∈ V1 ∪ V2)f'(v, s)

26.2-2
flow is 11 + 1 + 7 + 4 - 4 = 19
capacity is 16 + 4 + 7 + 4 = 31

26.2-3
Before iteration 0
s -> v1: 0/16
s -> v2: 0/13
v1 -> v3: 0/12
v2 -> v1: 0/4
v2 -> v4: 0/14
v3 -> v2: 0/9
v3 -> t: 0/20
v4 -> v3: 0/7
v4 -> t: 0/4
The maximum flow is 0
Before iteration 1
s -> v1: 12/16
s -> v2: 0/13
v1 -> v3: 12/12
v2 -> v1: 0/4
v2 -> v4: 0/14
v3 -> v2: 0/9
v3 -> t: 12/20
v4 -> v3: 0/7
v4 -> t: 0/4
The maximum flow is 12
Before iteration 2
s -> v1: 12/16
s -> v2: 4/13
v1 -> v3: 12/12
v2 -> v1: 0/4
v2 -> v4: 4/14
v3 -> v2: 0/9
v3 -> t: 12/20
v4 -> v3: 0/7
v4 -> t: 4/4
The maximum flow is 16
Final result
s -> v1: 12/16
s -> v2: 11/13
v1 -> v3: 12/12
v2 -> v1: 0/4
v2 -> v4: 11/14
v3 -> v2: 0/9
v3 -> t: 19/20
v4 -> v3: 7/7
v4 -> t: 4/4
The maximum flow is 23

26.2-4
the minimum cut is ({s, v1, v2, v4}, {v3, t}) with capacity 23
c cancels f(v3, v2)

26.2-5
by lemma 26.4, any cut has net flow |f|
assume there are no edges with infinite capacity in the original multi-source/sink graph G
|f| must be finite in G
let ss, st be the supersource and supersink in the resulting graph G'
the cut ({ss}, G'.V - {ss}) has net flow |f|
the only edges crossing the cut are (ss, si) for each source si in G, no edge entering ss
hence |f| = Σf(ss, si) is finite, f(ss, si) >= 0, each f(ss, si) is finite
similarly |f| = Σ(ti, st) is finite, each f(ti, st) is finite
those are the only edges added to G', all flow in G' is finite

26.2-6
define c(ss, si) = pi and c(ti, st) = qi
by an argument similar to 26.1-2, a valid flow in G is a valid flow in G'

26.2-7
as p is a simple path, each edge other than s and t on p is entered and leaved exactly once
for a vertex u != s or t on p, 
    exactly one edge entering u has flow cf(p)
    exactly one edge leaving u has flow cf(p)
Σfp(v, u) = cf(p) = Σfp(u, v), flow conservation is satisfied
for a vertex u != s or t not on p,
    all edges entering u have flow 0
    all edges leaving u have flow 0
Σfp(v, u) = 0 = Σfp(u, v), flow conservation is satisfied
cf(p) is defined as min{cf(u, v) | (u, v) on p}
for all edges (u, v) on p, cf(u, v) >= cf(p), capacity constraints are satisfied
for all edges (u, v) not on p, cf(u, v) >= 0, capacity constraints are satisfied
therefore fp is a valid flow defined on Gf
there is exactly one edge leaving s and no edge entering s on p,
    |f| = Σfp(s, v) - Σfp(v, s) = cf(p) - 0 = cf(p) >= 0

26.2-8
an augment path is simple and starts from s
such edges will never be on any augment path, hence will not affect the algorithm

26.2-9
if (u, v) ∈ E, by definition of flow network (v, u) ∉ E
    (f↑f')(u, v) = f(u, v) + f'(u, v) - f'(v, u) = f(u, v) + f'(u, v)
    Σ(f↑f')(u, v) = Σf(u, v) + Σf'(u, v) = Σf(v, u) + Σf'(v, u) = Σ(f↑f')(v, u)
flow conservation is still satisfied
however capacity constraints are violated
define G with vertices s, t and a single edge (s, t) with capacity 1
a flow f(s, t) = 1 is a valid and maximum flow defined on G
but (f↑f)(s, t) = 2 > c(s, t), the augmented flow is not valid

26.2-10
thanks http://www.cs.princeton.edu/courses/archive/spr03/cs423/download/PS5-solutions.doc
by 26.2-8, there is always a maximum flow with no flow entering the source 
given the maximum flow f, find any path p = s ~> t where f(u, v) > 0 for all (u, v) on p
such a path always exist as long as |f| > 0:
    if otherwise |f| > 0 and there's no path p = s ~> t where f(u, v) > 0 for all (u, v) on p
    considering only the edges with positive flow, the graph is disconnected
    thereby there must be a cut (S, T) that all edges crossing it has zero flow
    by Lemma 26.4 |f| = 0, in contradiction to |f| > 0
take fp = min{f(u, v) | (u, v) on p} > 0, decrease all f(u, v) on p by fp results another flow f'
the residual graph Gf' must contain an augment path p, since c(u, v) >= f(u, v) > f(u, v) - fp = f'(u, v) on p
f can be computed from augmenting f' with the path p and path flow fp
also f' contains at least one more edge with zero flow compared to f
after at most |E| such augmentation, |f| = 0
by assumption no flow enters the source, Σf(s, v) - Σf(v, s) = Σf(s, v) = 0, f(s, v) = 0 for all v ∈ V
if f(u, v) > 0 for some (u, v), it must be on a loop without reaching s or t
since the procedure only decreases flows, it must contain such a loop back when it is still the maximum flow
choose a maximum flow with no redundent loop of flow then guarentees f(u, v) = 0 for all (u, v) ∈ E after |E| steps
reverse the process, an initial empty flow can be augmented to a maximum flow with at most |E| augmentation

26.2-11
on a directed graph with c(u, v) = 1 for each (u, v) ∈ E
for any two vertices s and t, let f be the maximum flow from s to t
similar to 26.1-6, |f| is the maximum number of edge-disjoint paths from s to t
claim: |f| is the minimum number of edges have to be removed before there's no path between s and t
    since there are |f| edge-disjoint paths from s to t
    removing k < |f| edges will leave at least |f| - k > 0 such paths intact
    thus removing k < |f| edges will not disconnect s and t
    by max-flow min-cut theorem, there is a cut (S, T) of G with capacity |f|
    Σ(u ∈ S, v ∈ T)c(u, v) = |f|, as c(u, v) = 1 for all (u, v), there are |f| edges from S to T
    remove all of them thereby disconnects S and T, no path from s to t after the removing
for a particular vertex s on an undirected graph G is disconnected iff there's some t that no path s ~> t exists
hence by choosing randomly a source s and for all t ∈ V, calculate min{|f| | t ∈ V} gives the connectivity of G
G have to be transformed first to a valid flow network G' with no antiparallel edges
|G'.V| = |V| + |E|, |G'.E| = 3|E|
(is there any way to remove antiparallel edges while keeping |G'.V| = O(V)?)

26.2-12
EDMONDS-KARP correctly computes a maximum flow by repeatedly finding augment paths
an edge (v, s) will never appear on an augment path, hence EDMONDS-KARP will never set f(v, s) > 0
the maximum flow computed by EDMONDS-KARP must have f(v, s) = 0 for all v ∈ V
construct a new graph G' containing only edges (u, v) ∈ G.E where f(u, v) > 0 in O(V + E)
do dfs from s, there must be a tree path s ~> v:
    let S be the set of vertices v where there is a path s ~> v with positive flow
    V - S is the set of vertices with no positive flow path from s
    Σ(u ∈ V - S)Σ(v ∈ V)f(u, v) = Σ(u ∈ V - S)Σ(v ∈ V)f(v, u)
    Σ(u ∈ V - S)Σ(v ∈ V - S)f(u, v) + Σ(u ∈ V - S)Σ(v ∈ S)f(u, v)
    = Σ(u ∈ V - S)Σ(v ∈ V - S)f(v, u) + Σ(u ∈ V - S)Σ(v ∈ S)f(v, u)
each flow within V - S enters and leaves exactly one vertex, thus
    Σ(u ∈ V - S)Σ(v ∈ V - S)f(u, v) = Σ(u ∈ V - S)Σ(v ∈ V - S)f(v, u)
    Σ(u ∈ V - S)Σ(v ∈ S)f(u, v) = Σ(u ∈ V - S)Σ(v ∈ S)f(v, u)
by definition of V - S, 
    Σ(u ∈ V - S)Σ(v ∈ S)f(v, u) = 0, otherwise u ∈ S
    Σ(u ∈ V - S)Σ(v ∈ S)f(u, v) = Σ(u ∈ V - S)Σ(v ∈ S)f(v, u) = 0
    f(u, v) = 0 for all u ∈ V - S and v ∈ S
as f(v, s) > 0, there must be a positive path s ~> v, v is reachable from s during the dfs by white path theorem
the tree path s ~> v and (v, s) forms a cycle
by assumption, f(x, y) > 0 for (x, y) on the cycle, f(x, y) is integral, f(x, y) >= 1
decrease all these f(x, y) by 1 sets f(v, s) to 0 and gives an equally optimal f'

26.2-13
define c'(u, v) = |E|c(u, v) + 1
let C be a minimum cut of G, let C' be another cut that c(C') > c(C)
as capacities are integral, c(C') >= c(C) + 1
let |C| be the number of forward edges crossing C, 1 <= |C| <= |E|, |C| - |C'| < |E|
    c'(C')  = Σ(C' cuts (u, v))c'(u, v)   
            = Σ(C' cuts (u, v))(|E|c(u, v) + 1)
            = |E|Σ(C' cuts (u, v))c(u, v) + |C'|
            > |E|Σ(C cuts (u, v))c(u, v) + |E| + |C| - |E|
            = Σ(C cuts (u, v))(|E|c(u, v) + 1)
            = c'(C)
a non-minimum cut in G is also a non-minimum cut in G'
among all minimum cut of G, Σ(C cuts (u, v))(|E|c(u, v)) is the same
the minimum cut of G' must be the the minimum cut of G that minimizes the additional constant
which sums up to the number of forward edges crossing the cut

26.3-1
Before iteration 0
1 -> 6: 1
2 -> 6: 1
2 -> 8: 1
3 -> 7: 1
3 -> 8: 1
3 -> 9: 1
4 -> 8: 1
5 -> 8: 1
6 -> t: 1
7 -> t: 1
8 -> t: 1
9 -> t: 1
s -> 1: 1
s -> 2: 1
s -> 3: 1
s -> 4: 1
s -> 5: 1
Before iteration 1
1 -> s: 1
2 -> 6: 1
2 -> 8: 1
3 -> 7: 1
3 -> 8: 1
3 -> 9: 1
4 -> 8: 1
5 -> 8: 1
6 -> 1: 1
7 -> t: 1
8 -> t: 1
9 -> t: 1
s -> 2: 1
s -> 3: 1
s -> 4: 1
s -> 5: 1
t -> 6: 1
Before iteration 2
1 -> s: 1
2 -> 6: 1
2 -> s: 1
3 -> 7: 1
3 -> 8: 1
3 -> 9: 1
4 -> 8: 1
5 -> 8: 1
6 -> 1: 1
7 -> t: 1
8 -> 2: 1
9 -> t: 1
s -> 3: 1
s -> 4: 1
s -> 5: 1
t -> 6: 1
t -> 8: 1

26.3-2
initially f(u, v) = 0 is an integer
assume f(u, v) for all (u, v) ∈ E are integers
EDMONDS-KARP increases or decreases the flow along an augment path by cf(p)
cf(p) = f(v, u) or c(u, v) - f(u, v)
both c(u, v) and f(u, v) are integers, cf(p) must also be an integer
therefore f(u, v) + cf(p) and f(u, v) - cf(p) are all integers
on termination, f(u, v) are integers for all (u, v) ∈ E
|f| = Σf(s, v) - Σf(v, s) must be an integer

26.3-3
let M* be a maximum matching of G
|p| <= 2|M*| + 1
the augment path p must contain exactly one edge leaving s and exactly one edge entering t
there's only edges from L to R in G, a back edge in residual graph must be from R to L
let f be the flow before augmentation, f' be the flow after augmentation
M be the matching corresponding to f, M' to f', |M'| > |M| by definition of augment path
each back edge on the augment path (an edge that decreases flow when augmented with f) negates an edge in M
each forward edge (an edge that increases flow when augmented with f) adds an edge to M
the augment path negates |M| edges and adds |M'| edges at most
|M'| <= |M*|, |M| <= |M*| - 1, |M| + |M'| <= 2|M*| - 1

26.3-4
for A ⊆ L, as G is bipartite, N(A) ⊆ R
if |A| > |N(A)| for some subset A ⊆ L, assume there is a perfect match M of G
each vertex x ∈ A must be connected to a distinct vertex y in G, |N(A)| >= |A|, contradiction
claim: if |A| <= |N(A)|, there is a matching between A and N(A) that covers all vertices in A
when |A| = 1, A = {x}, choose any (x, y) incidents with A (must exist as |N(A)| >= |A| = 1) will satisfy the condition
assume for all |A| < k, if |A| <= |N(A)| there is a matching that covers all vertices in A
if for |A| = k, for all (x, y) between N and N(A), removing y and incident edges makes N(A - {x}) < |A - {x}|
    |A - {x}| = k - 1, removing y and incident edges reduces |N(A - {x})| by at most 1
    y ∈ N(A - {x}) for all y where (x, y) ∈ E, N({x}) ⊆ N(A - {x}), |N(A - {x})| = k - 1
    naturally for all z ∈ A - {x}, N({z}) ⊆ N(A - {x})
    but N(A) = ∪{N({x}) | x ∈ A}, the relations above makes N(A) ⊆ N(A - {x}), |N(A)| <= |N(A - {x})| = k - 1
    in contradiction to |N(A)| >= |A| = k
    hence there is an (x, y) that removing (x, y) and all incident edges will still give |N(A - {x})| >= |A - {x}|
|N(A - {x}| >= |A - {x}| inductively gives a matching that covers all of A - {x}
the matching also must not include y as it is already removed
add (x, y) to the matching then gives a matching covers all of A
for |L| = |R|, |N(A)| >= |A| for all A ⊆ L
|N(L)| >= |L|, there is a matching that covers all of L
the matching must connect each vertex in L to a distinct vertex in R, as |L| = |R|, it is a perfect matching

26.3-5
an easier proof: by Hall's theorem
for each subset A ⊆ L, there are d|A| edges from A to N(A)
if |N(A)| < |A|, all d|A| edges connect to vertices in N(A) ⊆ R
there must be a vertex in N(A) with degree higher than d, in contradiction to G being d-regular
therefore |N(A) >= |A| for all A ⊆ L, there is a matching of cardinality |L| by 26.3-4

26.4-1
s.e = Σc(s, v) = c(s, V - {s}), by min-cut max-flow theorem s.e >= |f*|

26.4-2
./CLRS/graph/maximum-flow.ts#pushRelabel
relabel can be implemented by explicitly examining all edges in Adj[u] in Gf
    |Adj[u]| <= |V|, relabel can be done in O(V)
    additionally relabel searchs for new pushable edges and edges no longer pushable after the relabel
    remove and append them to corresponding lists described below
    both can be done in O(1) traversal of Adj[u], O(V) in total
by taking pointer to an edge, all operations within push takes constant time
    push can be done in O(1)
    additionally push adds or removes node from a doubly linked list described below in O(1)
maintain 
    a doubly linked list of all the vertices with u.e > 0
    an array of doubly linked list of edges pushable from each vertex
by lemma 26.14, if u.e > 0, either push or relabel applies
by theorem 26.18, if no u.e > 0, the procedure terminates with a valid maximum flow
by checking the head of the corrsponding pushable list of a node is NIL or not (in O(1))
the procedure can determine whether a push or a relabel operation may apply, and the edge to apply push operation
(it's equivalent to maintaining a dynamic admissible graph)

26.4-3
each vertex is relabeled at most O(V) times
relabel each vertex in a graph once traverses Adj[u] once for each u, takes O(E) time
relabel each vertex in a graph O(V) times then costs O(VE) time

26.4-4
by lemma 26.17, on termination of GENERIC-PUSH-RELABEL, there's no path s ~> t in the residual graph Gf
which means when running dfs from s in G' = (V, E ∩ Ef), t is not reachable from s
define a cut (S, V - S) where S = {v | v reachable from s in the dfs}, s ∈ S, t ∈ V - S
for all edge (u, v) crossing the cut, it must have (u, v) ∉ Ef and cf(u, v) = 0, or it will be reachable from s
as (u, v) ∉ E ∩ Ef, (u, v) is a forward edge, cf(u, v) = c(u, v) - f(u, v), c(u, v) = f(u, v)
each edge (u, v) from S to V - S is saturated
    |f| = Σ(u ∈ S, v ∈ S - V)f(u, v) - Σ(u ∈ S, v ∈ S - V)f(v, u)
        >= Σ(u ∈ S, v ∈ S - V)f(u, v)
        = Σ(u ∈ S, v ∈ S - V)c(u, v)
        = c((S, V - S))
since |f| <= c((S, T)) for each cut (S, T) by min-cut max-flow theorem, (S, V - S) is a minimum cut
it can be find in O(V + E)

26.4-5
thanks http://i.stanford.edu/pub/cstr/reports/cs/tr/95/1556/CS-TR-95-1556.pdf
derive flow network G' from the bipartite graph G
do normal GENERIC-PUSH-RELABEL on G'
claim: GENERIC-PUSH-RELABEL will only set integral flows with integral capacities
    INITIALIZE-PREFLOW sets (s, v).f = c(s, v) which is integral
    v.e = c(s, v) or 0 is integral, s.e -= c(s, v) is integral
    similarly cf(u, v) is integral for all (u, v) ∈ Ef
    after that flow is only changed by PUSH
    by induction u.e, v.e, (u, v).f, (v, u).f, cf(u, v), cf(v, u) all integral
    Δf(u, v) = min(u.e, cf(u, v)) is integral
    (u, v).f, (v, u).f, u.e and v.e all integral after PUSH
as each edge has capacity 1, cf = c - f or f = 0 or 1, a push must be saturating, PUSH costs O(VE)
by 26.4-3, all relabel operations can be done in O(VE) aggregated time
O(VE) in total

26.4-6
by claim in 26.4-5, a nonsaturating push decreases cf(u, v) by at least 1
at most k nonsaturating push can be performed before a relabel which makes v.h = u.h + 1
by a similar argument to lemma 26.22, each edge may support no more than 2k|V| nonsaturating pushs
the total cost will be O(VE + VE + kVE) = O(kVE)

26.4-7
initially all edge (s, v) is saturated by INITIALIZE-PREFLOW
if there is a path s ~> t in the residual graph, there must be an edge (s, v) that's not saturated
only a push (v, s) for some v ∈ Adj[s] may cause (v, s) to be nonsaturating
but that means v.h = s.h + 1, if s.h is set to |V| - 2, v.h = |V| - 1
by lemma 26.17, a simple path v ~> t with at most |V| - 2 edges implies v.h <= t.h + |V| - 2 = |V| - 2
in contradiction to v.h = |V| - 1, therefore lemma 26.17 still holds, Theorem 26.18 still correct
lemma 26.18 unchanged, lemma 26.19 now implies that u.h <= s.h + (|V| - 1) = 2|V| - 3 <= 2|V| - 1 the old bound
no more relabel or push can be performed, asymptotic running time still the same

26.4-8
initialization:
    s.h = |V|, u.h = 0 for u ∈ V - {s}
    for all u with u.h < |V| (i.e. V - {s}), u.h = 0 <= δf(u, t)
    s.h - |V| = 0 <= δf(s, s)
maintenance:
    before either operation, u must be overflowing, thereby u != s and u != t
    for PUSH(u, v) operation where u.h < |V|:
        by invariant u.h = v.h + 1 <= δf(u, t), v.h <= δf(v, t) if u.h, v.h < |V|
        PUSH(u, v) may create an edge (v, u) in Gf
        any simple path v -> u ~> t have length at least 1 + δf(u, t) >= v.h + 2 > v.h
        v.h is smaller than length of any new simple path, v.h still a lower bound of δf(v, t)
        starting from some w != v, if a path w ~> v -> u ~> t traverses (v, u) and have length < w.h
        replace v -> u ~> t with shortest path before the addition of (v, u) will not increase the length
        there must be a path w ~> v ~> t shorter than w.h before the addition, in contradiction to the invariant
        PUSH(u, v) also may remove (u, v) from Ef, but removing an edge may not shorten any paths
    for PUSH(u, v) operation where u.h >= |V|:
        as u.h >= |V|, there's no path u ~> t in Gf, (v, u) will not change any δf(w, t)
        u.h - |V| = 0 <= δf(u, s), for all p = w ~> v -> u ~> s in Gf, by definition of height function
            w.h <= v.h + |w ~> v|
            |p| = |w ~> v| + 1 + |u ~> s| 
                >= |w ~> v| + 1 + δf(u, s) 
                = |w ~> v| + 1
                >= w.h - v.h + 1
                = w.h - |V| + 2
        thereby w.h - |V| is still a lower bound of δf(w, s)
    for RELABEL(u) operation:
        before RELABEL(u) operation, for all (u, v) ∈ Ef, u.h <= v.h
        u.h is set to min{v.h | (u, v) ∈ Ef} + 1
        a shortest path u ~> t must visit one v with (u, v) ∈ Ef first
        if there is any v.h < |V| for some (u, v) ∈ f
        δf(u, t)    >= min{δf(v, t) | (u, v) ∈ Ef} + 1
                    >= min{v.h | (u, v) ∈ Ef} + 1
                    = u.h
        u.h is still a lower bound of δf(u, t) after RELABEL(u, t)
        if v.h >= |V| for all (u, v) ∈ Ef, u.h will be updated to a lower bound of δf(u, s)
        δf(u, s)    >= min{δf(v, s) | (u, v) ∈ Ef} + 1
                    >= min{v.h - |V| | (u, v) ∈ Ef} + 1
                    = u.h

26.4-9
still set s.h = |V| and t.h = 0 on initialization, s.h - |V| = 0 = δf(s, s), t.h = 0 = δf(t, t)
Ef contains no (s, v) and an edge (v, s) for each (s, v) ∈ E as (s, v) ∈ E are saturated
run bfs on transpose of current Gf from t, assign u.h to the minimial distance computed by bfs = δf(u, t)
by triangle property, if (u, v) ∈ Ef
    for u.h < |V|, 
        if v.h < |V|, u.h = δf(u, t) <= δf(v, t) + 1 = v.h + 1 for all (u, v)
        if v.h >= |V|, u.h <= |V| + 1 <= v.h + 1
    for u.h >= |V|,
        if v.h >= |V|, u.h = δf(u, s) + |V| <= δf(v, s) + 1 + |V| = v.h + 1 for all (u, v) and v.h >= |V|
        if v.h < |V|, u -> v ~> t is a path u ~> t, in contradiction to the invariant proved below
u.h is still a valid height function
invariant:
    if u.h < |V|, u.h = δf(u, t)
    if u.h >= |V|, u.h = δf(u, s)
    there's no path u ~> t in the residual graph for all u.h >= |V|
initialization:
    modified INITIALIZE-PREFLOW set s.h = |V| which is the only u.h >= |V|
    no path s ~> t since all edge leaving s in E is saturated
PUSH:
    by 26.4-8, adding (v, u) will not change any δf(w, t)
    if the push is saturating, (u, v) is removed from Gf
    if that is the last pushable edge from u (can be determined in O(1) as described in 26.4-2)
    there's no longer (u, w) in Ef where w.h + 1 = u.h
    if u.h < |V|, u.h = δf(u, t) before push, w.h = δf(w, t) (shortest path w ~> t cannot visit u)
    the new shortest path u ~> t must first visit one w where (u, w) ∈ Ef, δf(u, t) = min{δf(w, t)} + 1
    it may have w.h >= |V| for all (u, w) ∈ Ef after push
    u.h in this case will be updated to δf(u, s) as described below
    if u.h = |V|, δf(u, s) = u.h - |V| = 0, u must be s, but u is also overflowing, hence u.h != |V|
    if u.h > |V|, v.h = u.h - 1 >= |V|, u.h - |V| = δf(u, s), v.h - |V| = δf(v, s)
    if (u, v) is the last pushable edge from u, u.h have to be updated
    by definition of height function, w.h >= u.h - 1 = |V| for all (u, w) ∈ Ef, w.h - |V| = δf(w, s)
    a shortest path w ~> s cannot traverse (u, w), w.h = δf(w, s) after push
    u.h = δf(u, s) has to be updated to min{δf(w, s)} + 1 = min{w.h} + 1
    u.h and recursively w.h for all predecessors w of u has to be fixed immediately by a relabel
    for all w.h >= |V|, if w ~> t doesn't exist before (v, u), δf(w, t) = +∞
    by 26.4-8 (v, u) will not change any δf(w, t), δf(w, t) is still +∞
RELABEL:
    relabel effectively computes u.h = δf(u, t) or δf(u, s) = min{w.h | (u, w) ∈ Ef}
    then for each x where (x, u) ∈ Ef, if (x, u) is the last pushable edge from x before relabel
    recursively x.h also have to be updated, no matter whether x is overflowing or not
    all such x can be find by traversing Adj[u] in Gf and filter for backward edges
    a single non-recursive call to RELABEL still can be done in O(|Adj[u]|), O(VE) aggregated
    relabel operation does not modify Gf, if w ~> t doesn't exist before relabel it doesn't exist after
correctness of this algorithm can be derived from the fact that h is still a valid height function
the only difference in complexity is now RELABEL may be performed on non-overflowing vertices
such non-overflowing relabels still increases u.h by at least 1
since u.h = δf(u, t) or δf(u, s) + |V|, u.h <= 2|V| - 1
the bound in Corollary 26.21 applies on non-overflowing relabels as well as overflowing relabels
furthermore the O(VE) aggregated time by 26.4-3 holds for the same reason
O(VE) additional work is required

26.4-10
RELABEL cannot decrease the height of a vertex, v.h <= 2|V| - 1
RELABEL only applies on overflowing vertices
all O(V^2) RELABEL operations only increases Φ by at most 
    (2|V| - 1)(|V - {s, t}|) 
    = (2|V| - 1)(|V| - 2)
    = 2|V|^2 - 5|V| + 2
by definition of flow network, there is at least one edge entering t and one edge leaving s
s.h and t.h will not be updated, saturating pushes between (s, v) or (v, t) may only happen twice
the number of saturating pushes in lemma 26.22 can be improved to 2|V|(|E| - 2) + 4
saturating pushes increases Φ by at most 
    (2|V|(|E| - 2) + 4)(2|V| - 1)
    = (2|V||E| - 4|V| + 4)(2|V| - 1)
    = 4|V|^2|E| - 8|V|^2 + 8|V| - 2|V||E| + 4|V| - 4
    = 4|V|^2|E| - 8|V|^2 + 12|V| - 2|V||E| - 4
combine both RELABEL and PUSH,
    2|V|^2 - 5|V| + 2 + 4|V|^2|E| - 8|V|^2 + 12|V| - 2|V||E| - 4
    = 4|V|^2|E| - 6|V|^2 + 7|V| - 2|V||E| - 4
    <= 4|V|^2|E| - 6|V|^2 + 7|V|
    when |V| >= 4, - 6|V|^2 + 7|V| < 0
    <= 4|V|^2|E|
which is an upper bound of unsaturating pushes

26.5-1
too long, see ./CLRS/graph/index.ts#problem_26_5_1

26.5-2
./CLRS/graph/maximum-flow.ts#relabelFIFO
make DISCHARGE also record the vertices v that v.e = 0 before DISCHARGE(u) and (u, v) is pushed (i.e. v.e > 0)
when RELABEL-FIFO terminates, no vertex has positive overflow, neither PUSH or RELABEL applies to any vertex
Theorem 26.30 is independent to the order of DISCHARGE, the O(V^3) bound also applies to relabel FIFO algorithm
the additional work recording new overflowing vertices is proportional to the number of pushes
the number of pushes is at most O(V^3), time spend on the additional work thus is O(V^3), O(V^3) in total

26.5-3
redo the proof of correctness of push-relabel algorithm
lemma 26.15:
    vertex height still will never decrease
    RELABEL(u) increases u.h by at least (exactly) 1
lemma 26.16:
    if h is a height function before RELABEL(u), by assumption of RELABEL, u.h <= v.h for all (u, v) ∈ Ef
    for all (u, v) ∈ Ef, u.h + 1 <= v.h + 1
    for all (w, u) ∈ Ef, w.h <= u.h + 1 < u.h + 1 + 1
    h is still a height function after RELABEL(u)
lemma 26.17:
    no change
Theorem 26.18 (the correctness of push-relabel algorithm):
    no change
redo the analysis of relabel-to-front algorithm:
u.h <= 2|V| - 1, RELABEL(u) increases u.h by at least 1, there are still O(V^2) phases and O(V^3) call to DICHARGE
since RELABEL is now O(1), O(V^2) RELABEL operations take O(V^2) aggregated time
push still costs O(V^3) in total, the O(V^3) bound is not improved

26.5-4
relabel operations will not create overflowing vertices, only push may change the order of overflowing vertices
if v.e changed from 0 to a positive value during DISCHARGE(u), it must be changed by PUSH(u, v)
as PUSH(u, v) only applies to admissible edges, v.h = u.h - 1, u is still the highest vertex
by maintaining an array of doubly linked lists of vertices keyed by their height ("buckets")
also maintain another doubly linked list that links all the non-empty lists in the array in decreasing order of height
("root list")
initially all overflowing vertices have height 0, thus they are in the same list, the root list has a single node
a vertex of maximum height can be picked by take any item in the first bucket in the root list in O(1)
PUSH(u, v) only applies to a vertex u of maximum height, if PUSH(u, v) overflows v,
if v is the only item in its bucket, since height is integral
the bucket must have second maximum height, can be appended to the head of the root list
RELABEL(u) removes u from its bucket, removes the bucket from the head of the root list if it becomes empty
then allocate a new bucket keyed by the new u.h and prepend the bucket to the root list
only O(1) additional work is required for each PUSH and RELABEL, the O(V^3) bound proved in Theorem 26.30 still holds

26.5-5
if there's no vertices with height k where 0 < k < |V| - 1, for all v.h > k there's no v ~> t in the residual graph
let (v0, .., vp), v0 = v, vp = t be a path v ~> t in the residual graph, by the definition of height function
(vi, vi+1) ∈ Ef implies that vi.h <= vi+1.h + 1
height function is integral, if vi+1.h < k, vi.h must also < k since
    if vi+1.h < k-1, vi.h < k - 1 + 1 < k
    if vi+1.h = k-1, vi.h <= k
    but there's no vertex with height k, vi.h <= k-1 < k
inductively v0.h < k, in contradiction to v.h > k
let T be the set of vertices v where there is a path v ~> t in Gf
during push relabel algorithm there's no path s ~> t, s ∈ V - T, t ∈ T
(V - T, T) cuts no edges in residual graph since if there is an edge (u, v) where u ∈ V - T and v ∈ T
u -> v ~> t is a path u ~> t, u ∈ T
hence all edges from V - T to T in graph G is saturated, |f| = f(V - T, T) >= c(V - T, T), (V - T, T) is a minimum cut
all v.h > k has no path v ~> t in Gf, v ∈ V - T, the source side of a minimum cut
let v.h > k, for any (u, v) ∈ Ef, u.h <= v.h + 1, increasing will not change the inequality
for any (v, u) ∈ Ef, if u.h <= k, u.h < k since there's no vertex with height k
v.h <= u.h + 1 <= k, in contradiction to v.h > k
thereby u.h > k, both v.h and u.h will be updated to |V| + 1, v.h <= u.h + 1
h is still a valid height function

26-1
a.  same to 26.1-7
    |V'| = (|V| - 2) + |V| = O(V), |E'| = (|V| - 2) + |E| = O(E)
b.  a multiple-source multiple-sink maximum flow problem with unit weighted edges
    each starting point is a source, each point on the boundary is a sink
    can be solved in O(V^3) by any algorithm in this chapter as E = O(V) in a grid

26-2
a.  assign unit capacity to each edge in E' defined in the problem
    define a set of edges E* where
        (i, j) ∈ E* if (xi, yj) has positive flow in a valid flow of G'
    claim 1: E* is a path cover
        by flow conservation, Σf(v, u) = Σf(u, v)
        for a particular i, there is an edge (j, i) entering i iff f(xj, yi) = 1
        the only edge leaving yi is (yi, y0) with capacity 1, Σf(v, yi) = Σf(yi, v) <= 1
        (xj, yi) is the only edge entering yi, (j, i) is the only edge entering i
        similarly there is at most one edge leaving each vertex i ∈ V
        the paths formed by E* are vertex-disjoint
    define a flow f on G' from a path cover P of a dag as
        f(x0, xi) = f(xi, yj) = f(yj, y0) = 1 iff (i, j) is an edge in the path cover
    claim 2: f is a valid flow, |f| equals the number of edges in the path cover
        f(x, y) <= 1, capacity constraints are satisfied
        for any xi, if f(x0, xi) = 0, there must not be any f(xi, yj) > 0, Σf(v, xi) = Σf(xi, v) = 0
        if f(x0, xi) = 1, there must be some yi that f(xi, yj) = 1
        which means (i, j) ∈ P
        each path in a dag is simple, by definition of path cover (i, j) is the only edge leaving i
        f(xi, yi) is the only edge with positive flow leaving xi, Σf(v, xi) = Σf(xi, v) = 1
        similarly for each yj, Σf(v, yi) = Σf(yi, v)
        flow conservation is satisfied
        furthermore, since there are at most one edge leaving each vertex i ∈ V
        f(x0, xi) = 1 iff there is an edge leaving i, f(x0, xi) = 0 iff there is no edge leaving i
        each edge in P leaves a distinct vertex, |f| = Σf(x0, xi) equals the number of edges in P
    claim 3: |Ep| the number of edges in a path cover P of a dag equals |V| - |P|, |P| is the number of paths
        prove by induction on |V|
        when |V| = 1, the only possible path cover is a single path of length 0, |Ep| = |V| - 1 = 0
        assume |Ep| = |V| - |P| for all |V| < k
        for a dag with |V| vertices, take any path p in the path cover P
        p must be simple, let |p| be the number of edges in p, the number of vertices covered by p is |p| + 1
        P - {p} is a path cover of |V| - |p| - 1 < k vertices
        P - {p} by induction has |V| - |p| - 1 - |P| + 1 = |V| - |p| - |P| edges
        |Ep| = |V| - |p| - |P| + |p| = |V| - |P|
    for a flow f and a set E* computed by maximum flow algorithm
    by claim 1 E* is a path cover
    by claim 2 and 3 if there is another path cover with fewer paths, it must have flow > |f|
    therefore E* is a minimum path cover
b.  claim 2 and 3 no longer true

26-3
a.  for each Ak ∈ Ri, c(Ak, Ji) = +∞
    if Ji ∈ T but some Ak ∈ Ri and Ak ∉ T, (Ak, Ji) crosses the cut, c(S, T) >= c(Ak, Ji) = +∞
    thereby a finite cut must include all Ak ∈ Ri in T if Ji ∈ T
b.  since c({s}, V - {s}) = Σc(s, v) = Σck is finite, a minimum cut is finite
    a minimum cut (S, T) must have Ji ∈ T => Ri ⊆ T by part a
    hiring all Ak ∈ T and accept all Ji ∈ T thus is a valid solution to the problem
    the net revenue defined by a finite capacity cut (S, T) is Σ(Ji ∈ T)pi - Σ(Ak ∈ T)ck
    the cut (S, T) cuts all edges (Ji, t) for Ji ∉ T and all (s, Ak) for Ak ∈ T
        c(S, T) = Σ(Ak ∈ T)ck + Σ(Ji ∉ T)pi
                = Σ(Ak ∈ T)ck + Σ(Ji)pi - Σ(Ji ∈ T)pi
                = Σ(Ji)pi - (Σ(Ji ∈ T)pi - Σ(Ak ∈ T)ck)
    Σ(Ji)pi over all jobs is a constant, a minimum cut hence maximizes the net revenue
c.  |V| = 2 + m + n = O(m + n), |E| = m + n + r = O(m + n + r)
    the maximum flow can be computed in O(V^3) = O((m + n)^3)
    the minimum cut can be found given the maximum flow in O(V + E) = O(m + n + r) by 26.4-4
    O((m + n)^3 + r) = O(m^3 + n^3 + r) in total

26-4
a.  by min-cut max-flow theorem
    increase the capacity of an edge by 1 at most increases min-cut by 1 as well as max-flow
    there's no augmenting path in Gf before the increase
    if f(u, v) < c(u, v), (u, v) ∈ Ef, increase c(u, v) will not create new edge in Gf
    otherwise f(u, v) = c(u, v), increase c(u, v) by 1 will add (u, v) to Ef
    if (u, v) form a new augmenting path in residual graph (can be found in O(V + E) by bfs)
    augment f↑f' will increase the flow
    by Theorem 26.10, the flow computed by Ford-Fulkerson method must be integral given integral capacities
    |f↑f'| >= |f| + 1, f↑f' must be the new maximum flow
    the residual graph can be constructed from f and G in O(V + E)
    bfs costs O(V + E), augmenting a path f' to f costs O(V)
    O(V + E) in total
b.  by min-cut max-flow theorem, decrease the capacity of an edge by 1 will not increase the maximum flow
    if f(u, v) < c(u, v), decrease c(u, v) by 1 will not invalidate f, f is still a maximum flow
    otherwise f(u, v) = c(u, v) >= 1, f is no longer valid
    define G' = (V, E') where E' = {(u, v) | f(u, v) > 0}
    if (u, v) is on a cycle in G', one unit of flow can be removed from every edge on the cycle
    the cycle can be found by single source dfs from v in G', if the dfs find a path v ~> u there is a cycle
    if (u, v) is not on a cycle, there must be a path s ~> u -> v ~> w:
        the maximum flow of G has no flow leaving t or can be transformed into one in O(E) by 26.2-12
        extend flow conservation to set of vertices
        for U a set of vertices where s, t ∉ U
        Σ(u ∈ U, v ∈ V)f(u, v) = Σ(u ∈ U, v ∈ V)f(v, u)
        Σ(u ∈ U, v ∈ U)f(u, v) + Σ(u ∈ U, v ∈ V - U)f(u, v) = Σ(u ∈ U, v ∈ U)f(v, u) + Σ(u ∈ U, v ∈ V - U)f(v, u) 
        since Σ(u ∈ U, v ∈ U)f(u, v) and Σ(u ∈ U, v ∈ U)f(v, u) are the same set of flows
        Σ(u ∈ U, v ∈ V - U)f(u, v) = Σ(u ∈ U, v ∈ V - U)f(v, u)
        flow leaving set U equals flow entering set U
        assume there's no path s ~> u in G', define u be the set with positive flow path to u
        s, t ∉ U by assumption, v ∉ U or (u, v) on a cycle in G'
        there's no flow entering U, Σ(u ∈ U, v ∈ V - U)f(v, u) = 0
        there is a flow f(u, v) leaving u, in contradiction to the extended flow conservation
        hence there must be a path s ~> u, similarly there must be a path v ~> t
    such a path can be found by dfs from u to s in transpose of G' and dfs from v to t in G'
    combine these two paths and dedup gives a simple path s ~> u -> v ~> t
    remove one unit of flow from the path decreases |f| by 1, f is now valid again
    either f is already a maximum flow or a maximum flow can be computed within one augmenting path

26-5
a.  a cut (S, T) at most cuts each edge in G once
    c(S, T) <= Σ((u, v) ∈ E)c(u, v) <= C|E|
b.  filter (u, v) ∈ Ef by cf(u, v) >= K, costs O(E)
    run bfs on the filtered Gf
    if bfs finds an augmenting path, each edge has residual capacity cf(u, v) >= K, cf(p) >= K
    if there is such a path beforehand, filter Ef will not remove any edge on the path, it still can be found
c.  in the last iteration of the outer loop, K = 1
    by Theorem 26.10, each augmenting path has path flow >= 1
    the last iteration searches all possible augmenting paths in Gf
    on termination of the loop, there's no augmenting path in Gf, f is a maximum flow
d.  initialization:
        K = 2^[lgC] >= 2^(lgC - 1) >= C / 2
        cf(u, v) = c(u, v) - f(u, v) = c(u, v) for each (u, v) ∈ E, no backward edges in Ef
        by part a c(S, T) <= 2K|E| for any cut (S, T) of Gf
    maintenance:
        before line 7, all augmenting path of flow capacity at least K is augmented to f
        there's no path s ~> t with flow capacity >= K, the minimum cf(u, v) along any path < K
        as long as there is a path s ~> t in Gf, an edge with cf(u, v) < K can be removed from the path
        then eventually there will be no path s ~> t in Gf, Gf is disconnected
        V can be divided to two sets (S, T) cutting no edge
        thereby only (u, v) with cf(u, v) < K crosses (S, T) in Ef
        for pairs of edges in Ef, at most one in each pair may contribute to c(S, T)
        c(S, T) < K|E|
        K is set to K / 2 on line 7, c(S, T) < 2K|E|, the minimum cut has capacity <= c(S, T)
e.  let p be an augment path s ~> t, any cut (S, T) must cut at least one edge on p
    claim: augment f along p will decrease each c(S, T) by at least K
        let cf(p) = K*, for each (u, v) on p, the augmentation will change capacity of a cut (S, T) in two ways
        1.  if u ∈ S, v ∈ T, cf(u, v) decreased by K* as well as c(S, T)
        2.  if u ∈ T, v ∈ S, cf(v, u) increased by K* as well as c(S, T)
        case 1 happens one more time than case 2:
        let p = (e0 .. ek), let ei be the last backward edge to (S, T) (i.e. ei = (u, v), u ∈ T, v ∈ S)
        since t ∉ S, p = s ~> t, there must be a forward edge after ei that leaves S
        between any two backward edge in the middle, the path have to leave S once
        s ∈ S implies that the first backward edge (if any) is preceded by a forward edge
        thereby c(S, T) is decreased by K* >= K
        min{c(S, T)} <= 2K|E|, c(S, T) >= 0, at most 2|E| = O(E) augmentations may happen
f.  line 7 sets K = K / 2, the outer loop executes lgC times
    each inner loop executes O(E) iterations, each iteration augments f along p in O(E)
    O(E^2lgC) in total

26-6
A ⊕ B = B ⊕ A
(A ⊕ B) ⊕ C = A ⊕ (B ⊕ C)
A ⊕ A = ∅, (A ⊕ B) ⊕ B = A ⊕ (B ⊕ B) = A ⊕ ∅ = A
for disjoint B1 and B2, B1 ⊕ B2 = B1 ∪ B2, A ⊕ (B1 ∪ B2) = A ⊕ B1 ⊕ B2
a.  let P = (e0 .. ek), edges in p belong to M and E - M alternately
    M ⊕ P is the edges with odd indices in P
    let ei = (x, u), ei+1 = (u, v), ei+2 = (v, y) be three consecutive edges in P, ei+1 ∈ M, ei, ei+2 ∈ E - M
    by definition of matching, there's no other edges in M sharing end points with ei = (u, v)
    if ei is the first edge in P, x is the only vertex in P that x ∈ L ∩ E - M
    otherwise x is also an endpoint of an edge in M
    similarly y is the only vertex in P that y ∈ R ∩ E - M or y is an endpoint in M
    thereby all x, u, v, y are distinct in P, P is a simple path = (v0, v1 .. vk+1)
    P - M = ((v0, v1), (v2, v3) .. (vk, vk+1)) are edges with distinct endpoints i.e. a matching
    let e = (u, v) ∈ M - P, if either u or v on P, it cannot be the first and last unmatched vertices
    but then there is an edge (u, w) ∈ M ∩ P, M = (M - P) ∪ (M ∩ P) contains two edges incident on the same vertex
    in contradiction to M being a matching
    therefore M - P contains only edges that are vertex-disjoint to P, M ⊕ P = M - P ∪ P - M is a matching
    every second edge in P ∈ M, the first and the last edge in P ∉ M
    |P - M| = 1 + |P ∩ M|
    |M ⊕ P| = |M - P| + |P - M| // M - P and P - M are disjoint
            = |M| - |P ∩ M| + 1 + |P ∩ M|
            = |M| + 1
    M ⊕ (P1 ∪ .. ∪ Pk) = M ⊕ P1 ⊕ .. ⊕ Pk
    for any Pi, if M ⊕ P1 .. Pi-1 is a matching, Pi is an augmenting path to M ⊕ P1 .. Pi-1
        the starting vertex of Pi is not in M or any P1 .. Pi-1, similarly the end vertex
        every second edge of Pi ∈ M, other edges not in M or any P1 .. Pi-1
    thus M ⊕ P1 ⊕ .. ⊕ Pi is also a matching, |M ⊕ P1 .. Pi| = |M ⊕ P1 .. Pi-1| + 1
    |M ⊕ {P1 ∪ .. ∪ Pk}| = |M| + k
b.  for a vertex u, there are at most one edge incident to u in M or M'
    M ⊕ M ⊆ M ∪ M*, u has degree at most 2 in G = (V, M ⊕ M*)
    let p = (v0 .. vk) be any non-simple path, vi = vj be the first repeated vertex on p for some different i and j
    assume i < j, if i != 0, (vi-1, vi), (vi, vi+1), (vj-1, vj) are distinct edges incident on vi unless i+1 = j
    when i+1 = j, (vi, vi+1) is a self loop on vi, contributes two degrees
    either case vi has degree 3 > 2
    similarly if j != k, vj has degree 3 > 2
    thereby p is either a simple path with no repeated vertex or a simple cycle with a single repeated vertex v0 = vk
    if two consecutive edges (u, v), (v, w) on p both belongs to M or M*
    M or M* has two edges incident to a single vertex, in contradiction to M and M* being matchings
    edges on p must belong to M and M* alternately
    a simple cycle has same number of edges from M and M*
    a simple path may have one more edge from M than from M* or vice versa
    let P be the set of simple paths in G = (V, M ⊕ M*)
    if any two path p and p' are not vertex-disjoint
    either they are actually parts of the same path or a vertex has degree > 2
    hence paths in P are mutually vertex-disjoint
    all edges in M ⊕ M* = M - M* ∪ M* - M are on some simple path or cycle in G
    a simple path p may contain one more edge from M* - M than M - M* iff it starts and ends with edges in M* - M
    let the first edge of p be (u, v) ∈ M* - M, the last be (x, y) ∈ M* - M ((u, v) may equal to (x, y))
    (u, v), (x, y) ∉ M or they are not in M* - M
    if there's (w, u) ∈ M that w != v, (w, u) ∈ M - M* since there's no other edge incident on u in M*
    p will be suffix of a longer path, (u, v) is not the first edge
    similarly if there's (y, z) ∈ M that z != x, (x, y) will not be the last edge, u and y are unmatched in M
    p is undirected with odd number of edges, all edges are between L and R in G
    p or the reverse of p starts from L and ends in R, p is an augmenting path to M in G
    thus if |M*| - |M| > 0, there are at least |M*| - |M| vertex-disjoint augmenting paths to M in G
c.  assume P has length l
    claim 1: if u is matched in M, it is also matched in M ⊕ P for some augmenting path p with respect to M
        if no edge e ∈ P ∩ M incident on u, it will not be affected by the augmentation
        otherwise e ∈ P ∩ M incidents on u, let it be (u, v)
        there must be another edge (w, u) ∈ P as u cannot be starting or ending point of P
        (w, u) ∉ M, (w, u) will be included in M ⊕ P, u is still matched in the new matching
    claim 2: M ⊕ P contains exactly two more matched vertices than M, which are the starting and ending point of P
        the starting and ending vertices of P are the only endpoint in P but not in M
        the first and last edges are in P - M, both endpoints will be matched in M ⊕ P
        other vertices in M still in M ⊕ P by claim 1
    claim 3: if e ∉ P and e ∉ M ⊕ P, e ∉ M
        assume e ∈ M, e ∈ M - P, e ∈ M - P ∪ P - M = M ⊕ P, contradiction
    as P is vertex-disjoint to P1 .. Pk, the starting and ending vertices of P are unmatched in M by claim 2
    P is automatically edge-disjoint to P1 .. Pk
    if an edge e ∈ P ∩ M', e ∉ P1 .. Pk, e ∈ M' = M ⊕ P1 .. Pk ⊆ M ∪ P1 .. Pk, e must ∈ M
    if an edge e ∈ P - M', e ∉ P1 .. Pk and M', by claim 3 e ∉ M
    edges in P alternately belongs to M and E - M, P is an augmenting path with respect to M
    then { P1 .. Pk } ∪ {P} is a larger set of vertex-disjoint paths of length l with respect to M
    thus P must have length > l
d.  (M ⊕ M') ⊕ P = M ⊕ (M ⊕ (P1 ∪ .. ∪ Pk)) ⊕ P = (P1 ∪ .. ∪ Pk) ⊕ P
    (M ⊕ M') ⊕ P = M ⊕ (M' ⊕ P) = M ⊕ (M ⊕ {P1 ∪ .. ∪ Pk} ⊕ P)
    by part a, |M'| = |M| + k, |M' ⊕ P| = |M| + k + 1
    by part b, A = M ⊕ (M' ⊕ P) contains at least k + 1 edge disjoint augmenting paths with respect to M
    there are at most k augmenting paths of minimum length l
    there must be an augmenting path in A longer than l, |A| > (k + 1)l
    |P1| + .. + |Pk| + |P| >= |P1 ∪ .. ∪ Pk ∪ P| >= |(P1 ∪ .. ∪ Pk) ⊕ P| > (k + 1)l
    |P| > (k + 1)l - kl = l, P has length longer than l
e.  let M* be a maximum matching of G
    M ⊕ M* has at least |M*| - |M| vertex-disjoint augmenting path with respect to M
    each of them has at least l edges and (l+1) disjoint vertices, (l+1)(|M*| - |M|) distinct vertices in total
    if |M*| > |M| + |V| / (l + 1), there more than (l + 1)|V| / (l + 1) = |V| vertices in |M ⊕ M*|, which is impossible
f.  each iteration augments M with at least one path P or terminates
    after |V|^(1/2) iteration, |M| >= |V|^(1/2)
    by part c and d, at the beginning of iteration k
    the shortest augmenting path with respect to M has length at least k = |V|^(1/2)
    a maximum matching has |M*| <= |M| + |V| / (l + 1) <= |M| + |V|^(1/2)
    |M*| - |M| <= |V|^(1/2), each iteration increases |M| by at least 1
    at most |V|^(1/2) iterations may happen after |V|^(1/2)th iteration, 2|V|^(1/2) in total
g.  thanks 
        http://www.cs.princeton.edu/courses/archive/fall09/cos521/Handouts/Hopcroft.pdf
        https://resources.mpi-inf.mpg.de/departments/d1/teaching/ss12/AdvancedGraphAlgorithms/Slides05.pdf
        http://www.cs.cornell.edu/courses/cs6820/2016fa/handouts/matchings.pdf
    (no proof of correctness)
    let U be the set of unmatched vertices with respect to M
    length of shortest augmenting path in G can be found by modified bfs that alternately follows edge in M and E - M
    the bfs starts from some u ∈ U and terminates on reaching another different v ∈ U
    by starting bfs from each unmatched vertex and visit each v ∈ V once
    l the length of shortest augmenting path starting from each u ∈ U can be computed in O(E) 
    level assigned to each vertex in V during the bfs defines a level graph on G
    traverse backward from vertices on level l to level 0, delete edges and vertices on the traversal
    a maximum set of minimum length augmenting paths can be enumerated in this way

Chapter 27
//  node.js is single threaded by design, implementations in this chapter are just proof of concept with async API
27.1-1
some call edges are replaced by spawn edges, T1, T∞ and parallelism won't change

27.1-2
one P-FIB(4) and one P-FIB(3) spawned from a vertex
T1(5) = T1(4) + T1(3) + 3 = 17 + 9 + 3 = 29
T∞(5) = max(T∞(4), T∞(3)) + 2 = max(8, 5) + 2 = 10
T1 / T∞ = 29/10

27.1-3
assume there are i incomplete steps and c compete steps during the execution
each incomplete step performs at least one unit of work
complete steps can perform at most T1 - i units of works
    c   <= (T1 - i) / P
    Tp  <= (T1 - i) / P + i
        = T1 / P + (P - 1)i / P
    as P >= 1, Tp is monotonically increasing with respect to i, i <= T∞
    Tp  <= (T1 - T∞) / P + T∞

27.1-4
let the starting strand s has three successors v1, v2 and v3, each is a linear chain of length k, let P = 2
if a greedy scheduler executes strand on (v2, v3), (v1, v3), (v1, v2) alternately
the scheduler executes a portion of length 2 from each chain in every 3 steps
3k / 2 steps are performed in total
otherwise if a greedy scheduler executes (v1, v2) until the two chains are exhausted
works on chain v3 have to be performed seperately, k + k = 2k steps are performed in total
generally if it has P = p and p+1 chains of length k
the two strategies will perform (p+1)k / p and 2k steps
2k / ((p+1)k / p) = 2p / (p+1) approachs 2 as p -> ∞

27.1-5
work law gives T1 <= T4 * 4 = 80 * 4 = 320
span law gives T∞ <= T64 = 10
T10 <= (T1 - T∞) / 10 + T∞ = T1 / 10 + (10 - 1)T∞ / 10
    <= 32 + 9 * 10 / 10 = 41
in contradiction to T10 = 42

27.1-6
take the sum Σ(j)aij * xj by divide and conquer for each i in Θ(lgn) span and Θ(n) work
    split the summation in the middle, recursively spawn two children that calculates each half of the sum
    the computation dag is an almost complete binary tree with height Θ(lgn)
    the overhead (internal nodes in computation dag) is no more than twice the number of j, Θ(n) works in total
each sum can be calculated independently, T∞ = Θ(lgn + lgn) = Θ(lgn) in total, T1 = Θ(n^2) in total
parallelism is T1 / T∞ = Θ(n^2 / lgn)

27.1-7
T1 = Θ(n^2) by the nested loop
T∞ = Θ(lgn + lgn) = Θ(lgn) as each exchange can be performed independently
parallelism = T1 / T∞ = Θ(n^2 / lgn)

27.1-8
T1 = Θ(n^2) unchanged
T∞ = Θ(lgn + n) = Θ(n) as the span from line 2 to a inner loop is Θ(lgn), the inner loop has T1 = T∞ = Θ(n)
parallelism = T1 / T∞ = Θ(n)

27.1-9
2048 / P + 1 = 1024 / P + 8
2048 + P = 1024 + 8P
1024 = 7P
P = 1024/7 is not an integer, the two algorithms will never run equally fast

27.2-1
//  graphviz code
digraph G {
    subgraph cluster_0 {
        stype=filled;
        shape=square;
        color=lightgrey;
        s1 -> s2 -> s3;
        label="start";
    }
    subgraph cluster_1 {
        color = blue;
        w11 -> w12 -> w13;
        label = "first row";
    }
    subgraph cluster_2 {
        color = blue;
        w21 -> w22 -> w23;
        label = "second row";
    }
    s1 -> w11 [style=bold, fillcolor = grey];
    w13 -> s3 [style=bold, fillcolor = grey];
    s2 -> w21 [style=bold, fillcolor = grey];
    w23 -> s3 [style=bold, fillcolor = grey];
    w11 -> a [style=bold, fillcolor = grey];
    w12 -> b [style=bold, fillcolor = grey];
    a -> w13 [style=bold, fillcolor = grey];
    b -> w13 [style=bold, fillcolor = grey];
    w21 -> c [style=bold, fillcolor = grey];
    w22 -> d [style=bold, fillcolor = grey];
    c -> w23 [style=bold, fillcolor = grey];
    d -> w23 [style=bold, fillcolor = grey];
}
if any constant time portion of program between calls or spawns is a unit strand
T1 = 13, T∞ = 5, parallelism is T1 / T∞  = 13 / 5

27.2-2
the definition of a strand is rough, T1 and T∞ can be anything

27.2-3
./CLRS/multithreaded/p-matrix-multiply.ts#pMatrixMultiply
use a divide and conquer sum function with O(lgn) span
the innermost loop new have span O(lgn) instead of O(n), O(3lgn) = O(lgn) span in total

27.2-4
./CLRS/multithreaded/p-matrix-multiply.ts#pMatrixMultiply
the algorithm in 27.2-3 can be easily modified to support non-square matrices
the total work is Θ(pqr), total span is Θ(lgp + lgr + lgq) = Θ(lg(pqr))
as long as one of p, q or r is not 1, the parallelism Θ(pqr / lg(pqr)) is nearly linear

27.2-5
./CLRS/multithreaded/p-matrix-multiply.ts#pTranspose
if A.rows = 1, the transpose of A is A itself
if A.rows >= 2, partition A into
    A11 A12
    A21 A22
the transpose of A will be
    T(A11)  T(A21)
    T(A12)  T(A22)
transpose the four sub matrices inplace then swap each entry in A12 and A21 in parallel
T1(n) = 4T1(n/2) + Θ(n^2) = Θ(n^2lgn)
T∞(n) = T∞(n/2) + Θ(lgn) = Θ(lg^2n)
parallelism = T1 / T∞ = Θ(n^2 / lgn)

27.2-6
./CLRS/multithreaded/p-matrix-multiply.ts#pFloydWarshall
simply make the two inner loops parallel
by 25.2-4, D[i, k] and D[k, j] will not be updated in iteration k of the outmost loop
each iteration of the outmost loop depends on the result of the last iteration, it has to be serial
T1 = Θ(V^3) as the serial implementation
the two inner loops now have span Θ(lgV)
T∞ = n * Θ(lgn) = Θ(VlgV)
parallelism is T1 / T∞ = Θ(V^2 / lgV)

27.3-1
when n1 + n2 is small enough, merge T[p1 .. r1] and T[p2 .. p2] with serial implementation MERGE to A

27.3-2
let A[p1 .. r1] and A[p2 .. r2] be two disjoint sorted slices in A
give a mutual median mid of the two slices, it must have
    let A[q1] be the last item smaller than mid in A[p1 .. r1]
    there are l1 = q1 - p1 + 1 items <= mid in A[p1 .. r1], g1 = r1 - q1 items >= mid
    mid is the mutual median, let l2 and g2 defined similarly in A[p2 .. r2]
        |(l1 + l2) - (g1 + g2)| <= 1
        |(l1 - g1) + (l2 - g2)| <= 1
        |(l1 - g1) - (g2 - l2)| <= 1
given (l1 - g1), (g2 - l2) may only take one of the three values: (l1 - g1), (l1 - g1) - 1 and (l1 - g1) + 1
given l2 + g2 = r2 - p2 + 1, l2 and g2 can be solved
for one of the solution it must have A[p2 + l2 - 1] <= mid and A[p2 + l2] >= mid
the test can be done in O(1) given q1, p1, r1, p2 and r2
binary search on A[p1 .. r1] with the test then computes the mutual median in O(lgn1), n1 = r1 - p1 + 1
if the mutual median is not in A[p1 .. r1], run the same procedure on A[p2 .. r2] must find the mutual median
O(lgn1 + lgn2) = O(lgn + lgn) = O(lgn) in total
binary search A[p1 .. r1] and A[p2 .. r2] for the median makes the two subproblems have equal size
asymptotic T∞(n) and T1(n) will not change

27.3-3
./CLRS/multithreaded/p-merge-sort.ts#pPartition
let 2^h be the next power of 2 greater than |A|
2^h <= 2|A|, 2^h = O(|A|)
construct a complete trie with 2^h leaves and 2^h - 1 internal nodes in parallel, keyed by the index of A
(basically a two-way order statistics tree / trie)
the height of the trie will be O(log2(2^h)) = O(h) = O(lg|A|)
each internal node corresponding to a prefix p stores:
    le: the number of items less than or equal to the pivot in A with index starting with p
    gt: the number of items greater than the pivot in A with index starting with p
the root has p = ε
both le and gt can be computed recursively from le and gt of the children of a node
if a leaf corresponds to a prefix (index) p,
    le = 1, gt = 0 if A[p] <= pivot
    le = 0, gt = 1 if A[p] > pivot
    le = 0, gt = 0 if p is out of boundary
let TC(h) be the construction time of a trie with height h
    TC1(h)  = 2TC1(h - 1) + O(1)
            = 2^h
            = O(2^lg|A|) = O(|A|)
    TC∞(h)  = TC∞(h - 1) + O(1)
            = O(h) = O(lg|A|)
for a particular index p
the number of items less or equal to pivot before index p can be computed by traversing a path from root to p:
    let sum = 0 initially
    if a right-going edge if followed, le of the left child is added to sum
similarly the number of items greater than the pivot before p can be computed in O(lg|A|)
by simultaneously traverse all the paths, all the items in A can be assigned to T in O(|A|) with O(lg|A|) span
elements less than or equal to the pivot is assigned from left end to right into T
elements greater than the pivot is assigned from right end to left into T

27.3-4
//  wait for Chapter 30

27.3-5
./CLRS/multithreaded/p-merge-sort.ts#pRandomizedSelect
a stright forward translation of RANDOMIZED-SELECT
by the same argument in 9.2
    E[T1(n)] = O(n)
    E[T∞(n)] <= 2/n * Σ(k = {n/2 .. n - 1})E[T∞(k)] + O(lgn)
    assume T∞(n) <= cln(n)
    E[T∞(n)]    <= 2/n * Σ(k = {n/2 .. n - 1})clgk + O(ln(n))
                <= 2/n * int(n/2 <= k <= n, clgk) + aln(n)
                = c/n * (nln(n) - n - n/2ln(n/2) + n/2) + aln(n)
                = c(ln(n) - 1 - ln(n/2)/2 + 1/2) + aln(n)
                = cln(n) - c/2(ln(n/2) + 1) + aln(n)
                = cln(n) - c/2(ln(n) - ln2 + 1) + aln(n)
sufficiently large c gives c/2(ln(n) - ln2 + 1) >= aln(n)
    E[T∞(n)]    = O(lgn)

27.3-6
./CLRS/multithreaded/p-merge-sort.ts#pSelect
the non-recursive works of the pivoter can be done in O(lgn) span
by the same argument in 9.3
    T1(n) = O(n)
    T∞(n)   <= T∞(n/5) + T∞(7n/10 + 6) + O(lgn)
            = O(n)

27-1
a.  a simple application of parallelFor defined as
    ./CLRS/multithreaded/p-matrix-multiply.ts#parallelFor
    T1(n) = O(n), T∞(n) = O(lgn)
b.  the same to SUM-ARRAYS
    the serial loop `for k = 0 to r - 1` dominates both T1(n) and T∞(n)
    T1(n) = T∞(n) = O(n), T1(n) / T∞(n) = O(1)
c.  the serial loop in SUM-ARRAYS' has r = n / grain-size iteration
    each sub procedure spawned from the loop has T1 = T∞ = O(grain-size)
    the span of SUM-ARRAYS' is max(O(n / grain-size), O(grain-size))
    which is minimized when grain-size = Θ(n^(1/2)), T∞(n) = O(n^(1/2)), T1(n) / T∞(n) = Θ(n^(1/2))

27-2
a.  ./CLRS/multithreaded/p-matrix-multiply.ts#pMatrixMultiplyDivide2
    insert a sync after line 9
    perform both addition and multiplication in the base case
b.  the matrix C can be initialized to zero matrix in O(n^2) work and O(lgn) span
    the span now contains two recursive calls instead of one
    T1(n) = Θ(n^3)
    T∞(n) = 2T∞(n) + O(1) = Θ(n)
c.  T1(n) / T∞(n) = Θ(n^2)
    when n = 1000, parallelism is about 10^6

27-3
//  wait for Chapter 29

27-4
a.  ./CLRS/multithreaded/p-loop.ts#pReduce
    assume the operation can be done in O(1)
    T1(n) = 2T1(n/2) + O(1) = Θ(n)
    T∞(n) = T∞(n/2) + O(1) = Θ(lgn)
    T1(n) / T∞(n) = Θ(n/lgn)
b.  P-REDUCE(A, 1, l) is calculated for each l ∈ {1 .. n}
    T1(n) = Σ(l ∈ {1 .. n})Θ(l) = Θ(n^2)
    T∞(n) = max{Θ(lgn + lgl)} = Θ(lgn)
    T1(n) / T∞(n) = Θ(n^2 / lgn)
c.  induct on j - i
    assume y[k] is the sum of x[i .. k] for i <= k <= j
    when j - i = 0, i = j, y[i] = x[i] is the sum of x[i .. i]
    when j - i > 0, by induction
        P-SCAN-2-AUX(x, y, i, k) computes y[p] = sum of x[i .. p] for i <= p <= k
        P-SCAN-2-AUX(x, y, k+1, j) computes y[p] = sum of x[k+1 .. p] for k+1 <= p <= j
        add y[k] to each y[p], k+1 <= p <= j
        y[p] = Σ(r ∈ {i .. k})x[r] + Σ(r ∈ {k+1 .. r})x[r] = Σ(r ∈ {i .. p})x[r]
    thereby y[p] = Σ(r ∈ {i .. p})x[r] for each i <= p <= j
    P-SCAN-2-AUX(x, y, 1, n) correctly computes the sum of prefixes
    T1(n) = 2T1(n/2) + Θ(n) = Θ(nlgn)
    T∞(n) = T1(n/2) + Θ(lgn) = Θ(lg^2n)
    T1(n) / T∞(n) = Θ(n / lgn)
d.  P-SCAN-UP:
        return t[k] * right;
    P-SCAN-DOWN:
        spawn P-SCAN-DOWN(v, x, t, y, i, k)
        P-SCAN-DOWN(v * t[k], x, t, y, k + 1, j)
    induct on j - i: P-SCAN-UP(x, t, i, j) returns Σ(p ∈ {i .. j})x[p]
    when j - i = 0, Σ(p ∈ {i})x[p] = x[i] is returned
    when j - 1 > 0, by induction
        t[k] = P-SCAN-UP(x, t, i, k) = Σ(p ∈ {i .. k})x[p]
        right = P-SCAN-UP(x, t, k+1, j) = Σ(p ∈ {k+1 .. j})x[p]
        t[k] * right = Σ(p ∈ {i .. j})x[p]
    P-SCAN-UP and P-SCAN-DOWN have exacly the same computation graph
    upon calling P-SCAN-DOWN(v, x, t, y, i, j), P-SCAN-UP(x, t, i, j) is already computed
    k = [(i + j) / 2], t[k] = Σ(p ∈ {i .. k})x[p] for each call to P-SCAN-DOWN(v, x, t, y, i, j)
    induct on depth of recurrence: v = Σ(p = {1 .. i - 1})x[p] upon calling P-SCAN-DOWN(v, x, t, y, i, j)
    on depth 0, the initial call P-SCAN-DOWN(x[1], x, t, y, 2, n) gives v = x[1] = Σ(p = {1 .. 2-1})x[p]
    on depth d
        recursive call on depth d must be issued by a recursive call P-SCAN-DOWN(v, x, t, y, i, j) on depth d - 1
        by induction v = Σ(p ∈ {1 .. i - 1})x[p]
        if current call is P-SCAN-DOWN(v, x, t, y, i, k), v = Σ(p ∈ {1 .. i - 1})x[p] still holds
        if current call is P-SCAN-DOWN(v * t[k], x, t, y, k+1, j)
        v * t[k]    = Σ(p ∈ {1 .. i - 1})x[p] + Σ(p ∈ {i .. k})x[p]
                    = Σ(p ∈ {1 .. k})x[p]
    thereby when i == j, y[i] is assigned v * x[i] = Σ(p ∈ {1 .. i})x[p], the prefix sum of x[1 .. i]
e.  T1(n) = 2T1(n/2) + O(1) = Θ(n)
    T∞(n) = Θ(lgn) + Θ(lgn) = Θ(lgn)
    T1 / T∞ = Θ(n / lgn)

27-5
a.  divide the array into four subarrays in O(1)
    recursively compute stencil of A11
    recursively compute stencil of A12 and A21 in parallel
    recursively compute stencil of A22
    T1(n) = 4T1(n/2) + O(1) = Θ(n^2)
    T∞(n) = 3T∞(n/2) + O(1) = Θ(n^(3/2))
    T1 / T∞ = Θ(n^(1/2))
b.  recursively compute stencil of A11
    recursively compute stencil of A12 and A21 in parallel
    recursively compute stencil of A13, A22 and A31 in parallel
    recursively compute stencil of A23 and A32 in parallel
    recursively compute stencil of A33
    T1(n) = 9T1(n/3) + O(1) = Θ(n^2)
    T∞(n) = 5T∞(n/3) + O(1) = Θ(n^log(3, 5))
    T1 / T∞ = Θ(n^(2 - log(3, 5)))
c.  T1(n) = Θ(n^2) regardless of b
    T∞(n) = (2b - 1)Θ(n/b) + O(1) = Θ(n^log(b, 2b - 1))
    T1 / T∞ = Θ(n^(2 - log(b, 2b - 1)))
    for b >= 2, log(b, 2b - 1) > 1, 2 - log(b, 2b - 1) < 1
    the parallelism is o(n)
d.  take b = n
    T1(n) = Θ(n^2)
    now 1 x 1 submatrices (entries) on a diagonal is proportional to n, a parallel loop has span Θ(lgn)
    T∞(n) = (2n - 1) * Θ(lgn) * O(1) = Θ(nlgn)
    T1 / T∞ = Θ(n^2 / nlgn) = Θ(n / lgn)
    no entry in the stencil has non-linear span
    induct on i + j, assume the span of the stencil of A[i, j] <= i + j
    stencil of A[1, 1] cannot depend on any other entries, span of A[1, 1] <= 1 <= 1 + 1
    stencil of A[i, j] can be computed in O(1), hence only depends on a constant number of entries on left / top
    span of A[i, j] is defined as 1 + max{span of A[i', j'] | A[i, j] is calculated from A[i', j']}
    by definition of stencil, i' + j' < i + j, inductively span of A[i', j'] <= i' + j' < i + j
    1 + span of A[i', j'] <= i + j
    each entry in the stencil has span <= i + j <= 2n = O(n), the span of the whole calculation must be O(n)
    the parallelism may achieve Θ(n)

27-6
a.  a particular sample in the sample space is defined by fixing all the random bits used on the random tape
    thus for a particular sample, the computation graph is fixed, work law and span law still applies
        Tp >= T1 / P, Tp >= T∞, Tp <= T1 / P + T∞
    take expectation of both sides
        E[Tp] >= E[T1] / P, E[Tp] >= E[T∞], E[Tp] <= E[T1] / P + E[T∞]
    rearranging the equations also gives:
        E[Tp / T1] >= P, E[Tp / T∞] >= 1, E[(Tp - T∞) / T1] <= 1/P
b.  E[T1 / Tp] = 1% * 10^4 + 99% * 1 = 100.99
    E[T1] / E[Tp] ≒ 10^9 / 10^9 = 1 
    most of the time (99%) T1 = Tp, 1 is a better measure than 100.99
c.  similar to part b
d.  ./CLRS/multithreaded/p-quicksort.ts#pRandomizedQuicksort
e.  expected work is still Θ(nlgn)
    span recurs into the bigger subproblem, the analysis of RANDOMIZED-SELECT applies
    expected span is Θ(n), E[T∞] / E[T1] = Θ(lgn)
