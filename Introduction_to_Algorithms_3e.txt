// rereading, lost all the notes / code of the previous reading

Chapter 2
2.1-1
[ 31, 41, 59, 59, 41, 58 ]
[ 31, 41, 41, 59, 41, 58 ]
[ 31, 31, 41, 59, 41, 58 ]
[ 26, 31, 41, 59, 59, 58 ]
[ 26, 31, 41, 41, 59, 59 ]
[ 26, 31, 41, 41, 58, 59 ]

2.1-2
change line 5 to
    while i > 0 and A[i] < key

2.1-3
./CLRS/start/index.ts#linearSearch
loop invariant: 
    none of the first i items in the array equals to v
initialization:
    i == 0, loop invariant holds trivially
maintenance:
    every iteration, A[i] == v is checked
    the loop continues only when A[i] == v is false
termination:
    if the loop terminated early, then A[i] == v and the first i elements in the array are not equal to v
    therefore arr[i] is the first element equals to v
    if the loop terminated on loop condition, i = arr.length, the first i elements are the whole array
    none of the items in the array equals to v, so function returns NIL
    
2.1-4
Input: Two n-element sequence of 0 and 1s, representing two numbers a and b in binary form
Output: a (n+1)-element sequence of 0 and 1s, representing a number c == a + b in binary form
assumes little-endian
./CLRS/start/index.ts#addBinary

2.2-1
Θ(n^3)

2.2-2
invariant: 
    each of the the first i elements of the array are the ith smallest elements
the last element is greater or equal to all the previous items, thereby must be the greatest element
best-case and worst-case both Θ(n^2)

2.2-3
Σip(i) = (Σi) / n = (1 + n) * n / 2n = (1 + n) / 2 on average, Θ(n)
n on worst case, Θ(n)
assuming comparsion of integers takes constant time

2.2-4
hard-code the solution of a specific input in the program 
if input equals to the specific input, returns that solution immediately
base-case running time reduced to time required to check the input

2.3-1
[ 3, 41, 52, 26, 38, 57, 9, 49 ]
[ 3, 41, 26, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 38, 57, 9, 49 ]
[ 3, 26, 41, 52, 9, 38, 49, 57 ]
[ 3, 9, 26, 38, 41, 49, 52, 57 ]
[ 3, 9, 26, 38, 41, 49, 52, 57 ]

2.3-2
./CLRS/start/mergesort.ts#mergeNoSentinel

2.3-3
when n = 2,
    T(n) = 2 = 2lg2
assume for n = 2^k, T(n) = nlgn = 2^klg(2^k) = k * 2^k
    T(2n)   = T(2^(k+1))
            = 2T(2^k) + 2^(k+1)
            = 2 * 2^k * k + 2^(k+1)
            = k * 2^(k+1) + 2^(k+1)
            = (k+1) * 2^(k+1)
            = (2n)lg(2n)
therefore for n = 2^k, k > 1,
    T(n) = nlgn

2.3-4
T(n)    = Θ(1)          when n = 1
        = T(n-1) + Θ(n) when n > 1

2.3-5
./CLRS/start/index.ts#binarySearch
T(n)    = Θ(1)          when n = 1
        = Θ(n/2) + Θ(1) when n > 1
by master theorem T(n) = Θ(lgn)

2.3-6
cannot improve
each iteration in insertion sort has to shift Θ(n) elements one place to the right
even if the right position to insert the element can be find in time Θ(lgn), shifting still takes Θ(n)
overall time still Θ(n^2)

2.3-7
first sort the array by mergesort
then for every element g in the sorted array, binary search x - g
(if the index of x - g equals the index of g, x = 2g, and such a pair doesn't exist)
./CLRS/index.ts#pairSum
mergesort: Θ(nlgn)
binary search: n * Θ(lgn) = Θ(nlgn)
overall: Θ(nlgn) + Θ(nlgn) = Θ(nlgn)

2-1
a.  insertion sort on an array of length k takes time Θ(k^2)
    there are n/k such arrays, overall running time Θ(k^2) * n / k = Θ(nk)
b.  the same with figure 2.5
    the height of the tree is reduced to lg(n/k)
    base level performs n/2k merges with arrays of length k, n/2k * Θ(k) = Θ(n) in total
    overall running time Θ(nlg(n/k))
c.  T(n) = Θ(nk + nlg(n/k))
    if k = ω(lgn), Θ(nk) = ω(nlgn), the running time of the modified algorithm then is asymptotically greater
d.  k = O(lgn), the precise value of k depends on cache size, hidden constant factors in algorithms, etc.

2-2
a.  it terminates in finite time on all inputs
b.  invariant:
        at the end of each iteration, the smallest element in slice A[j-1 .. A.length] is in A[j-1]
    initialization:
        j = A.length, the first iteration compares A[A.length] to A[A.length - 1], put the smaller in A[A.length - 1]
        which is the smallest in A[A.length - 1 .. A.length]
    maintenance:
        the smallest number is compared with A[j-1], the smaller of the two is swapped to A[j-1]
        inductively A[j-1] now holds the smallest number in A[j-1 .. A.length]
    termination:
        j = i+1, A[i] holds the smallest number in A[i .. A.length]
c.  invariant:
        at the end of each iteration, A[1..i] consists of ith smallest elements and is sorted in non-decreasing order
    initialization:
        at the end of the first iteration, the smallest element in A[1 .. A.length] is placed in A[1]
        it is the smallest element, A[1 .. 1] is trivially sorted
    maintenance:
        the inner loop finds the smallest number in A[i .. A.length] and puts it in A[i]
        A[1 .. i-1] consists of the i-1 smallest elements in A, therefore A[i] is the ith smallest number
        by definition A[i] >= A[i-1], A[1..i] is sorted 
    termination:
        A[1..A.length - 1] consists of A.length - 1 smallest numbers and is sorted
        thereby A[A.length] must be the greatest element in A
        A is sorted in non-decreasing order
d.  inner loop Θ(n)
    outer loop run n - 1 times
    overall running time (n-1) * Θ(n) = Θ(n^2)
    both the best and worst case of bubble sort performs Θ(n^2) comparsions
    while insertion sort in best case has running time Θ(n)

2-3
a.  the algorithm performs n additions and n multiplications
    assuming both are contant time, T(n) = Θ(n)
b.  ./CLRS/start/index.ts#naivePolynomial
    k = Θ(n), Math.pow(x, k) = x^k if implemented naively takes time Θ(k), if not takes time Θ(lgk)
    overall running time Θ(n^2) or Θ(nlgn)
c.  initialization:
        initially y = 0, the sum of an empty sequence
    maintenance:
        y = Σ(k ∈ {0..n-(i+1)})a(k+i+1)x^k = a(i+1) + a(i+2)x + .. + an * x^(n-(i+1)) at the start
        y = ai + xy = ai + a(i+1)x + .. + an * x^(n-i) = Σ(k ∈ {0..n-i})a(k+i)x^k
    termination:
        the end of the last iteration ends with i = 0, the next iteration (not executed) starts with i = -1
        y = Σ(k ∈ {0..n})ak * x^k
d.  as above

2-4
a.  (0, 4), (1, 4), (2, 3), (2, 4), (3, 4)  // zero-based index
b.  [n, n-1 .. 1], every pair is an inversion
    there are C(n, 2) = n(n-1)/2 pairs
c.  every assignment in the insertion sort reduces the total number of inversions by 1
    the sorted array has no inversions
    therefore the number of inversions in an array is a lower bound of the running time of insertion sort
d.  assume an array A[p..r] and some q that p < q < r, then the total number of inversions in A[p..r] is the sum of
    1.  the number of inersions in A[p..q]
    2.  the number of inversions in A[q+1..r]
    3.  the number of inversions as pairs (i, j) such that
            i in {p..q}, j in {q+1..r}
    for every index i in {p..q}, the number of type 3 inversions is exactly the number of indices j in {q+1..r} that
        A[i] > A[j]
    which can be calculated in a slightly modified merge function
    ./CLRS/start/mergesort.ts#inversionCount

Chapter 3
3.1-1
max(f(n), g(n)) <= f(n) + g(n) for all n
assume f(n) >= g(n), max(f(n), g(n)) = f(n) = (f(n) + f(n)) / 2 >= (f(n) + g(n)) / 2
symmetrically when g(n) >= f(n), max(f(n), g(n)) >= (f(n) + g(n)) / 2
therefore let c1 = 1, c2 = 1/2,
    1/2(f(n) + g(n)) <= max(f(n), g(n)) <= f(n) + g(n)
    max(f(n), g(n)) = Θ(f(n) + g(n))

3.1-2
(n + a)^b = Σ(k ∈ {0..b})(C(b, k) * n^k * a^(b-k))
the term with highest order is n^b, therefore (n + a)^b = Θ(n^b)

3.1-3
O(n^2) is an upper bound, the set O(n^2) contains all functions from constant to quadratic
denote the running time of algorithm A by T(n)
the statement can then be translated to "T(n) is at least any constant (including 0) for large enough n"
which is meaningless for a asymptotically non-negative function

3.1-4
2^(n+1) = 2 * 2^n = Θ(2^n)
2^2n / 2^n = 2^n, no constant can be asymptotically larger than 2^n

3.1-5
=>: f(n) = Θ(g(n)), c1g(n) <= f(n) <= c2g(n) for n >= n0, then trivally f(n) = O(g(n)) and f(n) = Ω(g(n))
<=: f(n) = O(g(n)) implies f(n) <= c1g(n) for n >= n1
    f(n) = Ω(g(n)) implies f(n) >= c2g(n) for n >= n2
    take n3 = max(n1, n2), c1g(n) <= f(n) <= c2g(n) for n >= n3, f(n) = Θ(g(n))
therefore f(n) = Θ(n) <=> f(n) = O(g(n)) and f(n) = Ω(g(n))

3.1-6
f(n) = Θ(g(n)) => f(n) = O(g(n)), for all inputs g(n) is an upper bound of f(n)
so even in the worst case f(n) = O(g(n))
similarly in the best case f(n) = Ω(g(n))

3.1-7
assume f(n) = o(g(n)) and f(n) = ω(g(n)), then
lim(f(n) / g(n)) = ∞ and lim(f(n) / g(n)) = 0, contradiction
so no such f(n) exist, o(g(n)) ∩ ω(g(n)) = ∅

3.1-8
Ω(g(n, m)) = {f(n, m):  there exist positive constants c, n0, m0 such that
                        cg(n, m) <= f(n, m) for all n >= n0 or m >= m0 }
Θ(g(n, m)) = O(g(n, m)) ∩ Ω(g(n, m))

3.2-1
for n1 >= n2, f(n1) >= f(n2), g(n1) >= g(n2), f(n1) + g(n1) >= f(n2) + g(n2)
thereby f(n) + g(n) is monotonically increasing
for n1 >= n2, g(n1) >= g(n2), f(g(n1)) >= f(g(n2))
thereby f(g(n)) is monotonically increasing

3.2-2
a^log(b, c) = (b^log(b, a))^log(b, c)
            = (b^log(b, c))^log(b, a)
            = c^log(b, a)

3.2-3
n! = (2πn)^(1/2) * (n/e)^n * (1 + Θ(1/n))
lg(n!)  = lg((2πn)^(1/2)) + lg((n/e)^n) + lg(1 + Θ(1/n))
        = 1/2lg(2πn) + nlg(n/e) + lg(1 + Θ(1/n))
        = Θ(lgn) + Θ(nlgn) + O(lgn)
        = Θ(nlgn)
n! = Πn, n! / 2^n = Π(k/2), lim(Π(k/2)) -> ∞, n! = ω(2^n)
n! / n^n = Π(k/n) <= 1/n, lim(1/n) -> 0, n! = o(n^n)

3.2-4
lg((lgn)!) = Θ(lgn * lglgn)
for large enough n,
c1(lgn * lglgn) <= lg((lgn)!) <= c2(lgn * lglgn)
e^c1(lgn * lglgn) <= (lgn)!
n^(c1 * lglgn) <= (lgn)!
as c1 * lglgn is not a constant, (lgn)! cannot be polynomially bounded
thanks https://ita.skanev.com/03/02/04.html
lg((lglgn)!)    = Θ(lglgn * lglglgn)
                = o(lglgn * lglgn)
                = o((lglgn)^2)
                = o(lgn)
asymptotically,
lg((lglgn)!) < lgn
(lglgn)! < e^lgn = n
therefore (lglgn)! is polynomially bounded

3.2-5
by definition, 
lg*lgn = lg*n - 1 = Θ(lg*n)
lg(lg*n) = Θ(lg(lg*n))
therefore lg*lgn is asymptotically larger

3.2-6
((1 + 5^(1/2)) / 2)^2 = (1 + 2 * 5^(1/2) + 5) / 4 = (3 + 5^(1/2)) / 2 = (1 + 5^(1/2)) / 2 + 1
((1 - 5^(1/2)) / 2)^2 = (1 - 2 * 5^(1/2) + 5) / 4 = (3 - 5^(1/2)) / 2 = (1 - 5^(1/2)) / 2 + 1

3.2-7
let g and g' denote golden ratio and its conjugate, r5 denotes square root of 5
g - g' = r5
g^2 = g + 1, g'^2 = g' + 1
F1 = 1 = (g - g') / r5
F2 = 1 = (g^2 - g'^2) / r5
assume Fi = (g^i - g'^i) / r5, Fi+1 = (g^(i+1) - g'^(i+1)) / r5
    (g^(i+2) - g'^(i+2)) / r5   = (g^i * g^2 - g'^i * g'^2) / r5
                                = (g^i * (g + 1) - g'^i * (g' + 1)) / r5
                                = (g^i - g'^i) / r5 + (g^(i+1) - g'^(i+1)) / r5
                                = Fi + Fi+1
                                = Fi+2

3.2-8
by symmetry law
    n = Θ(klnk)
as ln(n) is monotonically increasing
    c1klnk <= n <= c2klnk
    ln(c1klnk) <= ln(n) <= ln(c2klnk)
    lnk + lnlnk + c1' <= ln(n) <= lnk + lnlnk + c2'
    ln(n) = Θ(lnk)
    lnk = Θ(ln(n))
    k = Θ(n) / lnk = Θ(n) / Θ(ln(n)) = Θ(n / ln(n))

3-1
a.  k >= d, p(n) / n^k = Σ(ai * n^(i - k))
    each i - k <= 0, therefore lim(p(n) / n^k) <= ad
    take c = ad + ε, where ε is an arbitrary positive number
    there must exist n0 such that for n >= n0, lim(p(n) / cn^k) < 1, p(n) = O(n^k)
b.  k <= d, p(n) / n^k = Σ(ai * n^(i - k))
    lim(p(n) / n^k) >= ad
    take c = ad/2, with large enough n, cn^k <= p(n), p(n) = Ω(n^k)
c.  k == d <=> k <= d && k >= d
    combine a and b, p(n) = O(n^k) && p(n) = Ω(n^k) => p(n) = Θ(n^k)
d.  if k > d, lim(p(n) / n^k) = 0, p(n) = o(n^k)
e.  if k < d, lim(p(n) / n^k) = ∞, p(n) = ω(n^k)

3-2
a.  y   y   n   n   n
b.  y   y   n   n   n
c.  n   n   n   n   n
    n^sin(n) swings between 1/n and n
d.  n   n   y   y   n
    lim(2^n / 2^(n/2)) = lim(2^(n/2)) = ∞
e.  y   n   y   n   y 
    by 3.16, n^lgc = c^lgn
f.  y   n   y   n   y
    lg(n!) = Θ(nlgn)
    lg(n^n) = nlgn = Θ(nlgn)

3-3
a.  1
    lg(lg*n)
    lg*(lgn) = lg*n - 1
    lg*n
    lnln(n)
    // polylogarithmics:
    2^lg*n // unsure
    (lgn)^(1/2)
    ln(n)
    (lgn)! = Θ(lgn * lglgn)
    (lgn)^2
    2^((2lgn)^(1/2)) // unsure
    // polynomials:
    n^(1/lgn)
    (2^(1/2))^lgn = n^(1/2)
    n 
    2^lgn = n
    nlgn
    lg(n!) = Θ(nlgn)
    n^2
    4^lgn = n^2
    n^3
    // exponentials:
    (lgn)^lgn = n^lglgn
    n^lglgn
    (3/2)^n
    2^n
    n * 2^n
    e^n
    // super-exponentials:
    n!
    (n+1)!
    2^(2^n)
    2^(2^(n+1))
b.  the fastest growing function here is 2^(2^(n+1))
    2^(2^(n+2)) * |sin(n)|

3-4
a.  false, n = O(n^2), n^2 != O(n)
b.  false, let f(n) = n^2, g(n) = n, min(f(n), g(n)) = n, f(n) + g(n) = Θ(n^2) != Θ(n)
c.  f(n) = O(g(n))
    f(n) <= c1g(n)
    lg(f(n)) <= lg(c1(g(n))) = lg(g(n)) + c2 = O(lg(g(n)))
d.  false
    f(n) = O(g(n))
    f(n) <= c1g(n)
    2^f(n) <= 2^c1g(n) = (2^c1)^g(n)
    let f(n) = 2n, g(n) = n
    2^f(n) = 2^2n, 2^g(n) = 2^n, 2^2n != O(2^n)
e.  false, let f(n) = 1/n, f^2(n) = (1/n)^2 = 1/n^2
    lim(f(n) / f^2(n)) = ∞, f(n) = ω(f^2(n))
f.  f(n) = O(g(n))
    f(n) <= c1g(n)
    (1/c1)f(n) <= g(n)
    g(n) = Ω(f(n))
g.  false, let f(n) = 2^n, f(n/2) = 2^(n/2), lim(f(n) / f(n/2)) = ∞, f(n) = ω(f(n/2))
h.  for any g(n) ∈ o(f(n)), lim(g(n) / f(n)) = 0
    for large enough n, lim((f(n) + g(n)) / f(n)) = 1
    let c1 = 1, c2 = 1 + ε, there must exist n0 such that for n >= n0,
        c1f(n) <= f(n) + g(n) <= c2f(n)
    and f(n) + g(n) = Θ(f(n))

3-5
a.  if f(n) != O(g(n)), there's no n0 and c that cg(n) >= f(n) for all n >= n0
    let c be an arbitrary constant, if only for finitely many n, f(n) >= cg(n)
    then take n0 be the greatest of such n, then for n >= n0+1, f(n) < cg(n), f(n) = O(g(n))
    thereby there must be infinitely many n such that f(n) >= cg(n) >= 0, f(n) = ∞Ω(g(n))
    so f(n) != O(g(n)) => f(n) = ∞Ω(g(n)) and f(n) != ∞Ω(g(n)) => f(n) = O(g(n))
    if f(n) = n and g(n) = n * |sin(n)|, then neither f(n) = O(g(n)) or f(n) = Ω(g(n))
b.  it is the complement of set O(g(n)), may work smoother in certain proofs
    it's harder to reason and less intuitive, f(n) = ∞Ω(g(n)) no longer gives an lower bound of f(n)
c.  =>: f(n) = Θ(g(n))  => c1g(n) <= f(n) <= c2g(n)
                        => f(n) = Ω(g(n)) and f(n) <= c2g(n)
        let f(n) = -n, g(n) = -n, take c1 = c2 = 1, f(n) = Θ(g(n))
        but |f(n)| = n > cg(n) for all positive constant c, f(n) != O'(g(n))
    <=: |f(n)| <= cg(n) => f(n) <= cg(n), so f(n) = O'(g(n)) implies f(n) = O(g(n))
        therefore f(n) = O'(g(n)) && f(n) = Ω(g(n)) => f(n) = Θ(g(n))
d.  ~Ωg(n) = {f(n): there exist positive constants c, k and n0 that
                    0 <= cg(n)lg^k(n) <= f(n) for all n >= n0 }
    ~Θg(n) = {f(n): there exist positive constants c1, c2, k1, k2 and n0 that
                    c1g(n)lg^k1(n) <= f(n) <= c2g(n)lg^k2(n) for all n >= n0 }
    =>: f(n) = ~Θ(g(n)) => c1g(n)lg^k1(n) <= f(n) <= c2g(n)lg^k2(n) for all n >= n0
        instantly f(n) = ~O(g(n)) and f(n) = ~Ω(g(n))
    <=: similar, take n0 be the maximum of the two

3-6
a.  n - c
b.  lg*(n)
c.  lg(n)
d.  lg(n) - 1
e.  lglg(n)
f.  ∞
g.  lglg(n)
h.  iter 0: n
    iter 1: n / lgn = (iter 0) / lgn
    iter 2: (n / lgn) / lg(n / lgn) = (iter 1) / (lgn - lglgn)
    iter 3: ((n / lgn) / (lgn - lglgn)) / lg((n / lgn) / (lgn - lglgn))
            = (iter 2) / ((lgn - lglgn) - lg(lgn - lglgn))
    the divisor is never greater than lgn
    therefore lower bound is Ω(log(lgn, n)) = Ω(lgn / lglgn)
    upper bound unknown

Chapter 4
4.1-1
still the maximum subarray, the left and right sum is initialized to -Infinity

4.1-2
./CLRS/start/max-subarray.ts#findMaximumSubarrayBrute

4.1-3
crossover happens between 1024 <= n0 <= 2048
mixing recursive and brute-force algorithms then the crossover is no longer noticeable

4.1-4
return tuple of (Option<Idx>, Option<Idx>, Sum)
initialize left-sum and right-sum to 0, left-max and right-max to None
if all array elements between low and high are non-positive, left-max and right-max will not be reassigned
return value will be (None, None, 0)

4.1-5
if tail_max is the maximum subarray of the form A[i .. j] in an array A[0 .. j]
then all subarrays of the form A[k .. i-1] where k <= i-1 sum to non-positive value
all subarrays of the form A[i .. k] where k < j sum to non-negative value
considering A[k .. j+1],
    if k < i
        the slice can be divided to A[k .. i-1] and A[i .. j+1], where A[k .. i-1] sums to non-positive
        sum of this slice is not greater than A[i .. j+1]
    if i < k <= j
        the sum of the slice can be expressed as sum(A[i .. j+1]) - sum(A[i .. k-1])
        where k - 1 < j and sum(A[i .. k-1]) >= 0
        sum of this slice is not greater than A[i .. j+1]
    if k = j+1
        the sum equals to A[j+1], and A[j+1] > sum(A[i .. j+1]) only if sum(A[i .. j]) < 0
therefore the new tail_max is either A[i .. j+1] or A[j+1], depends on whether sum(A[i .. j]) < 0
./CLRS/start/max-subarray.ts#findMaximumSubarrayLinear

4.2-1
((1, 3), (7, 5)) * ((6, 8), (4, 2))
S1 = B12 - B22 = 6;
S2 = A11 + A12 = 4;
S3 = A21 + A22 = 12;
S4 = B21 - B11 = -2;
S5 = A11 + A22 = 6;
S6 = B11 + B22 = 8;
S7 = A12 - A22 = -2;
S8 = B21 + B22 = 6;
S9 = A11 - A21 = -6;
S10 = B11 + B12 = 14;
P1 = A11 . S1 = 6;
P2 = S2 . B22 = 8;
P3 = S3 . B11 = 72;
P4 = A22 . S4 = -10;
P5 = S5 . S6 = 48;
P6 = S7 . S8 = -12;
P7 = S9 . S10 = -84;
C11 = P5 + P4 - P2 + P6 = 18;
C12 = P1 + P2 = 14;
C21 = P3 + P4 = 62;
C22 = P5 + P1 - P3 - P7 = 66;

4.2-2
./CLRS/start/matrix-mul.ts#strassen

4.2-3
extend size of matrix to the next power of 2
assume size of matrix A is n x n, the next power of 2 is 2^b, the extended matrix looks like
    A   0   and     B   0
    0T  1           0T  1
where 1 denotes (2^b - n) x (2^b - n) unit matrix, 0 denotes n x (2^b - n) zero metrix
the multiplication will be
    AB  0
    0T  1
then AB can be extracted from the result
as n is not a power of 2, 2^b <= 2n
the extension takes time at most (2^b)^2 = O((2n)^2) = O(n^2)
O(n^2) + T(2^b) <= O(n^2) + T(2n) = O(n^2) + Θ((2n)^lg7) = Θ(n^lg7) using strassen's method

4.2-4
assume the meaning of matrices in "3 x 3 matrices" and "n x n matrices" are the same
the problem didn't state what will happen when n > 3
it just gives a potentially easier way to calculate the base case, which is Θ(1) anyway
it's not even clear what kind of multiplication it means by "k multiplications", maybe scalar
so this assumption will not affect the overall complexity, o(n^lg7) not achievable this way

4.2-5
same as above, only gives an easier way to calculate the base case, which is Θ(1) anyway
states nothing about the complexity when n > 72

4.2-6
let A = transpose([A1, A2 .. Ak]), B = [B1, B2 .. Bk], each of Ai, Bj is a n x n matrix
then A . B will be kn x kn
A . B = (Ai . Bj), k^2 multiplications of n x n matrices in total
assume matrix multiplication is O(g(n)), then T(n) = O(k^2 * g(n))
using strassen's algorithm g(n) = n^ln7, T(n) = O(k^2 * n^ln7)

4.2-7
let p1 = (a+b)(c+d) = ac + ad + bc + bd
    p2 = ac
    p3 = bd
then 
    real component = p2 - p3
    complex component = p1 - p2 - p3

4.3-1
assume T(n-1) <= (n-1)^2
T(n)    <= (n-1)^2 + n
        = n^2 - 2n + 1 + n 
        <= n^2
implicitly c = 1, T(n) = O(n^2)

4.3-2
assume T([n/2]) <= lg(n/2)
T(n)    <= lg(n/2) + 1
        = lgn - 1 + 1
        <= lgn
implicitly c = 1, T(n) = O(lgn)

4.3-3
assume T([n/2]) >= c(n/2)lg(n/2)
T(n)    >= 2 * c(n/2)lg(n/2) + n
        = cnlg(n/2) + n
        = cnlgn - cn + n
take 0 < c < 1, then T(n) >= cnlgn, T(n) = Ω(nlgn)

4.3-4
assume T([n/2]) <= c(n/2)lg(n/2) + 1
T(n)    <= 2 * c(n/2)lg(n/2) + 1 + n
        = cnlgn - cn + n + 1
        <= cnlgn when c > 2
T(1)    <= clg1 + 1 = 1

4.3-5
assume T([n/2]) <= c(n/2)lg(n/2) + 1
for any c1n = Θ(n)
T(n)    <= 2 * c(n/2)lg(n/2) + 1 + c1n
        = cnlgn - cn + 1 + c1n
        <= cnlgn + 1 when c <= c1
thus T(n) = O(nlgn)
assume T([n/2]) >= c(n/2)lg(n/2) + 1
for any c1n = Θ(n)
T(n)    >= 2 * c(n/2)lg(n/2) + 1 + c1n
        = cnlgn - cn + 1 + c1n
        >= cnlgn + 1 when c <= c1
thus T(n) = Ω(nlgn)
T(1)    = clg1 + 1 = 1 = Θ(1)
therefore T(n) = Θ(nlgn)

4.3-6
assume T(n) <= nlgn
T(n)    <= 2 * (n/2 + 17)lg(n/2 + 17) + n
        <= 2 * (n/2)lg(n/2) + n
        = nlgn - n + n = nlgn
T(n) = O(nlgn)

4.3-7
assume T(n) <= cn^log(3, 4)
T(n)    <= 4 * c(n/3)^log(3,4) + n
        = 4 * n^log(3,4)/4 + n
        = n^log(3,4) + n
        stuck
assume T(n) <= c1n^log(3,4) - c2n
T(n)    <= 4 * c1(n/3)^log(3,4) - 4 * c2(n/3) + n
        = c1n^log(3,4) - (4/3) * c2n + n
        <= c1n^log(3,4) - c2n when 4/3 * c2 - 1 >= c2
T(n) = O(n^log(3,4))
assume T(n) >= c1n^log(3,4) + c2n
T(n)    >= 4 * (c1(n/3)^log(3,4) - c2(n/3)) + n
        = c1n^log(3,4) - (4/3)*c2n + n
        >= c1n^log(3,4) - c2n when (4/3) * c2 - 1 <= c2
T(n) = Ω(n^log(3,4))
therefore T(n) = Θ(n^log(3,4))

4.3-8
the conclusion is incorrect, the recurrence
    T(n) = 4T(n/2) + n^2
is case 2 of master theorem, which means T(n) = Θ(n^2lgn)

4.3-9
T(n) = 3T(n^(1/2)) + lgn
let m = lgn, then
T(2^m)  = 3T((2^m)^(1/2)) + m
        = 3T(2^(m/2)) + m
let S(m) = T(2^m)
S(m) = 3S(m/2) + m
assume S(m) <= c1m^lg3 - c2m
S(m)    <= 3c1(m/2)^lg3  - (3/2)c2m + m
        = c1m^lg3 - (3/2)c2m + m
        <= c1m^lg3 - c2m when (3/2)c1 - 1 < c2
S(m) = O(m^lg3)
similarly, S(m) = Ω(m^lg3), S(m) = Θ(m^lg3)
T(n) = T(2^m) = S(m) = Θ(m^lg3) = Θ(lgn^lg3)

4.4-1
T(n) = 3T(n/2) + n
n/2 every level, lgn levels in total
for i = 0, 1 .. lgn, ith level has 3^i nodes, each node is n/2^i
the base level has 3^lgn = n^lg3 nodes, each is Θ(1)
Σ(3/2)^i will not converge
T(n)    = Σ(i = {0..lgn - 1})(3/2)^i * n + Θ(n^lg3)
        = ((3/2)^lgn - 1) / (3/2 - 1) * n + Θ(n^lg3)
        = 2(n^(lg3 - 1)/(3/2) - 1) * n + Θ(n^lg3)
        = O(n^lg3) + Θ(n^lg3)
        = O(n^lg3)
assume T(n) <= c1n^lg3 - c2n
T(n)    <= 3c1(n/2)^lg3 - 3c2(n/2) + n
        = cn^lg3 - (3/2)c2n + n
        <= c1n^lg3 - c2n when (3/2)c2 - 1 >= c2

4.4-2
T(n) = T(n/2) + n^2
n/2 every level, lgn levels in total 
for i = 0, 1 .. lgn, ith level has a single node, each node is (n/2^i)^2 = n^2/4^i
the base level has 1 node, Θ(1) in total
T(n)    = Σ(i = {0 .. lgn - 1})(1/4)^i * n^2 + Θ(1)
        <= 1/(1 - 1/4) * n^2 + Θ(1)
        = O(n^2)
without induction, this must be a tight bound, as T(n) >= n^2

4.4-3
T(n) = 4T(n/2 + 2) + n, assume it's asymptotically equivalent to T(n) = 4T(n/2) + n
n/2 every level, lgn levels in total
for i = 0, 1 .. lgn, ith level has 4^i nodes, each node is n/2^i
the base level has 4^lgn nodes, Θ(n^2) in total
T(n)    = Σ(i = {0 .. lgn - 1})2^i * n + Θ(n^2)
        = (2^lgn - 1) / (2 - 1) * n + Θ(n^2)
        = (n/2 - 1) * n + Θ(n^2)
        = O(n^2)
assume T(n) <= c1n^2 - c2n
T(n)    <= 4 * (c1(n/2 + 2)^2 - c2(n/2 + 2)) + n
        = c1n^2 + 4c1n + 4c1 - 2c2n + 8 + n
        = c1n^2 + (4c1 - 2c2 + 1)n + 8
        <= c1n^2 - c2n when 2c2 - 4c1 >= c2 and n is big enough
therefore T(n) = O(n^2)

4.4-4
n - 1 every level, n levels in total
for i = 0, 1, .. n, ith level has 2^i nodes, each node is 1
the base level has 2^n nodes, Θ(2^n) in total
T(n)    = Σ(i = {0 .. n - 1})2^i + Θ(2^n)
        = O(2^n)
assume T(n) <= c2^n - 1
T(n)    <= 2 * c2^(n-1) - 2 + 1
        = c2^n - 1
therefore T(n) = O(2^n)

4.4-5
T(n) = T(n-1) + T(n/2) + n
the longest simple path is of length n
as n - 1 + n/2 < (3/2)n, the sum of ith level is smaller than (3/2)^i * n
the base level have at most 2^n nodes, Θ(2^n) in total
T(n)    <= Σ(i = {0 .. n - 1})(3/2)^i * n + Θ(2^n)
        = Θ((3/2)^n * n) + Θ(2^n)
        = O(2^n)
assume T(n) <= c2^n
T(n)    <= c2^(n-1) + c2^(n/2) + n
        <= c2^n for large enough n

4.4-6
thanks https://ita.skanev.com/04/04/06.html
the shortest simple path from top to root is log(3, n)
each level still cn, this time the binary tree up to level log(3, n) is guarenteed to be complete
therefore T(n) >= log(3, n) * cn = Θ(nlgn), T(n) = Ω(nlgn)

4.4-7
cn
4c(n/2)
16c(n/4)
...
lgn levels in total, ith level has 4^i nodes, each is cn/2^i
the base level has 4^lgn = n^2 nodes, Θ(n^2) in total
T(n)    = Σ(i = {0 .. lgn - 1})2^i * cn + Θ(n^2)
        = Θ(2^lgn * cn) + Θ(n^2)
        = Θ(n^2)
assume T(n) <= c1n^2 - c2n
T(n)    <= 4 * (c1(n/2)^2 - c2(n/2)) + cn
        = c1n^2 - 2c2n + cn
        <= c1n^2 - c2n when c2 >= c
T(n) = O(n^2), similarly T(n) = Ω(n^2)

4.4-8
as a is a constant, T(a) = Θ(1)
T(n) = T(n-a) + Θ(1) + cn
n/a levels in total, ith level has 1 node, each node c(n - ia) + Θ(1)
base level a single node of Θ(1)
T(n)    = Σ(i = {0 .. n/a - 1})(c(n - ia) + Θ(1)) + Θ(1)
        = Θ(n/a) + cn^2/a - Θ((n/a)^2) * a
        = Θ(n^2)

4.4-9
reuse the solution to T(n) = T(n/3) + T(2n/3) + O(n) in text and problem 4.4-6
T(n) = Ω(nlgn) and T(n) = O(nlgn), T(n) = Θ(nlgn)

4.5-1
a.  a = 2, b = 4, log(b, a) = 1/2, f(n) = 1 = n^0, 1/2 > 0
    T(n) = Θ(n^(1/2))
b.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n^(1/2) = n^log(b, a)
    T(n) = Θ(n^(1/2)lgn)
c.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n, 1/2 < 1
    T(n) = Θ(n)
d.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n^2, 1/2 < 2
    T(n) = Θ(n^2)

4.5-2
log(4, a) < log(2, 7) = log(4, 49)
a < 49

4.5-3
a = 1, b = 2, log(b, a) = 0, f(n) = 1 = n^0 = n^log(b, a)
T(n) = Θ(lgn)

4.5-4
a = 4, b = 2, log(b, a) = 2, f(n) = n^2lgn
f(n) = Ω(n^log(b, a)) but not polynomially larger, master method doesn't apply

4.5-5
f(n) = n^(ε + |sin(n)|), a = 1, b = 2 for some small positive constant very close to 0
log(b, a) = 0, ε + |sin(n)| >= ε > 0
for any n/2 = 2kπ + π/2, sin(n/2) = 1, sin(n) = 0
af(n/b) = (n/2)^(ε + |sin(n/2)|) >= n/2
when n is large
cf(n) <= n^ε < n/2 for infinitely many k and n

4.6-1
n0 = n, n1 = [n/b], n2 = [[n/b]/b]
if b is an integer
n = kb + r, 0 <= r < b, [n/b] = k if r = 0, k + 1 if r > 0
as b is an integer, b > 1 => b >= 2, k = k'b + r', 0 <= r' < b, then
[[n/b]/b]   = k' if r = r' = 0
            = k' + 1 otherwise
n = kb + r = (k'b + r')b + r = k'b^2 + r'b + r
where r < b, r'b <= b(b-1), r'b + r < b^2
[n/b^2] = k' if r = r' = 0
        = k' + 1 otherwise
therefore [[n/b]/b] = [n/b^2] for all integer n
n2 = [[n/b]/b] = [n/b^2]
n3 = [n2/b] = [n/b^3]
...
nj = [n/b^j]

4.6-2
f(n) = Θ(n^log(b, a)lg^kn), then
g(n)    = Σ(j{0 .. log(b, n) - 1})(a^j * f(n/b^j))
        = {0 .. log(b, n) - 1})(a^j * Θ((n/b^j)^log(b, a) * lg^k(n/b^j)))
where
    Θ((n/b^j)^log(b, a) * lg^k(n/b^j)))
    = Θ((n^log(b, a) / a^j) * lg^k(n/b^j)
g(n)    <= Σ{0 .. log(b, n) - 1})(a^j * c1(n^log(b, a) / a^j) * lg^k(n/b^j))
        = Σ(c1 * n^log(b, a) * lg^k(n/b^j))
        = Σ(c1 * n^log(b, a) * lg(n / b^j)^k)
        = Σ(c1 * n^log(b, a) * (lgn - j * lgb)^k)
        <= c1 * n^log(b, a) * Σ(lg^k(n))
        = c1n^log(b, a) * log(b, n) * lg^k(n)
        = c1n^log(b, a) * O(lgn) * O(lg^k(n))
        = O(n^log(b, a) * lg^k+1(n))
similarly
g(n)    >= c2 * n^log(b, a) * Σ(j = {0 .. log(b, n) - 1})(lg(n/b^j)^k)
take only half of the summation terms,
g(n)    >= c2 * n^log(b, a) * Σ(j = {0 .. log(b, n) / 2})(lg(n/b^j)^k)
take the minimum over all summation terms
        >= c2 * n^log(b, a) * Σ(j = {0 .. log(b, n) / 2})(lg(n/b^(log(b, n) / 2))^k)
        = c2 * n^log(b, a) * (log(b, n) / 2)(lg(n^(1/2))^k)
        = c2 * n^log(b, a) * (log(b, n) / 2)(1/2 * lgn)^k
        = c2 * n^log(b, a) * Θ(lgn) * Θ(lg^kn)
        = Ω(n^log(b, a) * lg^(k+1)(n))
therefore T(n) = Θ(n^log(b, a)) + Θ(n^log(b, a) * lg^(k+1)(n))

4.6-3
thanks https://ita.skanev.com/04/06/03.html
af(n/b) <= cf(n)
=>  f(n)    >= (a/c)f(n/b)
            >= (a/c)^2f(n/b^2)
            ..
            >= (a/c)^if(n/b^i)
take i = log(b, n)
    f(n)    >= (a/c)^log(b, n)f(1)
            = Θ((a/c)^log(b, n))
            = Θ(n^log(b, a/c))
            = Θ(n^(log(b, a) - log(b, c)))
as c < 1, log(b, c) < 0, -log(b, c) > 0, let ε = -log(b, c)
    f(n)    >= Θ(n^(log(b, a) + ε))
    f(n)    = Ω(n^log(b, a) + ε)

4-1
a.  a = 2, b = 2, log(b, a) = 1, f(n) = n^4, 4 > 1
    2f(n/2) = n^4 / 8, c = 1/8 < 1
    T(n) = Θ(n^4)
b.  a = 1, b = 10/7, log(b, a) = 0, f(n) = n^1, 1 > 0
    c = 7/10 < 1
    T(n) = Θ(n)
c.  a = 16, b = 4, log(b, a) = 2, f(n) = n^2
    T(n) = Θ(n^2lgn)
d.  a = 7, b = 3, log(b, a) = log(3, 7) < 2, f(n) = n^2
    7f(n/3) = 7n/9, c = 7/9
    T(n) = Θ(n^2)
e.  a = 7, b = 2, log(b, a) = lg7 > 2, f(n) = n^2
    T(n) = Θ(n^lg7)
f.  a = 2, b = 4, log(b, a) = 1/2, f(n) = n^(1/2)
    T(n) = Θ(n^(1/2)lgn)
g.  n - 2 every level, n/2 levels in total
    every level has a single node, each contributes (n - 2i)^2
    the base level has a single node of Θ(1)
    T(n)    = Σ(i = {0 .. n/2 - 1})(n - 2i)^2 + Θ(1)
            <= n/2 * n^2 + Θ(1) = O(n^3)
    similarly, take only half of the summation terms
    T(n)    >= Σ(i = {0 .. n/4})(n - 2i)^2 + Θ(1)
            >= Σ(i = {0 .. n/4})(n - n/2)^2 + Θ(1)
            = (n/4) * (n/2)^2 + Θ(1) = Ω(n^3)
    therefore T(n) = Θ(n^3)

4-2
a.  1.  Θ(lgN) as usual
    2.  now each call have to copy the whole array
        the function at most recursively call itself lgn times
        total cost NlgN
    3.  T(n) = T(n/2) + n
        T(N) = Θ(N) by master method
b.  1.  Θ(NlgN) as usual
    2.  all recursive call to the function itself forms a complete binary tree of height lgN
        2^lgN = N nodes in total, the whole array is copied at each node, contributes N * N = N^2 in total
        the number of merge is half the number of nodes, again Θ(N) and contributes Θ(N^2) in total
        overall running time Θ(N^2)
    3.  T(n) = 2T(n/2) + 2 * (n/2) + n = 2T(n/2) + Θ(n) = Θ(nlgn)

4-3
a.  a = 4, b = 3, log(b, a) = log(3, 4) > 1, f(n) = nlgn = O(n^(1 + ε))
    T(n) = Θ(n^log(3, 4))
b.  T(n) = 3T(n/3) + n/lgn = 3T(n/3) + cn/log(3, n)
    3/n every level, log(3, n) levels in total
    ith level has 3^i nodes, contributes 3^i * c(n/3^i) / log(3, (n/3^i)) = cn / (log(3, n) - i)
    the base level has 3^log(3, n) = n nodes, Θ(n) in total
    T(n)    = Σ(i = {0 .. log(3, n) - 1})(n / (log(3, n) - i)) + Θ(n)
            = n * Σ(i = {0 .. log(3, n) - 1}(1 / (log(3, n) - i))) + Θ(n)
    the harmonic series Σ(i = {0 .. n - 1})(1 / (n - i)) is asymptotically close to ln(n)
    T(n)    = n * Θ(ln(log(3, n))) + Θ(n)
            = Θ(nlglgn)
c.  a = 4, b = 2, log(b, a) = 2, f(n) = n^2.5, 2.5 > 2
    T(n) = Θ(n^2.5)
d.  guess with master method, T(n) = Θ(nlgn)
    assume T(n) <= nlgn - cn
    T(n)    <= 3((n/3 - 2) * lg(n/3 - 2)) - 3c(n/3 - 2) + n/2
            <= nlg(n/3 - 2) - 3c(n/3 - 2) + n/2
            <= nlg(n/3) - cn + 6c + n/2
            = nlgn - (c + lg3 - 1/2)n + 6c
            <= nlgn - cn when n is large enough
    simlarly T(n) >= nlgn + cn for some c, T(n) = Θ(nlgn)
e.  similar to b, T(n) = Θ(nlglgn)
f.  the shortest path from bottom to root passes log(8, n) = lgn/3 nodes
    immediate children of a node of value p has combined value p/2 + p/4 + p/8 = 7p/8
    therefore ith level contributes (7/8)^i * n
    there are 3^(lgn/3) = n^(lg3 / 3) nodes at level lgn/3, each Θ(1), Θ(n^(lg3 / 3)) in total
    T(n)    >= Σ(i = {0 .. lgn/3 - 1})((7/8)^i * n) + Θ(n^(lg3 / 3))
            = n * ((7/8)^(lgn/3) - 1) / (7/8 - 1) + Θ(n^(lg3 / 3))
            = 8n * (1 - n^(lg7 / 3) / n) + Θ(n^(lg3 / 3))
            = Θ(n) - Θ(n^(lg7 / 3)) + Θ(n^(lg3 / 3))
            = Ω(n)
    assume T(n) <= cn
    T(n)    <= c(n/2 + n/4 + n/8) + n
            = (7/8)cn + n
            <= cn when c >= 8
    therefore T(n) = Θ(n)
g.  n - 1 every level, n levels in total, a single node every level, contributes 1/(n-i)
    the base level is a single node of Θ(1)
    T(n)    = Σ(i = {0 .. n-1})(1/(n-i)) + Θ(1), the harmonic series
            = Θ(ln(n)) + Θ(1)
            = Θ(lgn)
h.  n - 1 every level, n levels in total, ith level sums to lg(n - i)
    T(n)    = Σ(i = {0 .. n-1})(lg(n-i)) + Θ(1)
            = Θ(nlgn) as int(lgx) = x(lgx - 1)
i.  n - 2 every level, n/2 levels in total, ith level sums to 1/lg(n - 2i)
    T(n)    = Σ(i = {0 .. n/2 - 1})(1/lg(n - 2i)) + Θ(1)
            = Θ(li(n)), where li(n) is the logarithmic integral function
j.  let n = 2^m, m = lgn, T(n) = T(2^m) = 2^(m/2)T(2^(m/2)) + 2^m
    let S(m) = T(2^m), S(m) = 2^(m/2)S(m/2) + 2^m
    m/2 each level, lgm levels in total
    first level has 2^(m/2) nodes, the next 2^(m/2) * 2^(m/4), ...
    ith level will have 2^(m - m/2^i) nodes, each contributes 2^(m/2^i), 2^m in total
    the base level has 2^(m - m/2^(lgm)) = 2^(m-1) nodes, Θ(2^(m-1)) in total
    S(m)    = lgm * 2^m + Θ(2^(m-1)) = Θ(lgm * 2^m)
    T(n) = T(2^m) = S(m) = Θ(2^mlgm) = Θ(nlglgn)

4-4
a.  F(z)    = Σ(i = {0 ..})(Fiz^i)
    zF(z)   = Σ(i = {0 ..})(Fiz^(i+1))
            = Σ(i = {1 ..})(F(i-1)z^i)
            = Σ(i = {2 ..})(F(i-1)z^i)  // as F(0) = 0
    z^2F(z) = Σ(i = {0 ..})(Fiz^(i+2))
            = Σ(i = {2 ..})(F(i-2)z^i)
    z + zF(z) + z^2F(z) = 0 + z + Σ(i = {2 ..})(F(i-1)z^i + F(i-2)^zi)
                        = 0 + z + Σ(i = {2 ..})(Fiz^i)
                        = F(z)
b.  F(z) = z + zF(z) + z^2F(z)
    F(z) - zF(z) - z^2F(z) = z
    F(z)(1 - z - z^2) = z
    F(z) = z / (1 - z - z^2)
    let g = (1 + r5) / 2, g' = (1 - r5) / 2, r5 = 5^(1/2)
    (1 - gz)(1 - g'z)   = (1 - (1 + r5)z / 2)(1 - (1 - r5)z / 2)
                        = 1 - (1 - r5)z / 2 - (1 + r5)z / 2 + (1 + r5)(1 - r5)z^2 / 4
                        = 1 - z + (-4)z^2 / 4
                        = 1 - z - z^2
    (1/r5) * (1 / (1 - gz) - 1 / (1 - g'z))
    = (1/r5) * (- g'z + gz) / ((1 - gz) * (1 - g'z))
    = (1/r5) * (r5z) / ((1 - gz)(1 - g'z))
    = z / ((1 - gz)(1 - g'z))

4-5
a.  divide the chips into three groups
    first group of size n1 are good chips, which will always give the correct answer
    second group of size n2 are bad chips imitating good chips
    third group of size n3 are other bad chips
    as the number of bad chips n2 + n3 > n/2
    it's always possible to take n2 = n1 and n3 = n - n1 - n2 will be non-negative, then
    1.  chips in the first group will always answer "good" for chips in that group, "bad" for chips in group 2 and 3
    2.  let chips in group 2 imitate group 1
        always answer "good" for chips in the same group, "bad" for chips in group 1 and 3
    3.  let chips in group 3 answer "good" for any chips
    then chips in group 3 can be easily detected: they answer "good" for all chips, but only n2 < n/2 chips are good
    however group 1 and 2 are totally symmetric:
        they both have size n1 = n2 < n/2
        they only answer "good" for chips in the same group, which also answers "good" in the pairwise test
        they answer "bad" for chips not in that group
    no matter how many test is performed, it's impossible to tell if a chip is in group 1 or group 2
b.  assume n1 in n chips are good, where n1 > n/2
    starting from a set S containing all n chips, and an empty set S'
    repeat the following procedure [n/2] times:
        randomly pop two chips c1 and c2 from S, perform the test
        if any of the two reports "bad", drop both chips
        if both chips answer "good", drop c2 and add c1 to S'
    let n1 be the number of good chips tested, n2 be the number of bad chips tested
    let n11 be the number of good chips tested against a good chip, n12 otherwise (tested against a bad chip)
    similarly define n21 and n22, obviously n12 = n21, both n11 and n22 have to be even
    when n is even, n1 = n11 + n12, n2 = n21 + n22, n1 > n2
    the number of good chips in S' at the end will be n11 / 2, bad chips n22 / 2
    n11 = n1 - n12, n22 = n2 - n21 = n2 - n12, so n11 > n22, still more good chips than bad chips in S'
    when n is odd, one chip cannot be tested
    if that chip is bad, n1 > n2 + 1, n1 >= n2 + 2, n11 >= n22 + 2, n11/2 >= n22/2 + 1
    the bad chip must be dropped or number of good and bad chips may equal when n11/2 = n22/2 + 1
    if that chip is good, n1 >= n2, n1 >= n2, n11/2 >= n22/2
    the good chip must be included or number of good and bad chips may equal when n11/2 = n22/2
    thanks http://cseweb.ucsd.edu/classes/su99/cse101/sample1.pdf
    (n11 + n12) / 2 can be observed as the number of tests answered "good, good"
    if (n11 + n12) / 2 is even, add the untested chip to S'
    if (n11 + n12) / 2 is odd, drop the untested chip
    then the algorithm will act correctly on boundary conditions
c.  the single good chip can be find in time T(n), where
        T(n) = T(n/2) + [n/2]
        T(n) = Θ(n) by master method
    once a single good chip is found, it can be used to find all other good chips in Θ(n) tests
    overall time cost Θ(n)

4-6
a.  given
        A[i, j] + A[i+1, j+1] <= A[i, j+1] + A[i+1, j]
    for some r that 1 <= r < m - i, assume
        A[i, j] + A[i+r, j+1] <= A[i, j+1] + A[i+r, j]
    combine the assumption with
        A[i+r, j] + A[i+r+1, j+1] <= A[i+r, j+1] + A[i+r+1, j]
        A[i, j] + A[i+r+1, j+1] <= A[i, j+1] + A[i+r+1, j]
    therefore for any 1 <= r < m,
        A[i, j] + A[i+r, j+1] <= A[i, j+1] + A[i+r, j]
    similarly, apply the induction on columns
        A[i, j] + A[i+r, j+c] <= A[i, j+c] + A[i+r, j]
    for 1 <= r < m - i and 1 <= c < n - j
    take r = k - i and c = l - j, this is exactly A[i, j] + A[k, l] <= A[i, l] + A[k, j]
    conversely, take k = i+1, l = j+1, 
        A[i, j] + A[k, l] <= A[i, l] + A[k, j] => A[i, j] + A[i+1, j+1] <= A[i, j+1] + A[i+1, j]
b.  change 22 on first line to 24
c.  if f(i) > f(i+1) instead, by definition of Monge,
        A[i, f(i+1)] + A[i+1, f(i)] <= A[i+1, f(i+1)] + A[i, f(i)]
    by definition of f(i), A[i, f(i)] and A[i+1, f(i+1)] is the leftmost minimum of their rows, therefore
        A[i, f(i+1)] >= A[i, f(i)], A[i+1, f(i)] >= A[i+1, f(i+1)]
        A[i, f(i+1)] = A[i, f(i)], A[i+1, f(i)] = A[i+1, f(i+1)]
    A[i, f(i+1)] is also the minimum of row i, but then f(i+1) < f(i) and A[i, f(i)] is not the leftmost minimum
d.  the leftmost minimum of even rows are
        f(2), f(4), f(6) ..
    as proven in part c, f(1) <= f(2) <= f(3) <= ..
    therefore it's sufficient to search f(1) as the minimum among A[1, 1] .. A[1, f(2)]
    f(3) among A[3, f(2)] .. A[3, f(4)] and so on
    at most 2m + n elements have to be compared, Θ(m + n) in total
e.  the base case of finding the minimum of a row is O(n)
    T(m, n) = O(n) when m = 1
            = T(m/2, n) + O(m + n) otherwise
    the recurrence tree has lgm levels
    ith level has a single node contributes O(m/2^i + n)
    the base level is a single node of O(n)
    T(n)    = Σ(i = {0 .. lgm})O(m/2^i + n) + O(n)
            <= Σ(i = {0 .. lgm})c1(m/2^i + n) + c2n
            <= c1m + c1nlgm + c2n
            = O(m + nlgm)

Chapter 5
5.1-1
it's always possible to decide the best between any two candidates
which means either a > b or b > a for two candidates a and b
for a fixed set of candidates C and any candidate c in it,
    its rank, as the number of candidates better than it - 1, is fixed
    no candidate can have rank lower than |C|
    no two candidates can have the same rank, assume c has rank k,
        exactly (k-1) candidates are better than it, none of which can have rank equal or higher than k
        exactly (|C| - k) candidates are worse than it, none of which can have rank equal or lower than k
    therefore the rank of the candidates are a bijection C => {1 .. |C|}, where {1 .. |C|} has a total order

5.1-2
let 2^p be the next power of two of b - a
take p results of RANDOM(0, 1), interpret the result array as a binary number n
than n is uniformly distributed between 0 and 2^p - 1
if n <= b - a, return n + a, otherwise run the procedure again
let x <- S denote that n follows discrete uniform distribution over set S
as n <- {0 .. 2^p - 1}, if n <= b - a, n <- {0 .. a - b}, n + a <- {a .. b}
as 2^p is the next power of two of b - a, 2^(p-1) <= b - a < 2^p
the number of calls to the procedure follows geometric distribution with success probability (b - a) / 2^p >= 1/2
therefore the expected number of calls is 2 and E[T(a, b)] = Θ(p) = Θ(lg(b - a))

5.1-3
let A and B denote two independent runs of procedure BIASED-RANDOM
p((A, B) = (1, 1)) = p^2
p((A, B) = (0, 0)) = (1-p)^2
p((A, B) = (1, 0)) = p((A, B) = (0, 1)) = p(1-p)
(A, B) = (1, 0) and (A, B) = (0, 1) happen with equal probability
define RANDOM as:
    run (a, b) = (BIASED-RANDOM(), BIASED-RANDOM());
    if (a, b) == (1, 0):
        return 1;
    else if (a, b) == (0, 1):
        return 0;
    else:
        recursively call RANDOM itself
the number of calls to RANDOM follows geometric distribution with success probability 2p(1-p) = 2p - 2p^2
the expected number of calls is 1/(2p - 2p^2), E[T(n)] = Θ(1/(p - p^2))

5.2-1
one time:   the first candidate is hired and is the best among all the candidates
            p = 1/n
n times:    every candidate is hired, implying the candidates are presented in strictly increasing order
            p = 1/n!

5.2-2
the first candidate is hired anyway
assume the first candidate has rank r1, the second hired candidate has rank r2
r1 != 1, or the second hire cannot happen
the best candidate definitely will be hired, thereby r2 = 1
candidate with rank 1 is the first candidate with rank higher than r1 after the first among all r1 - 1 such candidates
p = 1 / (r1 - 1)
as r1 <- {2 .. n}, p = (1 / n-1) * Σ(1 / (r1 - 1))

5.2-3
E[ΣXi] = Σ(E[Xi]) = 3.5n

5.2-4
define Xi as
Xi  = 1 when ith customer get back own hat
    = 0 otherwise 
for any Xi, E[Xi] = P(ith customer get back own hat) = 1/n
expected number of customers who get back their own hat = E[Σ(Xi)] = n * 1/n = 1

5.2-5
when A[1 .. n] is a uniform random permutation, A[i] <- {1 .. n} for all i when viewed independently
for a pair i < j, A[i] <- {1 .. n}, A[j] <- {1 .. n} - A[i]
p(A[i] < A[j])  = (1 / n) * Σ(A[i] = {1 .. n})((A[i] - 1) / (n - 1))
                = 1/2 asymptotically
p(inversion) = p(A[i] > A[j]) = 1 - p(A[i] < A[j]) = 1/2
define Ii,j as
Ii,j    = 1 when A[i] > A[j]
        = 0 otherwise
then the expected number of inversions is E[ΣIi,j] = C(n, 2) * E[Ii,j] = n(n - 1) / 4

5.3-1
swap A[1] with A[RANDOM(1, n)] before the loop, start the loop with i = 2
the same proof still implies but now starting with 1-permutations

5.3-2
after the procedure, A[1] is guarenteed not to be in it's original place
at least n!/n = (n-1)! permutations are excluded, not only the identity

5.3-3
for an array of size 3, A = [a1, a2, a3]
let A' denote the array after calling PERMUTE-WITH-ALL(A)
every swap may have 3 possible consequences, the procedure overall may have 3^3 = 27 consequences
there are 3! = 6 different permutations of A, there's no way to divide 27 comsequences evenly among 6 permutations
there must be some permutations Ap of A that p(A' = Ap) != 1/6

5.3-4
p(A[i] ends up in B[j]) = p(offset = (i - j) mod n) = 1/n
this procedure can only produce cyclic shifting of an array
for an array of size n, there are only n different shiftings
other n! - n permutations may never be produced

5.3-5
let Ei denote the event ith element in the array is different to all the previous elements
P[E1 ∩ E2 ∩ E3 .. En]   = P[E1] * P[E2 | E1] * P[E3 | E1 ∩ E2] .. 
                        = 1 * (n^3 - 1) / n^3 * .. * (n^3 - n) / n^3
                        >= ((n^3 - n) / n^3)^n
                        = ((n^2 - 1) / n^2)^n
                        = (1 - 1/n^2)^n
thanks https://ita.skanev.com/05/03/05.html
                        >= 1 - 1/n

5.3-6
the algorithm should also produce a uniform permutation when all n priorities are identical
the sorting algorithm is not specified, it can be stable or instable
when it's stable it will not change the array in that case, when unstable the result is UB
therefore the procedure either have to shuffle the array in other meanings (using no sorting)
or have to regenerate the priorities until no priorities are identical

5.3-7
when m = 0, ∅ is the only set with no member, returning ∅ is thus returning a uniformly random 0-subset
assume RANDOM-SAMPLE(m-1, m-1) returns uniformly random (m-1)-subset S' of U = {1 .. n-1}
each with probability 1/C(n-1, m-1)
i = RANDOM(1, n), p(i ∈ S) = (m-1) / n
for a particular m-subset S of U = {1 .. n}, S = {s1, s2 .. sm}
when n ∉ S, RANDOM-SAMPLE(m, n) will only return S when
    S'  = S - si for some i, p = 1/C(n-1, m-1)
        there are C(m, m-1) = m such (m-1)-subsets
    si  = RANDOM(1, n), p = 1/n, as si != n, si is included in S
p(S)    = 1/C(n-1, m-1) * m/n
        = (m-1)!(n-m)! / (n-1)! * m/n
        = m!(n-m)! / n! = C(n, m)
when n ∈ S, S' may not contain n, as S' is a subset of {1 .. n-1}
the only way RANDOM-SAMPLE(m, n) may return S is 
    S'  = S - n, only one (m-1)-subset satisfies, p = 1/C(n-1, m-1)
    i   = RANDOM(1, n) ∈ S' or i = n, p = (m-1)/n + 1/n = m/n
p(S)    = 1/C(n-1, m-1) * m/n = C(n, m)
therefore RANDOM-SAMPLE(m, n) returns any m-subset of {1 .. n} with probability C(n, m)

5.4-1
let n = 365 denote the days in a year
p(there is at least one in all k people whose birthday is a specific day i)
= 1 - ((n-1)/n)^k >= 1/2
1/2 >= ((n-1)/n)^k
-1 >= k(lg(n-1) - lgn)
k >= 1 / (lgn - lg(n-1))
k >= 253

5.4-2
// wasted several hours here by misintepreting the problem as "until all bins contain at least 2 balls"
the same as the birthday problem

5.4-3
the exact analysis depends on mutual independence of variables
the analysis with indicator variables will still apply if the variables are just pairwise independent
    for any i and j, E[Xij] still is 1/n

5.4-4
assume their birthday are mutually independent, n = 365, m people in total
define Xijk as
    Xijk    = 1 when i, j, k have the same birthday
            = 0 otherwise
p(i, j, k have the same birthday)
= p(j, k have the same birthday as i)
= 1/n^2
E[X]    = ΣΣΣ(Xijk)
        = C(m, 3) * 1/n^2
        = m!/6(m-3)! * 1/n^2
        = m(m-1)(m-2)/6n^2 >= 1
m(m-1)(m-2) > (m-2)^3 >= 6n^2
m >= 95

5.4-5
it's the complement of birthday paradox
a k-string is a k-permutation only if it doesn't repeat any set element
p   = 1 * 1/(n-1) * 1/(n-2) * .. * 1/(n-k+1)
    = 1/(n!/(n-k)!)
    = (n-k)!/n!

5.4-6
define indicator Xi as
Xi  = 1 when bin i is empty after n tosses
    = 0 otherwise
E[Xi]   = p(bin i is empty after n tosses)
        = ((n-1)/n)^n
        = (n-1)^n / n^n
E[X]    = E[ΣXi]
        = ΣE[Xi]
        = n * (n-1)^n / n^n
        = (n-1)^n / n^(n-1)
define indicator Yi as
Yi  = 1 when bin i has 1 ball after n tosses
    = 0 otherwise
E[Yi]   = p(bin i has 1 ball after n tosses)
        = C(n, 1) * (1/n) * ((n-1)/n)^(n-1)
        = (n-1)^(n-1) / n^(n-1)
E[Y]    = E[ΣYi]
        = Σ(E[Yi])
        = n * (n-1)^(n-1) / n^(n-1)
        = (n-1)^(n-1) / n^(n-2)

5.4-7
divide all coin tosses to slices of length lgn - 2lglgn, all slices are mutually independent
p(a single slice is all head)   = 1/2^(lgn - 2lglgn)
                                = 1/(2^lgn/2^2lglgn)
                                = lg^2n / n
p(no slice is all head) = (1 - lg^2n / n)^(n / (lgn - 2lglgn))
                        <= e^(-lg^2n / n * (n/(lgn - 2lglgn)))
                        = e^(-lg^2n / (lgn - 2lglgn))
                        = n^(-lgn / (lgn - 2lglgn))
                        <= n^(-lgn / lgn)
                        = 1/n
"no slice is all head" is the union of two events:
    1. the longest head streak is no longer than lgn - 2lglgn - 1
    2. the longest head streak is longer than lgn - 2lglgn - 1, but is divided between two slices
therefore 
p(longest head streak is no longer than lgn - 2lglgn - 1) <= 1/n
inject this into the original prove,
E[L] = Ω(lgn - 2lglgn)

5-1
a.  let Xk be the number represented by i after k increments
    X0 = 0 = n0
    assume E[Xk] = n
    X(k+1)  = Xk + (ni+1 - ni), p = 1/(ni+1 - ni)
            = Xk, p = 1 - (1 / (ni+1 - ni))
    E[X(k+1)]   = 1/(ni+1 - ni) * (E[Xk] + (ni+1 - ni)) + (1 - 1/(ni+1 - ni)) * E[Xk]
                = E[Xk] + 1/(ni+1 - ni) * (ni+1 - ni)
                = n + 1
    therefore E[Xn] = n
b.  i now follows binomial distribution with success rate 1/100
    Var(i) = np(1-p) = 99n/10000
    Var(Xn) = Var(100i) = 100^2Var(i) = 99n

5-2
a.  ./CLRS/start/index.ts#randomSearch
b.  the number of trials X follows geometric distribution with success rate 1/n
    E[X] = 1/(1/n) = n
c.  the same with part b but with success rate k/n
    E[X] = 1/(k/n) = n/k
d.  the same as balls and bins 
    E[X] = n(ln(n) + O(1))
e.  i <- {1 .. n}, the number of comparsions X = i
    E[X] = E[i] = (1/n) * n(1+n)/2 = (1+n)/2
f.  thanks http://sites.math.rutgers.edu/~ajl213/CLRS/Ch5.pdf
    define indicator variables {Xi} such that
    Xi  = 1 when A[i] is compared to x in a search
        = 0 otherwise
    when A[i] = x
        p(Xi = 1)   = p(i is the minimum among all indices that A[i] = x)
                    = 1/k
    when A[i] != x
        p(Xi = 1)   = 1/k+1 // why? what's the distribution of the minimum of a k-subset of {1 .. n}?
    which gives E[ΣXi] = ΣE[Xi] = k * 1/k + (n-k)/k+1 = (n+1)/(k+1)
    in worst case, the k matches are the last k elements of the array
    the algorithm has to perform n-k+1 comparsions
g.  both average and worst case running time Θ(n)
h.  exactly the same as part e, f and g
i.  DETERMINISTIC-SEARCH
    shuffling an array will cost Θ(n) time, which is similar to deterministic linear searching
    it's not worth the effort to do the shuffling unless comparsion is expensive and inputs are malicious
    and the first algorithm is O(nlgn), worse than deterministic linear searching

Chapter 6
6.1-1
a complete binary tree of height h has nodes 2^(h+1) - 1
of height h-1 has nodes 2^h - 1
therefore a heap with height h will have nodes between 2^h and 2^(h+1) - 1

6.1-2
as 6.1-1 shows, 2^h <= n <= 2^(h+1) - 1, where h is the height of the heap
therefore 
    lg(2^h) <= lgn <= lg(2^(h+1) - 1)
    h <= [lgn] <= h, h = [lgn]

6.1-3
for a subtree of a heap, there is a simple path from root to any child node
for an arbitrary node n, PARENT^(i)(n) = root for some i
as A[PARENT(n)] >= A[n] for all n, A[n] <= A[PARENT(n)] <= .. <= A[root]
root is the largest value occuring anywhere in that subtree

6.1-4
anywhere at the bottom level
it can not be anywhere else than the bottom level, or it will have children, and A[PARENT(i)] >= A[i] for all i
as the problem assumed that all values are distinct, A[i] won't be the minimum

6.1-5
yes, for a sorted array, for any i <= j, A[i] <= A[j]
as PARENT(i) < i for all i except the root, A[PARENT(i)] <= A[i] for all i, A is a min-heap

6.1-6
23
17  14
6   13  10  1
5   7   2
no, 6 has two children 5 and 7, where 7 > 6

6.1-7
for i >= [n/2] + 1, the index of its left child will be greater than 2 * ([n/2] + 1) = 2 * [n/2] + 2
if n is even, 2 * [n/2] + 2 = n + 2
if n is odd, 2 * [n/2] + 2 = n + 1 
both cases the index of its left child will be greater than n, thereby A[i] must be a leaf

6.2-1
                27
            17      3
        16  13      10  1
       5 7 12  4   8  9   0
3 = A[3] swapped with 10 = A[6], then with 9 = A[13]

6.2-2
./CLRS/sort/heap.ts#heapify
the code is abstract, subclasses can instantiate the abstract cmp function to anything they want

6.2-3
after comparing A[i], A[LEFT(i)] and A[RIGHT(i)]
the procedure terminates without modifying the underlying array or recursively call itself

6.2-4
both LEFT(i) and RIGHT(i) will be out-of-bound, defined by <= A.heap-size
the procedure terminates without modifying the underlying array or recursively call itself

6.2-5
./CLRS/sort/heap.ts#heapify

6.3-1
swapped A[3] = 10 and A[7] = 22
swapped A[2] = 17 and A[5] = 19
swapped A[1] = 3 and A[4] = 84
swapped A[0] = 5 and A[1] = 84
swapped A[1] = 5 and A[3] = 22
swapped A[3] = 5 and A[7] = 10

6.3-2
starting from 1, the non-leaf nodes may not meet the condition of MAX-HEAPIFY
which requires the left and right subtrees of A[i] be a max heap

6.3-3
for level i, there can only be 2^(h - i - 1) nodes if the level is complete
2^h <= n <= 2^(h+1) - 1
[n / 2^(i+1)] >= 2^(h-i-1) 

6.4-1
swapped A[2] = 2 and A[6] = 20
swapped A[1] = 13 and A[3] = 25
swapped A[0] = 5 and A[1] = 25
swapped A[1] = 5 and A[3] = 13
swapped A[3] = 5 and A[7] = 8
swapped A[0] = 4 and A[2] = 20
swapped A[2] = 4 and A[5] = 17
swapped A[0] = 5 and A[2] = 17
swapped A[0] = 2 and A[1] = 13
swapped A[1] = 2 and A[3] = 8
swapped A[0] = 4 and A[1] = 8
swapped A[1] = 4 and A[4] = 7
swapped A[0] = 4 and A[1] = 7
swapped A[0] = 2 and A[2] = 5
swapped A[0] = 2 and A[1] = 4
[ 2, 4, 5, 7, 8, 13, 17, 20, 25 ]

6.4-2
initialization:
    at the start of the first iteration, i = A.length, A[1 .. i] = A is a max heap
    A[i+1 ..] is empty
maintenance:
    assume at the start of ith iteration, A[1 .. i] is a max heap, A[i+1 .. n] contains n-i largest elements sorted
    the root of the max heap is largest among A[1 .. i] and smaller than any element in A[i+1 .. n]
    thereby A[1] is the n-i-1th largest element
    swapping A[1] with A[i], now A[i .. n] contains n-i+1 largest elemented in sorted order
    decrement the size of the heap and call MAX-HEAPIFY(1), elements in A[1 .. i-1] is now a max heap
termination:
    after the end of the last iteration, i = 1, A[2 .. n] contains n-1 largest elementes in sorted order
    A[1] thus must be the smallest element in A[1 .. n], and A is in sorted order

6.4-3
assuming all elements are distinct
increasing order:
    since all leaves are greater than internal nodes, at least half of the elements in the array have to be moved
    which takes Ω(n), increasing order is asymptotically the worst case of BUILD-MAX-HEAP
    every iteration will swap an element at bottom level to the root of the heap
    MAX-HEAPIFY have to pass it all the way down to the bottom level, taking time Θ(h) = Θ(lgi)
    Θ(Σlgi) = Θ(lg(n!)) = Θ(nlgn)
decreasing order:
    array in decreasing order is already a max heap, BUILD-MAX-HEAP takes constant time
    every iteration will swap the minimum element to the root of the heap
    MAX-HEAPIFY have to pass it all the way down to the bottom level, taking time Θ(h) = Θ(lgn)
    Θ(Σlgi) = Θ(lg(n!)) = Θ(nlgn)
if all elements are identical, both BUILD-MAX-HEAP and MAX-HEAPIFY will take constant time
overall running time Θ(n)    

6.4-4
the same to decreasing order in 6.4-3

6.4-5
The Analysis of Heapsort
Schaffer R., Sedgewick R. 
Journal of Algorithms Volume 15, Issue 1, July 1993, Pages 76-100

6.5-1
swapped A[0] = 15 and A[11] = 1
swapped A[0] = 1 and A[1] = 13
swapped A[1] = 1 and A[4] = 12
swapped A[4] = 1 and A[9] = 6
Maximum key is 15

6.5-2
Inserting 10...
swapped A[12] = 10 and A[5] = 8
swapped A[5] = 10 and A[2] = 9

6.5-3
again the code is abstract, subclasses can override cmp to anything they like (as long as it's a total order)
./CLRS/sort/heap.ts#MinPriorityQueue

6.5-4
because it's bad code
the function HEAP-INCREASE-KEY is multi-purpose: it both increases the key and fixes the position of the key in heap
the fix part should be seperated from HEAP-INCREASE-KEY, then HEAP-INSERT no longer has to call it or set -∞
the guard condition is only necessary when A[i] may have children, in this case it hasn't
the heap should work with all types which has a total order over it
many of them do not have a global minimum like -∞

6.5-5
initialization:
    A[1 .. A.heap-size] is a max heap, key is inserted to A[A.heap-size + 1], i = A.heap-size + 1
    then A[i] is guarenteed to have no left or right child
    A[1 .. A.heap-size+1] is a max heap except A[i] may be larger than A[PARENT(i)]
    then A.heap-size is incremented before the first iteration
maintenance:
    assume A[1 .. A.heap-size] is a max heap except A[i] may be larger than A[PARENT(i)]
    if A[PARENT(i)] >= A[i], then A[1 .. A.heap-size] is a max heap, loop terminated
    or A[PARENT(i)] is swapped with A[i], A[PARENT(i)] > A[i]
    but now A[PARENT(i)] may be larger than A[PARENT(PARENT(i))], otherwise A[1 .. A.heap-size] is a max heap
    by setting i = PARENT(i)
    A[1 .. heap-size] is a max heap except A[i] may be larger than A[PARENT(i)] at the start of next iteration
termination:
    i = 1 after the last iteration
    i has no parent, A[1 .. A.heap-size] is a max heap

6.5-6
./CLRS/sort/heap.ts#fix

6.5-7
FIFO queue: based on a min priority queue, data keyed with a auto incrementing counter
./CLRS/sort/heap.ts#FIFOQueue
Stack (or FILO queue): the same as FIFO queue, but based on a max priority queue
./CLRS/sort/heap.ts#Stack

6.5-8
swap A[i] with A[A.heap-size], decrement A.heap-size
the value of A[A.heap-size] is uncertain: it can be either greater than A[PARENT(i)] or not
depending on the value of A[A.heap-size], it may need to go up or down the heap to restore max heap property
if it's greater than its parent, call INCREASE-KEY(i, 0); otherwise call MAX-HEAPIFY(i)

6.5-9
if used a min heap instead of a max heap, elements have to be removed from the head of the array when extracting
there's no way to do this efficiently in javascript
(can be done in Haskell by tail and in Rust by slicing)
./CLRS/sort/heap.ts#mergeArrays
build the max heap with k arrays: O(k)
extract and heapify the heap: O(lnk), O(nlnk) in total

6-1
a.  Original input:   [ 1, 2, 3, 4, 5 ]
    BUILD-MAX-HEAP:   [ 5, 4, 3, 1, 2 ]
    Repeated insert:  [ 5, 4, 2, 1, 3 ]
b.  when inputs are in increasing order
    every element inserted at the bottom level have to be swapped all the way to the root
    T(n) = Σ(hi) = Σ(Θ(lgi)) = Θ(lg(n!)) = Θ(nlgn)

6-2
a.  assume array index starts from 0
    the first element is the root, next d are the children of the root, next d^2 are their children, etc.
    let FIRST-CHILD(i) return the index of the first child of A[i]
    FIRST-CHILD(i+1) = FIRST-CHLID(i) + d
    FIRST-CHILD(0) = 1, FIRST-CHILD(i) = id + 1
    FIRST-CHILD(i) to FIRST-CHILD(i) + d - 1 (if in bound) will be the children of A[i]
    let PARENT(i) be the index of parent of i
    PARENT(i + d) = 1 + PARENT(i)
    PARENT(1) = PARENT(d) = 0, PARENT(i) = Math.floor((i - 1)/d)
b.  the complete d-ary tree of height h-1 has Σ(i = {0 .. h})(d^h) = (d^h - 1) / (d - 1) nodes
    the complete d-ary tree of height h has (d^(h+1) - 1) / (d - 1) nodes
    (d^h - 1) / (d - 1) + 1 <= n <= (d^(h+1) - 1) / (d - 1)
    d^h + d - 2  <= (d-1)n <= d^(h+1) - 1
    d^h <= (d-1)n - d + 2 <= d^(h+1) - d + 1
    Math.floor(log(d, (d-1)n - d + 2)) = h
    h = Θ(log(d, dn)) = Θ(lgn)
c.  has to find the maximum among d nodes at every level, Θ(lgn) levels in total
    T(n) = Θ(dlgn)
d.  the working part is function fix, which is the same to part e
e.  at most have to swap all nodes on a simple path from root to bottom
    the length of the path is no longer than the height of the heap, which is Θ(lgn)
    T(n) = Θ(lgn)

6-3
a.  [[2, 3, 4, 5],
     [8, 9, 12, 14],
     [16, -, -, -],
     [-, -, -, -]]
b.  for any i <= l and j <= k
    A[i, j] <= A[i, k] <= A[l, k]
    therefore if Y[1, 1] = ∞, for all i, j, Y[i, j] >= ∞, all entries are empty
    if Y[m, n] < ∞, for all i, j, Y[i, j] <= Y[m, n] < ∞, all entries are filled
c.  ./CLRS/sort/young-tableau#extractMin
    correctness can be proved similarly as min heap
    the smallest of Y[i, j], Y[i+1, j] and Y[i, j+1] is swapped to Y[i, j]
    if Y[i+1, j] is the minimum, tableau property of ith row now restored, reduced to a problem one row smaller 
    if Y[i, j+1] is the minimum, tableau property of jth column now restored, reduced to a problem one column smaller
    p = m + n is incremented every iteration and every iteration takes constant time
    the loop ends either i = m and j = n (out of bound) or eariler
        T(p) = T(p-1) + Θ(1), T(p) = O(p) by stright-forward recurrence tree analysis
    T(m + n) = O(m + n) 
d.  ./CLRS/sort/young-tableau#insert
    correctness can be proved similarly as insert in priority queue
    p = m + n is decremented every iteration and every iteration takes constant time
        T(p) = T(p-1) + Θ(1), T(p) = O(p)
    T(m + n) = O(m + n)
e.  ./CLRS/sort/index.ts#problem_6_3
    insert n^2 numbers into the tableau, then call EXTRACT-MIN n^2 times
    T(p) = T(2n) = O(n) for both INSERT and EXTRACT-MIN, Θ(n^2) operations in total
    T(p) = O(n) * Θ(n^2) = O(n^3)
    which is not optimal since
        1. it's not in-place
        2. heapsort can be done in O(n^2lg(n^2)) = O(n^2lgn) time
f.  ./CLRS/sort/young-tableau#find
    starting from Y[i, j] = Y[0, n - 1]
    if Y[i, j] > key, the whole column j contains numbers greater than key 
    the problem size is reduced by 1 column
    if Y[i, j] == key, [i, j] is the answer
    if Y[i, j] < key, all numbers Y[0, j] to Y[i, j] < key
    the problem size is reduced by 1 row
    as p = i + j is decremented every iteration, overall running time is O(m + n)

Chapter 7
7.1-1
Swapped A[0] = 9, A[2] = 13
[ 9, 19, 13, 5, 12, 8, 7, 4, 21, 2, 6, 11 ]
Swapped A[1] = 5, A[3] = 19
[ 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 ]
Swapped A[2] = 8, A[5] = 13
[ 9, 5, 8, 19, 12, 13, 7, 4, 21, 2, 6, 11 ]
Swapped A[3] = 7, A[6] = 19
[ 9, 5, 8, 7, 12, 13, 19, 4, 21, 2, 6, 11 ]
Swapped A[4] = 4, A[7] = 12
[ 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 ]
Swapped A[5] = 2, A[9] = 13
[ 9, 5, 8, 7, 4, 2, 19, 12, 21, 13, 6, 11 ]
Swapped A[6] = 6, A[10] = 19
[ 9, 5, 8, 7, 4, 2, 6, 12, 21, 13, 19, 11 ]
Swapped A[7] = 11, A[11] = 12
[ 9, 5, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 ]
Swapped A[0] = 5, A[1] = 9
[ 5, 9, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 ]
Swapped A[1] = 4, A[4] = 9
[ 5, 4, 8, 7, 9, 2, 6, 11, 21, 13, 19, 12 ]
Swapped A[2] = 2, A[5] = 8
[ 5, 4, 2, 7, 9, 8, 6, 11, 21, 13, 19, 12 ]
Swapped A[3] = 6, A[6] = 7
[ 5, 4, 2, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[0] = 2, A[2] = 5
[ 2, 4, 5, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[1] = 4, A[1] = 4
[ 2, 4, 5, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[2] = 5, A[2] = 5
[ 2, 4, 5, 6, 9, 8, 7, 11, 21, 13, 19, 12 ]
Swapped A[4] = 7, A[6] = 9
[ 2, 4, 5, 6, 7, 8, 9, 11, 21, 13, 19, 12 ]
Swapped A[5] = 8, A[5] = 8
[ 2, 4, 5, 6, 7, 8, 9, 11, 21, 13, 19, 12 ]
Swapped A[6] = 9, A[6] = 9
[ 2, 4, 5, 6, 7, 8, 9, 11, 21, 13, 19, 12 ]
Swapped A[8] = 12, A[11] = 21
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[9] = 13, A[9] = 13
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[10] = 19, A[10] = 19
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[11] = 21, A[11] = 21
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[9] = 13, A[9] = 13
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]
Swapped A[10] = 19, A[10] = 19
[ 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 19, 21 ]

7.1-2
every number in the array is smaller or equal to pivot
i = j = r - 1 after the loop
A[r] exchanged with A[i + 1] = A[r], return value is r
adding a specific check if all array elements are equal to the pivot, return Math.floor((p + r) / 2)
./CLRS/sort/quicksort#partition

7.1-3
the loop on lines 3-6 runs at most p - r + 1 times, bounded by j
each iteration takes constant time
other operations not in the loop takes constant times
T(n)    = Θ(p - r + 1) + Θ(1)
        = Θ(n)

7.1-4
change line 4 of PARTITION to
    if A[j] >= x
now the loop invariants are
    1.  if p <= k <= i, A[k] >= x
    2.  if i+1 <= k <= j-1, A[k] < x 
    3.  if k = r, A[k] = x

7.2-1
assume T(n) <= cn^2
T(n)    <= c(n-1)^2 + dn
        = cn^2 - 2cn + 1 + dn
        <= cn^2 by taking c > d/2
similarly, T(n) >= cn^2 
T(n) = Θ(n^2)

7.2-2
without the modification in problem 7.1-2, the problem is partitioned into two subproblems of size n-1 and 0
T(n) = T(n-1) + Θ(n) = Θ(n^2)
with the modification in problem 7.1-2, the problem is partitioned into two subproblem of size n/2
T(n) = 2T(n/2) + Θ(n) = Θ(nlgn)

7.2-3
when PARTITION is called with a sorted subarray A[p .. r], all elements in the slice is smaller or equal to r
return value will be r, the problem is partitioned into two sub-problems of size n-1 and 0
T(n) = Θ(n^2)

7.2-4
quiksort will take time Θ(n^2) for an already sorted array
insertion sort will take time Θ(n)

7.2-5
as α <= 1/2, 1 - α >= α
the shortest path from root to bottom is has length log(1/α, n) = lgn / lg(1/α) = -lgn/lgα
the longest path form root to bottom has length log(1/(1-α), n) = -lgn/lg(1-α)

7.2-6
thanks https://ita.skanev.com/07/02/06.html
as the input is uniformly random
the chance that the pivot falls within the αn smallest numbers or (1-α)n largest numbers is
    p = α + α = 2α
thereby with probability 1 - 2α, αn <= q <= (1-α)n, the partition is better than α to 1-α

7.3-1
the worst case of a randomized algorithm occurs rarely, also independent to the input

7.3-2
RANDOM is called once for each call to PARTITION, PARTITION is called for each call to QUICKSORT
for worst case, all partition are of size n - 1 and 0, PARTITION is called Θ(n) times
for best case, all partition are of size n/2
the number of calls to PARTITION is the same to the internal nodes in the recurrence tree
which is the number of nodes of a binary tree of height Θ(lgn), which is Θ(n)

7.4-1
assume T(n) >= cn^2
T(n)    >= max(cq^2 + c(n - q - 1)^2) + dn
        >= cn^2 - 2cn + 1 + dn
        >= cn^2 when c < d/2
T(n) = Ω(n^2)

7.4-2
T(n) = Θ(nlgn) => T(n) = Ω(nlgn)

7.4-3
the text already showed that the function is convex

7.4-4
E[X]    = Σ(i = {1 .. n-1})(Σ(k = {1 .. n-i})(2/(k+1)))
        = Σ(i = {1 .. n-1})(Σ(k = {1 .. n-i})(2/k) - 2)
        = Σ(i = {1 .. n-1})(Θ(lg(n-i)))
        = Θ(lg((n-1)!))
        = Ω(nlgn)

7.4-5
this time Xij = 1 only if i - j + 1 > k
E[X]    = Σ(i = {1 .. n-1})(Σ(j = {k+1 .. n-i})(2/(j+1)))
        <= Σ(i = {1 .. n-1})(Σ(k = {k+1 .. n})(2/j))
        = Σ(i = {1 .. n-1})(O(lgn) - O(lgk))
        = O(nlg(n/k))
the quicksort stops when the subarray has length <= k
thereby an element A[i] is guarenteed to be greater or equal to A[i - k]
when looking for a position to insert A[i], the insertion sort procedure will never have to search beyond A[i-k]
for all 0 <= i <= n-1, O(nk) in total
overall O(nk + nlg(n/k))

7.4-6
the median order of the three random variables falls within [αn, (1-α)n] only if at least two of them is in that range
it's a binomial distribution with success rate (1 - 2α)
P(X >= 2)   = C(3, 2)(1-2α)^2(2α) + C(3, 3)(1-2α)^3
            = 6α(1-2α)^2 + (1-2α)^3

7-1
a.  Swapped A[0] = 6, A[10] = 13
    [ 6, 19, 9, 5, 12, 8, 7, 4, 11, 2, 13, 21 ]
    Swapped A[1] = 2, A[9] = 19
    [ 6, 2, 9, 5, 12, 8, 7, 4, 11, 19, 13, 21 ]
    Return value:  8
b.  invariant: at the end of each iteration, p <= i, j <= r
    initialization:
        repeat clauses will be executed at least once, so i and j start with p and r
        as x = A[p], i will always be p after the first iteration
        if there's no element <= A[p] in the subarray, j will be p after the first iteration, !(i < j) and termination
        if there's some element <= A[p] in the subarray, the procedure swaps the element with A[p]
        denote this index as q
        after the first iteration:
            A[i] = A[p] <= x
            A[j] = A[q] = x
    maintenance:
        i and j is incremented at least once in each iteration
        the value they originally point to will not be changed
    termination:
        either j first reaches p or i first reaches q
        as A[p] <= x, j will stop at j = p
        i = p in the first iteration, thus i > p in this iteration, !(i < j) and the procedure is terminated 
        similarly, as A[q] = x, i will stop at i = q
        !(i < j) and termination
        therefore p <= i, j <= q <= r
c.  refine part 1 a little bit with assumption that r >= p + 1, subarray at least has length 2
    the procedure is only terminated at the first iteration when i = j = p, which means j < r
    otherwise j is decremented at least twice, j <= (r+1) - 2 = r - 1, j < r
d.  invariant:
        at the start of each iteration,
        A[p .. i-1] contains elements smaller or equal to the pivot
        A[j+1 .. r] contains elements greater or equal to the pivot
    initialization:
        A[p .. i-1] and A[j+1 .. r] are empty
    maintenance:
        after the two repeat clauses:
            A[p .. i-1] contains elements <= x
            A[j+1 .. r] contains elements >= x
            A[i] >= x
            A[j] <= x
        swap A[i] and A[j] then invariant maintained
    termination:
        !(i < j) => i >= j
        i = j only when A[i] >= x and A[i] <= x, A[i] = x
        every element in A[p .. i] = A[p .. j] <= x
        every element in A[j+1 .. r] >= x
    therefore every element in A[p .. i-1] <= x <= every element in A[j+1 .. r]
e.  ./CLRS/sort/quicksort.ts#hoareQuicksort

7-2
a.  any pivot will be equal to all array elements 
    the return of PARTITION will always be r, subarray partitioned to size n-1 and 0
    T(n) = Θ(n^2)
b.  ./CLRS/sort/quicksort.ts#partition2
    k incremented every iteration, starting from p, end condition is k > r - 1
    T(n) = Θ(p - r)
c.  ./CLRS/sort/quicksort.ts#quicksort2
d.  with QUICKSORT',
        if zi < x < zj, zi and zj will not be compared
        if zi or zj is first chosen as a pivot among {zi .. zj}, zi and zj will be compared
        if x = zi but not zi itself is first chosen as a pivot among {zi .. zj}
            zi will be in neither subproblems, zi will not be compared to zj
        simlarly if x = zj but not zj itself, zi will not be compared to zj
    the expected time analysis is still valid, E[X] = Θ(nlgn)

7-3
a.  pivot is chosen randomly from A[1] to A[n], 1/n for any particular element
    E[Xi] = 1/n
b.  if the qth smallest element is chosen as the pivot
    exactly q elements are smaller or equal
    the problem is partitioned into two subproblems of size q-1 and n-q
    T(n)    = ΣXq(T(q-1) + T(n-q) + Θ(n))
    as for a particular run, only one Xq can be 1, all others 0
    E[T(n)] = E[ΣXq(T(q-1) + T(n-q) + Θ(n))]
c.  the subproblems are independent to the value of Xq
    E[T(n)] = Σ(E[Xq](E[T(q-1)] + E[T(n-q)] + Θ(n)))
            = 1/n * Σ(E[T(q-1)] + E[T(n-q)] + Θ(n))
            = 1/n * (ΣE[T(q-1)] + ΣE[T(n-q)] + nΘ(n))
            = 1/n * (2ΣE[T(q)] + nΘ(n))
            = 2/n * ΣE[T(q)] + Θ(n)
d.  Σ(k = {2 .. n-1})(klgk)
    = Σ(k = {2 .. n/2 - 1})(klgk) + Σ(k = {n/2 .. n-1})(klgk)
    <= Σ(k = {1 .. n/2 - 1})(klg(n/2)) + Σ(k = {n/2 .. n-1})(klgn)
    <= (lgn - 1)(n/2)n/4 + lgn(3n/2 - 1)n/4
    = n^2lgn/8 - n^2/8 + 3n^2lgn/8 - Θ(nlgn)
    <= n^2lgn/2 - n^2/8
e.  assume E[T(n)] <= anlgn
    E[T(n)] <= 2/n * Σ(q = {2 .. n-1})(aqlgq) + dn
            <= 2/n * a(n^2lgn/2 - n^2/8) + dn
            = anlgn - an/4 + dn
            <= anlgn when a/4 > d
    E[T(n)] = O(nlgn)
    the best case is Ω(nlgn), so E[T(n)] = Ω(nlgn), E[T(n)] = Θ(nlgn)

7-4
a.  induction on problem size
    when p == r, the slice of one element is trivially sorted
    when p < r, after calling PARTITION(A, p, r),
        every element in A[p .. q-1] <= A[q]
        every element in A[q+1 .. r] > A[q]
    by induction assumption, TAIL-RECURSIVE-QUICKSORT(A, p, q-1) correctly sorts A[p, q-1]
    p set to q+1, equivalent to calling TAIL-RECURSIVE-QUICKSORT(A, q+1, r)
    by induction assumption, starting from the second iteration the procedure correctly sorts A[q+1, r]
    every element in A[p .. q-1] <= A[q] <= every element in A[q+1 .. r], therefore A[p .. r] is sorted
b.  the worst case of partition, subproblems of size n-1 and 0
    q = r - 1, TAIL-RECURSIVE-QUICKSORT(A, p, r) calls TAIL-RECURSIVE(A, p, r-1) recursively
    the stack keeps growing until p = r
    stack depth r - p + 1 = Θ(n)
c.  recursively solve the smaller subproblem first
    as the smaller of the subproblem has size <= n/2, stack depth <= lgn

7-5
a.  the three element chosen can be any 3-subset {Ap, Aq, Ar} of {A'1 .. A'n}
    for the median to be a particular i, 
        one of them should be Ai
        one of them should be smaller than Ai
        one of them should be greater than Ai
    pi = 1/n * (i-1)/n * (n-i)/n * A(3, 3) = 6(i-1)(n-i)/n^3
b.  with ordinary implementation, A'[(n+1)/2] is chosen with probability 1/n, the same with any other element
    with median-of-3 implementation, p = 6(n/2)(n/2)/n^3 = 3/2 * 1/n
    limit = 3/2
c.  for random pivot: p = Σ(i = {n/3 .. 2n/3})(1/n) = 1/3
    for median-of-3 pivot:
        p   = Σ(i = {n/3 .. 2n/3})(6(i-1)(n-i)/n^3)
            ≒ int(n/3 <= i <= 2n/3, 6(i-1)(n-i)/n^3)
            = 13/27 - 1/n
    as n -> ∞, p -> 13/27, better than 1/3 = 9/27
d.  as best possible split still gives Ω(nlgn) running time
    median-of-3 implementation can only be a constant factor better than the ordinary implementation on average

7-6
a.  the same idea as problem 7-2 can be used here 
    by comparing with a pivot interval [ap, bp], an array of interval [a, b] can be divided into three groups:
        1. strictly smaller than the pivot, b < ap
        2. strictly greater than the pivot, a > bp
        3. overlaps with the pivot, b >= ap && a <= bp
    the problem is the relation of overlapping is not transitive:
        [1, 2] overlaps with [2, 3]
        [2, 3] overlaps with [3, 4]
        [1, 2] doesn't overlap with [3, 4]
    thus group 3 may not overlap with each other
    the procedure must then maintain an interval that's the intersection of all overlapping intervals
    an interval must not be added to group 3 if it will make the intersection interval empty
    the intersection also must be computed beforehand, the left bound of the pivot must be constant during partition    
    ./CLRS/sort/quicksort.ts#fuzzysort
b.  the procedure is equivalent to quicksort in 7-2 on running time 
    which is Θ(nlgn) on average, Θ(n) when all elements are equivalent (i.e. contains the intersection)

Chapter 8
8.1-1
let array elements be vertices of a undirected graph
edge exists between vertices a and b iff a and b are compared
if the graph is not connected, i.e. array elements can be divided into two non-empty sets A1 and A2 that,
comparsions are only performed within these two sets but not between them
then any comparsion result between an elements from A1 and A2 are uncertain, the sort may not be accurate
thus for the comparsion sort algorithm to be correct, the graph must be connected
connected graph with n vertices has at least n-1 edges, which is the lower bound of comparsions
this lower bound is achievable
assume the input array is in sorted order, insertion sort only performs n-1 comparsions

8.1-2
Σ(k = {1 .. n})(lgk)    = lg(n!)
Σ(k = {1 .. n})(lgk)    <= Σ(k = {1 .. n})(lgn)
                        = nlgn = O(nlgn)
Σ(k = {1 .. n})(lgk)    = Σ(k = {1 .. n/2})(lgk) + Σ(k = {n/2 + 1 .. n})(lgk)
                        >= n/2 * lg(n/2)
                        = n/2 * (lgn - 1)
                        = Ω(nlgn)
lg(n!) = Θ(nlgn)

8.1-3
the unique simple path to half of n! leaves of a comparsion tree have length O(n)
a complete binary tree of height O(n) may only have 2^O(n) leaves
if it's possible, n!/2 = O(2^O(n)), n! = O(2^O(n)), which is not true
change 1/2 to any constant fraction and the argument still holds
n!/2^n = (1/2) * (2/2) * (3/2) ... >= (n/2)! = ω(2^O(n/2)) = ω(2^O(n))
it's not possible even for a 2^n fraction

8.1-4
each permutation of each subsequence is a possible sub-result, there are k! permutations for a length k subsequence
overall there are (k!)^(n/k) results, height of the comparsion tree at least 
    lg((k!)^(n/k))  = n/k * lg(k!)
                    = n / k * Ω(klgk)
                    = Ω(nlgk)
which is a lower bound of the worst case performance

8.2-1
[ 2, 4, 6, 8, 9, 9, 11 ]
[ null, null, null, null, null, 2, null, null, null, null, null ]
[ 2, 4, 5, 8, 9, 9, 11 ]
[ null, null, null, null, null, 2, null, 3, null, null, null ]
[ 2, 4, 5, 7, 9, 9, 11 ]
[ null, null, null, 1, null, 2, null, 3, null, null, null ]
[ 2, 3, 5, 7, 9, 9, 11 ]
[ null, null, null, 1, null, 2, null, 3, null, null, 6 ]
[ 2, 3, 5, 7, 9, 9, 10 ]
[ null, null, null, 1, null, 2, null, 3, 4, null, 6 ]
[ 2, 3, 5, 7, 8, 9, 10 ]
[ null, null, null, 1, null, 2, 3, 3, 4, null, 6 ]
[ 2, 3, 5, 6, 8, 9, 10 ]
[ null, null, 1, 1, null, 2, 3, 3, 4, null, 6 ]
[ 2, 2, 5, 6, 8, 9, 10 ]
[ null, 0, 1, 1, null, 2, 3, 3, 4, null, 6 ]
[ 1, 2, 5, 6, 8, 9, 10 ]
[ null, 0, 1, 1, 2, 2, 3, 3, 4, null, 6 ]
[ 1, 2, 4, 6, 8, 9, 10 ]
[ 0, 0, 1, 1, 2, 2, 3, 3, 4, null, 6 ]
[ 0, 2, 4, 6, 8, 9, 10 ]
[ 0, 0, 1, 1, 2, 2, 3, 3, 4, 6, 6 ]

8.2-2
assume Al = Ak, l < k
as j iters from A.length down to 1
Ak will first be placed into B at B[C[A[k]]]
C[A[j]] is then decremented by 1
as l < k, when j = l, C[A[l]] = C[A[k]] is smaller than when it was in iteration j = k
therefore if Al = Ak, l < k, they are placed at Bi, Bj that i < j
for n elements that are mutually equivalent
as the relative order of any two of them are preserved, the overall order must also be preserved

8.2-3
the argument in the text didn't specify the order of A[j]
reversing the order of A[j] and the argument still holds
but this time any two Al = Ak, l < k, are placed in Bi, Bj, i > j
the sorting algorithm is no longer stable

8.2-4
the array C can be used to solve this problem:
the number of elements falls into the range [a .. b]
can be computed as the number of elements equal or less than b - the number of elements equal or less than a - 1
which in turn is C[b] - C[a-1]
./CLRS/sort/linear-sort.ts#countingRange

8.3-1
[ 'SEA', 'TEA', 'MOB', 'TAB', 'DOG', 'RUG', 'DIG', 'BIG', 'BAR', 'EAR', 'TAR', 'COW', 'ROW', 'NOW', 'BOX', 'FOX' ]
[ 'TAB', 'BAR', 'EAR', 'TAR', 'SEA', 'TEA', 'DIG', 'BIG', 'MOB', 'DOG', 'COW', 'ROW', 'NOW', 'BOX', 'FOX', 'RUG' ]
[ 'BAR', 'BIG', 'BOX', 'COW', 'DIG', 'DOG', 'EAR', 'FOX', 'MOB', 'NOW', 'ROW', 'RUG', 'SEA', 'TAB', 'TAR', 'TEA' ]

8.3-2
insertion sort:
    stable, two element will be swapped only if the later is smaller than the former
merge sort:
    stable, in the merge procedure
    when element from left subarray is equal to the right one, the left one is always copied to A first
    within left or right subarray, elements are copied to A in the original order
heap sort:
    not stable
    e.g. [1a, 1b, 3]
        after BUILD-MAX-HEAP, will be [3, 1b, 1a]
        after first swap, will be [1a, 1b, 3]
        after second swap, will be [1b, 1a, 3]
quick sort:
    not stable     
    an element can be swapped to arbitrary place to its right if
        it is currently the first element greater than the pivot
        the element examined in the current iteration is smaller or equal to the pivot
    the original order of equivalent elements may not hold
zip the array with its index
comparsion now compares the index in the original array whenever two elements are equal
as array indices are distinct, the sort must be stable

8.3-3
invariant:
    after ith iteration, the array is sorted according to the i least significant digits of elements
inititialization:
    considering 0 digits (nothing), all elements are equal, trivially satisfied
maintenance:
    at the beginning of ith iteration, elements are sorted according to i-1 least significant digits 
    they are then sorted according to ith least significant digit
    at the end of ith iteration
        1.  they are sorted according to the ith least significant digit
        2.  for elements that have same ith least significant digit, the original order is preserved (stable)
            i.e. they are sorted according to i-1 least significant digits
    therefore the array is sorted according to i least significant digits
termination:
    i = d, the array is sorted

8.3-4
b = lg(n^3) = 3lgn >= lgn
choose r = lgn yields Θ(bn/lgn) = Θ(3nlgn/lgn) = Θ(3n) = O(n)

8.3-5
assume in n elements, Xi starts with digit i, Σ(i = {0 .. r-1})Xi = n, r is the radix
T(n, d) = Σ(T(Xi, d-1)) + Θ(n)
the height of the recurrence tree solely depends on d
in the worst case, n is evenly divided, the recurrence tree is a complete d-ary tree
number of sorting pass equals the number of interal nodes = r^d
a single call to the procedure creates r bins
when called recursively, the procedure keeps track of bins at most r * number of nodes in longest path = d

8.4-1
bucket 0:  []
bucket 1:  [ 0.13, 0.16 ]
bucket 2:  [ 0.2 ]
bucket 3:  [ 0.39 ]
bucket 4:  [ 0.42 ]
bucket 5:  [ 0.53 ]
bucket 6:  [ 0.64 ]
bucket 7:  [ 0.79, 0.71 ]
bucket 8:  [ 0.89 ]
bucket 9:  []
bucket 0 (sorted):  []
bucket 1 (sorted):  [ 0.13, 0.16 ]
bucket 2 (sorted):  [ 0.2 ]
bucket 3 (sorted):  [ 0.39 ]
bucket 4 (sorted):  [ 0.42 ]
bucket 5 (sorted):  [ 0.53 ]
bucket 6 (sorted):  [ 0.64 ]
bucket 7 (sorted):  [ 0.71, 0.79 ]
bucket 8 (sorted):  [ 0.89 ]
bucket 9 (sorted):  []
[ 0.13, 0.16, 0.2, 0.39, 0.42, 0.53, 0.64, 0.71, 0.79, 0.89 ]

8.4-2
when every element falls within the same bucket, insertion sort have to sort n elements in Θ(n^2) time
use better in-place algorithm than insertion sort when sorting the buckets

8.4-3
X ~ Bin(2, 1/2)
E[X]^2 = 1
E[X^2] = Var[X] + E[X]^2 = 3/2

8.4-4
the area is proportional to d^2
kth bucket should contain elements that 
    k/n <= di^2 < (k+1)/n
    k <= di^2 * n < k+1
    k = Math.floor(di^2 * n)

8.4-5
same as 8.4-4,
kth bucket should contains elements that
    k/n <= P(x) <= (k+1)/n

8-1
a.  each one of n! permutations of orders happens equally likely, with probaility 1/n!
    each one corresponds to a leaf in the comparsion tree as that's the only way to distinguish between them
    also an order of input can only corresponds to a single leaf
    if two leaves have the same value, as they start from the same root, they must have branched somewhere
    but for a fixed input, no comparsion can give different result on two runs
    the comparsion tree thus has n! leaves with 1/n! probability, other leaves unreachable
b.  assume LT has lk leaves, RT has rk leaves
    lk + rk = k, also when considered as the leaves of T their depth is one higher than in LT or RT
    D(T)    = D(LT) + lk + D(RT) + rk
            = D(LT) + D(RT) + k
c.  basically dynamic programming 
    the minial tree may have 0 <= i <= k leaves in its left subtree and k-i leaves in its right subtree
    if the left subtree didn't achieve d(i), swap one that does will reduce D(T)
    similarly the right subtree must have achieved d(k-i)
    d(k) = min(0 <= i <= k)(d(i) + d(k-i) + k)
    also when i = 0 or k, d(i) + d(k-1) + k = d(k) + k, which cannot be the minimal
    d(k) = min(1 <= i <= k-1)(d(i) + d(k-i) + k)
d.  d/di(ilgi + (k-1)lg(k-1))   = (lni - ln(k - x))/ln(2)
    has root i = k/2
    for 1 <= i < k/2, the first derivative is negative
    for k/2 < i <= k-1, the first derivative is positive
    therefore i = k/2 is a local maximum in range [1, k-1]
    assume d(k) >= cklgk
    d(k)    >= min{cilgi + c(k-i)lg(k-i) + k}
            = c(k/2)lg(k/2) + c(k/2)lg(k/2) + k
            = ck(lgk - 1) + k
            = cklgk - ck + k
            >= cklgk when c < 1
    d(k) = Ω(klgk)
e.  as d(k) is the minimum of D(k), D(k) = Ω(klgk), the average depth of leaves is D(k)/k = Ω(klgk) / k = Ω(lgk)
    D(T) = D(n!) = Ω(n!lgn!)
    average-case time is Ω(lgn!) = Ω(nlgn)
f.  as a randomization node didn't compare any two elements of the array, it gains no information of the order
    thus a randomization node didn't eliminate any possbility of the order of an array
    all children of a randomization node therefore must have the same set of leaves
    replace all randomization node with its child with minimal D(T)
    the result is still a valid comparsion tree which contains all possible n! permutations as leaves
    but the resulting tree will be deterministic
    as the child is chosen as having minimal D(T), the resulting tree has no worse average-case performance

8-2
a.  counting sort
b.  one pass of quicksort
c.  heap sort
d.  with counting sort, each bit has maximum 2, counting sort takes Θ(n + 2) = Θ(n)
    radix sort in turn takes Θ(bΘ(n)) = Θ(bn)
e.  ./CLRS/sort/linear-sort.ts#inplaceCountingSort
    not stable, whatever in C[A[i]] is swapped with A[i], it may not be the last element that equals to A[i]

8-3
a.  1.  divide numbers of different order of magnitudes into different sets
        the order of magnitude of a number k can be find in time d = log10(k) by repeatedly dividing 10
        the order of all numbers thereby can be computed in time Σd = n
    2.  radix sort each set of size ni, contains number with di digits
        each set takes time Θ(di * ni), Σdini = n, Θ(n) in total
    3.  concat the results in the correct order
        a number with more digits is always greater than a number with less digits
        as Σni <= n, this step takes O(n) time
    overall Θ(n)
b.  1.  first divide strings of different length into different list L[i]
        the length of all strings can be computed in time O(n)
    2.  initialize i as length of the longeset string, S an empty list, repeatedly:
            S = concat L[i] and S
            counting sort S with ith character (by definition of L, all strings in S now has i characters at least)
            decrement i
        when i > 0
        let ni = number of strings with at least i characters 
        each iteration takes O(ni) and Σni = n
    3.  on termination, S contains all strings of length at least 1 in sorted order
        return concatenation of all empty strings in L[0] with S
    overall Θ(n)
    at the start of each iteration, S contains strings sorted according to i+1th and later characters
    strings in L[i] has length i, can be treated as empty strings (i.e. least) considering i+1th and later characters
    thus prepending L[i] to S, the resulting list still sorted according to i+1th and later characters
    counting sort with ith character and the invariant carried to the next iteration
    ./CLRS/sort/linear-sort.ts#stringSort

8-4
a.  pick one red jug, test it with all blue jugs, repeat for all red jugs
    T(n) = n * n = Θ(n^2)
b.  for a fixed permutation of red jugs, any permutation of blue jugs may be the correct grouping
    n! possible results and the comparsion tree has height at least nlgn
    the worst case performance of the algorithm is Ω(nlgn) 
c.  by simultaneously quicksort red and blue jugs, repeatedly:
        1.  pick a random red jug r, test with all blue jugs
            divide blue jugs into groups that 
                R1: hold more water than r
                R2: hold less water than r
                R3: hold the same amount water as r (denote this jug as b)
        2.  test all red jugs with b, similarly divide red jugs into three groups B1, B2 and B3
        3.  recursively solve (R1, B1), (R2, B2)
    the set of red and blue jugs R and B can be treated as two equivalent set of numbers
    by picking r randomly, r works as a random pivot of B
    as b = a, R is partitioned in exactly the same way as B
    expected running time analysis of quicksort than applies

8-5
a.  just sorted normally
b.  1, 3, 2, 4, 5, 6, 7, 8, 9, 10 
c.  for any i,
        Σ(j = {i .. i+k-1})(A[j])/k <= Σ(j = {i+1 .. i+k})(A[j])/k
        A[i] <= A[i+k]  // other array elements cancelled each other
    conversely,
        A[i] <= A[i+k]
        //  add Σ(j = {i+1 .. i+k-1})(A[j]) to both sides
        Σ(j = {i .. i+k-1})(A[j]) <= Σ(j = {i+1 .. i+k})(A[j])
        Σ(j = {i .. i+k-1})(A[j])/k <= Σ(j = {i+1 .. i+k})(A[j])/k
d.  divides the array into k subarrays, each of length n/k
    sort each in time O(n/klg(n/k)), O(nlg(n/k)) in total
    merge the sorted subarrays to form A' that
        A'[i], A'[i+k], A'[i+2k] .. are from ith sorted subarray
    overall running time O(nlg(n/k))
e.  for 0 <= i <= k-1,
        A[i] <= A[i+k] <= A[i+2k] ..
    hence a k-sorted array can be divided into k sorted arrays
    by 6.5-9, k sorted arrays of length n/k can be merged into a single sorted array in time O(nlgk)
f.  when k is a constant, O(nlgk) = O(n)
    if k-sorting an array of size n can be done in time o(nlgn)
    o(nlgn) + O(n) = o(nlgn), the array can be completely sorted in time o(nlgn)
    in contradiction to Ω(nlgn) lower bound
    therefore k-sorting an array of size n must take Ω(nlgn)

8-6
a.  there are C(2n, n) ways to choose n elements from 2n ones
    C(2n, n) = (2n)! / (n!)^2
    the other n elements are automatically what not chosen
b.  all C(2n, n) leaves must be reachable from the root of the decision tree
    height of the tree at least lg(C(2n, n))
    by Stirling's approximation, 
        (2n)! / (n!)^2  = (4πn)^(1/2) * (2n/e)^2n * (1 + d/2n) / (2πn * (n/e)^2n * (1 + d/n)^2)
                        = (2^2n / (πn)^(1/2)) * (1 + d/2n) / (1 + d/n)^2
    where
        (1 + d/2n) / (1 + d/n)^2    = (1 + d/n - d/2n) / (1 + d/n)^2
                                    = 1/(1 + d/n) + (d/2n) / (1 + d/n)^2
                                    = 1 + O(1/n) as n -> ∞
        (2n)! / (n!)^2  = (2^2n)(1 + O(1/n)) / (πn)^(1/2)
    lg(C(2n, n))    = 2n + lg(1 + O(1/n)) - lg(πn)/2
                    = 2n - o(n) as n -> ∞
c.  if two elements a and b should be placed consecutively in sorted order, a <= b
    if a and b are not compared
        any element smaller than or equal to a will be smaller than or equal to b
        any element greatre than a will be greater than b
    no matter how much comparsions are made, a and b cannot be distinguished
    for the result to be correctly sorted, a and b must be compared
d.  there are 2n-1 consecutive pairs in an array of size 2n
    thus 2n-1 comparsions at least

8-7
-

Chapter 9
9.1-1
thanks https://ita.skanev.com/09/01/01.html
recursively:
    1.  divide the array into two
    2.  recursively find minimum of both subarrays, each with a list of elements compared to the minimum associated 
    3.  find minimum of the array as smaller of the two
    4.  append the greater to the list associated to the smaller
when the procedure terminates, it returns a list of elements been comapared to the minimum so far
number of comparsions performed equals to the number of internal nodes, which is n - 1
the list has size equal to the height of the recurrence tree, which is lgn
invariant:
    at each level, all elements in the subarray except the minimum is greater of equal to an element in the list
initialization:
    array size is 1, consists of a single element, which is the minimum, trivially satistied
maintenance:
    assume l, r are minimum of subarrays
    L, R are lists associated to them
    without loss of generality, if l <= r, L' = L ∪ {r} is the new list
    all elements in the left subarray except l >= one element in L
    all elements in the right subarray are >= r, thus >= one element in L'
termination:
    -
therefore the second minimum must be in the list associated to the minimum value
the minimum of them can be find with lgn - 1 comparsions, n - 1 + lgn - 1 = n + lgn - 2 in total

9.1-2
initially any element can be the maximum or the minimum
the number of potential maximum and minimum both equal to n, denote them as MIN and MAX
assume all elements are distinct, elements can be divided into 4 groups:
    1.  may be either maximum or minimum
    2.  may be maximum, may not be minimum
    3.  may be minimum, may not be maximum
    4.  may be neither maximum or minimum
comparing two elements a and b from group 1 will:
    if a < b, a now in group 3, b now in group 2, both MIN and MAX decremented by 1
comparsions between any combinations of elements from 2, 3 or 4, depends on the result, may be in vain
comparsions between a from group 1 and b from group 2, 3 or 4, depends on the result, may only decrement MIN+MAX by 1
thereby in worst case, it's optimal to perform comparsions between elements in group 1 first until group 1 is empty
such comparsions can only be performed n/2 times, eliminate n/2 potential maximum and minimum independent of the input
then the input array is divided into two disjoint groups, n/2 potential maximum and n/2 potential minimum
in worst case, maximum and minimum can be find among them in (n/2 - 1) * 2 = n - 2 comparsions
n/2 + n - 2 = 3n/2 - 2 in total

9.2-1
assume initially p < r, 1 <= i <= r - p + 1
if the left subproblem is empty, q = p, k = p - p + 1 = 1
i cannot be less than 1, so the procedure will not recursively solve the left subproblem
if the right subproblem is empty, q = r, k = r - p + 1
i cannot be greater than r - p + 1, so the procedure will not recursively solve the right subproblem

9.2-2
T(max(k-1, n-k)) is the same whether Xk = 0 or Xk = 1

9.2-3
./CLRS/sort/order.ts#randomizedSelect2

9.2-4
when pivot is always the maximum value

9.3-1
when divided into groups of 7, 
number of elements greater than x is at least
    4(1/2 * n/7 - 2) >= 2n/7 - 8
T(n)    <= c[n/7] + c(5n/7 + 8) + dn
        = 6cn/7 + 8c + dn
        = cn + (8c + dn - cn/7)    
        <= cn for c > 7d and large enough n
when divided into groups of 3,
number of elements greater than x is at least
    2(1/2 * n/3 - 2) >= n/3 - 4
T(n)    = T(n/3) + T(2n/3 + 4) + O(n)
        >= T(n/3) + T(2n/3) + O(n)
        = Θ(nlgn), no longer linear

9.3-2
3n/10 - 6 >= n/4
6n - 120 >= 5n
n >= 120

9.3-3
the lower median can be find in time O(n)
by finding the index of the lower median in O(n)
the array can be partitioned into subproblems of equal size in O(n)
a good split is then guarenteed, worst case running time thus O(nlgn)

9.3-4
assume all elements are distinct
let array elements be vertices of a graph, an edge a -> b exists iff some comparsion gives a > b
for vertices a and b
    if there is a path a -> b, a > b
    if there is a path b -> a, b > a
    otherwise it's uncertain whether a > b or b > a
also the graph is acyclic, otherwise for some element a on the cycle, a > a
for a procedure to correctly find the ith smallest element x, the comparsions it makes must ensure that:
    i-1 elements are smaller than x
    n-i elements are greater than x
therefore by additionally update the graph according to every comparsion
the set of i-1 elements smaller than x can be found by dfs from the vertex x
the set of i-1 elements greater than x can be found by dfs the reverse graph from the vertex x

9.3-5
by simulating binary search
let mid be the order of lower median, to find the ith smallest element
    if i = mid, one run of black-box can solve the problem
    if i > mid:
        find the median in O(n)
        filter elements smaller than the median from the array, O(n)
        find (i - mid)th smallest element in the resulting array recursively
    if i < mid:
        similarly filter the array
        find ith smallest element in the resulting array recursively
T(n) = T(n/2) + O(n) = O(n)

9.3-6
using select, an array can be partitioned around the median in time O(n)
therefore in time O(nlgk), the array can be divided into n/k chunks that
    1.  each chunk is of length k
    2.  elements in each chunk is greater than elements in previous chunks, smaller than elements in succeeding ones
then the maximum of each chunk forms the kth quantiles, which can be found in time O(n)
O(nlgk) + O(n) = O(nlgk) in total

9.3-7
find S in O(n)
substract S from every element, take abstract value in O(n)
find kth smallest element x of these abstract value in O(n)
filter the original input array for elements that has distance to S smaller or equal to x in O(n)

9.3-8
thanks https://ita.skanev.com/09/03/08.html
recursively:
    1.  find the medians ma, mb of the two array A, B
    2.  if the two medians equal, return either one
    3.  if ma > mb
            more than half of the elements in both arrays are greater than mb
            more than half of the elements in both arrays are smaller than ma
            the median of both arrays must be between ma and mb
            by dropping same amount of elements > ma and < mb, the overall median will not change
        drop elements > ma in A, elements < mb in B, get a subproblem of half size

9.3-9
denote the y coordinate of the main pipeline as y0, then
    y0 = min{Σ(|yi - y|)}
where (xi, yi) is the coordiante of ith well
when y is greater than the y coordinate of k wells and smaller than n-k,
    increasing y by an amount of ε will increase cost by εk, while decrease cost by ε(n-k) at the same time
therefore before k = n/2, increasing y will improve the solution
starting from = n/2 + 1, increasing y will increase the total cost
the problem is equivalent to finding the median of all y coordinates
the optimal solution is either the median, or any value between lower and higher medians

9-1
a.  O(nlgn)
b.  O(n) + O(nlgi)
c.  O(n) + O(ilgi)

9-2
a.  the median satisfies that, exactly [n/2] - 1 numbers are smaller than it
    each one has weight 1/n
        Σ(i = {1 .. [n/2] - 1})(1/n)
        = ([n/2] - 1)(1/n) < 1/2
        Σ(i = {[n/2] + 1 .. n})(1/n)
        = ([n/2] - 1)(1/n) < 1/2
b.  sort the elements
    find the greatest k that Σwi < 1/2 in sorted order
    then
        Σ(i = {1 .. k})(wi) < 1/2
        Σ{i = {1 .. k+1}}(wi) >= 1/2
        Σ(i = {k+2 .. n})(wi) >= 1/2
    and xk+1 is the median
c.  by modifying SELECT
    still partition the array around median of medians
    then compute the weight sum of elements smaller and greater than the pivot wl and wg
    if wl < 1/2 and wg <= 1/2, pivot is the weighted median
    if wl >= 1/2, the weighted median is among elements smaller than the pivot
        recursively find the weighted median in the subproblem
    if wg > 1/2, the weighted median is among elements greater than the pivot
        recursively find the element that's greater than elements sum to (1/2 - wl) in the subproblem
    running time O(n) as in text
d.  the same argument as 9.3-9
    increasing p a small amount ε will increase the sum by Σ(pi < p)(wiε) and decrease the sum by Σ(pi > p)(wiε)
    thus when Σ(pi > p)(wi) > 1/2, increasing p will reduce the total sum of weights
    when Σ(pi > p) < 1/2, inceasing p will increase the total sum of weights
    the minimum therefore is at greatest p where Σ(pi > p) >= 1/2
d.  min(p){Σd(a, b)} = min(x, y){Σ|xi - x| + Σ|yi - y|} = min(x){Σ|xi - x|} + min(y){Σ|yi - y|}
    when the x or y coordiate of the solution is changed, the other half of the sum won't change
    by finding the weighted median of x and y seperately, both part are minimized

9-3
a.  ?
b.  assuming Ui(n) <= n + cT(2i)lg(n/i)
    Ui(n)   <= n/2 + (n/2 + cT(2i)lg(n/2i)) + T(2i)
            = n + cT(2i)(lg(n/i) - 1) + T(2i)
            = n + cT(2i)lg(n/i) - cT(2i) + T(2i)
            <= n + cT(2i)lg(n/i) when c >= 1
c.  when i is constant,
    Ui(n)   = n + O(T(2i)lg(n/i))
            = n + O(lgn)
d.  when when i = n/k,
    Ui(n)   = n + O(T(2i)lg(n/i))
            = n + O(T(2n/k)lg(n/(n/k)))
            = n + O(T(2n/k)lgk)

9-4
a.  same as quicksort, zi and zj will be compared only if either zi or zj is first chosen as the pivot
    but this time, if i < j < k and an element between zj and zk is first chosen as pivot,
    the procedure will not recurse into the subproblem containing zi and zj
    Xijk = 1 only when zi or zj are the first pivot among {zi .. zk}
    similarly when k < i < j and an element between zk and zi is first chosen as pivot, zi and zj will not be compared
    Xijk = 1 only when zi or zj are the first pivot among {zk .. zj}
    namely
        E[Xijk] = 2/(k - i + 1) if i < j < k
                = 2/(j - k + 1) if k < i < j
                = 2/(j - i + 1) if i <= k <= j, i < j
                = 0 if i = j
b.  E[Xk]   = E[Σ(i = {1 .. n})(Σ(j = {i .. n})(Xijk))]
            = E[Σ(i = {1 .. k})(Σ(j = {k .. n})(Xijk))]
            + E[Σ(i = {k+1 .. n})(Σ(j = {i .. n})(Xijk))]
            + E[Σ(i = {1 .. k})(Σ(j = {i .. k-1})(Xijk))]
            = Σ(i = {1 .. k})(Σ(j = {k .. n})(2/(j - i + 1)))
            + Σ(i = {k+1 .. n})(Σ(j = {i .. n})(2/(j - k + 1)))
            + Σ(i = {1 .. k-2})(Σ(j = {i .. k-1})(2/(k - i + 1)))
            = 2Σ(i = {1 .. k})(Σ(j = {k .. n})(1/(j - i + 1)))
            + 2Σ(j = {k+1 .. n})(Σ(i = {k+1 .. j})(1/(j - k + 1)))
            + 2Σ(i = {1 .. k-2}(k-i-1)/(k-i+1))
            <= 2Σ(i = {1 .. k})(Σ(j = {k .. n})(1/(j - i + 1)))
            + 2Σ(j = {k+1 .. n})(j - k - 1)/(j - k + 1))
            + 2Σ(i = {1 .. k-2}(k-i-1)/(k-i+1))
c.  (j - k - 1)/(j - k + 1) < 1
    (k - i - 1)/(k - i + 1) < 1
    2Σ(j = {k+1 .. n})(j - k - 1)/(j - k + 1)) + 2Σ(i = {1 .. k-2}(k-i-1)/(k-i+1))
    <= Σ(i = {1 .. n}1) = n
    2Σ(i = {1 .. k})(Σ(j = {k .. n})(1/(j - i + 1)))
    = 1/k + 1/(k+1) + .. + 1/n
    + 1/(k-1) + 1/k + .. + 1/(n-1)
    + ...
    + 1/1 + 1/2 + .. + 1/(n-k+1)
    starting from 1/1, the term 1/i appears at most on i lines
    therefore the sum <= Σi/i = n
    E[Xk] <= 2 * 2n = 4n
d.  as E[Xk] <= 4n = O(n)

Chapter 10 
10.1-1
[ 4 ]
[ 4, 1 ]
[ 4, 1, 3 ]
Pop:  3
[ 4, 1 ]
[ 4, 1, 8 ]
Pop:  8
[ 4, 1 ]

10.1-2
two stacks, one starts from index 0 and grows upward, one starts from index n - 1 and grows downward

10.1-3
head: 0, tail: 1
[ 4, <5 empty items> ]
head: 0, tail: 2
[ 4, 1, <4 empty items> ]
head: 0, tail: 3
[ 4, 1, 3, <3 empty items> ]
Dequeue:  4
head: 1, tail: 3
[ 4, 1, 3, <3 empty items> ]
head: 1, tail: 4
[ 4, 1, 3, 8, <2 empty items> ]
Dequeue:  1
head: 2, tail: 4
[ 4, 1, 3, 8, <2 empty items> ]

10.1-4
./CLRS/collection/queue.ts#Queue
by maintaining the size of the queue explicitly

10.1-5
./CLRS/collection/queue.ts#Deque

10.1-6
two stacks A and B
enqueue push elements into A
dequeue pops elements from B
when B is empty, pop all elements from A and push into B in turn
enqueue: constant time
dequeue: amortized constant time

10.1-7
keep track of the number of elements in the queue A
push enqueues an element into A
when currently there are n elements in the queue, repeat n - 1 times:
    dequeue an element
    enqueue it immediately
then the next element dequeued is the last element pushed 
one queue is sufficient 
push: constant time
pop: linear time

10.2-1
insert still O(1), inserted before the head of the list
delete is now O(n), the previous node of can no longer be find in O(1), must traverse the list

10.2-2
PUSH:   insert a new node at the head of the list
POP:    delete the head, make head.next the new head, return the value contained by the head
        if head = NIL, throw underflow

10.2-3
additionally maintain the tail of the list
ENQUEUE:    append a new node to the tail of the list, update the tail to the new node
DEQUEUE:    delete the head, make head.next the new head, return the value contained by the head
            if head = NIL, throw underflow

10.2-4
set L.nil.key = k before the loop
when the loop terminates, either x = L.nil or another node x.key = k is found

10.2-5
./CLRS/collection/slist.ts#SList
INSERT: O(1)
DELETE: O(n)
SEARCH: O(n)

10.2-6
by explicitly maintaining the tail of the list as a field
./CLRS/collection/dlist.ts#concat

10.2-7
traverse the list, repeatedly delete and insert nodes
./CLRS/collection/slist.ts#reverse

10.2-8
assume the pointer to NIL is NULL = 0x0
this.head.np = this.head.next ^ NULL = this.head.next
for any two conservative nodes x and y,
    x.np = x.prev ^ x.next = x.prev ^ &y
    y.np = y.prev ^ y.next = &x ^ y.next
starting from any two concervative nodes, the dlist can be traversed in any direction
hence search can be performed in a similar manner
insert updates this.head.np to this.head.np ^ &new_head
delete cannot be performed in constant time, must traverse the list to get two conservative nodes
by swapping the value of head and tail, the list is reversed in constant time

10.3-1
double list
        1   2   3   4   5   6
next    2   3   4   5   6   -
key     13  4   8   19  5   11
prev    -   1   2   3   4   5
single list
        1   2   3   4   5   6
next    2   3   4   5   6   -
key     13  4   8   19  5   11

10.3-2
the same procedure
x.next now points to the word at x+1

10.3-3
the procedure never need to traverse the free list, only insert and delete from head
insertion only have to set the next of the new head
deletion only have to follow the next of the current head

10.3-4
will have to copy and move nodes as long as a node can be deleted from the middle of the list

10.3-5
assume a move operation that moves the content of node x to y
overwrites array entry y, while maintains the correctness of prev and next pointers
traverse the list L, for ith list node x
    inspect node in ith entry of the array
    if ith entry is free, move x to ith entry, free x
    if ith entry is another list node, allocate a new object y, move i to y, move x to i, free x
after n iterations, ith list node is stored at ith entry of the list

10.4-1
-

10.4-2
./CLRS/collection/tree.ts#printTree

10.4-3
./CLRS/collection/tree.ts#printTreeStack

10.4-4
./CLRS/collection/tree.ts#printSiblingTree

10.4-5
./CLRS/collection/tree.ts#printTreeConstant

10.4-6
use a boolean is_last_child to indicate whether this child is the last one of its parent
the first pointer is still the left-child
the second pointer, when is_last_child == false, is still the right-sibling
when is_last_child == true, the second pointer points to the parent

10-1
search      O(n)    O(n)    O(n)    O(n)
    search is always linear
    there's no way in linked list to reach a particular node without traversing all nodes before it
insert      O(1)    O(n)    O(1)    O(n)
    insert into unsorted lists can be done in constant time by inserting to the head
    while insertion into sorted lists have to maintain the order
delete      O(n)    O(1)    O(n)    O(1)
    deletion always has to modify the previous node
    in singly linked list, there's no efficient way to find the previous node, have to traverse from the head
successor   O(n)    O(1)    O(n)    O(1)
    in unsorted lists, successor have to search the whole list
    in sorted lists, successor only have to call next on the input node
predecessor O(n)    O(n)    O(n)    O(1)
    in unsorted lists, predecessor have to search the whole list
    in sorted lists, successor only have to call prev on the input node
    but singly linked list have no pointer points to the previous node, have to traverse the whole list
minimum     O(n)    O(1)    O(n)    O(1)
    have to search the whole lists when unsorted
    in sorted list minimum is just the head
maximum     O(n)    O(1)    O(n)    O(1)
    assumes the tail of the list is explicitly maintained
    otherwise if the lists are circular, O(1) for doubly linked sorted list, O(n) for singly linked
    both O(n) if the lists are not circular

10-2
a.  insert:         have to find the right position of the key, O(n)
    minimum:        at the head, O(1)
    extract-min:    delete and return the head, O(1)
    union:          merging two sorted list, O(n)
b.  insert:         insert at the head, O(1)
    minimum:        by searching the whole list, O(n)
    extract-min:    by searching the whole list then delete, O(n)
    union:          must dedup, O(n^2) by naive double loop
                    can be improved to O(nlgn) by first sort both lists then merge
c.  same to b except
    union:          dedup not necessary, simple concatenation will suffice, O(1)

10-3
a.  both algorithm is correct, returning the first node i in the list that key[i] = k or NIL if no such node exists
    if the while loop of COMPACT-LIST-SEARCH takes t iterations
    then none of the first t-1 random j can have key[j] == k
    otherwise by the loop condition, key[i] < k, i will be updated to j, the procedure will return at line 7
    therefore the for loop of COMPACT-LIST-SEARCH' will run for at least t iterations
b.  the for loop runs for t iterations, each O(1)
    after t iterations of the for loop, the distance from i to key k will be Xt
    the while loop sets i = next[i], thus will run for Xt iterations
    T(n) = O(n + Xt)
    E[T(n)] = O(n + E[Xt])
c.  for a particular r, there are
        1.  nodes with distance to the key greater or equal than r
        2.  nodes with distance to the key smaller than r
        3.  nodes after the key
    there are exactly r nodes in group 2
    if no random node j falls within group 2, Xt >= r
    E[Xt]   <= ΣP[Xt >= r]
            = Σ(1 - r/n)^t
d.  bounding by integral
    Σ(r = {0 .. n-1})(r^t)  <= int(0 <= r <= n, r^t)
                            = n^(t+1)/(t+1) - 0^(t+1)/(t+1)
                            <= n^(t+1)/(t+1)
e.  E[Xt]   <= Σ(1 - r/n)^t
            = Σ((n-r)/n)^t
            = Σ(r = {1 .. n})((n-r)^t)/n^t
            = Σ(r = {0 .. n-1})(r^t)/n^t
            <= n^(t+1)/(t+1)n^t
            = n/(t+1)
f.  combine b. and e.
g.  ?
h.  otherwise even node j is further down the list than i, i will not be updated to j as key[i] = key[j]
    when all but the last node contains the same key k
    searching any key smaller or equal to will be O(1)
    searching any key greater than k will be O(n) with or without random skip

Chapter 11
11.1-1
examine every slot, compare with the current maximum and update if not empty 
O(m), m the number of slots in the array

11.1-2
make a bijection between elements and a finite number set {0 .. n - 1} of size n
A[i] = 1 if i ∈ S, A[i] = 0 otherwise
INSERT(i) sets A[i] to 1, DELETE(i) set A[i] = 0
SEARCH(i) returns A[i] == 1

11.1-3
each slot stores a pointer to a doubly linked list
SEARCH(k) inspects whether the linked list at slot k is empty
INSERT(k) inserts at the head of the list
DELETE(x) removes the nodes from the list

11.1-4
thanks Instructor's Manual
by initializing two huge arrays which validate each other
array A stores index of array S, array S stores index of array A
additionally array S has a field S.top
if array A contains key k, A[k] < S.top && S[A[k]] == k, both insert and delete maintains this property
if array A does not contain key k, A[k] is not initialized thus can be any number
if A[k] >= S.top, A[k] is trivially invalid
if A[k] < S.top, it must be set by INSERT(k') with a different k', so S[A[k]] !== k

11.2-1
define indicator variable Xij as
    Xij = 1 when h(ki) = h(kj)
        = 0 otherwise
by simple uniform hashing, Pr{Xij = 1} = 1/m
the expected total number of collisions is
    E[Σ(i = {1 .. n})(Σ(j = {i+1 .. n})(Xij))]
    = C(n, 2)(1/m)
    = n(n-1)/m

11.2-2
A = [5, 28, 19, 15, 20, 33, 12, 17, 10]
B = A.map(n => n % 9);
B = [5, 1, 1, 6, 2, 6, 3, 8, 1]
three collisions in slot 1
two collisions in slot 6

11.2-3
deletion still O(1)
insertion now has to insert the key to the correct position, equivalent to unsuccessful search in average performance
unsuccessful search now stops when the node examined has a key larger than the search key
assume all inserted elements are drawn from a uniform distribution
about half of the α keys that collide with the search key will be smaller or greater than it
expected running time O(1 + α/2), which is O(1 + α)
performance of successful search now depends not on the keys inserted after k, but the keys smaller than k
    Pr{ Xij = 1 } = Pr{ h(ki) = h(kj) ∧ hi > hj } = 1/2m
expected number of nodes examined when searching ki is then
    E[1 + Σ(j = {1 .. n} - i)(Xij)]
    = 1 + (n-1)/2m
on average over all i:
    1/n * n(1 + (n-1)/2m)
    = 1 + (n-1)/2m
    = 1 + α/2 - 1/2m
    = Θ(1 + α)
so search and delete are asymptotically the same to unsorted lists
insertion performance is reduced from O(1) to (1 + α)

11.2-4
set a slot in the hash table not in the range of the hash function
that is, for a h_free, no k will give h(k) = h_free
can add 1 to any existing hash function, so the range changes from {1 .. m} to {2 .. m+1}
slot 1 then is the free slot now
search does not involve allocation and deallocation
insert operation now has to acquire a free node from the free list
    simply delete the head from list in slot h_free, O(1)
delete operation now has to return the node to the free node
    simply prepend the node to list in slot h_free, O(1)
a simply linked list will suffice

11.2-5
pigeonhole principle
if all slots contains list of length smaller than n
the total number of keys stored in the hash table is smaller than nm = |U|
thereby at least one slot contains a list of length greater or equal to n

11.2-6
thanks https://stackoverflow.com/questions/8629447/
repeatedly:
    1.  select a slot <- {1 .. m}
    2.  select a number i <- {1 .. L}
        if i <= length of the list in this slot, take the key in ith node
        otherwise go back to 1.
the procedure succeeds when i <= length of the list
as the expected length of list is α, success rate is α/L
expected number of trials is then L/α by geometric distribution
then ith node is retrieved in time O(L), O(L + L/α) = O(L(1 + 1/α)) in total
consider the hash table as a m x L 2d-array with n entries non-empty
this procedure than uniformly chooses a slot from all m x L entries, both empty and non-empty
as the procedure is redone when an empty entry is chosen, it's a uniform sampling of non-empty entry

11.3-1
compare h(k) to the stored hash value first, only compare the stored strings when hash values equal
comparing two strings will take time proportional to the length of the shorter string
comparing two hash values usually will be constant time

11.3-2
the number s represented by the string is Σ(i = {1 .. r})128^(r-i)ci
assume rk is the number represented by the first k characters of the string modulo m
    rk      = Σ(i = {0 .. k})(128^(r-i)ci) mod m
    rk+1    = Σ(i = {0 .. k+1})(128^(r-i)ci) mod m
            = (Σ(i = {0 .. k})(128^(r-i)ci) + ck+1) mod m
            = 128rk mod m + ck+1 mod m
therefore s mod m can be computed iteratively

11.3-3
let x = c1 .. cn
h(x)    = Σ(i = {1 .. n})((2^p)^(n-i)ci) mod 2^p - 1
        = Σ(i = {1 .. n})(2^p(n-i)ci mod 2^p - 1)
        = Σ(i = {1 .. n})(ci)
therefore any permutation y of x has h(y) = h(x)

11.3-4
[ 700, 318, 936, 554, 172 ]

11.3-5
assume there are |B| slots containing b1 .. b|B| keys
    Σ(i = {1 .. |B|})bi = |U|
then the number of collisions is
    Σ(i = {1 .. |B|})(bi(bi - 1))
    = Σbi^2 - Σbi
by Cauchy-Schwarz inequality,
    (Σbi)^2 <= |B|Σbi^2
    Σbi^2 >= |U|^2 / |B|
    Σ(bi(bi - 1)) >= |U|^2/|B| - |U|
assume Pr{h(x) = h(y)} < 1/|B| - 1/|U|
then the total number of collisions is less than
    C(|U|, 2) * (1/|B| - 1/|U|)
    = |U|(|U| - 1) * (1/|B| - 1/|U|)
    = (|U| - 1)(|U|/|B| - 1)
    < |U|^2/|B| - |U|
therefore it must has Pr{h(x) = h(y)} >= 1/|B| - 1/|U|

11.3-6
-

11.4-1
Linear probing:
[ 22, 88, null, null, 4, 15, 28, 17, 59, 31, 10 ]
Quadratic probing:
[ 22, null, 88, 17, 4, null, 28, 59, 15, 31, 10 ]
Double hashing:
[ 22, null, 59, 17, 4, 15, 28, 88, null, 31, 10 ]

11.4-2
./CLRS/collection/hashtable.ts#OpenAddressing

11.4-3
α = 3/4:
    unsuccessful:   1/(1 - α) = 4
    successful:     (1/α)ln(1/(1-α)) < 2
α = 7/8:
    unsuccessful:   1/(1 - α) = 8
    successful:     (1/α)ln(1/(1-α)) < 3

11.4-4
when gcd(h2(k), m) = d, the subgroup of Zm generated by h2(k) has m/d elements
which means c = m/d is the smallest positive c such that ch2(k) ≡ 0 mod m
hash value is once again h1(k) when i = c = m/d, therefore 1/dth of m entries of the hashtable

11.4-5
1/(1-α) = (2/α)ln(1/(1-α))
α/(1-α) = 2ln(1/(1-α))
e^(α/(1-α)) = 1/(1-α)^2
α = 0.71533 by wolframalpha

11.5-1
"no collisions occur" means insert operation never increments i
for the set of key K inserted, all {h(k): k ∈ K} are distinct
assuming uniform hashing,
    Pr{h(k) = h(k')} <= 1/m for all distinct k and k'
    p(n, m) = (1 - 1/m)^C(n, 2)
            <= e^(-1/m * C(n, 2))
            = e^(-n(n-1)/2m)
therefore when n >= m^(1/2) + 1, p(n, m) <= e^(-1/2) decreases exponentially

11-1
a.  the hashtable is at most half-full when i = n
    by uniform hashing
    all n = 2m keys inserted before, and all k probes of the current key points to uniformly random position in the array
    for any probe, Pr{ith probe hashes the key to an occupied slot} <= 1/2
    the insertion operation will do more than k probes if all the first k probes hashes the key to an occupied slot
    which happens with probability (1/2)^k = 2^-k
b.  2^-2lgn = n^-2 = O(1/n^2)
c.  Pr{X > 2lgn}    = Pr{X1 > 2lgn ∨ X2 <= 2lgn ∨ .. ∨ Xn <= 2lgn}
                    <= ΣPr{Xi > 2lgn}
                    = nO(1/n^2)
                    = O(1/n)
d.  X is capped by n, the total number of keys inserted
    therefore
        E[X]    <= 2lgn * (1 - O(1/n)) + nO(1/n)
                = O(lgn) + O(1)
                = O(lgn)

11-2
a.  the probability that a certain key is hashed to a certain slot is 1/n
    so the number of keys in a slot after n insertion follows binomial distribution with success rate 1/n
        Qk  = P(X = k) 
            = C(n, k)p^k(1-p)^(n-k)
            = C(n, k)(1/n)^k(1 - 1/n)^(n-k)
b.  M = k means at least a slot contains exactly k keys
    let Mi be the number of keys in slot i
    Pr{M = k}   = Pr{∃i, Mi = k ∧ ∀j != i, Mi <= k}
                <= Pr{∃i, Mi = k}
                = Pr{M1 = k ∨ M2 = k .. Mn = k}
                <= ΣPr{Mi = k}
                = nQk
c.  as n -> ∞, (1 - 1/n) -> 1
    (1 - 1/n)^(n-k) < 1
    C(n, k) = n!/k!(n-k)! <= n^k/k!
    Qk  = C(n, k)(1/n)^k(1 - 1/n)^(n-k)
        <= n^k/k! * (1/n)^k
        = 1/k!
        = 1/((2πn)^(1/2) * (k/e)^k * (1 + Θ(1/k)))
        <= 1/(k/e)^k
        = e^k/k^k
d.  when k0 = clgn/lglgn,
        lg(e^k0 / k0^k0)    = k0lge - k0lgk0
                            = clgnlge/lglgn - clgn/lglgn * (lgc + lglgn - lglglgn)
                            = c(lgelgn / lglgn - lgn/lglgn * (lgc + lglgn - lglglgn))
                            = clgn(lge / lglgn - 1/lglgn * (lgc + lglgn - lglglgn))
                            = clgn(lge / lglgn - lgc/lglgn - 1 + lglglgn/lglgn)
        lg(1/n^3) = -3lgn > lgQk0
        -3 > c(lge / lglgn - lgc/lglgn - 1 + lglglgn/lglgn)
    as n -> ∞, lge/lglgn -> 0, lgc/lglgn -> 0, lglglgn/lglgn -> 0
    c(lge / lglgn - lgc/lglgn - 1 + lglglgn/lglgn) -> -c
    so for c > 3, Qk0 < 1/n^3 for large enough n
    as c > 3 > e, e/k < 1, e^k/k^k monotonically decreasing
    Qk <= Qk0 when k <= k0
    Pk <= nQk <= nQk0 = 1/n^2
e.  M is capped by n
    E[M]    <= Pr{M > clgn/lglgn} * n + Pr{M <= clgn/lglgn} * clgn/lglgn
            <= n * 1/n^2 * n + 1 * clgn/lglgn for large enough n
            = 1 + O(lgn/lglgn)
            = O(lgn/lglgn)

11-3
a.  the ith probe examines position 
        (j + 1 + 2 + .. + i) mod m
        = (j + i(i+1)/2) mod m
        = (j + i/2 + i^2/2) mod m
    c1 = 1/2, c2 = 1/2
b.  assume (j + k(k+1)/2) ≡ (j + l(l+1)/2) mod m for some 0 <= k < l <= m-1
        k(k+1)/2 ≡ l(l+1)/2 mod m
        k(k+1)/2 - l(l+1)/2 ≡ 0 mod m
    thanks Instructor's Manual
        (j-i)(j+i+1)/2 ≡ 0 mod m
        (j-i)(j+i+1) = 2dm = 2d2^p = d2^(p+1)
    exactly one of j-i and j+i+1 must be even, the other must be odd
    which means 2^(p+1) must divide one of j-i and j+i+1
    as j-i < m, 2^(p+1) cannot divide j-i
    as j+i <= 2m-2, j+i+1 <= 2m-1 < 2^(p+1), 2^(p+1) cannot divide j+i+1
    therefore k(k+1)/2 !≡ l(l+1)/2 mod m for all distinct k and l
    for all i ∈ {0 .. m-1}, (j + i(i+1)/2) mod m is distinct

11-4
-

Chapter 12
12.1-1
Height 2:
  ┌-21
┌-17
| └-16
10
| ┌-5
└-4
  └-1
Height 3:
  ┌-21
┌-17
| └-16
10
└-5
  | ┌-4
  └-1
Height 4:
21
|   ┌-17
|   | └-16
| ┌-10
└-5
  └-4
    └-1
Height 5:
┌-21
17
└-16
  | ┌-10
  | | | ┌-5
  | | └-4
  └-1
Height 6:
  ┌-21
  | | ┌-17
  | | | | ┌-16
  | | | └-10
  | └-5
┌-4
1
12.1-2
binary search tree:
    left <= parent <= right
min heap:
    left >= parent, right >= parent
it cannot print elements in sorted order in O(n)
as building a min heap takes time O(n)
if it can traverse its element in sorted order in O(n), n elements can be sorted in time O(n)
which is lower than the lower bound of comparsion sort Ω(nlgn)

12.1-3
the same idea as 10.4-5
adjusted when a the key is yielded as 10.4-5 is preorder
./CLRS/collection/tree.ts#inorder

12.1-4
./CLRS/collection/tree.ts#preorder
./CLRS/collection/tree.ts#postorder

12.1-5
as inorder traverses the tree keys in sorted order in O(n)
if a binary search tree can be built from n elements in time o(nlgn), n elements can be sorted in time o(nlgn)
therefore building a binary search tree must take time Ω(nlgn)

12.2-1
by the nature of search tree structure, for any sequences of nodes examined and an index i,
    if A[i] >= A[i+1], A[i] >= A[j] for all j >= i+1
    if A[i] <= A[i+1], A[i] <= A[j] for all j >= i+1
c is not possible as 911 > 240 but 911 < 912

12.2-2
./CLRS/collection/tree.ts#treeMinimum
./CLRS/collection/tree.ts#treeMaximum

12.2-3
./CLRS/collection/tree.ts#treePredecessor

12.2-4
  ┌-21
┌-17
| └-16
10
| ┌-5
└-4
  └-1
search for 21, 10 on path, 16 to the left, 16 > 10
search for 1, 10 on path, 5 to the right, 5 < 10

12.2-5
assume all keys are distinct, current node x has key k
the successor s has key greater than k
it's either 
    1.  in the right subtree of x, or, 
    2.  at some level from the root, x is in the left subtree and s is the parent
then all nodes in case 1 have keys smaller than all nodes in case 2
as x has two children, it's successor must be case 1, in the right subtree of x
then if s has a left child, it has key smaller than s but greater than x
in contradiction to s being the successor of x
similarly the predecessor has no right child

12.2-6
any node s greater than x is either
    1.  in the right subtree of x, or, 
    2.  at some level from the root, x is in the left subtree and s is the parent / in the right subtree
in case 2, as all nodes in the right subtree are greater than the parent, case 2 can be refined to
    2.  at some level from the root, x is in the left subtree and s is the parent
when there's no nodes in case 1, the smallest node in case 2 is the successor of x
consider two nodes s1 and s2, where s2 is higher in the tree than s1
by definition s1 is in the left subtree of s2, s2 > s1
thereby the smallest node in case 2 is just the lowest node in case 2
which in turn is the lowest ancestor of x that has x in its left subtree / whose left child is also an ancestor of x

12.2-7
see 12.2-8, as h = O(n), k = O(n), k + h = O(2n) = O(n)

12.2-8
consider the subtree formed by nodes traversed by the k calls to SUCCESSOR
exactly k nodes is yielded during the traverse
the subtree may contain nodes smaller than the minimum of the k nodes on the path from root to the minimum
similary the subtree may contains nodes greater than the maximum
both path has at most h nodes
the number of nodes in the subtree is at most O(k + 2h)
for any node x in the subtree, x is traversed first from its parent p
either during call to SUCCESSOR(p) or SUCCESSOR(y) for some y
both case the left subtree of x will be traversed before x is visited again
then if x has a right child, its right subtree will be traversed before x is visited the third time
once m the maximum node in the right subtree of x is traversed, SUCCESSOR will go directly to p
if x = p.left, SUCCESSOR(m) = p, the left subtree of p will never be visited again
if x = p.right, SUCCESSOR(m) = p' for which x is in the left subtree of p', the same to above
therefore a node will be visited at most 3 times
T(k) = 3O(k + 2h) = O(k + h)

12.2-9
when x is a leaf node, it has no left nor right child
either SUCCESSOR(x) will return y or PREDECESSOR(x) will return y, depends on x being left or right child of y

12.3-1
./CLRS/collection/tree.ts#treeInsertRecur

12.3-2
TREE-INSERT and TREE-SEARCH are equivalent until a null leaf
search operation will traverse exactly the same path as insert
plus one final node which was NIL at the end of insertion

12.3-3
worst case:
    the element inserted is strictly increasing or decreasing
    before the ith insertion, the tree has height i-2, so ith insertion takes time Θ(i - 2) = Θ(i)
    T(n) = T(n-1) + Θ(n) = Θ(n^2)
best case:
    the tree is perfectly balanced
    T(n) = T(n-1) + Θ(lgn) = Θ(nlgn)

12.3-4
let x be a node with two children, y be its left child, y is a leaf
deleting x first will replace x with its successor without modifying its left child
then deleting y simply removes y
deleting y first will simply remove y
then x becomes case a in figure 12.4, the x is replaced by its right subtree
two results may be different if the right child of x has a left child

12.3-5
search doesn't reference parents
insert also doesn't reference parents
succ, the successor of the maximum node in the subtree rooted at x will be the first "right ancestor" of x
thereby if succ is NIL, x on the right-most branch of the tree
parent of x can be found by travering downward from the root and check if right child of the cursor is x
if succ is not NIL, x is on the right-most branch of the subtree rooted at the left child of succ
./CLRS/collection/tree.ts#treeParent
this procedure will only traverse from root to the bottom at most twice, running time is O(h)
both TRANSPLANT and TREE-DELETE only refers parent costant times, so TREE-DELETE takes time O(h)

12.3-6
the predecessor pred will be the maximum of the subtree rooted at the left child of z
then pred have to be first replaced by its left child before replacing z, if pred is not the left child of z
the two strategy can be defined as two seperate procedures, which are called with 1/2 probability each

12.4-1
when n = 1, ΣC(i+3, 3) = C(3, 3) = C(n+3, 4) = C(4, 4) = 1
assume Σ(i = {0 .. n-1})C(i+3, 3) = C(n+3, 4)
Σ(i = {0 .. n})C(i+3, 3)
= Σ(i = {0 .. n-1})C(i+3, 3) + C(n+3, 3)
= C(n+3, 4) + C(n+3, 3)
= C(n+4, 4)
so for all n >= 1, Σ(i = {0 .. n-1})C(i+3, 3) = C(n+3, 4)

12.4-2
let tree T' be a perfectly balanced tree with k nodes
then the height of T' will be O(lgk)
let tree T be a tree that:
    left subtree of T is T'
    right subtree of T is s single chain of length n - k - 1
then the average height in left subtree will be 1 + O(lgk) = O(lgk)
the average height in right subtree will be 1 + O(n-k) = O(n-k)
overall average will be
    (kO(lgk) + (n-k)O(n-k)) / n
    <= (nO(lgk) + (n-k)O(n-k)) / n
    = O(lgk) + O((n-k)^2 / n)
for the overall average be O(lgn), O((n-k)^2 / n) must be O(lgn)
    (n-k)^2 = O(nlgn)
    n-k = O((nlgn)^(1/2))
therefore the height of a tree of average height O(lgn) is O((nlgn)^(1/2))

12.4-3
let V(n) denote the variations of trees of n nodes
when the root has ith order, its left subtree has i-1 nodes, its right subtree has n-i nodes
    V(n) = Σ(i = {1 .. n})(V(i-1)V(n-i))
specifically:
    V(0) = 1
    V(1) = 1
    V(2) = 2
    V(3) = 5
when n = 3, a randomly chosen tree has 5 variations:
v1:
    3
    │ ┌─2
    └─1
v2:
    ┌─3
    2
    └─1
v3:
    3
    └─2
      └─1
v4:
    ┌─3
    │ └─2
    1
v5:
      ┌─3
    ┌─2
    1
when the tree is randomly chosen, each tree will be picked with probability 1/5
but 3! = 6, the 6 variations of inputs cannot be evenly divided into the 5 cases

12.4-4
f(x) = 2^x
(d/d^2x)f(x) = 2^x(ln2)^2 >= 0
2^x is convex

12.4-5
the randomized quicksort 
    1.  chooses a random pivot
    2.  compare all other elements with the pivot, partitions them into two subarray
    3.  repeat step 1. for both subarrays
this procedure is equivalent to
    1.  choose a random element as the root of the tree
    2.  compare all other elements with the root, decide whether they belongs to the left or right subtree
    3.  repeat step 1. for the two groups to build the two subtrees
which is in turn equivalent to build a random tree by insertion
the height of an element then is number of comparsions performed with its key as an operand
the total height bounds the total number of comparsions performed by randomized quicksort
as the expected height of the tree is O(lgn), the total height is O(nlgn)
thanks https://cs.stackexchange.com/questions/86149
the original bound O(lgn) in 12.4 can be strengthened 
as E[Yn] = E[2^Xn] <= C(n+3, 3)/4 = O(n^3), by Markov's inequality
    Pr{Xn >= lgt} = Pr{2^Xn >= t} <= E[2^Xn] / t = O(n^3/t)
choose t = n^(3+k),
    Pr{Xn >= (k+3)lgn} = O(n^-k)
therefore probability of running time of quicksort exceeds (k+3)nlgn = O(nlgn) is O(1/n^k) for any k > 0

12-1
a.  as k is inserted to the left of the current node when k <= node.key, the resulting tree will be a single chain
    ith insertion takes time Ω(i), total time Ω(n^2)
b.  for each node in the tree, the left subtree may have nodes at most one more than the right subtree
    let T(h) be the lower bound of number of nodes in such a tree of height h, then
        T(1)    = 1
        T(h)    = 1 + T(h-1) + (T(h-1) - 1) = 2T(h-1)
                = 2^(h-1) = Ω(2^h)
    therefore h = O(lgn), the tree is balanced
    expected time of n insertion is then nO(h) = O(nlgn)
c.  all nodes will be in the list associated with the root
    each insertion takes time O(1) of list insertion, O(n) in total
d.  in worst case the resulting tree is a single chain, Ω(n^2) in total
    on average it's roughly equivalent to building a tree by random sequences of insertion
    average height h = O(lgn), expected running time O(nlgn)

12-2
insertion is stright forward, takes time proportional to the length of the string, O(n) in total
nodes have to keep an additional boolean field indicating whether they are end of a string
then the total nodes in the tree is O(n)
instead of inorder, the tree has to be traversed in preorder to give sorted sequence
the path taken so far during the preorder traverse can be maintained in a stack-like structure
as both pop and push to the top of the stack takes O(1), this will not change the performance asymptotically
reconstructing the string from nodes takes time proportional to the length of the path
which in turn is the length of the originally inserted string, the combined length is O(n)
hence overall running time is O(n)
./CLRS/collection/radix-tree.ts#RadixTree

12-3
a.  there are n nodes in total
    each node x has depth d(x, T)
    average depth is Σd(x, T) / n = P(T) / n
b.  the root has depth 0
    total depth of nodes in left subtree is P(TL) + |TL|
    total depth of nodes in right subtree is P(TR) + |TR|
    |TL| + |TR| = n - 1
    P(T) = P(TL) + P(TR) + |TL| + |TR| = P(TL) + P(TR) + n - 1
c.  the root of the tree can have order from 1 to n uniformly
    if the root has order i, its left subtree has i-1 nodes, right subtree n-i nodes
    combine with part b.
    P(n)    = Σ(i = {1 .. n})(P(i-1) + P(n-i) + n - 1)
            = Σ(i = {0 .. n-1})(P(i) + P(n-i-1) + n - 1)
d.  each P(i) for i = {0 .. n-1} appears twice
    P(n)    = (2Σ(i = {0 .. n-1})P(i) + n(n-1)) / n
            = 2Σ(i = {0 .. n-1})P(i) / n + (n-1)
            = 2Σ(i = {0 .. n-1})P(i) / n + Θ(n)
e.  the same as 7-3
f.  randomized quicksort, as argued in 12.4-5

12-4
a.  see 12.4-3
b.  B(x)^2  = (Σ(n = {0..})bnx^n)^2
            = Σ(i = {0 ..})(Σ(j = {0 ..})(bibjx^(i+j)))
    which is the sum of a matrix where entry (i, j) is bibjx^(i+j)
    consider each diagonal line on which i+j = n >= 1, entries on that line sums to
    Σ(k = {0 .. n})bkbn-kx^n = bn+1x^n
    for n = 0, b0 = 1, b0b0x^0 = b1x^0
    B(x)^2  = Σ(n = {0 ..})(bn+1x^n)
    xB(x)^2 = Σ(n = {0 ..})(bn+1x^n+1)
            = Σ(n = {1 ..})(bnx^n)
    xB(x)^2 + 1 = B(x)
c.  routine
d.  Stirling's approximation

Chapter 13
13.1-1
bh(T.root) = 2:
    ┌─15r
  ┌─14b
  │ └─13r
┌─12r
│ │ ┌─11r
│ └─10b
│   └─9r
8b
│   ┌─7r
│ ┌─6b
│ │ └─5r
└─4r
  │ ┌─3r
  └─2b
    └─1r
bh(T.root) = 3:
    ┌─15b
  ┌─14r
  │ └─13b
┌─12b
│ │ ┌─11b
│ └─10r
│   └─9b
8b
│   ┌─7b
│ ┌─6r
│ │ └─5b
└─4b
  │ ┌─3b
  └─2r
    └─1b
bh(T.root) = 4:
    ┌─15b
  ┌─14b
  │ └─13b
┌─12b
│ │ ┌─11b
│ └─10b
│   └─9b
8b
│   ┌─7b
│ ┌─6b
│ │ └─5b
└─4b
  │ ┌─3b
  └─2b
    └─1b

13.1-2
36 will be inserted as the right child of 35
as 35 is red
    if 36 is inserted as a red node, property 4 is broken
    the red node 35 has a red child
    if 36 is inserted as a black node, property 5 is broken
    the path from root to 36 contains one more black node than other paths

13.1-3
property 1
    trivially satisfied
property 2
    satisfied
property 3
    satisfied
property 4
    only the root is colored black from red
    each red node in either the left or right subtree satisfies property 4
    the root is now black, so each red node satisfies property 4
property 5
    each node satisfies property 5 before the root is re-colored
    for each node in either left or right subtree, property 5 still holds after the re-coloring
    for the root, each path has one more black node after the re-coloring, property 5 still holds for all nodes

13.1-4
as red nodes must have black children, the parent of a red node must be black
then the red node itself also can only have black children
so after the "absorb", the parent of a red node may have at most 4 black children

13.1-5
both paths contain the same amount of black nodes bh(x)
as each red node must have black children
the longest such path may contain at most bh(x) red nodes, 2bh(x) combined
the shortest path has at least bh(x) nodes

13.1-6
by 13.1-5
T is at most a complete tree of height 2bh(T.root) = 2k, 2^(2k + 1) - 1 nodes in total
T is at least a complete tree of height bh(T.root)-1 = k-1, 2^k - 1 nodes in total

13.1-7
largest possible:
    root is black or red
    if one level contains red / black nodes only, the next level contains only the opposite color
    let 2^k - 1 <= n < 2^(k+1) - 1
    if k is odd, the first k levels contains 1/3 or 2/3 red nodes if the root is not counted
    the ratio increases / decreases towards 2/3 or 1/3 depends on whether root is red or black
    if k is even, the same to when k is odd, but this time the root can be counted
    the largest possible ratio then is between 2/3 and 1/2
smallest possible:
    if n = 2^k - 1, the tree is complete and all nodes can be black
    if 2^k - 1 < n < 2^(k+1) - 1, red nodes have to be inserted so property 5 holds
    if exactly half of the bottom level is filled and they are all decendents of the same subtree of the root
    by color the corresponding child of root to red, property 5 is maintained, red ratio is 1/n
    starting from a tree where all but the bottom nodes are black
    then for any black node one level above the bottom level which has two red children:
        color the children black and the parent red still maintains property 5
    thus the necessary number of red nodes is no more than lg(number of nodes at bottom level), which is <= lg(n/2)
    smallest possible red ratio is somewhere between 0 and (lgn - 1)/n

13.2-1
./CLRS/collection/redblack-tree.ts#rightRotate

13.2-2
there is a rotation for each edge between two internal nodes
for a tree with n nodes, there are exactly n-1 edges between internal nodes, thus n-1 possible rotations

13.2-3
depth of a increases by 1
depth of b won't change
depth of c decreases by 1

13.2-4
if the tree is not a right-going chain, some node on the right-most branch of the tree has a left child
then by right-rotating the node, its left child is now on the right-most branch
the right-most branch contains exactly one more node
so after at most n-1 right rotations, every tree can be transformed into a right-going chain
a right-going chain can be transformed into any tree by reversing the right rotations by left rotations
then every tree can be transformed into any other tree within 2n-2 rotations

13.2-5
let T1 be a right-going chain
no right rotation can be performed within T1, as no node in T1 has a left child
therefore T1 cannot be right-converted to any T2 that's not a right-going chain
thanks http://sites.math.rutgers.edu/~ajl213/CLRS/Ch13.pdf
consider nodes x, y and a, b, and c in subtrees α, β and γ, after a right rotation
the path from root to the node contains certain numbers of left-going edges, denote it by l(x)
    for a and x, l(a) and l(x) is decreased by 1
    for b, l(b) won't change (the order of one left-going and one right-going edge exchanged)
    for y and c, l(y) and l(c) won't change (the number of right-going edges increased by 1)
thus Σl(x) for all x in the tree is decreased by at least 1 after a right rotation
when Σl(x) = 0, the tree is a right-going chain, cannot be right-converted to any other trees
as l(x) can be at most n-1 for any node in a tree containing n keys
Σl(x) = O(n^2), any tree will become a right-going chain after at most O(n^2) right rotation
therefore if T1 can be right-converted to T2, it must happen within O(n^2) right rotaions

13.3-1
property 5 is violated then

13.3-2
┌─41b
38b
│ ┌─31b
└─19r
  └─12b
    └─8r

13.3-3
13.5:
    α:  the color of A and C swapped, bh still k
    β:  still only one black node among A, B and C, bh still k
    γ:  still only one black node among A, B and C, bh still k
    δ:  the color of C and D swapped, bh still k
    ε:  the color of C and D swapped, bh still k
13.6:
    as both A and B are red, the left rotation will not change bh of any node
    considering only the transformation from case 3 to a proper red black tree
    α:  C replaced by B, both black, bh still k
    β:  same to α
    γ:  before Cb -> Br -> γ, now Bb -> Cr -> γ, bh still k
    δ:  path from root to δ now contains one more red node, bh still k

13.3-4
by loop invariation, z is red at the start of each iteration, so z cannot be T.nil
then immediately the loop tests whether z.p.color == RED, terminates if z.p.color != RED
by loop invariation, if z and z.p both red, T.root is black, which means neither z or z.p is the root
so z, z.p, z.p.p all exists, none of them is T.nil
the uncle of z, z.p.p.right, is only modified when its color is red, so it cannot be T.nil too
as these are the only nodes ever get modified in an iteration of RB-INSERT-FIXUP, T.nil will not be colored red

13.3-5
the node will first be inserted as a red node
during RB-INSERT-FIXUP, the node z is always a red node
z may only be colored black at the end of RB-INSERT-FIXUP if z is the new root
which only happens when the tree is previously empty, n = 1
so for n > 1, z will point to a red node when RB-INSERT-FIXUP terminates, the tree contains at least one red node

13.3-6
during the initial insertion, where the new node z is placed somewhere at the bottom level
the path traversed are all ancestors of the node z
by pushing all the ancestors into a stack, z.p can be poped from the stack in time O(1)

13.4-1
root can only be red if y was the root and its red child x becomes the new root
then in RB-DELETE-FIXUP, the loop immediately terminates, and x is set to black
so the root after deletion is guarenteed to be black

13.4-2
if both x and x.p are red, the extra black can be fixed by setting x to black
then both property 1 and property 4 are restored

13.4-3
┌─41b
38b
│ ┌─31b
└─19r
  └─12b
    └─8r
Deleting 8
┌─41b
38b
│ ┌─31b
└─19r
  └─12b
Deleting 12
┌─41b
38b
│ ┌─31r
└─19b
Deleting 19
┌─41b
38b
└─31b
Deleting 31
┌─41r
38b
Deleting 38
41b
Deleting 41
Empty tree

13.4-4
anywhere examines or modifies x, as x may be T.nil

13.4-5
case 1:
    α:  A + B = 2 + 1 = 3 => A + B + D = 2 + 0 + 1 = 3
    β:  same to α
    γ:  B + D + C = 1 + 0 + 1 = 2 => D + B + C = 1 + 0 + 1 = 2
    δ:  same to γ
    ε:  B + D + E = 1 + 0 + 1 = 2 => D + E = 1 + 1 = 2
    ζ:  same to ε
case 2:
    α:  B + A = count(c) + 2 => B + A = count(c) + 1 + 1 = count(c) + 2
    β:  same to α
    γ:  B + D + C = count(c) + 1 + 1 = count(c) + 2 => B + D + C = count(c) + 1 + 0 + 1 = count(c) + 2
    δ:  same to γ
    ε:  B + D + E = count(c) + 1 + 1 = count(c) + 2 => B + D + E = count(c) + 1 + 0 + 1 = count(c) + 2
    ζ:  same to ε
case 3:
    α:  not modified
    β:  same to α
    γ:  B + D + C = count(c) + 1 + 0 = count(c) + 1 => B + C = count(c) + 1
    δ:  B + D + C = count(c) + 1 + 0 = count(c) + 1 => B + C + D = count(c) + 1 + 0 = count(c) + 1
    ε:  B + D + E = count(c) + 1 + 1 = count(c) + 2 => B + C + D + E = count(c) + 1 + 0 + 1 = count(c) + 2
    ζ:  same to ε
case 4:
    α:  B + A = count(c) + 2 => D + B + A = count(c) + 1 + 1 = count(c) + 2
    β:  same to α
    γ:  B + D + C = count(c) + 1 + count(c') => D + B + C = count(c) + 1 + count(c')
    δ:  same to γ
    ε:  B + D + E = count(c) + 1 + 0 = count(c) + 1 => D + E = count(c) + 1
    ζ:  same to ε

13.4-6
before calling RB-DELETE-FIXUP, the tree up to x is correct in property 4
as the sibling of x is red, x.p must be black

13.4-7
consider case (a) in figure 13.5
after insertion of node B, A and D is colored black, then C the root is colored red
RB-INSERT-FIXUP then fixes the color of C to black
deletion of node B will be simple, without calling RB-DELETE-FIXUP
but now the color of A and D is modified

13-1
a.  insertion:
        the new node will be inserted at the bottom level of the tree
        its parent must be copied or the new node may not be accessible from the root
        similarly its grandparent must be copied or the parent of the new node may not be accessible
        all O(h) ancestors of the new node must be copied
    deletion:
        if the deleted node y has no or one child, only its parent have to be copied and modified
        its parent now has NIL or y's sole child as a new child
        no modification to the subtree rooted at y's sole child is necessary
        if y has two children, then the successor of y is moved to its place
        every node on the path then has to be modified and copied, O(h) nodes at most
b.  ./CLRS/collection/persistent-tree.ts#PTree
c.  shallow copying the current node is O(1)
    if z.key <= copy.key and copy.left == null, the loop recursion terminates, similarly when z.key > copy.key
    otherwise the procedure recursively calls itself with copy.left or copy.right, which has depth one more than copy
    time complexity is T(h) = T(h-1) + O(1) = O(h)
    each recursive call shallow copies the current node, which consistents of a key and two pointers, takes O(1) space
    space complexity is T(h) = T(h-1) + O(1) = O(h) 
d.  PERSISTENT-TREE-INSERT must modify and copy the root to make a new tree
    then the left and right child of the root must be modified too
    otherwise their parent pointer points to the old root, the tree is no longer persistent
    so the children of the root also have to be copied
    recursively all nodes in the tree have to be copied, the procedure will take O(n) time and space
e.  RB-INSERT-FIXUP and RB-DELETE-FIXUP only ever modifies nodes as far as children of siblings
    when copying all the ancestors, also copy all the siblings and chlidren of siblings
    at most 4 times more nodes are copied, time and space complexity still O(h)
    then apply reb black fixup to the tree after insertion and deleteion, the height of the tree is O(lgn)

13-2
a.  as insertion and deletion in reb black tree maintains property 5
    the black height after insertion or deletion can be determined by traversing s simple path from root to any leaf
    length of that path will be O(lgn) as the property of red black tree
    so the complexity of insertion or deletion still O(lgn)
    by keeping a counter of black nodes, starting from T.bh, decrements every time a black node is visited
    T(n) = T(l) + T(r) + O(1), where l + r + 1 = n
    therefore T(n) = O(n) and T(n)/n = O(1)
b.  as each simple path from root to leaf contains the same number of black nodes
    also descending one level in the tree at most decreases the black height by 1
    thereby each simple path from root to leaf contains node with black height k for each k in {1 .. T.bh}
    starting from x = T.root and always go from x to x.right
    the last node has black height T2.bh is the maximum among all such nodes
c.  as node in Ty <= x <= node in T2, make a new tree that:
        x is the root
        left subtree is Ty
        right subtree is T2
    then as binary search tree property holds within Ty and T2, it also holds in the new tree
    y is chosen as the maximum node among all nodes in T1 with bh = T2.bh, each node in Ty is greater than y.p
    therefore each node in the new tree is greater than y.p, the new tree is in the right position within T1
d.  Ty has black height T2.bh, same to T2
    T2.root is black, the color of y uncertain
    x must be red or any simple path through x contains one more black node
    but then
        x may be the root after join
        x and T2.root may both be red
    if x is the new root, coloring it black will fix both proerty 2 and 4
    if x is not root and both T2.root and x are red
    the tree now satisfies the invariants in RB-INSERT-FIXUP with z = T2.root, or z = x.right
    so calling RB-INSERT-FIXUP(x.right) will fix the red black tree properties
e.  then T1 is inserted to the maximum node in T2 which has bh = T1.bh
f.  finding y: O(lgn)
    constructing the new subtree: O(1)
    RB-INSERT-FIXUP: O(lgn)
    overall runtime O(lgn)

13-3
a.  let L(h) denote the least number of nodes a height-h AVL tree may contain
        L(0) = 1
    x.h = 1 + max(x.left.h, x.right.h), so the height of subtrees are at least x.h - 1 and x.h - 2
        L(h)    = L(h-1) + L(h-2) + 1
                > L(h-1) + L(h-2)
                = Fh
    as Fibonacci number grows exponentially, h = O(lgn)
b.  when |x.right.h - x.left.h| <= 1, the subtree is balanced
    when x.right.h = x.left.h + 2, both subtree are balanced but the height of right subtree is 2 higher than left
    denote:
        h = x.h, the current height of x
        y = x.right
        α = left subtree of x
        β = left subtree of y
        γ = right subtree of y
    then
        h(x) = h 
        h(y) = h - 1
        h(α) = h - 3
        h - 3 <= h(β), h(γ) <= h - 2
    after a left rotation at x,
        h(x.left) = h(α) = h - 3
        h - 3 <= h(x.right) = h(β) <= h - 2
    x is balanced
        h - 2 <= h(y.left) = 1 + max(h(α), h(β)) <= h - 1
        h - 3 <= h(y.right) = h(γ) <= h - 2
    y may not be balanced if h(γ) = h - 3 and h(β) = h - 2
    a right rotation at y can prevent that situation, but also possibly introduces unbalance to y
    // stuck
    thanks http://courses.csail.mit.edu/6.046/spring04/handouts/ps5-sol.pdf
    the possible unbalance introduced by right rotation at y will be eliminated by a left rotation
    denote
        h = x.h
        z = y.left
        α = left subtree of x
        β = left subtree of z
        γ = right subtree of z
        δ = right subtree of y
    then
        h(α) = h - 3
        h(z) = h - 2
        h(δ) = h - 3
        h - 4 <= h(β), h(γ) <= h - 3
    after right rotation at y,
        h - 4 <= h(y.left) = h(γ) <= h - 3
        h(y.right) = h(δ) = h - 3
    y is balanced, and h(y) = h - 2
    after left rotation at z,
        h(x.left) = h(α) = h - 3
        h - 4 <= h(x.right) = h(β) <= h - 3
    x is balanced, and h(x) = h - 2
    z now is balanced with h(z) = h - 1
    ./CLRS/collection/avl-tree.ts#balance
c.  ./CLRS/collection/avl-tree.ts#insert
d.  depends on whether it's necessary to fix the height of all the ancestors after calling BALANCE
    BALANCE(x) decreases height of the subtree rooted at x by 1
    this decrement may be propagated all the way to the top, change the height of all the ancestors of x
    but during insertion, this propagation can be compensated by fix the height of roots recursively
    so BALANCE takes O(1), INSERT takes O(h) = O(lgn)
    as BALANCE(x) decrements the height of subtree rooted at x by 1
    once BALANCE(x) is called for some node x, none of the ancestors of x will be unbalanced
    (they are balanced before the insertion
    and the possible height increase caused by insertion solved at x)
    so BALANCE(x) may only be fully executed once and O(1) rotations are performed

13-4
a.  given distinct priorities, all permutations of x1 .. xn has a same sorted order 
    by the property of treap, the resulting treap is the tree formed if x1 .. xn is inserted in sorted order
    therefore for a set of elements x1 .. xn with fixed keys and priorities, the resulting treap is the same
    if the keys are different, the resulting tree or treap is certainly different
    if the priorities are different up to ordering, the resulting treap may not be different
    both insertion sequence [2, 1, 3] and [2, 3, 1] results in the same tree
b.  as elements are given random priorities
    sorted form of x1 .. xn can be any of n! permutations of x1 .. xn with equal probability
    when n = 1:
        trivial
    assume the sorted form of x1 .. xn can be any of n! permutations of x1 .. xn with probability 1/n!:
        next element xn+1, given random priority, may appear anywhere in the sorted form with equal probability
        there are n+1 places xn+1 may end up at in any permutations, n! * (n+1) = (n+1)! new permutations in total
        each with probability 1/(n! * (n+1)) = 1/(n+1)!
    as the sorted form of x1 .. xn is a uniformly random permutation of x1 .. xn
    theoriem 12.4 applies, the expected height of the treap is O(lgn)
c.  first the normal insertion of z is performed according to binary search tree property
    but then the priority of z.p may be greater than z, and this is the only possible violation of treap properties
    which means the other child of z.p and its descendents have priority greater than z.p
    the descendents of z (if any) has priority greater than z
    if z == z.p.let, perform RIGHT-ROTATE(z.p), otherwise LEFT-ROTATE(z.p)
    then z becomes the new parent of z.p, the treap property violation between z and z.p is fixed
    the possible violation is now between z and its new parent, one level higher in the treap
    ./CLRS/collection/treap.ts#insert
d.  the initial insertion takes O(h)
    each iteration of INSERT-FIXUP moves z one level up the treap, O(h) iterations at most
    each iteration performs constant number of rotations, which takes O(1)
    O(h) + O(h) = O(h) in total
    as the expected height of a treap is O(lgn), insertion takes time O(lgn) on average
    if insertion takes time o(lgn), n keys can be sorted in time o(nlgn), thereby insertion is Ω(lgn) and Θ(lgn)
e.  x is initially a leaf to be inserted into the treap, C + D = 0
    after a right rotation at x.p when x = x.p.right
        x.p is added to the left spine of x's right subtree
        left subtree of x and its right spine unchanged
        C + D incremented by 1
    similary, a right rotation at x.p when x = x.p.left increments C + D by 1
    so C + D equals to the number of rotations performed
f.  =>: as y is in the left subtree of x, x.priority < y.priority, x.key > y.key
        as y is in the right spine of the subtree
        each z that y.key < z.key < x.key is in right subtree of y
        otherwise it will be traversed inorder before y or after x
        therefore x.priority > y.priority
    <=: as y.priority > x.priority and y.key < x.key,
        either y is in the left subtree of x or y is in the left subtree of some left ancestor of x
        then this left ancestor satisfies y.key < z.key < x.key, y.priority < z.priority
        y cannot be in the left subtree of z, so y must be in the left subtree of x
        if y is not in the right spine of the left subtree of x
        there must be some ancestor z of y that y.key < z.key < x.key
        which implies y.priority < z.priority, in contradiction to z being y's ancestor
        therefore y must be in the right spine of the left subtree of x
g.  the problem implicitly assumes k > i or (k - i - 1)! is not well defined
    as priority is randomly chosen, independent from the key
    considering only the order of these priorities, mapping from set of random numbers to {1 .. n}
    for each z that y.key < z.key < x.key, y.priority_order < z.priority_order
    there are k-i-1 such z, k-i+1 with x and y
    there are (k-i+1)! possible permutations of their priority orders
    only those in which x.priority is the minimum and y.priority is the second smallest is valid
    so only (k-i-1)! permutations satisfies the conditions in part f
    Pr{Xik = 1} = (k-i-1)! / (k-i+1)! = 1/(k-i+1)(k-i)
h.  for x and k = x.key,
        E[C]    = E[Σ(i = {1 .. k-1})Xik]
                = Σ(i = {1 .. k-1})E[Xik]
                = Σ(i = {1 .. k-1})(1 / (k-i+1)(k-i))
                = Σ(i = {1 .. k-1})(1 / i(i+1))
                = Σ(i = {1 .. k-1})(1/i - 1/(i+1))
                = 1 - 1/k
i.  for y != x, k = x.key, i = y.key, define
        Yik = I{y is in the left spine of the right subtree of x}
    then Yik = 1 iff y.priority > x.priority, y.key > x.key and 
    for every z such that x.key < z.key < y.key, y.priority > z.priority
        Yik = 0 if i <= k, otherwise
        Pr{Yik = 1} = 1/(k-i+1)(k-i)
        E[D]    = E[Σ(i = {k+1 .. n})E[Yik]]
                = 1 - 1/(n - k + 1)
j.  E[C + D]    = E[C] + E[D]
                = 2 - O(1) <= 2

Chapter 14
14.1-1
26, 12 + 1 = 13 > 10, go left
17, 7 + 1 = 8 < 10, go right, 10 - 8 = 2
21, 2 + 1 = 3 > 2, go left
19, 0 + 1 = 1 < 2, go right, 2 - 1 = 1
20, 0 + 1 = 1, selected node 20

14.1-2
35, r = 1, not root, go up
38, not root, go up
30, 1 + 1 = 2, r = 1 + 2 = 3, not root, go up
41, not root, go up
26, 12 + 1 = 13, r = 3 + 13 = 16, root, terminate

14.1-3
./CLRS/collection/augmented-redblack-tree.ts#select

14.1-4
traverse the tree from root to find where a node with key k is or will be inserted into the tree
while counting the number of elements smaller than k in sibling subtree and up during the course
if followed a right edge, add x.left.size + 1 to the counter
terminate when x = T.nil
./CLRS/collection/augmented-redblack-tree.ts#keyRank

14.1-5
find the order k of node x, find the node with order k+i
overall running time O(lgn) + O(lgn) = O(lgn)

14.1-6
insertion:
    if z is inserted into the left subtree of x, x.rank += 1
    otherwise x.rank won't change, x.rank is the local rank in the subtree rooted at x, not global rank
deletion:
    traverse from the deleted node p = y to the root
    if followed a left edge, decrement p.rank by 1
rotation:
    in left rotation, x.rank is unchanged
    y.rank changes from size of β to the size of α, β and x combined
    which is x.rank + y.rank + 1
    right rotation is symmetric

14.1-7
for an array [x1 .. xn], assume x1 .. xn-1 is already inserted into the OS tree
after insertion of xn, the rank of xn is 1 + the number of elements smaller than xn in x1 .. xn-1
each element in x1 .. xn-1 that is greater than xn causes an inversion
therefore the total number of inversions can be counted by inserting x1 .. xn that:
    after insertion of xi, get the current rank ri of xi, add (i - ri - 1) to the counter
then after insertion of xn, the counter is the number of inversions in the array
each iteration performs one insertion, one search and one OS-RANK, all of them O(lgn), O(lgn) in total
overall running time O(nlgn)

14.1-8
each chord has two end points, give them keys as their angular coordinate in polar system
for a chord c let c.x be the smaller between the two end points, c.y be the greater
two chords c and d, assuming d.y > c.y, can be one of three cases:
    1.  d.x > c.y > c.x, no intersection, both c.y and c.x smaller than d.x
    2.  c.y > d.x > c.x, intersection, c.x smaller than d.x but not c.y
    3.  c.y > c.x > d.x, no intersection, both c.x and c.y greater than d.x
thereby if two OS trees for both .x and .y are maintained
the number of intersections will be the difference between number of keys smaller than d.x in the two trees
sort the chords according to c.y, for each chord c and two OS trees T1 and T2:
    insert c.x into T1, get rx the rank of c.x in T1
    use OS-KEY-RANK(T2, c.x) from problem 14.1-4 to find ry the rank of c.x in T2
    insert c.y into T2
    add rx - ry to a global counter cnt
at the end the global counter cnt holds the number of intersections
initial sorting O(nlgn)
constant number of insertion, rank operation and search are performed each iteration, O(lgn) in total
overall running time O(nlgn)

14.2-1
minimum in subtree rooted at x can be calculated as the minimum of left subtree or x itself
maximum in subtree rooted at x can be calculated as the maximum of right subtree or x itself
both easily maintained as described in the text
insertion or deletion at most changes the predecessor and successor of two nodes
during first stage of insertion of a node z:
    if z end up as the left child of a node p
        set p.predecessor.successor = z (if p.predecessor exists)
        set z.predecessor = p.predecessor (same condition as above)
        set z.successor = p
        set p.predecessor = z
    if z end up as the right child of a node p
        set p.successor.predecessor = z (same condition)
        set z.successor = p.successor
        set z.predecessor = p;
        set p.successor = z;
during first stage of deletion, assume a node z is removed:
    z.predecessor.successor = z.successor
    z.successor.predecessor = z.predecessor
rotation will not change the nodes stored in a tree, thereby will not change predecessor or successor of any node
all operations take constant time, fixing predecessors and successors will not propagate to ancestors

14.2-2
since second stage of insertion and deleteion changes color of some nodes
it cannot be simply maintained by method described in the text
but the black height still can be maintained without changing the asymptotic performance of set operations
the first stage of insertion will not change black height of any node, as the new node z is colored red
in case 1 of RB-INESRT-FIXUP:
    bh of A and D incremented by 1
in case 2 of RB-INSERT-FIXUP:
    no bh change
in case 3 of RB-INSERT-FIXUP:
    bh of C decremented by 1
    bh of B incremented by 1
the first stage will not change bh of any node if x has an extra black
in case 1 of RB-DELETE-FIXUP:
    decrement bh(B)
in case 2 of RB-DELETE-FIXUP:
    decrement bh(A)
    decrement bh(D)
in case 3 of RB-DELETE-FIXUP:
    increment bh(C)
    decrement bh(D)
in case 4 of RB-DELETE-FiXUP:
    decrement bh(A)
    increment bh(E)
when x = T.root, the extra black is removed, bh(T.root) stays intact

14.2-3
the inorder listing of subtree rooted at x is
    xl1.a * .. * xml.a * x.a * x xr1.a  * .. * xrm.a
where 
    xl1.a .. xml.a is the inorder listing of its left subtree
    xr1.a .. xrm.a is the inorder listing of its right subtree
as the operation is associative, x.left.f * x.a * x.right.f = x.f
set x.a = 1 for all node x, * = +, this scheme calculates the size of the subtree

14.2-4
use 14.2-1, add pointers to successor and predecessor to nodes, set operations still O(lgn)
search for the initial key a is O(lgn)
traversing m successors between b and a is O(m)
O(m + lgn) in total

14.3-1
abstract over all augmented red black trees
./CLRS/collection/augmented-redblack-tree.ts#leftRotate

14.3-2
the meaning of overlapping changes a little bit, from
    !(a.high < b.low || b.high < a.low)
to
    !(a.high <= b.low || b.high <= a.low)
then change line 3 of INTERVAL-SEARCH to
    if x.left !== T.nil and x.left.max > i.low
theorem 14.2 still holds since:
    if line 5 is executed, x.left.max <= i.low    
    by definition of overlapping of open intervals, no intervals in left subtree of x will overlap with i
    if line 4 is executed, x.left.max > i.low
    if no interval in left subtree overlaps with i, there must be some i' that
        i'.high = x.left.max > i.low
        i'.low >= i.high
    then for every interval i'' in right subtree of x,
        i''.low >= i'.low >= i.high
    so no intervals in right subtree of x overlaps with i

14.3-3
basically INTERVAL-SEARCH, but not terminate on finding an overlapping intervals
but keeps track of the interval that overlaps with i which has minimal low end point
only terminates when x == T.nil
by theorem 14.2, either 
    1.  there are no overlapping intervals in left subtree (line 5), or
    2.  there are overlapping intervals in left subtree and possibly right subtree too (line 4)
in case 2, to find the overlapping interval with minimal low end point, x should be x.left in next iteration
./CLRS/collection/interval-tree.ts#minimalOverlappingInterval

14.3-4
thanks http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/tut2.pdf
do INTERVAL-SEARCH(i), delete the node returned, do next INTERVAL-SEARCH(i)
re-insert all the nodes back to the tree when INTERVAL-SEARCH returns null
do this at most n/lgn times, then re-insert removed nodes and traverse all the nodes instead
running time = O(klgn) when k < n/lgn, O(n) when k >= n/lgn, O(min(n, klgn)) overall

14.3-5
when comparing two intervals, compare their high end points if their low end points equal
by searching in this new ordering, the exact key (interval) can be find in time O(lgn)
namely
    le(a: Interval, b: Interval): boolean {
        if (a.low === b.low) {
            return a.high <= b.high;
        } else {
            return a.low <= b.low;
        }
    }
    eq(a: Interval, b: Interval): boolean {
        return a.low === b.low && a.high === b.high;
    }
the normal search operation then works like the definition of INTERVAL-SEARCH-EXACTLY

14.3-6
store:
    minimum in the subtree rooted at x
    maximum in the subtree rooted at x
    min gap in the subtree rooted at x
in the node x alongside the key
by search tree property, left subtree of x contains number smaller than x, right subtree of x contains number greater
the min gap is one of:
    the min gap of the left subtree
    the min gap of the right subtree
    x.key - x.left.maximum 
    x.right.minimum - x.key
all the three additional fields can be calculated from information in current node, left and right children in O(1)

14.3-7
assume a rectangle is a structure like
    interface Rect {
        vert: Interval;
        horiz: Interval;
    }
    interface Interval {
        low: number;
        high: number;
    }
define a new structure RectNode that 
    interface RectNode {
        rect: Rect;
        node: IntNode;
    }
where IntNode is node in an interval tree
make two copies of points to all RectNodes rn, name them L and H
sort L according to rn.rect.horiz.low, sort H according to rn.horiz.vert.high
create an initially empty interval tree T that stores vertical intervals of rectangles
until both L and H is empty:
    compare the top of L and H
    if rn the top (minimum) of L is smaller, pop it from the set L
        search intervals overlaps rn.rect.vert in T, if exists terminate and return true
        insert rn.rect.vert into T, assign the inserted node to rn.node
    if rn the top of H is smaller of L is empty, pop it from the set H
        as rn.rect.horiz.low <= rn.rect.horiz.high, rn.node always points to an existing node
        delete rn.node from T
any time during the procedure, T contains vertical intervals of rectangles that overlap with a certain horizontal line
as they already overlaps horizontally, if they overlaps vertically, the two rectangles overlaps

14-1
a.  assume the point of maximum overlap overlaps with a set of intervals {ik}
    let p = max{ik.low}, it must have:
        p <= ik.high for all k
    otherwise the interval with low end point p will not overlap with ik, hence do not contain a common point
    then p overlaps with all intervals in {ik}, p is a point of maximum overlap
b.  if an interval i does not overlap with a point p, then
        i.low > p or i.high < p
    assuming distinct end points, additionally store:
        size_low, number of low end points in subtree
        size_high, number of high end points in subtree
        pom, point of maximum overlap in subtree
        non_overlap, number of intervals that may not overlap with point of maximum in subtree
    calculating size_low and size_high from children and current node is strightforward
    for a node x, compare x.left.size_high + x.right.non_overlap and x.right.size_low + x.left.non_overlap
    pom of subtree rooted at x may be one of:
        x.left.pom, not overlapping with x.left.non_overlap + x.right.low_size intervals
        x.right.pom, not overlapping with x.right.non_overlap + x.left.high_size intervals
        x.key, not overlapping with x.left.high_size + x.right.low_size intervals
    update x.pom and x.non_overlap accordingly, T.root then contains pom of the whole tree

14-2
a.  put all numbers in a circular doubly linked list in sorted order
    starting from x.key = 1, repeat the procedure below until no node remains in the linked list:
        1.  follow x = x.next m times
        2.  delete node pointed by x.prev and yield its key
    since m is constant, step 1 takes O(1)
    step 2 also O(1) as deletion of a node in doubly linked list is O(1)
    the procedure runs for at most n iterations until the linked list is empty, hence O(n) total running time
b.  rank starting from 0 for convinence here
    if the previously removed node has rank i, there are k numbers not removed yet
    node with rank i + m - 1 mod k should be removed next
    [1, 2, 3, 4, 5, 6, 7]
    remove 3 with rank 2
    [1, 2, 4, 5, 6, 7]
    2 + 3 - 1 mod 6 = 4
    remove 6 with rank 4
    [1, 2, 4, 5, 7]
    4 + 3 - 1 mod 5 = 1
    remove 2 with rank 1
    [1, 4, 5, 7]
    1 + 3 - 1 mod 4 = 3
    remove 7 with rank 3
    [1, 4, 5]
    3 + 3 - 1 mod 3 = 2
    remove 5 with rank 2
    [1, 4]
    2 + 3 - 1 mod 2 = 0
    remove 1 with rank 0
    [4]
    0 + 3 - 1 mod 1 = 0
    remove 4 with rank 0
    using an order statistic tree, deletion and searching by rank both O(lgn)
    O(nlgn) in total
    ./CLRS/collection/augmented-rebblack-tree.ts#josephus

Chapter 15
15.1-1
T(0) = 1
assume T(k) = 2^k for all k <= n
T(n+1)  = 1 + Σ(j = {0 .. n})T(j)
        = 1 + Σ(j = {0 .. n})2^j
        = 1 + 2^(n+1) - 1
        = 2^(n+1)

15.1-2
let price table be [0, 0, 4, 5] for rod length 0 ~ 3
let n = 3
greedy method will first cut a piece of length 2, which has the highest price density 4/2 = 2
but then the remaining rod with length 1 is valueless, total price 4
the optimal solution will not cut the rod, total price 5

15.1-3
./CLRS/technique/rod.ts#cutRodCC

15.1-4
./CLRS/technique/rod.ts#extendedMemoizedCutRod

15.1-5
problem n depends on problem n-1 and problem n-2
./CLRS/technique/rod.ts#fibonacci

15.2-1
2010 '((A0A1)((A2A3)(A4A5)))'

15.2-2
./CLRS/technique/matrix-chain.ts#matrixChainMultiply

15.2-3
assume P(k) >= 2^n for k < n
    P(n)    >= Σ(k = {1 .. n-1})c2^(n-k)
            = Σ(k = {1 .. n-1})2^n
            = (n-1)2^n
            >= 2^n
therefore P(n) = Ω(2^n)

15.2-4
the subproblem (i, j) refers solutions to subproblems (i, k) and (k+1, j) for all i <= k < j
there are j - i such k and vertex (i, j) has degree 2(j-i)
vertices on the same diagonal has the same j - i, which ranges from 0 to n-1
each diagonal contains n - (j - i) vertices
    Σ(k = {0 .. n-1})2k(n-k)
    = 2nΣ(k = {0 .. n-1})k - 2Σ(k = {0 .. n-1})k^2
    = 2n * n(n-1) / 2 - 2(n-1)n(2n-1) / 6
    = n^2(n-1) - n(n-1)(2n-1)/3
    = n(n-1)(3n - 2n + 1)/3
    = n(n-1)(n+1)/3
    = n(n^2 - 1)/3
    = (n^3 - n)/3
edges in total
C(n, 2) + n = n(n-1) + n = n^2 vertices in total

15.2-5
equals to the number of edges in the subproblem graph

15.2-6
the number of pairs of parentheses as a function of the number of matrices is
    P(1) = 0
    P(n) = P(k) + P(n-k) + 1
P(n) = n - 1 easily proved by induction

15.3-1
RECURSIVE-MATRIX-CHAIN enumerates fewer results than a complete enumeration
instead all subproblem of Ai..k and Ak+1..j, only the optimal subproblem is returned
but a complete enumeration performs one comparsion for each possible parenthesization
while RECURSIVE-MATRIX-CHAIN performs many comparsion for each subproblem
enumerating all possibilities is at least Ω(4^n / n^(3/2)), the Catalan numbers
redo 15.8 with different setting
line 1-2, line 5 and line 6-7 takes at most constant time, so
    T(1)    <= c
    T(n)    <= c + Σ(k = {1 .. n-1})(T(k) + T(n-k) + c)
            = c + c(n-1) + 2Σ(k = {1 .. n-1})T(k)
            = cn + 2Σ(k = {1 .. n-1})T(k)
thanks Instructor's Manual
assume T(n) <= cn3^(n-1)
    T(n)    <= cn + 2Σ(k = {1 .. n-1})(ck3^(k-1))
            = c(n + 2Σ(ck3^(k-1)))
            = c(2(n3^(n-1) / (3 - 1) + (1 - 3^n) / (3 - 1)^n) + n)
            = cn3^(n-1) + c(2n + 1 - 3^n)/2
            <= cn3^(n-1)
T(n) = O(n3^(n-1)), RECURSIVE-MATRIX-CHAIN is more efficient than complete enumeration

15.3-2
subproblems in merge sort do not overlap
each subproblem is solved only once, hence memoization won't improve the performance

15.3-3
this problem do has optimal substructure
the subproblems are independent and overlaps

15.3-4
when i and j fixed, minimizing pi-1pkpj equivalent to minimizing pk
for p = [30, 20, 10, 1]
    greedy method returns (A0A1)A2, 6300 multiplications
    optimal solution is A0(A1A2), 800 multiplications

15.3-5
the subproblems no longer overlap as much as in 15.1
a subproblem now is described by the length of the rod and the the remaining allowance of length i rods
for example the two subproblem:
    1.  a length 3 rod after cutting length 3 from a length 6 rod
    2.  a length 3 rod after cutting length 1 from a length 4 rod
are no longer the same subproblem

15.3-6
consider currencies as vertices in a graph and rij be directed edges
the problem only makes sence when:
    for any cycle in the graph, the production of rij along the cycle equals to 1
    (especially rij = 1/rji for all i and j)
otherwise looping in the cycle will result in infinite profit 
thereby for each optimal solution i ~> j, there must be a equally optimal solution i ~> j that is a simple path
the problem space then is bounded and has size O(n^2)
problem can be solved by 
    for each pair (i, j)
    finding the optimal exchange rate with at most k trades for k ranges from 0 to n-1
for at most k+1 trades, search each vertex v that
    has an optimal exchange rate riv[k] with at most k trades from i to v 
    there is an edge rvj from v to j
then the maximum of riv[k] * rvj is the solution for at most k+1 trades from i to j
only one subproblem each iteration, trivially independent
subproblems overlap a lot with each other as optimal solutions for all i and j is computed
when ck != 0, the argument above no longer holds
optimal solution of size k+1 does not necessary come from optimal solutions of size k
when the gap between ck and ck+1 is huge
for k+1 trades it may need to choose a less optimal solution of size k which makes fewer trades
instead of an optimal k-trade solution that uses exactly k trades

15.4-1
100110

15.4-2
./CLRS/technique/substring.ts#constrSubstring

15.4-3
./CLRS/technique/substring.ts#memoizedLcs

15.4-4
only the case with min(m, n) entries and O(1) extra space
./CLRS/technique/substring.ts#linearSpaceLcs

15.4-5
let Y be X in sorted order, run LCS with X and Y
total running time O(nlgn) + O(n^2) = O(n^2)

15.4-6
thanks https://www.csie.ntu.edu.tw/~kmchao/seq06fall/dp.pdf
maintain a list best_end while traversing the elements in input array A
right before examining element A[i]
best_end[k] contains the minimum among last element of increasing subsequences of length k in A[1 .. i-1]
then best_end must be in sorted order:
    if increasing subsequences of length k in A[1 .. j] all end with an element >= p
    then no increasing subsequences of length k+1 can have last element <= p
    since the first k elements in the k+1-element sequence is, a k-element increasing subsequence
    so kth p <= kth element <= k+1th element as the increasing subsequence is in increasing order
then A[i] may update best_end in two ways:
    if A[i] >= any element in best_end, for all best_end[k] currently defined
        appending A[i] to any of the subsequences of length k will not change best_end[k+1], as best_end[k+1] <= A[i]
        for max_k the maximum such k, best_end[max_k + 1] is not defined so far
        which means no increasing subsequence of length max_k + 1 exists in A[1 .. i-1]
        then appending A[i] to any of the max_k-element subsequences will update best_end[max_k + 1] to A[i]
    if A[i] < best_end[k] for some k
        let min_k be the minimal such k that best_end[k] > A[i]
        for all k < min_k, as the same argument above, best_end[k] will not be updated by A[i]
        by appending A[i] to one of the increasing sequence of length min_k - 1
        (possible since best_end[min_k - 1] <= A[i])
        best_end[min_k] can be updated to A[i]
        for sequences of length k >= min_k, A[i] can be appended to no sequences, hence updates no best_end[k]
        as best_end[k] > A[i] and best_end[k] stores the minimal last element among all sequences
as max_k is just the current length of best_end[k], min_k can be find in O(lgn) time by binary search
each iteration above runs in time O(lgn), O(nlgn) in total
as best_end[k] records the minimum among last element of increasing subsequences of length k
the greatest k defined then is the length of the longest increasing subsequence of A
the last element of which is stored in corresponding best_end[k]
maintain another array prev that starts with all entries = null
    when A[i] updates best_end[k], set prev[A[i]] = best_end[k-1]
    (best_end[0] set to some null value)
invariant:
    for i = best_end[k]
    traversing prev[i] -> prev[prev[i]] -> .. before null value yields a k-element decreasing subsequence of A
initialization:
    all entries of prev = null, best_end has length 0, invariant trivially satisfied
maintenance:
    prev[best_end[k-1]] has been set already, as the only way best_end grows is appending to the end
    then prev[best_end[k-1]] points to a k-1 element decreasing subsequence of A
    as A[i] replaces best_end[k] by appending to the increasing sequence indicated by best_end[k-1]
    prepending the decreasing sequence with A[i] yields a decreasing sequence of length k
termination:
    let best_end[k] be the content of the last entry of best_end
    traversing prev[best_end[k]] then yields a length k decreasing subsequence of A
    reverse of this sequence then is the longest increasing subsequence of A
even if the numbers are large, prev can be a efficient dynamic set and operations on it O(lgn)
overall running time still O(nlgn) 

15.5-1
./CLRS/technique/optimal-bst.ts#constructOptimalBST

15.5-2
3.12
  ┌─d7
┌─k7
│ │ ┌─d6
│ └─k6
│   └─d5
k5
│     ┌─d4
│   ┌─k4
│   │ └─d3
│ ┌─k3
│ │ └─d2
└─k2
  │ ┌─d1
  └─k1
    └─d0

15.5-3
computation of w[i, j] takes time j - i + 1 = O(n)
it's computed for at most O(n^2) times, O(n^3) in total
the asymptotic running time will not change

15.5-4
initially root[i][i] = i for all 1 <= i <= n
starting with l = 1, j = i + l for all i
    root[i][i] <= root[i][i+1] <= root[i+1][i+1]
    i <= root[i][i+1] <= i+1
if for l >= 1, root[i][i+l] is in sorted order, root[i][i+l] <= root[i+1][i+l+1] for all i
    root[i][i+l] <= root[i][i+l+1] <= root[i+1][l+l+1]
    root[i+1][i+l+1] <= root[i+1][i+l+2] <= root[i+2][i+l+2]
the next diagonal in the table (as shown in figure 15.10) is also in sorted order
sets of possible candidates for root[i][i+l+1] and root[i+1][i+l+2] only overlaps at end points (at most one)
fix l, the number of candidates for each root[i][i+l] combined is at most n + entries on the last diagonal = O(n)
as l = 1 to n, O(n^2) in total
as least Ω(n^2) entries are filled in three tables, overall running time Θ(n^2)

15-1
topologically sort the vertices
remove all the vertices before s (won't appear in a path s ~> t) and after t (similar) 
if an edge (s, u) exists, solve subproblem u ~> t, add w(s, u) to the solution, take the maxmimum among all such u
a bottom-up solution traverses the sorted list reversely from t to s, solve subproblems of smallest size first
topological sort is O(|V| + |E|), removing nodes O(|V|)
the graph is naturally a subproblem graph, subproblems are referred to at most O(|E|) times
ovarall time complexity O(|V| + |E|)
one subproblem at a time is examined, tivially independent
O(|E|) possible references to O(|V|) subproblems, naturally overlaps


15-2
longest common subsequence of a string and its reverse
O(n^2)

15-3
thanks Instructor's Manual
for a sorted list of points [p1 .. pn]
define a bitonic path Pij as a path that:
    visits all points in {p1 .. pj}
    starts from pi and go strictly left to p1
    then go srtictly right from p1 to pj
let |pipj| be the distance between pi and pj
let b[i, j] be the length of the shortest among all bitonic path Pij
for i < j - 1:
    pj-1 must be on the right going subpath of Pij, preceeding pj
    if Pij is a shortest bitonic path, then its subpath Pij-1 must also be a shortest bitonic path
    otherwise by cut and paste an optimal Pij-1, b[i, j] can be reduced
    b[i, j] = b[i, j-1] + |pj-1pj|
for i = j - 1:
    pi is the first point on the left going subpath of Pij
    the point preceeding pj on the right subpath can be pk for any 1 <= k < j - 1
    again Pkj-1 must be the optimal bitonic path or cut and paste can reduce b[i, j]
    b[j-1, j] = min{b[k, j-1] + |pkpj|} over all k
for i = j:
    the bitonic path starts and ends with pi
    its subpath Pj-1j, for the same reason above, must be a optimal bitonic path
    b[j, j] = b[j-1, j] + |pj-1, pj|
at the end b[n, n] is the length of the optimal bitonic path pn ~> p1 ~> pn, which is also a bitonic tour
by recording each minimal choice of k during the computation, the bitonic tour can be reconstructed
r[i][j] stores k that pk preceeding pj on an optimal bitonic path i ~> 1 ~> j, where i < j
if i > j, the optimal bitonic path is Pji, r[j][i] stores k that preceeding pi 
as initially i = n-1, j = n, if i > j then pk is on the left going subpath of Pn-1n, should be printed later
./CLRS/technique/bitonic-tour.ts

15-4
denote the length of ith word by li for 1 <= i <= n
define ws[i, j] as the number of white spaces at the end of of line with word i to j
    ws[i, j]    = M - (Σ(k = {i .. j})li + j - i)
                = ws[i, j-1] - (lj + 1)
so ws[i, j] can be computed in time O(n^2)
if words from i to j do not fit in a line, ws[i, j] will be negative
define the whitespace cost wc[i, j] for a line containing words i to j as:
    wc[i, j]    = +∞ if ws[i, j] < 0, words i to j will not fit in a line
                = 0 if ws[i, j] >= 0 and j = n, this is the last line, contributes nothing to the cost
                = ws[i, j]^3 otherwise
define the total cost cost[j] as the optimal cost of words 1 to j
if the last line starts with kth word
then the paragraph without the last line (which contains words k to j) has optimal cost[k-1]
otherwise cut and paste will reduce cost[j]
the optimal solution then is min{cost[k-1] + wc[k, j]} over all k < j
assume each word no longer than M, there is always an finite solution
if wc[k, j] = ∞ the solution will never be chosen
by recording optimal choice of k, the paragraph can be reconstructed
both O(n^2) in time and space
./CLRS/technique/print-neatly.ts

15-5
a.  a recursive solution is to work backwards from z = y
    as the operations never backtracks or decrements i or j
    once i > i* and j > j*, x[1 .. i*] can not be examined, z[1 .. j*] cannot be edited
    whenever the operations increments j, it must have z[j] == y[j]
    hence only a few operations are legal for each step
        only when x[i] == y[j], a copy operation can be performed
        delete operation only possible when i <= x.length
        replace operation may only replace z[j] = y[j]
        insert operation may only insert z[j] = y[j]
        twiddle operation only possible when x[i] == y[j+1] && x[i+1] == y[j]
        kill operation only possible when z = y
    thereby subproblems are defined solely by i and j, it's unnecessary to maintain z in subproblems or anywhere
    by determining the possible last operation and work backwards
        only when x[i-1] = y[j-1], i-1 >= 1, j-1 >= 1, the last operation may be copy
        only when i >= 2, the last operation may be delete
        only when i >= 2 and j >= 2, the last operation may be replace
        only when j >= 2, the last operation may be insert
        only when x[i-1] = y[j-2], x[i-2] = y[j-1] i >= 3, j >= 3, the last operation may be twiddle
        only when j = y.length + 1 and i = x.length + 1, the last operation may be kill
    each operation at least increments one of i or j
    current problem can then be solved by optimal solutions of subproblems
    ./CLRS/technique/edit-distance.ts
b.  a space in y is equivalent to delete operation (j incremented but i not)
    a space in x is equivalent to insert operation (i incremented but j not)
    then the problem can be casted to edit distance problems with operations:
        insert:     cost +2
        delete:     cost +2
        replace:    cost +1
        copy:       cost -1
    the inverse of the cost is then the score of the two sequence

15-6
define two subproblem for each subtree rooted at x for some node x
    1.  maximizes the sum of conviviality with the node x
    2.  maximizes the sum of conviviality without the node x
then for a node x and its children y1 .. yk,
    the solution to problem 1 is the sum of subproblem 2 of its children
    the solution to problem 2 is the sum of the greater between solution to subproblem 1 and 2 of its chilren
whether a guest should join the party can be solely defined top-down by 
    whether the immediate supervisor joins the party
    a comparsion between solutions to subproblem 1 and 2 
complexity proportional to the number of edges, O(n) in total

15-7
a.  bfs doesn't apply here as the path may not be simple
    define subproblem (v, j) as if there is a path labeled <σj, σj+1 .. σk> starting from vertex v
    starting from vertex s, for each edge (s, u) with label σi = σ1:
        solve the subproblem (u, i+1)
        choose randomly from all vertices u which (u, 2) has a solution, record the choice
    total running time O(n|E|) as each subproblem traverses at most |E| edges
b.  store not only a boolean, but also the probability of the subproblem
    record the u which (u, i+1) yields maximal probability instead of a random one
    still O(n|E|) as real number operations take unit time

15-8
a.  let n = 2
    no matter which pixel is chosen on the last row
    the pixel removed can be either the first or the second pixel on each row
    2^m possibilities in total
    for n > 2, each possibility in the case n = 2 still valid, O(2^m) possibilities
b.  define subproblem (i, j) as the minimal disruption seam starting from row i and column j
    when i = m, optimal solution to subproblem (i, j) is simply d(i, j)
    when i < m, the optimal solution is d(i, j) plus the minimum among optimal solutions to
        (i+1, j-1), (i+1, j) and (i+1, j+1)
    if j-1 or j+1 falls within the bound
    the problem then easily solved by a bottom up algorithm starting with i = m and goes upwards
    mn subproblems, each refers to at most 3 subproblems, O(mn) in total

15-9
complexity of this problem only depends on the length of the list of break points, not n the length of string S
add the two end points (1 and n) to the list of break point bp, sort the list
assume bp has length m, define subproblem (i, j) as optimally break the substring S[bp[i], bp[j]] according to bp
the original problem is then (1, m) and for all 1 < k < m:
    the solution to (1, m) is the minimal of solutions to (1, k) + (k, m) over all k plus bp[m] - bp[1] + 1
and all subproblems can be solved similarly
problems (i, i+1) has no break point between, solution is 0
the optimal way to break the string then can be reconstructed from choices of k
total number of solution is O(m^2), each refers m subproblems at most, O(m^3) in total
./CLRS/technique/break-string.ts

15-10
it not clear if the fee is deducted at the end of the 10 year period or deducted each year
if it's deducted each year, it must be be deducted from the money in one or more investment
which does no match quite well with the description "leave the money made in the previous year in the same investments"
so assume the fee is deducted from the profits only at the end of the 10 year period
a.  assume in a span from year i to year j, money is not moved among investments
    assume two investments s and t in the portfolio and a total amount of q dollars invested in them, qs and qt each
    the combined return equals to
        qsΠ(1 + rks) + qtΠ(1 + rkt)
    all rates fixed, it must have
        1.  Π(k = {i .. j})(1 + rks) >= Π(k = {i .. j})(1 + rkt), or
        2.  Π(k = {i .. j})(1 + rks) <= Π(k = {i .. j})(1 + rkt)
    in case 1, invest all the q dollars in investment s will not decrease the profit
    in case 2, invest all the q dollars in investment t will not decrease the profit
    in an optimal solution, the 10 years can be divided into spans in which money is not moved
    therefore there is also an optimal solution that only invests in a single investment each year
b.  define the subproblem (i, s) as 
        the return of investment strategy from year i if the investment in year i-1 is s
    then optimal solution to (i, s) can be derived from
        let k range over all investments
        get solutions to all subproblem (i+1, k)
        if k = s, deduct f1 from the solution; otherwise deduct f2
        take the maximum among all choice of k, record the choice
    //  the problem does not seem to have optimal substructure
    //  the optimal choice of k also depends on the total return of previous investments
    //  for example, if f2 - f1 = 10000 and the total amount of money at the start of year i is q = 20000 dollars
    //  no return rate <= 50% worths the additional fee charged
    //  but if q = 200000, a return rate of 5% will suffice
    //  the subproblems is defined also by the total amount of money can be invested in this year
    //  then the space of problem is enormous
c.  as the number of years is a constant 10
    it's possible to do a complete enumeration of arrangements about move investments or not at the end of each year
    2^9 = 512 different arrangements in total, which is a constant
    then the fee paid for each arrangement is a constant and can be ignored
    each arrangement can be solved in time O(n^3), where n is the total amount of investments, with scheme in part b
d.  at some point, the total aount of dollars at the start of year i exceeds 15000
    then the same argument in part a no longer applies

15-11
define subproblem (i, j) as
    the cost to satisfy demand from month i with j machines in the inventory
obviously at any point during the optimal strategy, the inventory should never exceed D
or the total demand can be satisfied solely with inventories and it's a waste to produce and store even one more unit
then optimal solution to (i, j) can be derived from
    calculate cost ck to produce k machines so k + j - di ranges from 0 to D, di - j <= k <= D + di - j
    get optimal solution to (i+1, k + j - di), plus ck
    minimize the total cost over all choices of k
there are at most nD subproblems, each refers to at most D subproblems
O(nD^2) in total

15-12
define subproblem (n, x) as
    maximize VORP from position n with x dollars
similar to any other dynamic problemming problem in this chapter
NX/1000000 subproblems, each refers to P subproblems at most, O(NXP) overall
why a greedy algorithm doesn't work:
let N = 2 with three players:
    1.  in position 1, VORP 10, sign $10
    2.  in position 1, VORP 6, sign $9
    3.  in position 2, VORP 6, sign $9
with an allowance $18, the optimal solution signs player 2 and 3
greedy solution signs player 1 first, as $10 / 10 is the highest VORP density

Chapter 16
16.1-1
./CLRS/technique/activity-selection.ts#activitySelection

16.1-2
optimal substructure still applies
let subproblem Sk = {ai ∈ S : fi <= sk}, Ak ⊆ Sk be an optimal solution to Sk
let am be the activity with greatest start time in Sk
let aj be the activity with greatest start time in Ak
as activities in Ak is mutually compatible, for all ai ∈ Ak, i != j
    f[ai] <= s[aj] <= s[am]
then Ak - {aj} ∪ {am} is still mutually compatible
as |Ak - {aj} ∪ {am}| = |Ak|, this is also an optimal solution to Sk
so a greedy strategy that always select the compatible activity with last start time also yields optimal solution

16.1-3
least duration:
        1   2   3
    s   1   9   10
    f   10  11  20
    if the activity of least duration is selected, activity 2 will be selected first
    then any other activity is not compatible with it, solution has size 1
    the optimal solution of size 2 contains activities 1 and 2
fewest overlaps:
        1   2   3   4   5   6   7   8   9   10  11
    s   1   2   2   2   3   4   5   6   6   6   7
    f   3   4   4   4   5   6   7   8   8   8   9
    solution by fewest overlaps: {a2, a6, a8}
    optimal solution: {a1, a5, a7, a11}
earliest start time:
        1   2   3
    s   1   2   3
    f   4   3   4
    solution by earliest start time: {a1}
    optimal solution: {a2, a3}

16.1-4
basically the same to half the problem 14.3-7, scan the start and finish times altogether in increasing order
assume an activity is represented by an object:
interface Act {
    s: number;
    f: number;
    room?: number;
}
make two copies of pointers to object representing activities S and F
sort S according to start time, F according to finish time
maintain a counter k and two index i, j all starting from 0
if S[i].s > F[j].f
    increment k
    assign k to S[i].room
    increment i
if F[j].f >= S[i].s
    decrement k
virtually equivalent to moving a point from past to future on the timeline
k counts the number of activities overlap the point
it's greedy in the sense that the activity is always assigned to the first (i.e. with smallest index) empty room

16.1-5
solution to 16.1-1 can be modified so instead of
    c[i, j] = c[i, k] + c[k, j] + 1
now it compute and maximize
    c[i, j] = c[i, k] + c[k, j] + vk

16.2-1
assume in an optimal solution, w of wm pounds of the item with highest value density is not in the knapsack
replace w pounds of anything in the knapsack with it will yield a equal or better solution
if the capacity W is higher than the total weight of the item with highest value density
all of it should be put into the knapsack and it is now a subproblem with W - wm capacity and all other items

16.2-2
define a subproblem (i, j) as
    maximize the total value from items i to n with capacity j
then the solution to (i, j) is the best among:
    1.  when wi <= j, take item i, add vi to the optimal solution of subproblem (i+1, j - wi)
    2.  do not take item i, exactly the optimal solution to subproblem (i+1, j)
at most nW subproblem that each refers to 2 subproblems at most, O(nW) in total
./CLRS/technique/knapsack.ts#knapsack

16.2-3
let I be the list of items sorted according to their weight
consider a solution which contains I[j] but not I[i], i < j
then it's always safe to replace I[j] by I[i] as the I is sorted by weight, I[i].weight <= I[j].weight
the replacement also improves the solution since I[i].value >= I[j].value as stated in the problem
therefore one of the optimal solutions must be defined by the largest i that Σ(k = {1 .. i})I[k].weight <= W

16.2-4
as there's no cost for carrying more water, assume the professor always refills 2 liters of water at water stop
define subproblem i as
    minimize the number of water stops starting from water stop k with 2 liters of water
then for i < j, the optimal solution to subproblem i is no better than the optimal solution to subproblem j
either the solution to i didn't refill at water stop j and starts from there with less than 2 liters of water
or it refills water at water stop j thus refills at least one more time than the solution to j
let di be the distance between water stop i and water stop i+1
a greedy choice that refills water at water stop j that's the greatest j such that Σ(k = {i .. j - 1})dk <= m
all other choices yields a subproblem j with smaller or equal j, hence is no better than the greedy choice
all di examined at most once, total running time O(n)

16.2-5
first, there must be an optimal solution in which all left end points of intervals is a point in the set 
assume some interval vi in an optimal solution does not start at a point in the set
if vi covers a set of points {xi .. xj}, it must have
    max{xi .. xj} - min{xi .. xj} <= 1
a unit interval starting at min{xi .. xj} then covers the same set of points or more
thereby it's safe to assume all intervals start at a point in the set 
let X be a sorted array of the points in the set, define subproblem i as:
    minimized the number of unit intervals to cover all points from X[i] to X[n]
the leftmost point X[i] must be the start point of an interval, otherwise it will not be covered by any interval
let j be the first point that's not covered by a unit interval starting at X[i]
the optimal solution is then 1 + an optimal solution to subproblem j
all points in the sorted array is examined at most once, O(n) in total, sorting takes O(nlgn)
overall running time O(nlgn) + O(n) = O(nlgn)

16.2-6
thanks Instructor's Manual
basically binary search on unsorted array with linear time median algorithm
first find the median m of value density (value / weight)
calculate the sum of weights w of items which has value density >= m
if w > W
    no item with value density lower than m should be chosen in an optimal solution
    the solution is then the optimal solution to a subproblem with weight limit W and half the items
if w <= W
    all items with value density higher than m should be included in an optimal solution
    solution is the sum of values of these items + an optimal solution with weight limit W - w and half the items
median, sum, filter and combining results of subproblems all take O(n)
T(n) = T(n/2) + O(n), T(n) = O(n) by master theorem
./CLRS/technique/knapsack.ts#linearKnapsack

16.2-7
take logarithm of the result, since ai > 0 for all i,
    lg(Πai^bi) = Σbilg(ai)
as logarithm function is monotonically increasing
maximizing the product is equivalent to maximizing the sum of logs
assume for two indices i and j, ai <= aj but bi >= bj
    bilg(ai) + bjlg(aj) - bilg(aj) - bjlg(ai)
    = bilg(ai / aj) + bjlg(aj / ai)
    = bilg(ai / aj) - bjlg(ai / aj)
    = (bi - bj)lg(ai / aj)
ai <= aj, lg(ai / aj) <= 0, (bi - bj)lg(ai / aj) <= 0
exchange the position of bi and bj then will improve or yield a equally optimal solution
if {ai} is in sorted order, then arrange {bi} also in sorted order yields an optimal solution

16.3-1
x is the character with least frequency, y is the character with second least frequency
which means x.freq <= y.freq <= a.freq <= b.freq
if x.freq = b.freq, all four characters have the same frequency

16.3-2
if the binary tree is not full, there must be some interal node n which has only one child
without loss of generality, assume the sole child is a left child
then replace n with its left subtree will reduce depth of leaves in the left subtree by 1
the resulting binary tree is still a valid encoding of the same alphabet since no leaf is removed

16.3-3
            ┌─b:1
          ┌─2
          │ └─a:1
        ┌─4
        │ └─c:2
      ┌─7
      │ └─d:3
    ┌─12
    │ └─e:5
  ┌─20
  │ └─f:8
┌─33
│ └─g:13
54
└─h:21
prove Σ(k = {1 .. i})fi < fi+2 by induction
    initially f1 + f2 = 2 < f4 = 3
    assume Σ(k = {1 .. i})fi < fi+2
    Σ(k = {1 .. i+1})   = Σ(k = {1 .. i}) + fi+1
                        < fi+2 + fi+1
                        = fi+3
assume in an alphabet C, character ci has frequency fi, where fi is the ith fibonacci number
initially c1:1 and c2:1 is extracted from the priority queue, an internal node with frequency 2 is inserted
assume in iteration i, the new internal node inserted into the queue has frequency Σ(k = {1 .. i+1})fi
which indicates that it has leaves c1 .. ci+1
as Σ(k = {1 .. i+1})fi < fi+3, the next character merged with the internal node must be ci+2
the resulting binary tree has d(ci) = |C| - i + 1

16.3-4
a leaf x with depth k contributes x.freq to all of its ancestors, and there are exactly k of them
by summing all frequences of internal nodes frequency of each leaf x.freq is added exactly d(x) times
formally define d(x, y) as the depth of x in subtree rooted at y
prove for all leaves in a subtree rooted at y, the sum of frequencies of internal nodes equals Σx.freq * d(x, y)
base case is d(x, x) = 0, the leaf has no internal nodes
inductively, for a node y
    Σx.freq * d(x, y.left) = sum of frequencies of internal nodes in left subtree 
    similarly the right subtree
    the tree rooted at y has one more internal nodes which is y
    y.freq = y.left.freq + y.right.freq, which is the sum of frequencies of all the leaves
    d(x, y) = d(x, y.left) + 1 if x is in the left subtree of y, symmetrically y.right
    Σ(x in left subtree)x.freq * d(x, y.left) + Σ(x in right subtree)x.freq * d(x, y.right) + Σ(x in both subtree)x.freq
    = Σ(x in both subtree)x.freq * d(x, y)
at the root d(x, root) = d(x)

16.3-5
if for ci and cj, i < j, d(ci) >= d(cj)
as frequencies of characters in C is in decreasing order, ci.freq >= cj.freq
by switching the prefix code of ci and cj, the total cost is changed by 
    ci.freq * d(cj) + cj.freq * d(ci) - ci.freq * d(ci) - cj.freq * d(cj)
    = ci.freq(d(cj) - d(ci)) + cj.freq(d(ci) - d(cj))
    = (ci.freq - cj.freq)(d(cj) - d(ci))
    <= 0
so there must be an optimal solution in which code length of C is in increasing order

16.3-6
a huffman tree has n leaves and n - 1 internal nodes, 2n - 1 nodes in total
use the heap-like notation to encode internal nodes, the tree can be represented in 2(n-1) + 1 = 2n - 1 bits
(which enumerates nodse level by level, output 1 for internal nodes and 0 for leaves)
leaves can be added back unambiguously since a huffman tree is full
then list all keys in preorder, each takes at most [lgn] bits
the keys can be plugged into the nodes unambiguously since preorder walk of a fixed tree structure is unique

16.3-7
an optimal ternay code tree may not be full
but only one internal node with maximal depth may have 2 children, all other internal nodes must have three
if any internal node has 1 or no children, by problem 16.3-2 the node can be eliminated
if two internal nodes have only 2 children, denote them by x and y, assume d(x) <= d(y)
move a child c of y to x will not increase the total cost of the tree as c.freq * (d(x)+1) <= c.freq * (d(y)+1)
then x is full and y has only 1 child, y can be eliminated
if the node with 2 children has less than maximal depth
move one leaf with maximal depth to it will reduce the total cost, hence the node must have maximal depth
replace a leaf with a tree with three children adds 2 leaves to the tree, thus full ternay tree has 3 + 2k leaves
if the number of leaves n = 3 + 2k for k >= 0, the tree is full
if otherwise n = 4 + 2k, the tree is full except one internal node have only two children
a ternay equivalence to Lemma 16.2 then have two cases
    when n = 4 + 2k, swap x, y with least frequency with a, b as siblings with maximal depth
    when n = 3 + 2k, swap x, y, z with least frequency with a, b, c as siblings with maximal depth (must exist)
by almost the same argument as the proof of Lemma 16.2, the tree is still optimal
a ternay equivalence to Lemma 16.3 is strightforward
the algorithm should merge two least frequent nodes in the first iteration if n = 4 + 2k
otherwise it should merge three least frequent nodes

16.3-8
by the HUFFMAN algorithm in this chapter
as 2min{ci.freq} > max{ci.freq}
once two leaves are merged into an internal node
the node will not be extracted from the queue until all leaves are extracted
all the 256 leaves are merged before any leaf is merged with an internal node
after 128 iterations, the queue contains 128 internal nodes, each has size 2
let {xi} be the set of internal nodes, min{xi.freq} >= 2min{ci.freq} > max{ci.freq} >= 1/2max{xi.freq}
again the maximum node frequency is less than twice the minimum node frequency
once two internal nodes are merged to a size-4 node, it will not be extracted until all size-2 nodes are extracted
recursively the huffman tree built will be a complete tree of height 8
each leaf has depth 8, the encoding is no better than 8-bit fix-sized encoding

16.3-9
an n-bit random file has 2^n variations
if it's compressed to a k-bit file that k < n
2^k < 2^n, there may not be a bijection between original files and compressed files
thus decoding cannot always succeed, the compression method is not valid

16.4-1
let A ∈ Ik, if B ⊆ A, |B| <= |A| <= k, B ∈ Ik, A is independent, Ik is hereditary
if A, B ∈ Ik, |A| < |B|, then |A| < k, A ∪ {x} ∈ Ik for any x ∈ S, including some x ∈ B - A

16.4-2
|S| = n, S is finite
if A ∈ I, B ⊆ A, by definition of linear independence, B ∈ I
let A, B be two set of linear independent columns, |A| < |B|
if there isn't any x ∈ B - A that A ∪ {x} ∈ I
all columns in B can be expressed as a linear combination of columns in A, A spans B
by Steinitz exchange lemma, |B| <= |A|, in contradiction to the assumptions
therefore there must be some x ∈ B that A ∪ {x} ∈ I

16.4-3
let A' ∈ I', B' ⊆ A', as A ⊆ S - A' ⊆ S - B', A ⊆ S - B', B' ∈ I'
let A', B' ∈ I', |A'| < |B'|
both S - A' and S - B' contain some maximal A, B ∈ I
if there exists some x ∈ B' - A' that x ∉ A, A ⊆ S - (A' ∪ {x}), A' ∪ {x} ∈ I
otherwise B' - A' ⊆ A
all maximal A ∈ I have the same size, |A| = |B|
thanks Instructor's Manual
B' - A' ⊆ A ⊆ S - A', B' ∩ A = B' - A', |B' ∩ A| = |B' - A'| = |B'| - |B' ∩ A'|
B ⊆ S - B', |B ∩ A'| <= |A' - B'|
    |A'| < |B'|
    |A'| - |B' ∩ A'| < |B'| - |B' ∩ A'|
    |B ∩ A'| < |B'| - |B' ∩ A'|
    |B ∩ A'| < |B' ∩ A|
    |B| - |B ∩ A'| > |A| - |B' ∩ A|
    |B - A'| > |A - B'|
as B - A' ⊆ B ∈ I, A - B' ⊆ A ∈ I, B - A', A - B' are independent
by exchange property, there must be some y ∈ B - A' that A - B' ∪ {y} ∈ I
if y ∈ A, as y ∉ A - B', y ∈ A ∩ B', but y ∈ B => y ∈ S - B' => y ∉ B', thereby y ∉ A
//  stuck
//  adding elements in B - A' to A - B' cannot always result in a set C, |C| = |A|
//  if |B - A'| = |A| = |B|, it must have B ∩ A' = ∅

16.4-4
let A ∈ I, obviously removing elements from A will not increase |A ∩ Si| for any i, so B ⊆ A => B ∈ I
let A, B ∈ I, |A| < |B|
both A and B has at most 1 element from each disjoint subset Si
let AS = {Si | |A ∩ Si| = 1}, similarly BS
ovbiously |A| = |AS|, |B| = |BS|
|AS| < |BS| implies |BS| contains more subset Si than AS, BS - AS != ∅
take any Sk ∈ BS - AS, add the only x ∈ B ∩ Sk to A, then A ∪ {x} ∈ I

16.4-5
let w0 be a constant > max{w(xi)}, define w'(x) = w0 - w(x)
let A be the greedy solution to a weighted matroid with weight function w'
    w'(A)   = Σ(w0 - w(x))
            = w0|A| - w(A)
each maximal independent has the same size, thus w0|A| is a constant
A maximizes -w(A) among all independent subset of S, hence minimizes w(A) among all A ∈ I

16.5-1
[ { index: 5, deadline: 1, weight: 50, original_weight: 30 },
  { index: 4, deadline: 3, weight: 40, original_weight: 40 },
  { index: 6, deadline: 4, weight: 60, original_weight: 20 },
  { index: 3, deadline: 4, weight: 30, original_weight: 50 },
  { index: 7, deadline: 6, weight: 70, original_weight: 10 } ]

16.5-2
assume declaring an array D of size n takes O(1) time (true in some languages but not all)
for each task t in A:
    increment D[t.d]
    if D[t.d] > t.d, return false
return true

16-1
a.  quarter:    25 cent
    dime:       10 cent
    nickel:     5 cent
    penny:      1 cent
    always choose the coin with most value <= n
    let subproblem n be the optimal solution to n cents
    when n >= 5, let C be an optimal solution using only nickels and pennies
        C may not contain more than 5 pennies, otherwise that 5 pennies can be replaced with a nickel
        if k < [n/5], the solution have more than 5 pennies
        if k > [n/5], k > n/5, 5k > n, the solution is not valid
        thereby k = [n/5], p = n % 5
    when n >= 10, let C be an optimal solution using no quarters
        let d be the number of dimes in C, C - {d dimes} is an optimal solution to n - 10d using no dime and above
        p = (n - 10d) % 5 = n % 5
        k = [(n - 10d) / 5]
        furthermore, if k >= 2, two nickels can be replaced by a dime, so k <= 1
        if d > [n / 10], 10d > n, the solution is not valid
        if d < [n / 10], n - 10d > 10, [(n - 10d) / 5] >= 2
        thereby d = [n / 10]
    when n >= 25, let C be an optimal solution
        let q be the number of quarters in C
        C - {q quarters} must be an optimal solution to n - 25q using no quarters
        p = (n - 25q) % 5 = n % 5
        d = [(n - 25q) / 10]
        k = [(n - 25q - 10d) / 5]
        furthermore, if d >= 3, 3 dimes can be replaced by a quarter and a nickel
        if d >= 2 and k >= 1, 2 dimes and 1 nickel can be replaced by a quarter
        if q > [n/25], 25q > n, the solution is not valid
        if q < [n/25], n - 25q > 25
            for 25 < n - 25q < 30, d = 2, k = 1
            for n - 25q >= 30, d >= 3
        thereby q = [n / 25]
    taking q = [n / 25], d = [(n - 25q) / 10], k = [(n - 25q - 10d) / 5]
    is equivalent to choose the most valuable coin <= n and solve the subproblem with the same strategy
b.  an optimal solution with coins {c^0, c^1} will have n1 = [n / c] coins of value c^1
    assume an optimal solution with coins {c^0 .. c^i}, ni = [n / c^i]
    an optimal solution with coins {c^0 .. c^i+1} then must have ni+1 = [n / c^i+1]
    if ni+1 > [n / c^i+1], the solution is invalid
    if ni+1 < [n / c^i+1]
    the remaining coins must be an optimal solution to n - ni+1 * c^i+1 with coins {c^0 .. c^i}
    ni+1 < [n / c^i+1] implies n - ni+1 * c^i+1 >= c^i+1
    ni = [(n - ni+1 * c^i+1) / c^i] >= c
    but c coins of value c^i can be replaced by a coin of value c^i+1
    therefore ni+1 = [n / c^i+1], equivalent to a greedy solution
c.  coins have face value (1, 7, 10)
    for n = 14, greedy solution is (10, 1, 1, 1, 1), optimal solution is (7, 7)
d.  with dynamic programming
    define subproblem i naturally as
        minimize the number of coins that add up to i cents
    optimal solution to problem i can be derived from
        for all k diffrent coins:
        choice one of them, compute j = i - its face value, get an optimal solution to subproblem j, plus 1
        minimize the sum over all k
    at most n subproblems, each refers to k subproblems, O(nk) in total

16-2
a.  always choose the task with least completion time
    n is fixed for each problem, minimizing average completion time is equivalent to minimizing total completion time
    assume A = (ai1, ai2 .. ain) is a solution that does not start with the task aik with least completion time
    consider A' that swap ai1 and aik in A
    since ths first k tasks still takes that much time
    the completion time for all aij that j >= k will not be affected
    but the completion time for all tasks ai1 .. aik-1 is improved by ai1 - aik > 0
    the subproblem with S' = S - {aik} can be solved similarly
    the optimal sum of completion time to S is the optimal time to S' + n * aik
    the optimal solution is S sorted according to completion time
    running time is O(nlgn)
b.  for each time unit, the task with least remaining completion time should be executed
    after rn, the problem is the same to part a
    preemption will not improve the optimal solution after rn:
        let S = (a1 .. an) sorted by completion time in increasing order
        assume in the first time unit, instead of a1, another task with completion time ak.c > a1.c is executed
        S reduced to a subproblem S' = (a1 .. ak' .. an) instead of S'' = (a1' .. an)
        where ak'.c = ak.c - 1, a1'.c = a1.c - 1
        let A' be an optimal solution to S'
        there are ak.c + a1.c - 1 time units in A' that executes a1 and ak'
        since ak.c > a1.c, the completion time of the earlier between a1 and ak' >= the a1.cth time unit
        the the later one completes at the last time unit among the ak.c + a1.c - 1
        let the first a1 - 1 time units execute a1' instead, A' changed into A'', a solution to A''
        the completion time of a1' is a1'.c = a1.c - 1th time unit
        the completion time of the later one unchanged
        thereby the optimal solution to S'' is better than the optimal solution to S'
        the task with least completion time should be executed at each time unit
    between rn-1 and rn, either
        all tasks except the last one can be fully completed before rn
        the optimal solution is the same to part 1 for tasks available between rn-1 and rn
        then execute the last one after rn
    or
        the tasks may not be completed in time rn-1 to rn
        the optimal solution should left a subproblem as small as possible to time after rn
        by the argument above
        //  guess, not prove
        execute rn - rn-1 time units of tasks with least remaining completion time leaves a smallest subproblem
        doing so also minimizes the completion time of the tasks completed between rn - rn-1
        //  guess, not prove
    both case the optimal strategy is to execute the task with least remaining completion time at each time unit
    recursively for each time span between ri ~ ri+1, the same strategy applies
    the scheduling can be computed with a min heap
    make a copy of tasks T sorted according to release time
    create a min heap Q initially empty according to remaining completion time
    set a variable t starting from 0, representing the current time
    if Q is empty, pop the next task a from T, insert it into Q, set t to a.r the release time of a
    if Q is not empty, extract the minimum a from Q, let b be the next task in T
    compare a.c the remaining completion time of a and b.r - t
    if a.c > b.r - t
        set a.c to a.c - (b.r - t)
        insert a back to Q
        pop b from T, insert b into Q
        set t to b.r
    if a.c <= b.r - t or T is empty
        record the completion time of a as t + a.c
        set t to t + a.c
    repeat until both Q and T empty
    each iteration O(lgn) by complexity of heap operations
    in each iteration, either a task popped from T or completed, 2n iterations at most
    overall running time O(nlgn)

16-3
a.  each column ci in M corresponds to an edge i
    ci[j] = 1 iff edge i and vertex j is incident
    obviously for each i, exactly two entries have value 1, others have value 0
    in the field of integers modulo 2, the only possible coefficient is 1 or 0
    the columns of M is linearly dependent iff for a column ci an a set of column C, ci ∉ C
        ci = Σ(cj ∈ C)cj
        ci + Σ(cj ∈ C)cj = 0    // over field of integers modulo 2
        Σ(cj ∈ C ∪ {ci})cj = 0
    in other words Σ(ci ∈ C)ci = 0 for some subset C of columns of M
    Σ(ci ∈ C)ci counts the number of edges in a subset of E incident on each vertex in V modulo 2
    if a set of edges forms a cycle, each vertex is incident to exactly two edges
    if a set of edges does not contain a cycle (i.e. is a forest)
    there must be some vertex v that only one edge is incident on v
    therefore Σ(ci ∈ C)ci = 0 iff the set of edges represented by C forms a cycle
    a set of columns of M is linearly independent iff the corresponding set of edges is acyclic
    by problem 16.4-2, define S be the set of columns, I = {A | A ⊆ S, A is linearly independent}
    (S, I) is a matroid
b.  sort E by weight, let E' = ∅, for each e in E:
        if E' ∪ {e} is acyclic, add e to E
    return E' after the traversal
c.  exchange property fails
    let E1 = {(v1, v2), (v2, v3), (v1, v3)}
    let E2 = {(v2, v1), (v3, v2)}
    E1, E2 ∈ I, but adding any edge in E1 to E2 forms a directed cycle
d.  if some set of edges forms a directed cycle
    they enter and leave each vertices in the cycle exactly once
    the sum of the corresponding columns = 0, the set of corresponding columns is not linearly independent
    if the set of columns is linearly independent, no set of edges forms a directed cycle in the graph
e.  the converse of d is not true
    in the field of integers, coefficient in linear combinations can be negative
    an edge in the sum can be interpreted as itself or its reverse

16-4
a.  case 1: such a slot t exists
        let A be an optimal solution to the problem that didn't assign a1 to the first empty slot before d1
        that slot t must be assigned to some ak that pk the penalty of ak <= p1
        if a1 is early in A, it must occupy a time slot earlier than t, as t is the latest such slot
        if ak is early in A, exchanging the position of a1 and ak will not make ak late
        if otherwise ak is late, ak may still be late after the exchange
        total penalty is not decreased
        if a1 is late in A, the exchange makes a1 early and possibly makes ak late
        total penalty changed by at most pk - p1 <= 0
        the resulting solution is at least as optimal as A
    case 2: such a slot doesn't exist
        let A be an optimal solution to the problem
        a1 must be late in A
        if the last task ak in A is early, exchanging a1 with ak will not make ak late
        if ak is late, ak may still be late after the exchange
        the total penalty is not increased
    the subproblem is defined by the set of remaining tasks and remaining time slots, optimal substructure applies
b.  wait for section 21.3

16-5
a.  preprocess:
        maintain a dynamic set S and an array N, scan the sequence of requests from tail to head
        upon a request ri:
            search key ri in S
            if ri is in S, retrieve the value and assign it to N[i]
            update key ri in S to i
        on termination, N[i] points to the next occurrence of ri in sequence of requests (if any)
    simulation:
        maintain a max heap H keyed by N[i] with value ri and a search tree T keyed by ri
        for each request ri in the sequence:
            if ri is in T, it is a cache hit
            if ri is not in T, if H.size < k, insert (N[i], ri) into H and ri into T
            otherwise extract the maximum from H, let it be rj
            if N[j] > N[i], delete rj from T, insert (N[i], ri) and ri into H and T, output "rj evicted"
            if N[j] < N[i], insert (N[j], rj) back to H, output "ri not cached"
    in preprocess, running time is dominated by O(n) search and insertion operation to S
    if S has O(1) search and insertion (e.g. hash table), running time is O(n)
    if S has O(lgn) search and insertion, running time is O(nlgn)
    in simulation, running time is dominated by O(n) heap and search tree operations, each O(lgn)
    O(nlgn) in total
b.  subproblem is defined by a tail of the requests and the set of items in the cache, a size-k subset of requests
    no matter how the current request is treated
    if the subproblem can be handled better, the solution can be improved
    assume the current request being processed is ri, |S| <= k is the cache items
    the possible treatment for the current request ri is:
        1.  a cache hit, subproblem is (i+1, S)
        2.  a cache miss and |S| < k, subproblem is (i+1, S ∪ {ri})
        3.  a cache miss and |S| = k, either ri or one rj ∈ S is evicted
            possible subproblems are (i+1, S) and (i+1, S - {rj} ∪ {ri}) for rj ∈ S
c.  thanks https://www.cs.princeton.edu/~wayne/kleinberg-tardos/pearson/04GreedyAlgorithms.pdf
    assume before the ith request, there is an optimal solution A that follows furtherest-in-future strategy up to i
    consider the ith request ri
    if A evicts an item rp in the cache that is not rk the furtherest in future (FF)
    let A' be a solution that evicts rk instead and imitates A as long as possible
    let AF be the solution derived from FF
    let j be the next time that A' can no longer imitate A (must involve rp or rk)
    case 1: rj = rk
        can't happen, rk is the furtherest in future, there must be a request to rp before rk
        in A' the cache won't contain rp, request to rp will be a cache hit in A but a cache miss in A'
    case 2: rj = rp, a cache miss
        rp is not in the cache of A, an element rq is evicted
        if rq = rk, A' accesses rp in the cache, A and A' has the same cache from now
        if rq != rk, let A' evicts rq and add rk into the cache, A and A' has the same cache from now
        let A' imitate A from now on, the total number of cache miss not increased
    case 3: rj != rp or rk, a cache miss
        A must evict rk or A' can imitate it
        let A' evict rk, A and A' has the same cache from now on
        the total number of cache miss is the same
    thereby A' is a equally optimal solution that follows FF before i+1th request
    inductively there is a solution that follows FF all the way to the end

Chapter 17
17.1-1
no, consider n MULTIPUSH(K) operation followed by n MULTIPOP(|K|) operations
the complexity is now Θ(n|K|), depending on |K|

17.1-2
increment from 1^k-1 to 10^k-1 changes k-1 bits
decrement from 10^k-1 to 1^k-1 changes k-1 bits
if the sequence of operations repeatedly increment and decrement around that two numbers
the total running time is Θ(nk)

17.1-3
the operation costs i when the binary representation of i is 10^k for some k
when incrementing i from 1 to n, i = 10^k happens exactly ||n|| = |b(n)| times for each k from 0 to ||n|| - 1
the sum of such i equals 2^||n|| - 1 < 2n
plus the O(1) cost for other i, the amortized running time is O(n) / n = O(1)

17.2-1
assign costs to operations:
    PUSH    3
    POP     1
    COPY    0
each POP is paid by previous PUSH just like in the text
each PUSH and POP also adds one extra credit, so after k operations there are k extra credits to pay for the copy

17.2-2
assign a contant cost of 3 to increment operation
between any two i = 2^k + 1 and j = 2^(k+1), the total number of credits accumulated is
    3 * (2^(k+1) - 2^k) = 3 * 2^k = 2^(k+1) + 2^k
deduct the costs of j - i - 1 = 2^k - 1 normal incremental operations
there are still 2^(k+1) + 1 credits, which is enough to pay for the cost incurred by jth increment operation

17.2-3
keep the index of the highest order 1 in the array, update the index at the end of INCREMENT
assign a constant cost of 3 to increment operation
so even after be set to 0 in INCREMENT, each bit that was once 1 has at least 1 credits on it
the RESET operation then set every bit from the highest order 1 to 0, which was once 1
the cost of RESET can be paid by previous INCREMENT operations

17.3-1
define Φ'(Di) = Φ(Di) - Φ(D0)
as Φ(Di) >= Φ(D0), Φ'(Di) >= 0 = Φ(D0) - Φ(D0) = Φ'(D0)
the amortized cost of ith operation is
    či  = ci + Φ'(Di) - Φ'(Di-1)
        = ci + Φ(Di) - Φ(D0) - Φ(Di-1) + Φ(D0)
        = ci + Φ(Di) - Φ(Di-1)

17.3-2
define Φ(Di) as
    Φ(Di) = Φ(Di) + 2 if i != 2^k
    Φ(Di) = 0 if i = 2^k or i = 0
as Φ(Di) either increases or be set to 0, Φ(Di) >= 0 = Φ(D0) for all i
    či  = ci + Φ(Di) - ΦD(Di-1)
        = 1 + 2 = 3 if i != 2^k
when i = 2^k, Φ(Di) = 0, Φ(Di-1) equals twice the number of operations between j = 2^k-1 and 2^k - 1
    Φ(Di-1) = 2(2^k - 2^k-1) = 2 * 2^k-1 = 2^k
    Φ(Di) - Φ(Di-1) = -2^k
    či  = ci + Φ(Di) - Φ(Di-1)
        = i - 2^k = 0
therefore Σci <= Σči <= Σ3 = O(n)

17.3-3
as the EXTRACT-MIN operations takes O(lgn) time, cost of the operation <= clgn for some c
let n be the size of the heap, define Φ(Di) = cnlgn when n >= 1, Φ(Di) = 0 when n = 0, Φ(Di) >= 0 = Φ(D0) for all i
for an insert operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = O(lgn) + cnlgn - c(n-1)lg(n-1)
        = O(lgn) + c(n-1)lg(n / (n-1)) + clgn
        = O(lgn) + c(n-1)lg(1 + 1/(n-1))
        <= O(lgn) + c(n-1)lg(e^(1/(n-1)))
        = O(lgn) + c(n-1)(1/(n-1))lge
        = O(lgn) + clge = O(lgn) if n >= 2
when n = 1, Φ(Di) = 0 = Φ(Di-1), či = ci = O(1)
for EXTRACT-MIN operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        <= clgn - (cnlgn - c(n-1)lg(n-1))
        = clgn - clgn + c(n-1)lg((n-1) / n)
        <= c(n-1)lg(n / n) = 0 = O(1) if n >= 2
when n = 1, či = ci = O(1)
the cost of EXTRACT-MIN is paid by previous insert operations

17.3-4
let n be the size of the stack, define Φ(Di) = n, Φ(Di) >= 0 = Φ(D0) for all i
for push operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = 1 + 1 = 2
for pop operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = 1 + -1 = 0
for MULTIPOP operation:
    či  = min(n, k) + Φ(Di) - Φ(Di-1)
        = min(n, k) + -min(s, k) = 0
the amortized cost for each operation <= 2
    Σci = Σči - Φ(Di) + Φ(D0)
        <= Σ2 - sn + s0
        = 2n - sn + s0

17.3-5
Σci <= 2n - bn + b, bn >= 0, if n = Ω(b) then 2n + b = O(n)

17.3-6
define two stack S1 and S2
ENQUEUE operation pushes items into S1
DEQUEUE operation pops items from S2, if S2 is empty, pop all items in S1 and push them into S2 in order
define Φ(Di) = 2|S1|, Φ(Di) >= 0 = Φ(D0)
for ENQUEUE operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = 1 + 1 = 2
for DEQUQUE operation, if |S2| > 0:
    či  = ci + Φ(Di) - Φ(Di-1) = ci = 1
if |S2| = 0, |S1| push and |S1| + 1 pop operations performed
    či  = ci + Φ(Di) - Φ(Di-1)
        = 2|S1| + 1 - 2|S1| = 1
    Σci <= Σči <= Σ2 = O(n)

17.3-7
use an unsorted array
insert simply puts the item to one beyond the current length and increment the length n
DELETE-LARGER-HALF operates as following:
    find the median of the array in O(n)
    filter the array by the median
    copy remaining items to a new array, set the length to n/2
each step O(n), O(n) in total, let cn be an upper bound of the cost of DELETE-LARGER-HALF
define Φ(Di) = 2cn, Φ(Di) >= 0
for insert operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        = O(1) + 2c = O(1)
for DELETE-LARGER-HALF operation:
    či  = ci + Φ(Di) - Φ(Di-1)
        <= cn + 2c(n/2) - 2cn
        = 0 = O(1)
therefore Σci <= Σči <= ΣO(1) = O(m)
may use the variable-size array introduced in the next section to support unlimited insertion

17.4-1
by Corollary 11.7, the expected probes of insertion in an open addressing hash table is 1 / (1 - α)
when α -> 1, 1 / (1 - α) -> ∞, the last few insertions before the table is full may be costy
by restricting α, the expected performance of insertion is guarenteed to be better than a constant
only allow insertion if α < α0, where α0 < 1 is a constant
then the expected number of probes in an insertion is at most 1/(1 - α) <= 1/(1 - α0) = O(1)
the underlying table can be extended easily as described in the text
the hash function however must be adjusted or the new slots will never be accessed
upon expansion, copy each item in slot i to slot 2i in the new array, for 0 <= i <= m - 1
originally the hash function h(k, i) produces a permutation of {0 .. m - 1}
let h'(k, i) be the new hash function that
    h'(k, i)    = 2h(k, i) if i <= m - 1
                = 2h(k, i) + 1 if i >= m
h'(k, i) thereby produces a permutation of {0, 2 .. 2m - 2} ∪ {1, 3 .. 2m - 1} = {0, 1, .. 2m - 1}
the slots containing items in the old table is probed in the same order as h first
only after that the new slots are accessed, so the validity of search operation is not damaged
table expansion is Ω(n), when α = α0, the table must be expanded before insertion
thus expected running time of insertion is not necessarily O(1)

17.4-2
when αi-1 >= 1/2, deletion will not cause contraction
if both αi and αi-1 >= 1/2
    či  = ci + Φi - Φi-1
        = 1 + (2numi - sizei) - (2numi-1 - sizei-1)
        = 1 + (2(numi-1 - 1) - sizei) - (2numi-1 - sizei)
        = -1
if αi < 1/2 and αi-1 >= 1/2
    či  = ci + Φi - Φi-1
        = 1 + (sizei / 2 - numi) - (2numi-1 - sizei-1)
        = 3sizei-1 / 2 - 3numi-1
        = 3sizei-1 / 2 - 3αi-1sizei-1
        <= 3sizei-1 / 2 - 3sizei-1 / 2
        = 0

17.4-3
if the deletion didn't cause a contraction,
    či  = ci + Φi - Φi-1
        = 1 + |2numi - sizei| - |2numi-1 - sizei-1|
        = 1 + |2numi-1 - 1 - sizei| - |2numi-1 - sizei|
        <= 1 + 1 = 2
if the deletion causes a contraction
    numi-1 / sizei-1 >= 1/3, (numi-1 - 1) / sizei-1 < 1/3
    for all except a few small sizei-1, 2numi-1 - sizei-1 < 0
    sizei = 2sizei-1 / 3
    numi-1 / sizei >= 1/2, (numi-1 - 1) / sizei < 1/2, 2numi / sizei < 0
    ci = numi + 1
    či  = ci + Φi - Φi-1
        = numi + |2numi - sizei| - |2numi-1 - sizei-1|
        = numi-1 - 2numi-1 + 2 + 2sizei-1 / 3 + 2numi-1 - sizei-1
        = numi-1 - 2 - sizei-1/3 
        = numi-1 - 1 - sizei-1/3 - 1
        < -1
the amortized running time of deletion is O(1)

17-1
a.  ./CLRS/technique/bit-reverse.ts#rev
    ./CLRS/technique/bit-reverse.ts#bitReversalPermutation
b.  ./CLRS/technique/bit-reverse.ts#BitReversedCounter
c.  shift by more than 1 place is only used when computing the initial mask 1 << (1 - k)
    which can be computed once and stored as a field of the counter
    amortized running time still O(1) assuming k = O(n)

17-2
a.  do binary search on each array
    worst case is unsuccessful search where all bits in b(n) is 1
    T(n)    = O(Σ(k = {0 .. lgn - 1})lg2^k)
            = O(Σ(k = {0 .. lgn - 1})k)
            = O(lg^2n)
b.  insert to A0, |A0| <= 1, the insertion will take constant time
    if |A0| = 2, starting from i = 1
        if Ai is empty, replace it will Ai-1, set Ai-1 = ∅ and terminate
        otherwise set Ai to the merge of Ai-1 and Ai, set Ai-1 = ∅, set i = i + 1, go to the next iteration
    after ith iteration
        the correspondence between up to ni-1 and Ai-1 is restored, |Ak| = 0 for k <= i-1
        the correspondence between ni and Ai restored (termination) or |Ai| = 2^(i+1)
    the iteration terminates only when Ai is empty, Ai replaced by Ai-1, |Ai-1| = 2^(i-1+1) = 2^i
    in worst case every array is merged, total cost Σ(k = {1 .. lgn - 1})O(2^k) = O(n)
    each merge up to Ai occurs once each 2^i insertions, takes O(2^i)
    for k = 2^j insertions, the cost of merge is no greater than
        Σ(O(2^i) * (2^j / 2^i))
        = Σ(O(2^i) * 2^(j-i))
        = Σ(O(2^j))
        = O(klgk)
    amortized cost for each insertion is then O(klgk) / k = O(lgk)
c.  if the deleted item is in array Aj, find the first nonempty array Ai 
    take the last element of Ai, insert it to the correct position in Aj in O(2^j) = O(n)
    now |Ai| = 2^i - 1
    cut Ai to i slices of length 2^0, 2^1 .. 2^i-1, set A0 .. Ai-1 to these slices in O(2^i) = O(n)
    as Ai is sorted, each A0 .. Ai-1 is automatically sorted
    ni = 0, n0 .. ni-1 = 1 now, the correspondence restored in O(n) total

17-3
a.  if x.size is even, x.left.size and x.right.size may differ by at most 1
    otherwise the greater between x.left.size and x.right.size > 1/2 * x.size
    if x.size is odd, x.left.size must equal x.right.size
    walk the tree in inorder, enumerate nodes in an array, set .parent, .left and .right of each node to NIL
    recursively:
        find the median x, the median than divide the array into two subarrays
        the left subarray has key < x.key, the right subarray has key > x.key
        construct the 1/2-balanced tree l and r of both subarray
        set x.left = l, x.right = r
        as x is the median, if x.size is odd, l.size = r.size, otherwise l.size and r.size differ by at most 1
        the tree rooted at x is a 1/2-balanced tree
    T(n) = 2T(n/2) + O(1) = O(n)
b.  a tree with height h = 0 has a single node
    if an α-balanced tree of height h has at least n nodes, for an α-balanced tree of height h+1
    one of the subtree of x the root has height h, let it be the left subtree
        α * x.size >= x.left.size >= n
        x.size >= n/α
    inductively, an α-balanced tree of height h has at least (1/α)^h nodes, where 1/α > 1
    n >= (1/α)^h, lgn >= hlg(-α), h <= lg(-α)lgn = O(lgn)
    a search at most examines h = O(lgn) nodes
c.  Δ(x) is a absolute value, Δ(x) >= 0 for all x, ΣΔ(x) >= 0
    consider a 2-node tree with root and its single left child
    the tree is 1/2-balanced, but Δ(root) = 1, Φ(T) = c
    assume x.left.size = x.right.size for each node in 1/2-balanced tree in next two parts
d.  in worst case, both left and right subtree is 1/2-balanced
    the cost have to be paid solely with potential of the root x
        Φ(T) = cΔ(x) = c|x.left.size - x.right.size|
    α * x.size = αm < max(x.left.size, x.right.size)
    assume x.left.size is greater, denote ml = x.left.size, mr = x.right.size
        m > ml > αm, mr = m - ml - 1, c|ml - mr| = c(2ml - m + 1) > c(2α - 1)m
    for c(2α - 1)m >= m, c(2α - 1) >= 1, c >= 1/(2α - 1)
e.  for insertion, if rebalance unnecessary:
        či  = ci + Φ(Ti) - Φ(Ti-1)
            = O(lgn) + O(lgn) (at most O(lgn) nodes has Δ(x) changed by 1)
            = O(lgn)
    if insertion caused a rebalance at level x, x.size = m:
        Φ(Ti)   = 0
        Φ(Ti-1) >= c|x.left.size - x.right.size| >= c|αm - (1-α)m| = c(2α - 1)m >= m
        či  = ci + Φ(Ti) - Φ(Ti-1)
            <= O(lgn) + m - c(2α - 1)m = O(lgn)
    for deletion, if rebalance unncessary:
        či  = ci + Φ(Ti) - Φ(Ti-1)
            = O(lgn) + O(lgn) = O(lgn)
    if deletion caused a rebalance at level x, x.size = m:
        Φ(Ti)   = 0
        Φ(Ti-1) = c(2α - 1)m
        či  = ci + Φ(Ti) - Φ(Ti-1)
            = O(lgn) + m - c(2α - 1)m
            = O(lgn)
    therefore amortized cost of insertion and deletion is O(lgn)

17-4
a.  for a newly inserted node:
        its parent is red and has a red sibling
        each black ancestor of it has a red parent and a red uncle
    then case 1 in RB-INSERT-FIXUP will propagate all the way to the top, make O(h) = O(lgn) color changes
    assume after deletion, node x is double black:
        all ancestors of x is black
    then the while loop in RB-DELETE-FIXUP will not terminate until x = T.root, run for O(h) = O(lgn) iterations
    in each iteration some nodes change color, O(lgn) color changes in total
b.  for RB-INSERT-FIXUP, case 2 and 3 are terminating
    for RB-DELETE-FIXUP, case 1, 3 and 4 are terminating, case 2 is terminating if new x is red
c.  in case 1 of RB-INSERT-FIXUP, two red nodes colored black, one black node colored red
    total number of red nodes decremented by 1, Φ(T') = Φ(T) - 1
d.  in case 1 (line 5 - 8):
        two red nodes colored black
        one black node colored red
        potential decreased by 1
    in case 2 (line 10 - 11):
        a left rotation
        no potential change
    in case 3 (line 12 - 14):
        one red node colored black
        one black node colored red
        a right rotation
        no potential change
e.  in case 1 of RB-INESRT-FIXUP, each iteration decreases potential by 1
    the cost is paid by the potential, či = O(1)
    case 2 and 3 takes O(1) and terminates
    amortized cost of RB-INESRT-FIXUP is then O(1)
f.  as w(x) = 0 if x is red, potential will only change if a red node is colored black
    in case 1 the only nonterminating case (referring nodes by their name in figure 13.5 - 13.7):
        w(C) changed from 2 to 0
        as A is originally red, its left child must be black
        w(A) 0 -> 0
        as D is originally red, both its children must be black
        w(D) 0 -> 1
        Φ(T') = Φ(T) - 1
    each iteration of case 1 is paid by the potential
    case 2 and 3 changes potential by at most a constant
        ci <= či = O(1)
g.  in case 2 the only nonterminating case:
        w(D) 1 -> 0
        B must be black or case 2 terminates
        w(B) 1 -> 0
        Φ(T') = Φ(T) - 2
    each iteration of case 2 is paid by the potential
    case 1, 3 and 4 change potential by at most a constant
        ci <= či = O(1)
h.  Σci <= Σči = ΣO(1) = O(m)

17-5
a.  when H does not know σ in advance
    the operation of H is totally defined by the current ordering and the search key σi
    after such an operation, H must choose some x as the last node of the list
    let a sequence σ that always searches key in x after H performed its reordering
    each search takes Ω(n), Ω(mn) in total
b.  searching x examines rank(x) nodes
    since each transposition decreases rank(L, x) by 1
    moving x to the head of the list uses at least rank(x) - 1 transposition
    by always transpose x with its prev, the lower bound can be achieved
        ci = rank(L, x) + rank(L, x) - 1 = 2rank(x) - 1
c.  obvious
d.  assume (x, y) is transposed
    if (x, y) was not an inversion, it becomes one after the transposition, Φ(Li) increased by 2
    if (x, y) was an inversion, after the transposition y precedes x, (y, x) is not an inversion
    Φ(Li) decreased by 2
e.  a node precedes x in Li-1 either precedes x in L*i-1 or not
    the number of nodes precede x thereby is |A| + |B|, rank(Li-1, x) = |A| + |B| + 1
    similarly rank(L*i-1, x) = |A| + |C| + 1
f.  after access σi, x is moved to the head of Li
    each nodes in set A now causes a new inversion with x as they follow x in Li but precede x in L*i-1
    each nodes in set B eliminates an inversion with x as they now follow x in both Li and L*i-1
    relative position between elements in C and D with x in Li not changed
    they cause inversions between Li and L*i-1 iff they cause inversions between Li-1 and L*i-1
    the relative position between any other (y, z) in Li that y != x and z != x is not changed from Li-1
    then i* transpositions in L*i-1 at most increases the potential by 2i*
        Φ(Li) - Φ(Li-1) <= 2(|A| - |B| + i*)
g.  ci + Φ(Li) - Φ(Li-1)    <= 2(|A| + |B| + 1) - 1 + 2(|A| - |B| + i*)
                            = 4|A| + 1 + i*
                            <= 4|A| + 4|C| + 4 + 4i*
                            = 4rank(L*i-1, x) + 4i*
                            = 4c*i
h.  summation

Chapter 18
18.1-1
then t-1 = 0 and some node will store no keys and only one child
the height of the tree is no longer O(lgn)

18.1-2
node stores 2 or 3 keys, t-1 <= 2, 2t - 1 >= 3
node has 3 or 4 children, t <= 3, 2t >= 4
t may be 2 or 3

18.1-3
the root contains 1-3 keys
1 key:
    2 children, each with 1-3 keys
    if any of them is not a leaf, it has at least 2 children with at least 1 key each
    the other child of root then may not have any child, leaves do not have the same height
    thus both children of the root must be a leaf
    ((1), 2, (3, 4, 5)), ((1, 2), 3, (4, 5)), ((1, 2, 3), 4, (5))
2 keys:
    3 children, each with 1 key
    ((1), 2, (3), 4, (5))
3 keys:
    impossible, must have 4 children with 1 key each, 7 > 5 keys in total

18.1-4
each internal node has 2t - 1 keys and 2t children
a complete B-tree of height h has (2t)^d nodes at depth d
    n   = 1 + (2t - 1)Σ(d = {1 .. h})((2t)^d)
        = 1 + (2t - 1)((2t)^(h+1) - 1) / (2t - 1)
        = (2t)^(h+1)
    
18.1-5
the children of red nodes must be black
a black node after absorbing red children has 4 children and 3 keys, 3 children and 2 keys, or 2 children and a key
c1 originally root of left subtree of left child, c2, c3 and c4 accordingly, property 3 of B-tree follows
the black height is universal among all leaves, property 4 of B-tree follows
therefore the resulting tree is a 2-3-4 tree

18.2-1
About to insert K
Empty BTree
About to insert L
┌──S
├Q
│ ┌K
└─├F
  └C
About to insert W
  ┌V
┌─├T
│ └S
├Q
│ ┌L
├─├K
│ └H
├F
└──C
About to insert M
    ┌W
  ┌─└V
┌─├T
│ └──S
├Q
│   ┌L
│ ┌─├K
└─│ └H
  ├F
  └──C
About to insert P
    ┌W
  ┌─└V
┌─├T
│ │ ┌S
│ └─└R
├Q
│   ┌N
│ ┌─├M
│ │ └L
└─├K
  ├──H
  ├F
  └──C
About to insert A
    ┌W
  ┌─└V
┌─├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
│ ├M
│ ├──L
└─├K
  ├──H
  ├F
  └──C
About to insert Y
    ┌X
  ┌─├W
  │ └V
┌─├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
├─├M
│ └──L
├K
│ ┌──H
│ ├F
└─│ ┌C
  └─├B
    └A
About to insert D
    ┌Y
  ┌─└X
  ├W
┌─├──V
│ ├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
├─├M
│ └──L
├K
│ ┌──H
│ ├F
└─│ ┌C
  └─├B
    └A
Final configuration
    ┌Z
  ┌─├Y
  │ └X
  ├W
┌─├──V
│ ├T
│ │ ┌S
│ └─└R
├Q
│   ┌P
│ ┌─└N
├─├M
│ └──L
├K
│ ┌──H
│ ├F
│ │ ┌E
└─├─├D
  │ └C
  ├B
  └──A

18.2-2
no DISK-READ or DISK-WRITE in the body of B-TREE-INSERT
B-TREE-SPLIT-CHILD modifies each of x, y and z, no DISK-WRITE is redundant
in B-TREE-INSERT-NONFULL:
    the only DISK-WRITE is after inserting the new key into a leaf
    the only DISK-READ(x.ci) reads a child of x in the current recursion
    as insertion is performed by a single pass down the tree, each x is visited only once
    there's no way to refer to the parent of a node, the only way to visit x.ci is through x
    therefore the read of x.ci must be the first time and the last time it is read
there doesn't seem to be any redundant read or write in insertion

18.2-3
minimum:
    traverse x.c1 until x is a leaf, then select x.key1
predecessor:
    if x is an internal node, predecessor of x.keyi is the minimum of x.ci+1
    if x is a leaf, predecessor of x.keyi is x.keyi+1
    when x.keyi+1 doesn't exist, predecessor is the deepest y.keyj+1 for which y.ci contains x

18.2-4
in worst case, the keys are in sorted order 
at least every 2 insertions cause a leaf to split
at least two splits of the same leaf cause a node with height 1 to split
at least two splits of the same node of height 1 causes a node with height 2 to split
the total number of splits caused by insertion is n/2 + n/4 + n/8 ... ≒ n
the tree after n insertion may have n nodes at most
in best case, each internal node contains 2 keys and each leaf 3
an internal node with 3 keys after B-TREE-SPLIT-CHILD has a child with only 1 key
any inserion into that child will cause the internal node to split
let h be the height of the tree
there are 3^d nodes at depth d
    2Σ(d = {0 .. h-1})3^d + 3 * 3^h = n
    (3^h - 1) + 3 * 3^h = n
    4 * 3^h - 1 = n
    3^h = (n + 1) / 4
so 3^h = (n + 1) / 4 leaves and (3^h - 1) / 2 = (n - 3) / 8 internal nodes
(3n - 1) / 8 nodes at least

18.2-5
modify B-TREE-SPLIT-CHILD so it now takes t as a parameter
./CLRS/structure/b-tree.ts#splitChild
and define different t for leaves and internal nodes, give different parameter to B-TREE-SPLIT-CHILD accordingly
instead of manually checking x.n == 2t - 1, make a method isFull(x) in the tree that chooses t according to x.leaf

18.2-6
searching the key in each iteration now takes time O(lgt)
O(lgt * log(t, n)) = O(lgt * lgn / lgt) = O(lgn)

18.2-7
a search operation at worst case performs O(h) DISK-READ
as the CPU time is just O(lgn), for reasonable n < 2^64, CPU time is negligible compared to disk operations
height of the tree is approximately log(t, n), optimal t then minimizes 
    (a + tb)log(t, n) = (a + tb)ln(n) / ln(t)
    d/dt((a + tb) / lnt) = 0
for a = 5, b = 10, the solution is approximately 3

18.3-1
  ┌Z
┌─└Y
├X
│ ┌V
├─└U
├T
│ ┌S
├─├R
│ └Q
├P
│ ┌O
├─└N
├L
│ ┌K
├─└J
├E
│ ┌C
└─└A
Deleting C
  ┌Z
┌─└Y
├X
│ ┌V
├─└U
├T
│ ┌S
├─├R
│ └Q
├P
│ ┌O
├─└N
├L
│ ┌K
│ ├J
└─├E
  └A
Deleting P
  ┌Z
┌─└Y
├X
│ ┌V
├─└U
├T
│ ┌S
├─└R
├Q
│ ┌O
├─└N
├L
│ ┌K
│ ├J
└─├E
  └A
Deleting V
  ┌Z
┌─└Y
├X
│ ┌U
│ ├T
├─├S
│ └R
├Q
│ ┌O
├─└N
├L
│ ┌K
│ ├J
└─├E
  └A

18.3-2
./CLRS/structure/b-tree.ts#delete

18-1
a.  Θ(n) disk operations, Θ(nm) CPU time
b.  each p push operations requires a disk read
    Θ(n/m) disk operations in total, Θ(n) CPU time
c.  a series of
        PUSH PUSH POP POP PUSH PUSH POP POP ...
    operations at the boundary between two pages
    every two operations incurs a disk access, Θ(n) in total, Θ(nm) CPU times
d.  only read a new page when the pointer is about to cross the boundary and enter a page not in memory
    the page at the opposite end is written to disk and freed from memory
    after each time this pair of read/write disk operations occur
    the pointer is at the boundary between two pages, both of them in memory
    at least m stack operations (m pop or m push) before the next time a disk operation is necessary
    2/m = O(1/m) amortized disk operations, (O(m) + 2O(m)) / m = O(1) amortized CPU time

18-2
a.  search will not change the structure
    insertion may change the height of nodes when:
        a child is splited, the new child z has the same height of y in B-TREE-SPLIT-CHILD
            z.height can be assigned in O(1)
        the tree grows upward, the new root s has height one more than the old root
    deletion may change the height of nodes when:
        the root is removed, no height in subtree is changed
b.  if either tree is empty, simply insert k into another, running time O(h) = O(|h - h'|) = O(1 + |h - h'|)
    insert the lower tree as a subtree of the higher tree
    if h' > h'', insert T'' as a child of the right-most node with height h'' + 1 in T', k as the last key
    if h' < h'', insert T' as a child of the left-most node with height h' + 1 in T'', k as the first key
    if h' = h'', allocate a new node x with sole key k, set x.c = [T1.root, T2.root]
    split child when necessary on the path
    at most max(1, |h' - h''|) nodes visited, spliting child takes O(t) = O(1), O(1 + |h' - h''|) in total
    ./CLRS/structure/b-tree.ts#join
c.  assume at some subtree rooted at x, p descends into a child x.c[i]
    define a new tree T' rooted at x' which has keys x.key[1] to x.key[i-2] and children x.c[1] to x.c[i-1]
    define another tree T'' rooted at x'' which has keys x.key[i+1] to x.key[x.n] and children x.c[i+1] to x.c[x.n+1]
    if any of T' and T'' has no key not a single child, replace root with that child
    either T' or T'' may be empty but not both of them as x has at least two children
    let k' = x.key[i-1], k'' = x.key[i]
    obviously each y' ∈ T' < k' since all keys are distinct, similarly y'' ∈ T'' then y'' > k''
    repeat the process at lower levels, for each key z in x.c[i], y' < k' < z, k < k'' < y''
    either k' or k'' may not exist but no both of them as x has at least one key
    if T' is empty, k' doesn't exist; if T'' is empty, k'' doesn't exist
    at the node x in which k is found, assume x.key[i] = k, split x into two subtree T' and T'' that
    T' contains x.key[1] to x.key[i-1] and x.c[1] to x.c[i]
    T'' contains x.key[i+1] to x.key[x.n] and x.c[i+1] to x.c[x.n+1]
    let T'i and T''i be subtrees splited at ith node x on the path p, similarly k'i and k''i
    the process splits the entirety of a subtree rooted at x into five parts: T', T'', k', k'' and x.c[i]
    a key in T then is
        1.  in T'i for some i
        2.  in T''i for some i
        3.  is k'i for some i
        4.  is k''i for some i 
        5.  is k
    therefore S' = 1 + 2, S'' = 3 + 4
    each subtree T'i or T''i, if not empty, has height equals to or one less than the corresponding x
    the difference in height between T'i and T'i+1 is bounded from above by 2
    consider only the set of non-empty T'i, the sum of differences in height between T'i and T'i+1 <= 2h = O(lgn)
d.  maintain two 2-3-4 tree S' and S'' initially empty
    for each node x on p from k to root,
        split x into T', T'', k', k'' as described in part c
        join (T', k', S') and (S'', k'', T''), assign result to S' and S''
    join operation takes O(1 + |h' - h''|)
    sum of differences between height of T'i and T'i+1 is O(lgn), therefore running time of split operation is O(lgn)
    ./CLRS/structure/b-tree.ts#split

Chapter 19
19.2-1
46
 ┌─23
 │   ┌─39
7┤ 18┴─38──41
 └─21──52
  ┌─30
17┴─26──35

19.3-1
x was once a root
then it's linked to another node y, later one of its children is removed
after zero or more EXTRACT-MIN, y became H.min
the next EXTRACT-MIN added x to the root list
no node in the root list has the same degree as x
CASCADING-CUT will not recurse into a root

19.3-2
each recursive call to CASCADING-CUT moves one child to the root list
at most n-1 recursive calls to CASCADING-CUT may happen before all nodes are roots in the heap
no further calls to FIB-HEAP-DECREASE-KEY will call CASCADING-CUT
m calls to FIB-HEAP-DECREASE-KEY then takes O(m + n) / m amortized time
if m = Ω(n), O(m + n) / m = O(m) / m = O(1)

19.4-1
from a heap H that is a single chain of length n
make H', an exact copy of H, by repeat the construction of H
union H and H', assign the result to H
insert -∞
now H contains three root: the x root of original H, x' the root of H' and -∞
call EXTRACT-MIN(H), -∞ is extracted, x and x' is linked
assume x' became a child of x, denote the other child of x by z
call FIB-HEAP-DELETE(H, z), the sub-heap rooted at z is cutted from x
the heap rooted at x now is a single chain of length n+1
later z removed and the child of z added to the rooted list, which soon linked to x as both have degree 1
repeat this procedure and the heap rooted at x will became a single chain of length n+1
for a fixed n, starting from a singleton heap, repeat the above procedure n-1 times

19.4-2
now yi.degree >= i - k
define z and sd(originally sk) similarly
    size(x) >= sd
            >= 1 + Σ(i = {1 .. d})(syi.degree)
            >= 1 + Σ(i = {1 .. d})(si-k)
            >= k + Σ(i = {k .. d})(si-k)
            = k + Σ(i = {0 .. d - k})(si)
compared to the old series of {s'i} where k = 2:
    s0 = 1 = s'0
    s1 = 2 = s'1
    s[k+1]  >= k + s0 + s1 >= s'3
    s[2k+1] >= k + s0 + s1 + s[k+1] >= s'4
inductively, if s[dk+1] >= s'[d+2]
    s[(d+1)k+1] >= k + s0 + s1 + ... + s[dk+1] >= s'[d+3]
therefore 
    skn = O(φ^(n))
    sn = O(φ^(n/k))
as long as k is a constant, D(n) = O(lgn)

19-1
a.  all parent pointers in the children of x have to be fixed to NIL
    the actual complexity is O(x.degree)
b.  PISANO-DELETE is the same to FIB-HEAP-DELETE except it will not consolidate the root list
    the first stage equivalent to call FIB-HEAP-DECREASE-KEY and decrease x.key to -∞, takes O(c)
    the second stage adds each child of x to root list, takes O(x.degree)
    O(c + x.degree) in total
c.  after the first stage, the change in potential is at most 4 - c
    the second stage adds x.degree and removes one node from root list without changing any mark
    total change in potential at most 4 - c + (x.degree - 1) = 3 - c + x.degree
d.  the potential change will not cover x.degree
    since x.degree can be as large as D(n), amortized complexity of PISANO-DELETE is O(D(n)) = O(lgn)

19-2
a.  1.  |B0| = 1 = 2^0, |Bk| = 2|Bk-1| = 2^k
    2.  height of B0 is 0
        let height of Bk be k constantly, Bk+1 consists of two Bk trees
        one is rooted at the root of Bk+1, another rooted at depth 1
        the height of Bk+1 thereby is k+1
    3.  for B0, C(0, 0) = 0
        assume at depth i there are C(k, i) nodes in Bk
        for Bk+1, there are a single root at depth 0, C(k, k) = 1 node at depth k+1
        every depth i between have C(k, i) + C(k, i-1) = C(k+1, i) nodes
    4.  each subtree in a binomial tree is also a binomial tree
            for B0, the only node is the root
            for Bk+1, each subtree in two Bk is a binomial tree by induction, the new root is the root of Bk+1
        the root of Bk has degree k since:
            B0 has degree 0
            Bk+1 is a Bk tree with a new child, degree of Bk+1 is k+1
        each subtree of Bk is a binomial tree Bk' strictly smaller than Bk, their root has degree k' < k
        assume children of root of Bk are Bi for i = k-1, k-2 .. 0
        Bk+1 adds another Bk as the left-most child of Bk
        children of root of Bk+1 are Bi for i = k, k-1 .. 0
b.  each binomial tree in H has distinct degree k, which means it is Bk with size 2^k
    n = Σ2^ki for distinct ki
    a binary number that has (ki+1)-th least significant bit 1 for all i and other bits 0 equals is b(n)
    since b(n) is unique to n, the existance of Bk can be determined by (k+1)th bit of b(n)
    i <= |b(n)| = [lgn] + 1
c.  most operations on binomial heaps has much resemblance to operations on binary numbers
    ./CLRS/structure/binomial-heap.ts
    most functions are based on an auxiliary function add that combines two root lists
    insert:
        equivalent to incrementing a binary number
        add the root list of the heap with [x]
    union:
        add the root list of the two heaps
    minimum:
        enumerate all roots in the root list, return the one with minimum key
    extractMin:
        find the minimum root
        remove it from the root list
        add its child list with the root list
    decreaseKey:
        the same to binary heap, move the node upward until the heap property is restored
    delete:
        decrease key to -∞ then extractMin
d.  EXTRACT-MIN only removes a root in the root list
    the children of a node is never removed from the structure
    let y1, ... yk denote the children of z in the order in which they were linked to z
        yi.degree = i-1
    let sk denote the number of nodes in a tree of degree k
    s0 = 1 = 2^0 is a constant
    if sk = 2^k is a constant for all k <= d, for k = d+1,
        sk  = 1 + Σ(i = {1 .. k})(syi.degree)
            = 1 + Σ(i = {1 .. k})(si-1)
            = 1 + Σ(i = {1 .. k})(2^(i-1))
            = 2^k
    therefore k = lgn, trees in fibonacci heap is just binomial trees
    the difference is, since CONSOLIDATE is delayed as far as possible
    there may be several trees with the same degree in the root list of a fibonacci tree
e.  McGee heap is exactly binomial heap without DECREASE-KEY and DELETE operations

19-3
a.  delete the node, change the key and insert it back
    one deletion and one insertion, O(lgn) amortized time
    if k <= x.key, may call DECREASE-KEY instead, O(1) amortized time
b.  maintain another pair of left / right pointers in nodes
    use that pair of pointers to maintain a leaf list, which contains leaves in the heap
    H.leaf points to one of the leaves in the leaf list, or NIL if the heap is empty
    a few operations have to be adjusted to maintain the leaf list
    (LINK):
        if a leaf had a child through linking, remove it from the leaf list
    INSERT:
        the new node x is appended to H.leaf
    UNION:
        the leaf lists of the two heap is concatenated
    EXTRACT-MIN:
        CONSOLIDATE using the adjusted LINK instead
    DECREASE-KEY:
        if x is the last child of y, CUT also adds y to leaf list
    DELETE: DECREASE-KEY + EXTRACT-MIN
    add the size of the heap to the potential function
    it's only increased by a constant in INSERT and decreased by a constant in DELETE
    the amortized cost of existing operations will not be affected
    then for x = H.leaf and y = x.parent
    PRUNE repeat
        CUT(x)
        CASCADING-CUT(x, y)
        remove x from the root list
    min(r, H.n) times
    like in DECREASE-KEY, CASCADING-CUT(x, y) will be paid by the potential function
    each iteration of PRUNE takes constant amortized time and decrements n
    all min(r, H.n) iterations then is also paid by the potential function
    PRUNE takes amortized O(1) time in total

19-4
by Theorem 18.1, 2-3-4 heaps have height h = O(lgn)
assume each node has a pointer x.parent points to its parent
a.  compare x.small for all children x of the current node y
    descend into the node with smallest x.small
    height of y decreases each iteration, O(h) = O(lgn) in total
b.  set x.small and x.key to k
    update y.small for all ancestors of x, O(h) = O(lgn) in total
c.  similar to inserting a new minimum in 2-3-4 trees, split full child on the way down to a leaf
    update y.small for y on the path if necessary, O(h) = O(lgn)
d.  update .small for all its ancestors in O(lgn) time
    deletion in 2-3-4 heaps is the bottom-up version of case 3 in B-tree deletion
    delete the leaf, denote y = x.parent, z = y.parent
    if y now has only one child, move one child from its sibling to it
    if all its siblings has only 2 child, merge y with one of them
    now z may have only 1 child, repeat the process up to the root
    if the root has only one child, set the sole child as the new root, delete the old root
    O(h) = O(lgn) in total
e.  MINIMUM -> DELETE
f.  same to 2-3-4 tree join, update x.small
    O(1 + |h - h'|) = O(lgn)

Chapter 20
20.1-1
store linked lists in the underlying array instead of bare integers
deletion will not cause update to summary if the linked list is not empty

20.1-2
store pointers to objects with field key and value in the underlying array instead of bare integers

20.1-3
from root, at each node x:
    if x.key == k, terminate
    if x.key > k, descend into x.left or terminate if x.left == NIL
    if x.key < k, descend into x.right or terminate if x.right == NIL
the procedure will find an x with greatest x.key <= k
assume there is a y that x.key < y.key <= k and the procedure selects x
if x is an ancestor of y, x.right exists and y belongs to the right subtree of x
the procedure will descends into x.right as x.key < k
if y is an ancestor of x, x belongs to the left subtree of y
the procedure must visit y to reach x, but the procedure will not descend into y.left as y.key <= k
otherwise let z be the lowest common ancestor of x and y, x.key <= z.key <= y.key
x and y must be in different subtrees of z or it will not be the lowest common ancestor
x belongs to left subtree, y belongs to right subtree of z
to reach x the procedure must visit z, but then it will not descend into z.left as z.key <= y.key <= k
its successor is the successor of k in the tree

20.1-4
there are u^(1/k) nodes at depth 1, (u^(1/k))^2 = u^(2/k) at depth 2, u^(2^(i-1)/k) at depth i
the bottom level contains u^(2^(i-1)/k) = u nodes, 2^(i-1)/k = 1, 2^(i-1) = k, i = lgk + 1
depth of the bottom level equals height of the top level, h = lgk + 1
MINIMUM:
    search the u^(1/k) nodes of the current node, find the first with nonzero key
    descend that the child and repeat
    u^(1/k) * (lgk + 1) = O(lgk * u^(1/k))
MAXIMUM: symmetric to MINIMUM
SUCCESSOR:
    starting from a leaf x, search y the first right siblings with nonzero key of x
    if no such y exists, go up a level by setting x = x.parent
    otherwise find the minimum in the subtree rooted at y
    O(lgk * u^(1/k))
PREDECESSOR: symmetric to SUCCESSOR

20.2-1
./CLRS/structure/proto-veb-tree.ts#maximum
./CLRS/structure/proto-veb-tree.ts#predecessor

20.2-2
let DELETE procedure report whether after the deletion the cluster become empty
for the base case (u = 2) it sets A[x] = 0 and checks both A[0] and A[1]
for u = 2^(2^k), if deleting low(x) from cluster high(x) reports true, delete high(x) from the summary
if the deletion from summary also reports true, return true as the result
otherwise report false
at most two recursive calls, running time is the same to insertion
    T(u) = 2T(u^(1/2)) + O(1) = O(lgu)

20.2-3
./CLRS/structure/proto-veb-tree.ts#delete
two recursive calls in worst case, running time is the same to insertion
    T(u) = 2T(u^(1/2)) + O(1) = O(lgu)
INSERT should be adjusted so now it reports whether a key is truely inserted or an existing key is overwritten
in the first case .n is incremented
asymptotic running time still O(lgu) since both report and incrementing n takes constant time

20.2-4
each slot is incremented / decremented in insertion and deleteion instead of being set to 1 and 0

20.2-5
./CLRS/structure/proto-veb-tree.ts#ProtoVEBTree
summary now have different structure to the main tree since it stores no satellite data
MEMBER is replaced by SEARCH that return NIL or value stored under the input key

20.2-6
./CLRS/structure/proto-veb-tree.ts#factory
for u = 2 it creates a base case tree ProtoVEBBase
otherwise it creates an internal node ProtoVEBNode
only a super type of the two is exposed

20.2-7
when u = 2, PROTO-vEB-MINIMUM returns NIL iff A is empty, the tree contains no key
for V.u = 2^(2^k), assume proto vEB tree with universe < u, PROTO-vEB-MINIMUM return NIL iff the tree is empty
V.summary is a proto vEB tree with smaller universe
line 9 is only executed when PROTO-vEB-MINIMUM(V.summary) return NIL, which means V.summary is empty
if any of its cluster contains at least one key, the summary will not be empty

20.2-8
there are now u^(3/4) clusters in each tree, and the summary has universe u^(3/4)
high(x)     = [x / u^(1/4)]
low(x)      = x % u^(1/4)
index(h, l) = h * u^(1/4) + l
MEMBER:
    search cluster high(x) for low(x)
    T(n)    = T(n^(1/4)) + O(1)
    T(2^m)  = T(2^(m/4)) + O(1)
    S(m)    = S(m/4) + O(1)
            = Θ(lgm) = Θ(lglgn)
MINIMUM and MAXIMUM:
    at most two recursive calls, one to the summary and one to a cluster
    T(n)    = T(n^(1/4)) + T(n^(3/4)) + O(1)
    S(m)    = S(m/4) + S(3m/4) + O(1)
    by Akra-Bazzi method, (1/4)^p + (3/4)^p = 1, p = 1
    S(m)    = Θ(m(1 + int(1, m, 1/u^2)))
            = Θ(m(1 + 1 - 1/m))
            = Θ(m - 1) = Θ(m)
    T(n)    = S(m) = Θ(lgn)
SUCCESSOR and PREDECESSOR:
    at most two recursive calls and a call to MINIMUM
    T(n)    = T(n^(3/4)) + T(n^(1/4)) + Θ(lgn)
    S(m)    = S(3m/4) + S(m/4) + Θ(m)
    by Akra-Bazzi method, p = 1, g(m) = Θ(m) >= cm
    S(m)    = Ω(m(1 + int(1, m, cu / u^2)))
            = Ω(m(1 + ln(m) - 0)
            = Ω(mlgm)
    similarly S(m) = O(mlgm), S(m) = Θ(mlgm)
    T(n)    = S(m) = Θ(lgnlglgn)
INSERT:
    two recursive calls, similar to MINIMUM, Θ(lgn)
DELETE:
    follow implementation of 20.2-3, two recursive calls, similar to MINIMUM, Θ(lgn)

20.3-1
same to 20.2-4

20.3-2
./CLRS/structure/veb-tree.ts

20.3-3
./CLRS/structure/veb-tree.ts#factory

20.3-4
insertion with duplicate keys will cause exceptions
at line 1-2
    the current tree is empty, no duplicate keys
at line 3-4
    x < V.min, no duplicate keys
at line 6-8
    the cluster that x will go into is empty, no duplicate keys
at line 9
    insertion into a tree with strictly smaller universe
    the only line in the procedure that may be executed when inserting a duplicate key
    the procedure will recursively call line 9 until V.cluster doesn't exist (i.e. the base case)
line 10-11 executes only when x > V.max, does not handle duplicate keys
deletion of non-exist keys will cause trouble when V has a single key V.min and x !== V.min
at line 1-3 the only key in the tree will be deleted, without checking whether it is x
at line 4-8
    V.min != V.max, both 0 and 1 exist in the base case, x may only be 0 or 1
line 9-12 will not execute as x cannot be equal to V.min
at line 13
    deletion from a tree with strictly smaller universe, by induction only cause inconsistency at line 1-3
at line 14-15
    deletion from summary with strictly smaller universe, by induction only cause inconsistency at line 1-3
line 16-22 will not execute as x cannot be equal to V.max
by maintaining another array A of size u, set the bit A[x] when inserting x, reset A[x] when deleting x
A[x] indicates if x is in the tree and can be checked in constant time

20.3-5
MEMBER:
    T(u)    = T(u^(1/k)) + O(1)
    T(2^m)  = T(2^(m/k)) + O(1)
    S(m)    = S(m/k) + O(1) = O(lgm) as k > 1
    T(u)    = S(m) = O(lglgu)
MINIMUM and MAXIMUM:
    still O(1)
SUCCESSOR and PREDECESSOR:
    T(u)    <= max{T(u^(1/k)), T(u^(1 - 1/k))} + O(1)
    for k < 2
    T(u)    <= T(u^(1/k)) + O(1)
            = O(lglgu)
    for k >= 2
    T(u)    <= T(u^(1 - 1/k)) + O(1)
            = T(u^((k-1)/k)) + O(1)
    S(m)    = S(m(k-1)/k) + O(1)
            = O(lgm)
    T(u)    = O(lglgu)
INSERTION:
    similar to SUCCESSOR
    T(u)    <= max{T(u^(1/k)), T(u^(1 - 1/k))} + O(1)
            = O(lglgu)
DELETION:
    similar to INSERTION

20.3-6
(O(u) + nO(lglgu)) / n = O(lglgu)
O(u) / n = O(lglgu)
n = Ω(u/lglgu)

20-1
a.  each internal node allocates an array of size u^(1/2) has u^(1/2) clusters and a summary
    both clusters and summary are a vEB tree with universe u^(1/2)
    thus P(u) = (u^(1/2) + 1)P(u^(1/2)) + Θ(u^(1/2))
b.  assume P(u) <= cu - d
        P(u)    <= (u^(1/2) + 1)(c(u^(1/2)) - d) + Θ(u^(1/2))
                = cu - (d-1)(u^(1/2)) - d + Θ(u^(1/2))
                <= cu - d by ajusting d-1 so it covers Θ(u^(1/2))
        P(u)    = O(u)
c.  ./CLRS/structure/veb-tree.ts#insert
d.  ./CLRS/structure/veb-tree.ts#successor
e.  with universal hashing, accessing a dynamic hash table takes O(1) expected time
    the same analysis in text still applies, but now probabilistic
f.  for u = 2
        insertion into the base case will take O(1)
    for u = 2^k
        insertion either creates a new cluster or not
        if it creates a new cluster, the key will become min of the new cluster
        the new cluster contains no child clusters or summary, takes O(1) space
        the hash table may need to be extended, takes O(1) amortized space
        the summary may need to be updated
        as summary has strictly smaller universe, insertion into summray takes O(1) amortized time
        if it didn't create a new cluster, the insertion consumes no space at all
    inductively insertion will take O(1) amortized space, n insertion will take O(n) worst case space
g.  O(1), each line in CREATE-NEW-RS-vEB-TREE takes constant time (including creation of a dynamic hash table)

20-2
a.  each number and all its prefixes have 1 + 2 + .. lgu = O(lg^2u) characters in total
    each number also allocates a single node in the linked list
    if n numbers are stored, the structure takes at most nO(lg^2u + 1) = O(nlg^2u) space
    O(nlgu) if each string can be stored in constant space, this seems to be the supposed answer
b.  the doubly linked list should only contain actual numbers, not their prefixes
    or there's no way to distinguish a prefix and a number in the hash table
    MINIMUM and MAXIMUM:
        take the first or last node in the circular doubly linked list
    MEMBER:
        assume hashing x takes constant time, check if x is in the hash table takes O(1)
    SUCCESSOR:
        binary search p the longest prefix of x in the hash table that satisfies:
            1.  x = p||0||w
            2.  p||1 exists
        then p is the longest common prefix of x and its successor y since:
            any other number with a different prefix will be < x or > y
            assume there exists some z that x < z < y
            if z = p||1||w', p is still the longest common prefix of x and z
            if z = p||0||w', it must be different at some bit with x
            the first such difference must be z = p'||1||w and x = p'||0||w' since z > x
            then the binary search will find p' instead of p
        //  the structure introduced in this part is actually x-fast trie
        //  but instead of descendent pointer and leaf list this structure maintains a sorted linked list of all nodes
        //  no matter how the nodes are sorted, traversing down to the nearest child may take O(lgu)
        //  if they are sorted numerically, the next node is not even guarenteed to be in the same subtree
        //  if they are sorted lexicographically, traversing down to a leaf may take time O(lgu)
    INSERT:
        find y the predecessor of x in O(lglgu)
        allocate a node for x, insert it after the node for y, O(1)
        insert x and the pointer to its node into the hash table, O(1)
        insert each prefix of x into the hash table, O(lgu) prefixes of x must be inserted
        O(lgu) in total
    DELETE:
        delete the node from the linked list, delete the key from the hash table
        for each prefix p of x from the longest to shortest:
            if x = p||1, search p||0 in the hash table
            if p||0 doesn't exist, delete p as well
        each iteration takes O(1), O(lgu) prefixes may be deleted, O(lgu) in total
c.  there are only O(n/lgu) elements in the x-fast trie which occupies O((n/lgu)lgu) = O(n) space
    each representative corresponds to a O(lgu)-node red black tree which occupies O(lgu) space
    O(n) + (n/lgu)O(lgu) = O(n) space in total
d.  MINIMUM:
        find x the minimum in the x-fast trie
        the actual minimum must be in the binary tree repesented by x, since all other trees has keys greater than x
        as red black tree is balanced, h = O(lglgu), find the minimum in the tree takes O(h) = O(lglgu) time
        O(1) + O(lglgu) = O(lglgu) in total
    MAXIMUM: symmetric
e.  for input x, find its successor s in x-fast trie, or s = x if x is itself a representative
    x must be in the tree represented by s or nowhere in the structure, since
        all other keys in the x-fast trie either < x or > s
        if a representative y < x, all nodes in the tree represented by y contains key <= y < x
        if a representative y > s, all nodes in the tree represented by y > s >= x
    search the successor of x in x-fast trie takes time O(lglgu), search for x takes O(1)
    search x in the tree takes time O(lglgu), O(lglgu) in total
f.  SUCCESSOR:
        again for key x, find its successor s in x-fast trie, find successor ss of s
        the successor of x is either
            the successor of x in tree s, or
            the minimum of tree ss
        all four operations can be done in O(lglgu), O(lglgu) in total
    PREDECESSOR: symmetric
g.  they must update the red black tree of height Ω(lglgu)
    in which a node is always inserted to the bottom level
h.  use 2-3-4-trees instead of red black trees
    split and join trees if their sizes are too big or too small
    by definition and 18-2, a tree, its representative and the next tree can be joined in O(lglgu)
    if a tree contains more than 2lgu nodes, split it around the median results in two trees of size lgu
    (the size of each subtree can be easily maintained in each node to support order statistics operations)
    if a tree contains less than lgu/4 nodes, join it with a neighbour in O(lglgu)
    the resulting tree contains lgu/2 to 9lgu/4 nodes, split it if it contains more than 3lgu/2 nodes
    there will be at most 2n/lgu representatives, x-fast trie operations still O(lglgu)
    each tree has lgu/2 to 2lgu nodes, tree operations still O(lglgu)
    a join or split is only caused by at least lgu/4 insertion or deletion, O(lglgu) / Ω(lgu) = O(1)
    the join and split operation takes O(1) amortized time

Chapter 21
21.1-1
Unioned d and i
{a}
{b}
{c}
{i, d}
{e}
{f}
{g}
{h}
{j}
{k}
Unioned f and k
{a}
{b}
{c}
{i, d}
{e}
{k, f}
{g}
{h}
{j}
Unioned g and i
{a}
{b}
{c}
{i, d, g}
{e}
{k, f}
{h}
{j}
Unioned b and g
{a}
{i, d, g, b}
{c}
{e}
{k, f}
{h}
{j}
Unioned a and h
{h, a}
{i, d, g, b}
{c}
{e}
{k, f}
{j}
Unioned i and j
{h, a}
{i, d, g, b, j}
{c}
{e}
{k, f}
Unioned d and k
{h, a}
{i, d, g, b, j, k, f}
{c}
{e}
Unioned a and e
{h, a, e}
{i, d, g, b, j, k, f}
{c}

21.1-2
if two vertices v and u are in the same connected component
there is a simple path v ~> u in the graph, inductively,
if v ~> u has length 1, there is an edge (v, u)
    line 3-5 of CONNECTED-COMPONENTS calls UNION(v, u) if v and u is not already in the same set
    v and u must be in the same set after CONNECTED-COMPONENTS
if v ~> u has length k, assume it is v ~> w -> u where (w, u) is an edge
    by induction v ~> w has length k-1, v and w will be in the same set after CONNECTED-COMPONENTS
    as (w, u) is an edge, w and u will be in the same set after CONNECTED-COMPONENTS
therefore v and u will be in the same set after CONNECTED-COMPONENTS
for a vertex v, after k iteration of line 3-5 in CONNECTED-COMPONENTS
for each vertices u in the same set of v, there is a path u ~> v of length at most k
for k = 1,
    u may be in the same set with v iff edge (u, v) exists and is chosen in the first iteration
    u ~> v has length 1
inductively,
    each u in the same set with v has a path u ~> v of length at most k after k iterations
    if this iteration adds another vertex w to the set, there is an edge (u, w)
    then w -> u ~> v is a path of length at most k + 1
therefore after |E| iterations, u and v will be in the same set only if there is a path u ~> v
if u and v are not in the same connected component, there is no path u ~> v, u and v are not in the same set
A => B and ~A => ~B, A <=> B

21.1-3
FIND-SET is called twice for each edge, 2|E| in total
within each connected component of size n,
    at beginning there are n disjoint sets in the component
    each call to UNION decreases the number of disjoint sets by 1
    on termination there is only one disjoint set for each connected component
there are exactly n - 1 calls to UNION within each connected component
and no UNION may happen between between components
Σ(i = {1 .. k})(ni - 1) = |V| - k calls to UNION in total

21.2-1
./CLRS/structure/disjoint-set-list.ts

21.2-2
./CLRS/structure/index.ts#problem_21_2_2
set of x2
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}
set of x9
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}

21.2-3
O(nlgn) / n = O(lgn)

21.2-4
n MAKE-SET operations take Θ(n)
each UNION operation is performed with one set having size 1, O(1) for each, O(n) in total
Θ(n) overall running time

21.2-5
each list contains a single pointer to its tail
each list element contains two pointers, one to the set object, one to the previous element or NIL
union(g, e) updates pointers to set object in e by traversing prev pointers from tail to head
then set prev of head to tail of g, set tail of g to tail of e
union is still linear in the size of disjoint set containing e, Theorem 21.1 still applies

21.2-6
union(g, e) update pointers to set object in e by traversing next pointers from head of e to tail
on termination it has a pointer to the tail node in set e
set next of tail to head of g, set head of g to head of e
the list of e is prepended to the list of g
running time still linear in the size of e, Theorem 21.1 still applies

21.3-1
./CLRS/structure/index.ts#problem_21_3_1
set of x2
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}
set of x9
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}

21.3-2
store all ancestors traversed in a stack
./CLRS/structure/disjoint-set-forest.ts#findSet

21.3-3
first n-1 UNION operations form a single binomial tree of degree lgn
the deepest leaf in a binomial tree of degree k has depth k
obey union by rank by always union disjoint trees with same height from height 0
then n FIND-SET operations to the deepest leaf nodes, each traverses a path from leaf to root, Ω(h) = Ω(lgn) each
m = 2n - 1 = Θ(n), total running time is Ω(nlgn) = Ω(mlgn)

21.3-4
thanks https://stackoverflow.com/questions/22945058/
each node stores a new pointer next that originally points to itself after MAKE-SET
next links form a circular linked list of elements in a disjoint set
when linking two root x and y, exchange their next pointers
following x.next now traverses all nodes in disjoint set represented by y first
the last node traversed has its next pointer points to y
following y.next then traverses all nodes in the disjoint set represented by x, go back to x at the end

21.3-5
define r(x) as
    x.rank before the first call to FIND-SET(x)
    0 after the first call to FIND-SET(x)
define the potential function Φ(T) = Σr(x) for all node x in the disjoint set
each call to MAKE-SET(x) takes constant time and does not change the potential, O(1) amortized time
each call to LINK at most increases Φ(T) by 1, takes O(1) amortized time combined with the O(1) actual time
each call to FIND-SET(x) will recurse more than once only if it's the first call to FIND-SET(x)
otherwise x.p will be set to the representative, x.p.p = x.p (assuming x is not linked to other nodes later)
FIND-SET(x) recursively calls FIND-SET at most twice
the number of recursive calls in FIND-SET(x) <= 2 + k
where k is the number of nodes y on the path from x to the representative that FIND-SET(y) is never called before
for each recursive call, denote y = x.p, y.rank >= 1 since rank is an upper limit of height
calling FIND-SET(y) reduces r(x) to 0, Φ(T) decreased by at least 1
by adjusting the constant in Φ(T), each recursive call to FIND-SET is paid by the potential function
all three operations take O(1) amortized time, m operations take O(m) worst case time