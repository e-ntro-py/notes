1.1
a.  ability to act rationally based on knowledge
b.  intelligence originates not in biomass but in artifacts
c.  actor
d.  make near optimal decisions under certain criteria
e.  inference correct conclusions given a set of facts

1.2
https://www.csee.umbc.edu/courses/471/papers/turing.pdf
The Theological Objection:
    have no familiarity in christianity
    by my buddhist grandma, even the most insignificant kind of bugs have souls equal to human's
    so why not intelligent machines
The "Heads in the Sand" Objection
    this idea is even stronger today fueled by 70 years of sci-fi movies and literatures since then
    threat of a possible "AI domination" scenario is unlikely to stop the creation of AI (if ever possible)
    similar to nuclear weapons, AI technology is too much an advantage to any major power
The Mathematical Objection
    machines cannot solve the halting problem for certain, yet there's no proof whether human can do it better
    modern compilers can sometimes detect infinite loops which are unobvious to human eyes
The Argument from Consciousness
    seems to be the most relevant objection today and Turing's refutations on this topic were quite weak
    the idea was later expanded by the Chinese room argument
    recent breakthrough in ML and AI research made machines more rational, but not necessarily more humanly
Arguments from Various Disabilities
    either solved by now or not falsifiable at first place
Lady Lovelace's Objection
    if creativity on music composing or literature writing can be specified in terms of scores
    ML algorithms can train a machine to do it
    this objection lands more on ambiguity in human feelings / languages than ability of machines
Argument from Continuity in the Nervous System 
    no idea 
The Argument from Informality of Behaviour 
    AI today is already more error-prone than human while e.g. driving a car
The Argument from Extrasensory Perception
    can't believe they took ESP into consideration seriously
    was that common back in 1950s?
it turns out unskilled interrogator is too easy to fool:
    "Artificial Stupidity", The Economist, 324 (7770): 14, 1 August 1992
at this rate no one knows what will possibly happen in 50 years 

1.3
it's rational in the sense that it's based on practical reasons instead of emotions
it's not intelligent as it's not a decision by reasoning

1.4
IQ test is designed for normal human beings, not machines on a special task
the microworld of IQ test is overly restrictive

1.5
10^4 / 10^-3 = 10^7
human power: 10^14

1.6
1.  people lies for various reasons
2.  people who lacks certain knowledges cannot express their thought and emotion correctly
3.  people with satisfactory knowledge of thought and emotion is a biased sample

1.7
a.  pattern recognition for non-natural patterns (artifical barcodes)
b.  pattern recognition (NLP) & decision making (order of entries by preference / collected browse histories)
c.  pattern recognition for natural patterns (human voice)
d.  decision making

1.8
these computational models immitates result of human evolution instead of human knowledge
people don't have to understand the ATP cycle to lift and get fit

1.9
self-preservation as a population
by natural selection, entities without the urge of self-preservation are more likely to be eliminated by the environment
and in most case self-preservation is considered rational

1.10
both the theory and the realization of the theory should be considered AI
recent boom in ML is mostly boosted by powerful video cards 

1.11
the latter is true in the same way to 
"human thoughts are just a bunch of electrical and chemical signals being tossed around inside the brain"
thus the first sentence cannot be derived from the latter

1.12
the latter is not true, human thought and behavior is not that deterministic by gene

1.13
same to 1.11

1.14
List of entries not possible yet:
    h:  NLP or the ability of catching fuzzy ideas as "funny" is still lagging behind other aspects
    k:  not heard, but AI can help surgeons perform better
        by https://publishing.rcseng.ac.uk/doi/pdf/10.1308/rcsbull.2017.87, better computer vision is the missing piece

1.15
//  skipped

2.1
consider a vaccum cleaner with limited battery, in an environment where dust is generated randomly over time
performance is again measured by how clean the floor is on average 
if T is short compared to the battery capacity
the cleaner may choose to search very actively, consume more energy than optimal in the first T time steps
and do no work at all afterwards 

2.2
a.  there are 4 cases for the initial environment, 2 cases of initial place of the vaccum cleaner
    4 * 2 = 8 inital cases in total, where half the cases are symmetric
    assume the cleanliness is asserted at the start of each time step 
    A: Clean, B: Clean, Start: A,
        performance measure is 2000 for any agents
    A: Dirt, B: Clean, Start: A,
        the agent cleans dirt in square A in time step 1, move back and forth afterwards
        performance measure is 1999
        1999 is the optimal performance as the dirty square cannot be cleaned before time step 1
    A: Dirt, B: Clean, Start: B,
        the agent moves to square A at the end of time step 1, clean square A in time step 2
        performance measure is 1998
        1998 is optimal as any agent needs at least 2 time steps to move and clean the dirt in square A
    A: Dirt, B: Dirt, Start: A,
        1: clean A, 2: move to B, 3: clean B
        performance measure is 1996
        1996 is optimal as any agent needs at least 3 time steps to clean the two squares
b.  same to figure 2.3 but stop moving when no square is dirty
    if the agent can observe the cleanliness of both squares, no internal state required
    otherwise the agent has to remember the cleanliness of another square
c.  it may be beneficial to learn the dirt distribution and the geography of the room
    dirts will be cleaned earlier if the agent returns to the square with a highest posibility to generate dirt

2.3
a.  false, the vaccum cleaner agent in figure 2.2 is perfectly rational despite sensing only partial information
b.  true, an environment in which a button should be pushed when two parts are defected in a row
c.  true, a vaccum cleaner world as figure 2.3 in which no dirt are ever generated
d.  false, agent function always takes sequence of percepts, while agent program may be pure reflex
e.  false, it may be phisically impossible due to limitation on storage or computation power
f.  true, where the performance measure is independent to the actions of agents
g.  true, the vaccum cleaner agent in 2.2.b is also rational in the world defined in figure 2.3
h.  false, an agent that never clean squares cannot be rational in the vaccum cleaner world, with sensor or not
i.  false, it's logically impossible: what will happen when two of such agents play against each other?

2.4
a.  playing soccer (as a single player):
        P:  (depends on position) goals, assists, pass success rate, ...
        E:  position, momentum, ... of other players, ball, judges, ...
        A:  all possible moves of a human being
        S:  visual, sound and physical contact
b.  subsurface ocean of Titan (as a rover / submarine):
        P:  completeness and accuracy of aquired data, project cost, (if manned) casualty
        E:  Titan (atmosphere, climate, marine condition, ...)
        A:  sampling devices, thrust, ...
        S:  visual inputs
// skipped

2.5
agent function:         formal definition of the performance logic of an agent
agent program:          implementation of an agent function
autonomy:               ability to complete tasks / make decisions under no supervision
reflex agent:           agent whose action is independent to history of percepts
model-based agent:      agent which simulates unobservable part of environment as interal states from percepts
                        and make decision based on those simulations
goal-based agent:       agent which asserts and changes its actions by a binary performance measure (goal / defeat)
utility-based agent:    agent which asserts and changes its actions by a performance measure function
learning agent: agent   agent which asserts and improves its actions from experiences

2.6
a.  let F be an reflex agent function, P be an agent program implementing F
    let P' be a program that on a percept:
        runs P twice, return the result of the second run
    by definition P' is also an implementation of F
b.  none, ignoring storage and computation limitations, the table-driven method is always possible
c.  yes or no according to a more precise definition of randomized agent function / program
d.  infinite, there are infinite possible implementations of a trivial agent which yields a single action forever 
e.  no, by structure of the program it will only emit an action on percept inputs
    speed up the machine only boost the reaction speed of the agent
    if the agent is a valid implementation of the agent function at the first space it have to wait more

2.7
a.  function GOAL-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    DESIRABLE, boolean function with built-in goal
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if DESIRABLE(state)
                return action'
b.  function UTILITY-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    UTILITY, function from state to a performance measure
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if UTILITY(state) is maximized
                return action'

2.8
./AI/vaccum_cleaner

2.9
./AI/vaccum_cleaner
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 2000
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 2000
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1998
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 1998
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1996
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1996

2.10
a.  it cannot
    when there's initially no dirt in the environment, the simple reflex agent will move mindlessly
    an agent that does nothing at all will achieve score 2000 instead of 1000 by the simple reflex agent
b.  refer 2.2
c.  refer 2.2

2.11
./AI/vaccum_cleaner
a.  it cannot, a simple deterministic agent will only clean dirt on a fixed path
    if the initial dirt distribution generates no dirt along that path, no dirt will be cleaned by the reflex agent
    another agent may outperform it by cleaning even one square of dirt (by chance or better perception)
b.  it's possible as stated in part a
    results are concluded in part d
c.  a spiral environment
    map legend:
        #: obstacle
        %: dirty
         : clean
        @: start position of agent
    ##########
    #%%%%%%%%@
    #%########
    #%#%%%%%%#
    #%#%####%#
    #%#%#%%#%#
    #%#%%%%#%#
    #%######%#
    #%%%%%%%%#
    ##########
    after cleaning the square, the random cleaner will bump into walls frequently
    dispite there are only one correct direction to progress
d.  a stateful agent that records the visited part of the environment, decide the next moving direction by
        1. prioritize unvisited squares
        2. choose a random non-obstacle square if surrounded by visited squares
    all environments are 10 x 10 rectangles of clean and dirty squares or obstacles
    a square has
        1/8 chance to be an obstacle
        3/8 chance to be dirty
        4/8 chance to be clean
    score is measured by number of clean squares over a time period of 1000
    % ##%%# %%
    # # % % %#
    %%  %  %
    %   #  %
    %  #    %
    # %%#  %
    %###%#%###
    %% %% % #
    %#%#%  % #
    % #   #
    Bot position: (1, 0)
    RandomCleaner Score: 68262
    Stateful BumpCleaner Score: 73193
    #  %#% %
    % %# %
    %     %%
    # % %#% %%
    %% % #% %
    ## #%   %%
    % #% #%% %
    # % %%%
    % % %  %
    %#%% % # %
    Bot position: (4, 1)
    RandomCleaner Score: 70134
    Stateful BumpCleaner Score: 78198
    %  #  # %
    % # %  %
    ####  % %%
    %
    %%   %%%
    % #%#%% %
    #  %% %
    %%   % %
    %     %#%#
    ##%% % % %
    Bot position: (4, 8)
    RandomCleaner Score: 74664
    Stateful BumpCleaner Score: 80026
    %# %%  %#
    # #%%%#
    %  %#%%%#
    #   %  %
    %%# %
    %% #%%%%%%
    %%#%  %%#
    %%%%   %%%
    %%   %% %
    %% %% #%
    Bot position: (4, 3)
    RandomCleaner Score: 73039
    Stateful BumpCleaner Score: 82044
    #    %
    # %#% #%%#
    %%% % %%
    % % % % %
        # #  %
    %%% #%
    % % #  % #
    %%# %%
    %#%  %%%
    #  %   %
    Bot position: (6, 6)
    RandomCleaner Score: 74993
    Stateful BumpCleaner Score: 80545
    a cleaner cannot be perfectly rational without full percept of the environment

2.12
the agent in 2.11.d is already based on bump detection
if the bump detector fails, the cleaner should fall back to randomized reflex mode

2.13
a.  repeat the Clean action until the the dirt sensor gives "clean" twice in a row
b.  again a cleaner cannot be perfectly rational without full percept of the environment

3.1
without a goal, no action or state is rationally better than others, problem formulation is meaningless

3.2
let w be width of the maze, h be height of the maze
let i be the number of intersections
a.  4wh (4 orientations for each position)
b.  4i (4 orientations for each intersection)
c.  i, no need to keep track of orientations
d.  how the robot accelerate
    how the robot is controlled remotely
    position of the robot in the maze is in fact continuous 

3.3
a.  let G be a indirected graph
    each city is a node of G
    each road is an edge of G
    the state space is tuples of nodes (i, j) where i, j ∈ G.N
    starting state is (i0, j0) for some i0, j0 ∈ G.N
    for each state (i, j), an action is defined with 
        result: (i', j') where (i, i'), (j, j') ∈ G.E
        cost:   max(d(i, i'), d(j, j'))
b.  (i) is not admissible:
        let d(1, 2) = 1, d(2, 3) = 1, D(1, 3) = 2
        given start state (1, 3), the optimal solution has cost 1 < 2 = D(1, 3)
    (ii) >= (i), nor is (ii) admissible
    (iii):
        let start state be (i, j), let C*(i, j) be cost of the optimal solution
        if there's no solution to this instance, C*(i, j) is virtually infinite, greater than D(i, j) / 2 
        if there's an optimal solution to this instance
        basic: 
            i = j, C*(i, j) = D(i, j) / 2 = 0
        induction (on length of optimal solution):
            let the next state in the solution be (i', j')
            the cost of the next action is max(d(i, i'), d(j, j'))
            tail of the solution must be the optimal solution to the subproblem (i', j')
            (or a better solution can be constructed by replace the tail with a better solution to (i', j'))
            by induction on number of actions of optimal solution, C*(i', j') >= D(i', j') / 2
            C*(i, j)    = C*(i', j') + max(d(i, i'), d(j, j'))
                        >= D(i', j') / 2 + max(d(i, i'), d(j, j'))
                        >= (D(i', j') + d(i, i') + d(j, j')) / 2
                        >= D(i, j) / 2  // apply triangle inequality twice
c.  if they cannot stay in a city for a turn (i.e. there's no zero-cost self-cycle for each node)
        for each bi-partite graph with partitions {A, B}, if i0 ∈ A and j0 ∈ B
        if (i, j) is a successor of (i0, j0), by definition of bi-partite graph, i ∈ B and j ∈ A
        similarly, if (i', j') is a successor of (i, j), i ∈ A and j ∈ B
        since A and B are disjoint, there's no solution to the instance
    otherwise the shortest path i ~> j must contain infinite edges
d.  G.E = {(1, 1), (1, 2)}, start state = (1, 2)
    successors of (1, 2) are (1, 1) and (2, 1)
    (1, 1) is a goal state where 1 is visited twice by the first friend
    (2, 1) is symmetric to (1, 2), any state from (2, 1) will visit at least one city twice

3.4
//  only half of the proof
//  complete proof can be found in A Modern Treatment of the 15 Puzzle, AF Archer 1999
//  http://www.cs.cmu.edu/afs/cs/academic/class/15859-f01/www/notes/15-puzzle.pdf
represent the tiles as an array in row-major order
the goal state is a sorted array of numbers [1 - 8, E], where E is the empty tile
let I be the number of inversions in the state
I mod 2 is invariant by legal moves:
    a horizontal move will not change I
    when a tile A is moved downwards to the empty tile:
        there are two tiles B and C between A and the empty tile in row-major order
        if there are k inversions among (A, B), (A, C), after the move there will be 2 - k inversions
        I changeed by (2 - k) - k = 2 - 2k, which is even
the goal state [1, 2, 3, 4, 5, 6, 7, 8, E] has I mod 2 = 0
another state [1, 2, 3, 4, 5, 6, 8, 7, E] has I mod 2 = 1, both partitions are nonempty
by CLRS, number of inversions can be computed by modified merge sort in O(nlgn)
treat E as the tile number 9, then deduct the inversions caused by 9 from the result
search algorithms introduced in this chapter can only find an instance unsolvable after exhausting the state space
which is n! / 2 for an (n-1)-puzzle, the program might only terminate after years given enough space

3.5
a queen located left to a column may attack at most 3 squares in the column
hence there are at least n - 3i possible squares on ith column with n rows
S(n)    >= Π(n - 3i)
S^3(n)  >= Π(n - 3i)^3
        >= Π(n - 3i)(n - 3i - 1)(n - 3i - 2)
        = n!
S(n)    >= n^(1/3)
when n = 30, S(n) >= 10^10, any n greater may exhaust the memory of a personal computer

3.6
a.  let G be an indirected graph
    G.V = {i | i is a region on the map}
    G.E = {(i, j) | i and j are adjacent on the map}
    state of the problem is an array of length <= |V| of c ∈ Z4
    where 1 - 4 indicates different colors
    start state is the empty array []
    a state [c0 .. ck] is valid if:
        assign each ci to vertex i, no (i, j) ∈ G.E has ci = cj
    goal state is a valid state of length |V|
    for a state [c0, c1 .. ci], a possible action is defined as:
        result: [c0, c1 .. ci, ci+1] which is valid
        cost: 0
b.  state: position of monkey and crates (may be continuous)
    initial state: implied by the description of the problem
    goal state: monkey with banana
    actions: climb / leave the crates, move the crates around, grab the banana
    cost: unclear
c.  state: a set of records
    initial state: the file of records
    goal state: a singleton set of record
    actions: for a set of records R:
        result: R' ⊆ R, the program outputs "illegal input record" when fed R'
        cost: depends on how the result is computed
d.  state: amount of water in three jugs
    initial state: three empty jugs
    goal state: there's exactly one gallon of water in one of the jugs
    actions: given w0, w1, w2 as amount of water in jugs, c1, c2 and c3 be their capacities
        result: set wi = 0, or
                set wi = ci, or
                for some i != j, set wi = 0, wj = max(cj, wi + wj)
        cost: 1

3.7
a.  both uncountable
b.  when there are obstacles between two points on a plane, instead of the shortest (straight) path, the robot should 
    bypass the obstacle by turn away from the shortest path as little as possible, in which case the optimality of 
    polygon vertices over point on polygon sides and middle of nowhere can be roughly explained by triangle inequality
    the state space now contains all vertices of obstacles and the start and goal points
c.  let G be an indirected graph
    G.V = {i | i is a vertex of an obstacle, the start point or the goal point}
    G.E = {(i, j) | the straight line between point i and j is not blocked by any obstacle }
    state: any i ∈ G.V
    start state: the start point
    goal state: the goal point
    possible actions from a point i:
        result: j for all (i, j) ∈ G.E
        cost: straight line distance between i and j
d.  simply shortest path from single vertex after formulation, can be easily solved optimally with linear time and space

3.8
a.  consider only graph G that, for each states V, there is a path from start state to goal state passing V
    let start state be s, goal state be g
    let (v, u) be any transition in the graph, let G' be G without (v, u)
    let Cm be the maximum cost of any simple path in G'
    assume an algorithm which didn't examine (v, u) gave an optimal solution to G' with cost C*
    if the cost of (v, u) < -2Cm + C*, construct a path s ~> v -> u ~> g where s ~> v and u ~> g are simple paths
    by definition cost of such a path <= 2Cm + c(v, u) < C*
    hence ignoring any single transition in the graph may result in suboptimal solution
    an optimal algorithm must examine all transitions in the graph
b.  it won't help
    if the negative-cost edge forms a negative cost cycle in the graph
    then there's no optimal solution to the instance
c.  the optimal cost of the instance can be arbitrary low, i.e. no optimal solution
d.  the mental gain from repeatedly visiting the same scenery is diminishing (for sane people)
    formally the cost of an action is not independent to the history of actions
    enrich the state space with history of actions / states
    adjust action costs according to the history to reflect the diminishing gain in reality
e.  drug abuse
    the cost of taking drugs may not be significantly negative
    but the cost of not to take drugs will increase overtime and for each action of taking drugs
    to a point the drug abuser have no other choice
    
3.9
a.  state:  a five-tuple (M, C, M', C', B), where M, C, M' and C' are integers indicating missionaries and cannibals
            B are a binary value representing the position of the boat
    start state: (3, 3, 0, 0, 0)
    goal state: (0, 0, 3, 3, _)
    action: transfer at most two people from one side of the river to another while keeping M >= C and M' >= C'
            if B = 0, only transfer from M, C to M' and C' is valid and vice versa
            cost is uniform
b.  ./AI/searching
    this side: 3M + 3C, other side: 0M + 0C, boat on: This side
    this side: 2M + 2C, other side: 1M + 1C, boat on: Other side
    this side: 2M + 3C, other side: 1M + 0C, boat on: This side
    this side: 2M + 1C, other side: 1M + 2C, boat on: Other side
    this side: 2M + 2C, other side: 1M + 1C, boat on: This side
    this side: 1M + 1C, other side: 2M + 2C, boat on: Other side
    this side: 1M + 2C, other side: 2M + 1C, boat on: This side
    this side: 1M + 0C, other side: 2M + 3C, boat on: Other side
    this side: 1M + 1C, other side: 2M + 2C, boat on: This side
    this side: 0M + 0C, other side: 3M + 3C, boat on: Other side
    without checking repeated space, the searching will loop forever
c.  some move in an optimal solution is anti-intuitive

3.10
state: vertex, atomic description of the moving parts of the environment
state space: G.V, set of all possible states
search tree: tree derived from the state graph during searching
search node: a data structure keeping track of extra information tied to a state
goal: the destination in the state graph
action: transitions / edges in the state graph
transition model: adjacency list from a vertex in the state graph 
branching factor: out-degree of a vertex in the state graph

3.11
world state: the state in reality, without abstraction
state description: abstracted world state percepted and emulated by the agent
search node: data structure keeping track of state descriptions and else in search algorithm
none of any two of the above terms are equivalent:
several world state may correspond to the same state descriptions
there may be a lot search node for a single state description

3.12
for a problem with |V| states and branching factor b
making composite actions of n actions will increase total transitions from |V|b to |V|b^n
while only speed up the searching n times at best
the cost is not justified by the return

3.13
basic: 
    the frontier contains only the initial state s, path to any other state passes the initial state
induction:
    let u be the state expanded next, u is removed from the frontier after the iteration
    let s ~> v be the path from initial state to an unexplored state before expansion
    if u is not on s ~> v, as u is the only state removed from the frontier, s ~> v passes a state in the frontier
    if u is on s ~> v
        if v = u, v is nolonger unexplored
        if v != u, the path is s ~> u ~> v, where u ~> v must pass a successor of u
        by the algorithm, all successors of u are added to the frontier

3.14
a.  false, depth-first search may be quick to find suboptimal solutions
    if the initial state has an immediate goal state successor with high cost
    depth first search may accidentally explore it first and terminate
    A* search will only explore it after all node with lower-cost is explored
b.  true, h(n) = 0 is an admissible heuristic for any problem with non-negative costs
c.  false, not after proper abstraction
d.  true, breath-first search is ignorant to costs
e.  false, obvious

3.15
a.  digraph {
        0 [label="1"]
        1 [label="2"]
        2 [label="3"]
        3 [label="4"]
        4 [label="5"]
        5 [label="6"]
        6 [label="7"]
        7 [label="8"]
        8 [label="9"]
        9 [label="10"]
        10 [label="11"]
        11 [label="12"]
        12 [label="13"]
        13 [label="14"]
        14 [label="15"]
        0 -> 1
        0 -> 2
        1 -> 3
        1 -> 4
        2 -> 5
        2 -> 6
        3 -> 7
        3 -> 8
        4 -> 9
        4 -> 10
        5 -> 11
        5 -> 12
        6 -> 13
        6 -> 14
    }
b.  BFS:
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11
    depth-limited (3):
        1, 2, 4, 8, 9, 5, 10, 11
    iterative deepening:
        1,
        1, 2, 3,
        1, 2, 4, 5, 3, 6, 7,
        1, 2, 4, 8, 9, 5, 10, 11
c.  reverse transition model has branching factor 1
    the search will be trivial in reverse order
d.  see part e
e.  ./AI/searching

3.16
a.  state: a railway and a set of unconnected pieces
    start state: an empty railway and 32 unconnected pieces
    goal state: a railway with no overlapping and loose ends with no unconnected pieces
    action: plug one of the unconnected piece to a hole on the railway
            for unsymmatric pieces there are two ways to plug it to a hole
    cost: insignificant
b.  as all goal states are in the same depth (depth 32)
    DFS will be most memory-efficient without incurring suboptimal time complexity
    also there's no significant cost, hence no reasonable heuristic to guide the searching
c.  for there being no loose ends, the number of plugs and holes must match
    removing any one of the fork pieces then the instance won't meet the prerequisite above
d.  the railway under construction has at most 3 holes
    there are 12 + (16 + 2 + 2) * 2 = 52 ways to plug a piece to a hole
    the maximum branching factor is 52 * 3 = 168, with maximum depth 32 the state space is at most 168^32

3.17
a.  given a finite program instance, let the set of action costs be C
    number of subsets of C is also finite
    hence the minimum positive difference between the sum of any two subsets is greater than a constant ε
    in graph search, the cost of a path is the sum of a subset of C
    therefore each iteration of iterative lengthening search will increase the cost limit by ε
    let the optimal cost be C*, when the cost limit is smaller than C*, there's no solution
    the algorithm will retry with a greater cost limit
    when the cost limit is greater or equal to C*, given non-negative costs
    uniform-cost search, as an optimal search algorithm, will find the optimal solution
    as C* / ε is finite, the algorithm will definitely terminate
b.  with unit step costs, ε >= 1
    d iterations at most
c.  it depends on the minimum positive difference ε defind above
d.  ./AI/searching/ils

3.18
a state space with branching factor f in which all state at depth d is a goal state
DFS will always terminate in O(d) time, while IDDFS algorithm have to analysis Ω(f^d) states at least

3.19
//  skipped
//  tracing every link on a page is extremely complicated nowadays with all the front end arcane magics
//  the state space is infinite (dynamic generated pages)
astar with heuristic based on common parts of the url
one (cheat) strategy may be heading a search engine from the first page then search the url of the second page
bidirectional search is infeasible as there's no proper way to generate complete of a predecessor
even with a search engine (single page applications, js user interfaces e.g. React, etc.)

3.20
a.  state space is tiny, any algorithm will suffice
    a possible admissible heuristic is f(n) = dirty squares * 2 - 1
    each square requires at least two actions (move to and suck) to clean
    except the square under the cleaner which can be cleaned by a single action
    non-negative cost actions only, graph search is preferable
b.  %%%
    ...
    ...
    Cleaner: (1, 1)
    %%%
    ...
    ...
    Cleaner: (1, 0)
    %.%
    ...
    ...
    Cleaner: (1, 0)
    %.%
    ...
    ...
    Cleaner: (0, 0)
    ..%
    ...
    ...
    Cleaner: (0, 0)
    ..%
    ...
    ...
    Cleaner: (1, 0)
    ..%
    ...
    ...
    Cleaner: (2, 0)
    ...
    ...
    ...
    Cleaner: (2, 0)
c.  search cost is negligible in such a tiny state space (a few milliseconds at most)
    avarage cost of 1000 samples is 4.493
d.  the astar search agent is optimal
e.  state space has size 2^(n^2) * n^2, the search cost will dominate move cost quickly
    reflex agent will outperform the search agent for any moderately large n

3.21
a.  when the transition model has uniform cost for all actions
    g(n) = depth of the node n, the shallowest node will be expanded next
    that's exactly the behavior of BFS
b.  assume the depth of a node is stored in the node itself
    let f(n) = -n.depth, the deepest node will always be expanded next
    which is exactly the behavior of DFS
c.  when h(n) = 0, f(n) = g(n) + f(n) = g(n), as well as uniform-cost search

3.22
//  this line of the pseudo-code
//      (result, best.f) ← RBFS(problem, best, min(f.limit, alternative))
//  contains a very sneaky assignment to best.f
//  without this assignment the implementation will blow the stack up
./AI/searching/src/rbfs.rs
much slower
partially due to naive unoptimized RBFS vs A* from an optimized extern crate
also RBFS is a tree search, exploring much more redundant states compared to A*

3.23
//  skipped

3.24
start state: 0
goal state: 3
costs:
    0 -> 1: 0
    1 -> 2: 0
    0 -> 2: 1
    2 -> 3: 2
heuristics:
    1: 2
    2: 0
    3: 0
frontier = [1, 2] after expanding start state
g(2) + h(2) = 1 < g(1) + h(1), 2 will be expanded next
the path 0 -> 1 -> 2 will never be expanded as 0 -> 2 adds 2 to explored set

3.25
when w = 0, f(n) = 2g(n), the algorithm is uniform cost search, not complete in tree search
when 0 < w < 2, f(n) = (2 - w)(g(n) + w * h(n) / (2 - w))
the algorithm is A* search with heuristic w * h(n) / (2 - w)
depending on h(n), this algorithm may or may not be complete
when w = 2, f(n) = 2h(n), the algorithm is greedy best first search, not complete in tree search
when h(n) is admissible, for 0 <= w / (2 - w) <= 1, w <= 1, (w / (2 - w))h(n) is also admissible

3.26
a.  4
b.  4k in graph search
    2(k + 1)(k + 2) in tree search
c.  Θ(4^(x + y))
d.  2(x + y + 1)(x + y + 2)
e.  true (manhattan distance)
f.  all nodes in the rectance((0, 0), (x, y)), (x + 1)(y + 1) nodes
g.  true, shortest path p0 ~> p1 may only increase when some link is removed
h.  no, if there is a link ((0, 0), (x, y)) when |x| + |y| > 1, the heuristic is no longer admissible

3.27
a.  A(n^2, n) = (n^2)! / n!
b.  <= 5^n
c.  manhattan distance
    it cannot move more than one grid without another vehicle
d.  (i) is not admissible:
        when n = 2, Σh = 4 but the optimal solution has cost 2
    (ii) is not admissible:
        given a state:
            . 2 3
            . . 1
            . . .
        the goal state is:
            3 2 1
            . . .
            . . .
        max{h} = 2, but the state can reach the goal state in a single action
    (iii) is admissible:
        let s be an arbitrary state, s' be one of its successor
        let h = (h1 .. hn) be heuristics in state s, h' = (h1' .. hn') be heuristics in s'
        let hi = min{h},
            if the ith vehicle stay put, hi' = hi, min{h'} >= min{h}
            if the ith vehicle moves normally, hi' >= hi - 1, min{h'} >= min{h} - 1
            if the ith vehicle hops over the jth car, hj' = hj >= min{h}, min{h'} >= min{h}
        thus an action can at most deduce min{h} by 1
        the goal state has min{h} = 0, if (s0 .. sk) is an optimal solution, h(s0) <= k

3.28
a totally random heuristic can easily lead A* to suboptimal solutions
./AI/searching
the optimal solution has f(n*) = g(n*) + h(n*) <= C* + c
any node explored before the optimal solution must have f(n) <= f(n*) = C* + c
assume h(n) is non-negative, f(n) = g(n) + h(n) >= g(n), g(n) <= C* + c

3.29
h(n) <= c(n, a, n') + h(n')
for an arbitrary state n0, let (n0 .. nk) be an optimal solution
optimal cost is sum of action costs Σc(ni, ai, ni+1) = C*
for each pair (ni, ni+1), h(ni) <= c(ni, ai, ni+1) + h(ni+1)
    Σh(ni) <= Σ(c(ni, ai, ni+1) + h(ni+1))
    // all but h(n0) and h(nk) cancels each other
    h(n0) <= Σ(c(ni, ai, ni+1)) + h(nk)
    h(n0) <= C* + 0 = C*
the heuristic in exercise 3.24

3.30
a.  a relaxed version of TSP where the tour does not have to be a hamiltonian cycle
b.  no description for straight line distance as a heuristic function provided
c.  cities: random points from [0, 1]^2
    start city: one of the cities
    a state keeps track of the current position of the salesman and the traveled city 
d.  refer CLRS

3.31
moving a square to the blank square can at most fix one misplaced square
for a solvable puzzle
    4 5 3
    1 2 6
    7 8 .
h1 = 4, h2 = 4, Gaschnig's heuristic = 5, optimal solution has cost 16
if a square i is misplaced, it occupies the tile of another square j, which occupies tile of square k, ..
these misplacements therefore forms cycles
each misplacement cycle of length k requires k actions to fix if the blank square is on the cycle
otherwise k + 1 actions

3.32
//  skipped

4.1
a.  simple hill-climbing search
b.  BFS if no information are shared between threads
c.  first-choice hill climbing
d.  random walk
e.  random walk (with mutations instead of successors from actions)

4.2
change from 3.16:
when connecting the next piece to the track, the piece can be rotated by an angle a in the range [-10, 10]
an angle a is accepted as the next state with probability e^(a/T), where T = schedule(t) is a function on time

4.3
a.  ./AI/searching
    average cost ratio over 1000 samples is 1.2425902960619555
b.  // oddly nowhere in the paper mentioned the fitness function they used
    // skipped

4.4
over 1000 samples of eight queens:
Steepest ascent cost: 31.666097ms
Steepest ascent success ratio: 0.16
First choice cost: 33.931055ms
First choice success ratio: 0.157
Simulated annealing cost: 172.206779ms
Simulated annealing success ratio: 0.14
Random restart cost: 220.052079ms

4.5
