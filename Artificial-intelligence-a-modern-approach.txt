1.1
a.  ability to act rationally based on knowledge
b.  intelligence originates not in biomass but in artifacts
c.  actor
d.  make near optimal decisions under certain criteria
e.  inference correct conclusions given a set of facts

1.2
https://www.csee.umbc.edu/courses/471/papers/turing.pdf
The Theological Objection:
    have no familiarity in christianity
    by my buddhist grandma, even the most insignificant kind of bugs have souls equal to human's
    so why not intelligent machines
The "Heads in the Sand" Objection
    this idea is even stronger today fueled by 70 years of sci-fi movies and literatures since then
    threat of a possible "AI domination" scenario is unlikely to stop the creation of AI (if ever possible)
    similar to nuclear weapons, AI technology is too much an advantage to any major power
The Mathematical Objection
    machines cannot solve the halting problem for certain, yet there's no proof whether human can do it better
    modern compilers can sometimes detect infinite loops which are unobvious to human eyes
The Argument from Consciousness
    seems to be the most relevant objection today and Turing's refutations on this topic was quite weak
    the idea was later expanded by the Chinese room argument
    recent breakthrough in ML and AI research made machines more rational, but not necessarily more humanly
Arguments from Various Disabilities
    either solved by now or not falsifiable at first place
Lady Lovelace's Objection
    if creativity on music composing or literature writing can be specified in terms of scores
    ML algorithms can train a machine to do it
    this objection lands more on ambiguity in human feelings / languages than ability of machines
Argument from Continuity in the Nervous System 
    no idea 
The Argument from Informality of Behaviour 
    AI today is already more error-prone than human while e.g. driving a car
The Argument from Extrasensory Perception
    can't believe they took ESP into consideration seriously
    was that common back in 1950s?
it turns out unskilled interrogator is too easy to fool:
    "Artificial Stupidity", The Economist, 324 (7770): 14, 1 August 1992
at this rate no one knows what will possibly happen in 50 years 

1.3
it's rational in the sense that it's based on practical reasons instead of emotions
it's not intelligent as it's not a decision by reasoning

1.4
IQ test is designed for normal human beings, not machines on a special task
the microworld of IQ test is overly restrictive

1.5
10^4 / 10^-3 = 10^7
human power: 10^14

1.6
1.  people lies for various reasons
2.  people who lacks certain knowledges cannot express their thought and emotion correctly
3.  people with satisfactory knowledge of thought and emotion is a biased sample

1.7
a.  pattern recognition for non-natural patterns (artifical barcodes)
b.  pattern recognition (NLP) & decision making (order of entries by preference / collected browse histories)
c.  pattern recognition for natural patterns (human voice)
d.  decision making

1.8
these computational models immitates result of human evolution instead of human knowledge
people don't have to understand the ATP cycle to lift and get fit

1.9
self-preservation as a population
by natural selection, entities without the urge of self-preservation are more likely to be eliminated by the environment
and in most case self-preservation is considered rational

1.10
both the theory and the realization of the theory should be considered AI
recent boom in ML is mostly boosted by powerful video cards 

1.11
the latter is true in the same way to 
"human thoughts are just a bunch of electrical and chemical signals being tossed around inside the brain"
thus the first sentence cannot be derived from the latter

1.12
the latter is not true, human thought and behavior is not that deterministic by gene

1.13
same to 1.11

1.14
List of entries not possible yet:
    h:  NLP or the ability of catching fuzzy ideas as "funny" is still lagging behind other aspects
    k:  not heard, but AI can help surgeons perform better
        by https://publishing.rcseng.ac.uk/doi/pdf/10.1308/rcsbull.2017.87, better computer vision is the missing piece

1.15
//  skipped

2.1
consider a vaccum cleaner with limited battery, in an environment where dust is generated randomly over time
performance is again measured by how clean the floor is on average 
if T is short compared to the battery capacity
the cleaner may choose to search very actively, consume more energy than optimal in the first T time steps
and do no work at all afterwards 

2.2
a.  there are 4 cases for the initial environment, 2 cases of initial place of the vaccum cleaner
    4 * 2 = 8 inital cases in total, where half the cases are symmetric
    assume the cleanliness is asserted at the start of each time step 
    A: Clean, B: Clean, Start: A,
        performance measure is 2000 for any agents
    A: Dirt, B: Clean, Start: A,
        the agent cleans dirt in square A in time step 1, move back and forth afterwards
        performance measure is 1999
        1999 is the optimal performance as the dirty square cannot be cleaned before time step 1
    A: Dirt, B: Clean, Start: B,
        the agent moves to square A at the end of time step 1, clean square A in time step 2
        performance measure is 1998
        1998 is optimal as any agent needs at least 2 time steps to move and clean the dirt in square A
    A: Dirt, B: Dirt, Start: A,
        1: clean A, 2: move to B, 3: clean B
        performance measure is 1996
        1996 is optimal as any agent needs at least 3 time steps to clean the two squares
b.  same to figure 2.3 but stop moving when no square is dirty
    if the agent can observe the cleanliness of both squares, no internal state required
    otherwise the agent has to remember the cleanliness of another square
c.  it may be beneficial to learn the dirt distribution and the geography of the room
    dirts will be cleaned earlier if the agent returns to the square with a highest posibility to generate dirt

2.3
a.  false, the vaccum cleaner agent in figure 2.2 is perfectly rational despite sensing only partial information
b.  true, an environment in which a button should be pushed when two parts are defected in a row
c.  true, a vaccum cleaner world as figure 2.3 in which no dirt are ever generated
d.  false, agent function always takes sequence of percepts, while agent program may be pure reflex
e.  false, it may be phisically impossible due to limitation on storage or computation power
f.  true, where the performance measure is independent to the actions of agents
g.  true, the vaccum cleaner agent in 2.2.b is also rational in the world defined in figure 2.3
h.  false, an agent that never clean squares cannot be rational in the vaccum cleaner world, with sensor or not
i.  false, it's logically impossible: what will happen when two of such agents play against each other?

2.4
a.  playing soccer (as a single player):
        P:  (depends on position) goals, assists, pass success rate, ...
        E:  position, momentum, ... of other players, ball, judges, ...
        A:  all possible moves of a human being
        S:  visual, sound and physical contact
b.  subsurface ocean of Titan (as a rover / submarine):
        P:  completeness and accuracy of aquired data, project cost, (if manned) casualty
        E:  Titan (atmosphere, climate, marine condition, ...)
        A:  sampling devices, thrust, ...
        S:  visual inputs
// skipped

2.5
agent function:         formal definition of the performance logic of an agent
agent program:          implementation of an agent function
autonomy:               ability to complete tasks / make decisions under no supervision
reflex agent:           agent whose action is independent to history of percepts
model-based agent:      agent which simulates unobservable part of environment as interal states from percepts
                        and make decision based on those simulations
goal-based agent:       agent which asserts and changes its actions by a binary performance measure (goal / defeat)
utility-based agent:    agent which asserts and changes its actions by a performance measure function
learning agent: agent   agent which asserts and improves its actions from experiences

2.6
a.  let F be an reflex agent function, P be an agent program implementing F
    let P' be a program that on a percept:
        runs P twice, return the result of the second run
    by definition P' is also an implementation of F
b.  none, ignoring storage and computation limitations, the table-driven method is always possible
c.  yes or no according to a more precise definition of randomized agent function / program
d.  infinite, there are infinite possible implementations of a trivial agent which yields a single action forever 
e.  no, by structure of the program it will only emit an action on percept inputs
    speed up the machine only boost the reaction speed of the agent
    if the agent is a valid implementation of the agent function at the first space it have to wait more

2.7
a.  function GOAL-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    DESIRABLE, boolean function with built-in goal
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if DESIRABLE(state)
                return action'
b.  function UTILITY-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    UTILITY, function from state to a performance measure
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if UTILITY(state) is maximized
                return action'

2.8
./AI/vaccum_cleaner

2.9
./AI/vaccum_cleaner
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 2000
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 2000
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1998
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 1998
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1996
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1996

2.10
a.  it cannot
    when there's initially no dirt in the environment, the simple reflex agent will move mindlessly
    an agent that does nothing at all will achieve score 2000 instead of 1000 by the simple reflex agent
b.  refer 2.2
c.  refer 2.2

2.11
./AI/vaccum_cleaner
a.  it cannot, a simple deterministic agent will only clean dirt on a fixed path
    if the initial dirt distribution generates no dirt along that path, no dirt will be cleaned by the reflex agent
    another agent may outperform it by cleaning even one square of dirt (by chance or better perception)
b.  it's possible as stated in part a
    results are concluded in part d
c.  a spiral environment
    map legend:
        #: obstacle
        %: dirty
         : clean
        @: start position of agent
    ##########
    #%%%%%%%%@
    #%########
    #%#%%%%%%#
    #%#%####%#
    #%#%#%%#%#
    #%#%%%%#%#
    #%######%#
    #%%%%%%%%#
    ##########
    after cleaning the square, the random cleaner will bump into walls frequently
    dispite there are only one correct direction to progress
d.  a stateful agent that records the visited part of the environment, decide the next moving direction by
        1. prioritize unvisited squares
        2. choose a random non-obstacle square if surrounded by visited squares
    all environments are 10 x 10 rectangles of clean and dirty squares or obstacles
    a square has
        1/8 chance to be an obstacle
        3/8 chance to be dirty
        4/8 chance to be clean
    score is measured by number of clean squares over a time period of 1000
    % ##%%# %%
    # # % % %#
    %%  %  %
    %   #  %
    %  #    %
    # %%#  %
    %###%#%###
    %% %% % #
    %#%#%  % #
    % #   #
    Bot position: (1, 0)
    RandomCleaner Score: 68262
    Stateful BumpCleaner Score: 73193
    #  %#% %
    % %# %
    %     %%
    # % %#% %%
    %% % #% %
    ## #%   %%
    % #% #%% %
    # % %%%
    % % %  %
    %#%% % # %
    Bot position: (4, 1)
    RandomCleaner Score: 70134
    Stateful BumpCleaner Score: 78198
    %  #  # %
    % # %  %
    ####  % %%
    %
    %%   %%%
    % #%#%% %
    #  %% %
    %%   % %
    %     %#%#
    ##%% % % %
    Bot position: (4, 8)
    RandomCleaner Score: 74664
    Stateful BumpCleaner Score: 80026
    %# %%  %#
    # #%%%#
    %  %#%%%#
    #   %  %
    %%# %
    %% #%%%%%%
    %%#%  %%#
    %%%%   %%%
    %%   %% %
    %% %% #%
    Bot position: (4, 3)
    RandomCleaner Score: 73039
    Stateful BumpCleaner Score: 82044
    #    %
    # %#% #%%#
    %%% % %%
    % % % % %
        # #  %
    %%% #%
    % % #  % #
    %%# %%
    %#%  %%%
    #  %   %
    Bot position: (6, 6)
    RandomCleaner Score: 74993
    Stateful BumpCleaner Score: 80545
    a cleaner cannot be perfectly rational without full percept of the environment

2.12
the agent in 2.11.d is already based on bump detection
if the bump detector fails, the cleaner should fall back to randomized reflex mode

2.13
a.  repeat the Clean action until the the dirt sensor gives "clean" twice in a row
b.  again a cleaner cannot be perfectly rational without full percept of the environment

3.1
without a goal, no action or state is rationally better than others, problem formulation is meaningless

3.2
let w be width of the maze, h be height of the maze
let i be the number of intersections
a.  4wh (4 orientations for each position)
b.  4i (4 orientations for each intersection)
c.  i, no need to keep track of orientations
d.  how the robot accelerate
    how the robot is controlled remotely
    position of the robot in the maze is in fact continuous 

3.3
a.  let G be a indirected graph
    each city is a node of G
    each road is an edge of G
    the state space is tuples of nodes (i, j) where i, j ∈ G.N
    starting state is (i0, j0) for some i0, j0 ∈ G.N
    for each state (i, j), an action is defined with 
        result: (i', j') where (i, i'), (j, j') ∈ G.E
        cost:   max(d(i, i'), d(j, j'))
b.  (i) is not admissible:
        let d(1, 2) = 1, d(2, 3) = 1, D(1, 3) = 2
        given start state (1, 3), the optimal solution has cost 1 < 2 = D(1, 3)
    (ii) >= (i), nor is (ii) admissible
    (iii):
        let start state be (i, j), let C*(i, j) be cost of the optimal solution
        if there's no solution to this instance, C*(i, j) is virtually infinite, greater than D(i, j) / 2 
        if there's an optimal solution to this instance
        basic: 
            i = j, C*(i, j) = D(i, j) / 2 = 0
        induction (on length of optimal solution):
            let the next state in the solution be (i', j')
            the cost of the next action is max(d(i, i'), d(j, j'))
            tail of the solution must be the optimal solution to the subproblem (i', j')
            (or a better solution can be constructed by replace the tail with a better solution to (i', j'))
            by induction on number of actions of optimal solution, C*(i', j') >= D(i', j') / 2
            C*(i, j)    = C*(i', j') + max(d(i, i'), d(j, j'))
                        >= D(i', j') / 2 + max(d(i, i'), d(j, j'))
                        >= (D(i', j') + d(i, i') + d(j, j')) / 2
                        >= D(i, j) / 2  // apply triangle inequality twice
c.  if they cannot stay in a city for a turn (i.e. there's no zero-cost self-cycle for each node)
        for each bi-partite graph with partitions {A, B}, if i0 ∈ A and j0 ∈ B
        if (i, j) is a successor of (i0, j0), by definition of bi-partite graph, i ∈ B and j ∈ A
        similarly, if (i', j') is a successor of (i, j), i ∈ A and j ∈ B
        since A and B are disjoint, there's no solution to the instance
    otherwise the shortest path i ~> j must contain infinite edges
d.  G.E = {(1, 1), (1, 2)}, start state = (1, 2)
    successors of (1, 2) are (1, 1) and (2, 1)
    (1, 1) is a goal state where 1 is visited twice by the first friend
    (2, 1) is symmetric to (1, 2), any state from (2, 1) will visit at least one city twice

3.4
//  only half of the proof
//  complete proof can be found in A Modern Treatment of the 15 Puzzle, AF Archer 1999
//  http://www.cs.cmu.edu/afs/cs/academic/class/15859-f01/www/notes/15-puzzle.pdf
represent the tiles as an array in row-major order
the goal state is a sorted array of numbers [1 - 8, E], where E is the empty tile
let I be the number of inversions in the state, let R be the row number (0-based) of the empty tile in the state
(I + R) mod 2 is invariant by legal moves:
    a horizontal move will not change I or R
    when a tile A is moved downwards to the empty tile:
        there are three tiles B, C and D between A and the empty tile in row-major order
        if there are k inversions among (A, B), (A, C) and (A, D), after the move there will be 3 - k inversions
        I changeed by (3 - k) - k = 3 - 2k, which is odd
        R changeed by 1, I + R changed by an even number, (I + R) mod 2 is invariant
the goal state [1, 2, 3, 4, 5, 6, 7, 8, E] has (I + R) mod 2 = 0
another state [1, 2, 3, 4, 5, 6, 8, 7, E] has (I + R) mod 2 = 1, both partitions are nonempty
by CLRS, number of inversions can be computed by modified merge sort in O(nlgn)
treat E as the tile number 9, then deduct the inversions caused by 9 from the result
search algorithms introduced in this chapter can only find an instance unsolvable after exhausting the state space
which is n! / 2 for an (n-1)-puzzle, the program might only terminate after years given enough space

3.5
a queen located left to a column may attack at most 3 squares in the column
hence there are at least n - 3i possible squares on ith column with n rows
S(n)    >= Π(n - 3i)
S^3(n)  >= Π(n - 3i)^3
        >= Π(n - 3i)(n - 3i - 1)(n - 3i - 2)
        = n!
S(n)    >= n^(1/3)
when n = 30, S(n) >= 10^10, any n greater may exhaust the memory of a personal computer

3.6
a.  let G be an indirected graph
    G.V = {i | i is a region on the map}
    G.E = {(i, j) | i and j are adjacent on the map}
    state of the problem is an array of length <= |V| of c ∈ Z4
    where 1 - 4 indicates different colors
    start state is the empty array []
    a state [c0 .. ck] is valid if:
        assign each ci to vertex i, no (i, j) ∈ G.E has ci = cj
    goal state is a valid state of length |V|
    for a state [c0, c1 .. ci], a possible action is defined as:
        result: [c0, c1 .. ci, ci+1] which is valid
        cost: 0
b.  state: position of monkey and crates (may be continuous)
    initial state: implied by the description of the problem
    goal state: monkey with banana
    actions: climb / leave the crates, move the crates around, grab the banana
    cost: unclear
c.  state: a set of records
    initial state: the file of records
    goal state: a singleton set of record
    actions: for a set of records R:
        result: R' ⊆ R, the program outputs "illegal input record" when fed R'
        cost: depends on how the result is computed
d.  state: amount of water in three jugs
    initial state: three empty jugs
    goal state: there's exactly one gallon of water in one of the jugs
    actions: given w0, w1, w2 as amount of water in jugs, c1, c2 and c3 be their capacities
        result: set wi = 0, or
                set wi = ci, or
                for some i != j, set wi = 0, wj = max(cj, wi + wj)
        cost: 1

3.7