/// errata: http://aima.cs.berkeley.edu/errata.html

1.1
a.  ability to act rationally based on knowledge
b.  intelligence originates not in biomass but in artifacts
c.  actor
d.  make near optimal decisions under certain criteria
e.  inference correct conclusions given a set of facts

1.2
https://www.csee.umbc.edu/courses/471/papers/turing.pdf
The Theological Objection:
    have no familiarity in christianity
    by my buddhist grandma, even the most insignificant kind of bugs have souls equal to human's
    so why not intelligent machines
The "Heads in the Sand" Objection
    this idea is even stronger today fueled by 70 years of sci-fi movies and literatures since then
    threat of a possible "AI domination" scenario is unlikely to stop the creation of AI (if ever possible)
    similar to nuclear weapons, AI technology is too much an advantage to any major power
The Mathematical Objection
    machines cannot solve the halting problem for certain, yet there's no proof whether human can do it better
    modern compilers can sometimes detect infinite loops which are unobvious to human eyes
The Argument from Consciousness
    seems to be the most relevant objection today and Turing's refutations on this topic were quite weak
    the idea was later expanded by the Chinese room argument
    recent breakthrough in ML and AI research made machines more rational, but not necessarily more humanly
Arguments from Various Disabilities
    either solved by now or not falsifiable at first place
Lady Lovelace's Objection
    if creativity on music composing or literature writing can be specified in terms of scores
    ML algorithms can train a machine to do it
    this objection lands more on ambiguity in human feelings / languages than ability of machines
Argument from Continuity in the Nervous System 
    no idea 
The Argument from Informality of Behaviour 
    AI today is already more error-prone than human while e.g. driving a car
The Argument from Extrasensory Perception
    can't believe they took ESP into consideration seriously
    was that common back in 1950s?
it turns out unskilled interrogator is too easy to fool:
    "Artificial Stupidity", The Economist, 324 (7770): 14, 1 August 1992
at this rate no one knows what will possibly happen in 50 years 

1.3
it's rational in the sense that it's based on practical reasons instead of emotions
it's not intelligent as it's not a decision by reasoning

1.4
IQ test is designed for normal human beings, not machines on a special task
the microworld of IQ test is overly restrictive

1.5
10^4 / 10^-3 = 10^7
human power: 10^14

1.6
1.  people lies for various reasons
2.  people who lacks certain knowledges cannot express their thought and emotion correctly
3.  people with satisfactory knowledge of thought and emotion is a biased sample

1.7
a.  pattern recognition for non-natural patterns (artifical barcodes)
b.  pattern recognition (NLP) & decision making (order of entries by preference / collected browse histories)
c.  pattern recognition for natural patterns (human voice)
d.  decision making

1.8
these computational models immitates result of human evolution instead of human knowledge
people don't have to understand the ATP cycle to lift and get fit

1.9
self-preservation as a population
by natural selection, entities without the urge of self-preservation are more likely to be eliminated by the environment
and in most case self-preservation is considered rational

1.10
both the theory and the realization of the theory should be considered AI
recent boom in ML is mostly boosted by powerful video cards 

1.11
the latter is true in the same way to 
"human thoughts are just a bunch of electrical and chemical signals being tossed around inside the brain"
thus the first sentence cannot be derived from the latter

1.12
the latter is not true, human thought and behavior is not that deterministic by gene

1.13
same to 1.11

1.14
List of entries not possible yet:
    h:  NLP or the ability of catching fuzzy ideas as "funny" is still lagging behind other aspects
    k:  not heard, but AI can help surgeons perform better
        by https://publishing.rcseng.ac.uk/doi/pdf/10.1308/rcsbull.2017.87, better computer vision is the missing piece

1.15
//  skipped

2.1
consider a vaccum cleaner with limited battery, in an environment where dust is generated randomly over time
performance is again measured by how clean the floor is on average 
if T is short compared to the battery capacity
the cleaner may choose to search very actively, consume more energy than optimal in the first T time steps
and do no work at all afterwards 

2.2
a.  there are 4 cases for the initial environment, 2 cases of initial place of the vaccum cleaner
    4 * 2 = 8 inital cases in total, where half the cases are symmetric
    assume the cleanliness is asserted at the start of each time step 
    A: Clean, B: Clean, Start: A,
        performance measure is 2000 for any agents
    A: Dirt, B: Clean, Start: A,
        the agent cleans dirt in square A in time step 1, move back and forth afterwards
        performance measure is 1999
        1999 is the optimal performance as the dirty square cannot be cleaned before time step 1
    A: Dirt, B: Clean, Start: B,
        the agent moves to square A at the end of time step 1, clean square A in time step 2
        performance measure is 1998
        1998 is optimal as any agent needs at least 2 time steps to move and clean the dirt in square A
    A: Dirt, B: Dirt, Start: A,
        1: clean A, 2: move to B, 3: clean B
        performance measure is 1996
        1996 is optimal as any agent needs at least 3 time steps to clean the two squares
b.  same to figure 2.3 but stop moving when no square is dirty
    if the agent can observe the cleanliness of both squares, no internal state required
    otherwise the agent has to remember the cleanliness of another square
c.  it may be beneficial to learn the dirt distribution and the geography of the room
    dirts will be cleaned earlier if the agent returns to the square with a highest posibility to generate dirt

2.3
a.  false, the vaccum cleaner agent in figure 2.2 is perfectly rational despite sensing only partial information
b.  true, an environment in which a button should be pushed when two parts are defected in a row
c.  true, a vaccum cleaner world as figure 2.3 in which no dirt are ever generated
d.  false, agent function always takes sequence of percepts, while agent program may be pure reflex
e.  false, it may be phisically impossible due to limitation on storage or computation power
f.  true, where the performance measure is independent to the actions of agents
g.  true, the vaccum cleaner agent in 2.2.b is also rational in the world defined in figure 2.3
h.  false, an agent that never clean squares cannot be rational in the vaccum cleaner world, with sensor or not
i.  false, it's logically impossible: what will happen when two of such agents play against each other?

2.4
a.  playing soccer (as a single player):
        P:  (depends on position) goals, assists, pass success rate, ...
        E:  position, momentum, ... of other players, ball, judges, ...
        A:  all possible moves of a human being
        S:  visual, sound and physical contact
b.  subsurface ocean of Titan (as a rover / submarine):
        P:  completeness and accuracy of aquired data, project cost, (if manned) casualty
        E:  Titan (atmosphere, climate, marine condition, ...)
        A:  sampling devices, thrust, ...
        S:  visual inputs
// skipped

2.5
agent function:         formal definition of the performance logic of an agent
agent program:          implementation of an agent function
autonomy:               ability to complete tasks / make decisions under no supervision
reflex agent:           agent whose action is independent to history of percepts
model-based agent:      agent which simulates unobservable part of environment as interal states from percepts
                        and make decision based on those simulations
goal-based agent:       agent which asserts and changes its actions by a binary performance measure (goal / defeat)
utility-based agent:    agent which asserts and changes its actions by a performance measure function
learning agent: agent   agent which asserts and improves its actions from experiences

2.6
a.  let F be an reflex agent function, P be an agent program implementing F
    let P' be a program that on a percept:
        runs P twice, return the result of the second run
    by definition P' is also an implementation of F
b.  none, ignoring storage and computation limitations, the table-driven method is always possible
c.  yes or no according to a more precise definition of randomized agent function / program
d.  infinite, there are infinite possible implementations of a trivial agent which yields a single action forever 
e.  no, by structure of the program it will only emit an action on percept inputs
    speed up the machine only boost the reaction speed of the agent
    if the agent is a valid implementation of the agent function at the first space it have to wait more

2.7
a.  function GOAL-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    DESIRABLE, boolean function with built-in goal
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if DESIRABLE(state)
                return action'
b.  function UTILITY-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    UTILITY, function from state to a performance measure
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if UTILITY(state) is maximized
                return action'

2.8
./AI/vaccum_cleaner

2.9
./AI/vaccum_cleaner
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 2000
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 2000
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1998
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 1998
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1996
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1996

2.10
a.  it cannot
    when there's initially no dirt in the environment, the simple reflex agent will move mindlessly
    an agent that does nothing at all will achieve score 2000 instead of 1000 by the simple reflex agent
b.  refer 2.2
c.  refer 2.2

2.11
./AI/vaccum_cleaner
a.  it cannot, a simple deterministic agent will only clean dirt on a fixed path
    if the initial dirt distribution generates no dirt along that path, no dirt will be cleaned by the reflex agent
    another agent may outperform it by cleaning even one square of dirt (by chance or better perception)
b.  it's possible as stated in part a
    results are concluded in part d
c.  a spiral environment
    map legend:
        #: obstacle
        %: dirty
         : clean
        @: start position of agent
    ##########
    #%%%%%%%%@
    #%########
    #%#%%%%%%#
    #%#%####%#
    #%#%#%%#%#
    #%#%%%%#%#
    #%######%#
    #%%%%%%%%#
    ##########
    after cleaning the square, the random cleaner will bump into walls frequently
    dispite there are only one correct direction to progress
d.  a stateful agent that records the visited part of the environment, decide the next moving direction by
        1. prioritize unvisited squares
        2. choose a random non-obstacle square if surrounded by visited squares
    all environments are 10 x 10 rectangles of clean and dirty squares or obstacles
    a square has
        1/8 chance to be an obstacle
        3/8 chance to be dirty
        4/8 chance to be clean
    score is measured by number of clean squares over a time period of 1000
    % ##%%# %%
    # # % % %#
    %%  %  %
    %   #  %
    %  #    %
    # %%#  %
    %###%#%###
    %% %% % #
    %#%#%  % #
    % #   #
    Bot position: (1, 0)
    RandomCleaner Score: 68262
    Stateful BumpCleaner Score: 73193
    #  %#% %
    % %# %
    %     %%
    # % %#% %%
    %% % #% %
    ## #%   %%
    % #% #%% %
    # % %%%
    % % %  %
    %#%% % # %
    Bot position: (4, 1)
    RandomCleaner Score: 70134
    Stateful BumpCleaner Score: 78198
    %  #  # %
    % # %  %
    ####  % %%
    %
    %%   %%%
    % #%#%% %
    #  %% %
    %%   % %
    %     %#%#
    ##%% % % %
    Bot position: (4, 8)
    RandomCleaner Score: 74664
    Stateful BumpCleaner Score: 80026
    %# %%  %#
    # #%%%#
    %  %#%%%#
    #   %  %
    %%# %
    %% #%%%%%%
    %%#%  %%#
    %%%%   %%%
    %%   %% %
    %% %% #%
    Bot position: (4, 3)
    RandomCleaner Score: 73039
    Stateful BumpCleaner Score: 82044
    #    %
    # %#% #%%#
    %%% % %%
    % % % % %
        # #  %
    %%% #%
    % % #  % #
    %%# %%
    %#%  %%%
    #  %   %
    Bot position: (6, 6)
    RandomCleaner Score: 74993
    Stateful BumpCleaner Score: 80545
    a cleaner cannot be perfectly rational without full percept of the environment

2.12
the agent in 2.11.d is already based on bump detection
if the bump detector fails, the cleaner should fall back to randomized reflex mode

2.13
a.  repeat the Clean action until the the dirt sensor gives "clean" twice in a row
b.  again a cleaner cannot be perfectly rational without full percept of the environment

3.1
without a goal, no action or state is rationally better than others, problem formulation is meaningless

3.2
let w be width of the maze, h be height of the maze
let i be the number of intersections
a.  4wh (4 orientations for each position)
b.  4i (4 orientations for each intersection)
c.  i, no need to keep track of orientations
d.  how the robot accelerate
    how the robot is controlled remotely
    position of the robot in the maze is in fact continuous 

3.3
a.  let G be a indirected graph
    each city is a node of G
    each road is an edge of G
    the state space is tuples of nodes (i, j) where i, j ∈ G.N
    starting state is (i0, j0) for some i0, j0 ∈ G.N
    for each state (i, j), an action is defined with 
        result: (i', j') where (i, i'), (j, j') ∈ G.E
        cost:   max(d(i, i'), d(j, j'))
b.  (i) is not admissible:
        let d(1, 2) = 1, d(2, 3) = 1, D(1, 3) = 2
        given start state (1, 3), the optimal solution has cost 1 < 2 = D(1, 3)
    (ii) >= (i), nor is (ii) admissible
    (iii):
        let start state be (i, j), let C*(i, j) be cost of the optimal solution
        if there's no solution to this instance, C*(i, j) is virtually infinite, greater than D(i, j) / 2 
        if there's an optimal solution to this instance
        basic: 
            i = j, C*(i, j) = D(i, j) / 2 = 0
        induction (on length of optimal solution):
            let the next state in the solution be (i', j')
            the cost of the next action is max(d(i, i'), d(j, j'))
            tail of the solution must be the optimal solution to the subproblem (i', j')
            (or a better solution can be constructed by replace the tail with a better solution to (i', j'))
            by induction on number of actions of optimal solution, C*(i', j') >= D(i', j') / 2
            C*(i, j)    = C*(i', j') + max(d(i, i'), d(j, j'))
                        >= D(i', j') / 2 + max(d(i, i'), d(j, j'))
                        >= (D(i', j') + d(i, i') + d(j, j')) / 2
                        >= D(i, j) / 2  // apply triangle inequality twice
c.  if they cannot stay in a city for a turn (i.e. there's no zero-cost self-cycle for each node)
        for each bi-partite graph with partitions {A, B}, if i0 ∈ A and j0 ∈ B
        if (i, j) is a successor of (i0, j0), by definition of bi-partite graph, i ∈ B and j ∈ A
        similarly, if (i', j') is a successor of (i, j), i ∈ A and j ∈ B
        since A and B are disjoint, there's no solution to the instance
    otherwise the shortest path i ~> j must contain infinite edges
d.  G.E = {(1, 1), (1, 2)}, start state = (1, 2)
    successors of (1, 2) are (1, 1) and (2, 1)
    (1, 1) is a goal state where 1 is visited twice by the first friend
    (2, 1) is symmetric to (1, 2), any state from (2, 1) will visit at least one city twice

3.4
//  only half of the proof
//  complete proof can be found in A Modern Treatment of the 15 Puzzle, AF Archer 1999
//  http://www.cs.cmu.edu/afs/cs/academic/class/15859-f01/www/notes/15-puzzle.pdf
represent the tiles as an array in row-major order
the goal state is a sorted array of numbers [1 - 8, E], where E is the empty tile
let I be the number of inversions in the state
I mod 2 is invariant by legal moves:
    a horizontal move will not change I
    when a tile A is moved downwards to the empty tile:
        there are two tiles B and C between A and the empty tile in row-major order
        if there are k inversions among (A, B), (A, C), after the move there will be 2 - k inversions
        I changeed by (2 - k) - k = 2 - 2k, which is even
the goal state [1, 2, 3, 4, 5, 6, 7, 8, E] has I mod 2 = 0
another state [1, 2, 3, 4, 5, 6, 8, 7, E] has I mod 2 = 1, both partitions are nonempty
by CLRS, number of inversions can be computed by modified merge sort in O(nlgn)
treat E as the tile number 9, then deduct the inversions caused by 9 from the result
search algorithms introduced in this chapter can only find an instance unsolvable after exhausting the state space
which is n! / 2 for an (n-1)-puzzle, the program might only terminate after years given enough space

3.5
a queen located left to a column may attack at most 3 squares in the column
hence there are at least n - 3i possible squares on ith column with n rows
S(n)    >= Π(n - 3i)
S^3(n)  >= Π(n - 3i)^3
        >= Π(n - 3i)(n - 3i - 1)(n - 3i - 2)
        = n!
S(n)    >= n^(1/3)
when n = 30, S(n) >= 10^10, any n greater may exhaust the memory of a personal computer

3.6
a.  let G be an indirected graph
    G.V = {i | i is a region on the map}
    G.E = {(i, j) | i and j are adjacent on the map}
    state of the problem is an array of length <= |V| of c ∈ Z4
    where 1 - 4 indicates different colors
    start state is the empty array []
    a state [c0 .. ck] is valid if:
        assign each ci to vertex i, no (i, j) ∈ G.E has ci = cj
    goal state is a valid state of length |V|
    for a state [c0, c1 .. ci], a possible action is defined as:
        result: [c0, c1 .. ci, ci+1] which is valid
        cost: 0
b.  state: position of monkey and crates (may be continuous)
    initial state: implied by the description of the problem
    goal state: monkey with banana
    actions: climb / leave the crates, move the crates around, grab the banana
    cost: unclear
c.  state: a set of records
    initial state: the file of records
    goal state: a singleton set of record
    actions: for a set of records R:
        result: R' ⊆ R, the program outputs "illegal input record" when fed R'
        cost: depends on how the result is computed
d.  state: amount of water in three jugs
    initial state: three empty jugs
    goal state: there's exactly one gallon of water in one of the jugs
    actions: given w0, w1, w2 as amount of water in jugs, c1, c2 and c3 be their capacities
        result: set wi = 0, or
                set wi = ci, or
                for some i != j, set wi = 0, wj = max(cj, wi + wj)
        cost: 1

3.7
a.  both uncountable
b.  when there are obstacles between two points on a plane, instead of the shortest (straight) path, the robot should 
    bypass the obstacle by turn away from the shortest path as little as possible, in which case the optimality of 
    polygon vertices over point on polygon sides and middle of nowhere can be roughly explained by triangle inequality
    the state space now contains all vertices of obstacles and the start and goal points
c.  let G be an indirected graph
    G.V = {i | i is a vertex of an obstacle, the start point or the goal point}
    G.E = {(i, j) | the straight line between point i and j is not blocked by any obstacle }
    state: any i ∈ G.V
    start state: the start point
    goal state: the goal point
    possible actions from a point i:
        result: j for all (i, j) ∈ G.E
        cost: straight line distance between i and j
d.  simply shortest path from single vertex after formulation, can be easily solved optimally with linear time and space

3.8
a.  consider only graph G that, for each states V, there is a path from start state to goal state passing V
    let start state be s, goal state be g
    let (v, u) be any transition in the graph, let G' be G without (v, u)
    let Cm be the maximum cost of any simple path in G'
    assume an algorithm which didn't examine (v, u) gave an optimal solution to G' with cost C*
    if the cost of (v, u) < -2Cm + C*, construct a path s ~> v -> u ~> g where s ~> v and u ~> g are simple paths
    by definition cost of such a path <= 2Cm + c(v, u) < C*
    hence ignoring any single transition in the graph may result in suboptimal solution
    an optimal algorithm must examine all transitions in the graph
b.  it won't help
    if the negative-cost edge forms a negative cost cycle in the graph
    then there's no optimal solution to the instance
c.  the optimal cost of the instance can be arbitrary low, i.e. no optimal solution
d.  the mental gain from repeatedly visiting the same scenery is diminishing (for sane people)
    formally the cost of an action is not independent to the history of actions
    enrich the state space with history of actions / states
    adjust action costs according to the history to reflect the diminishing gain in reality
e.  drug abuse
    the cost of taking drugs may not be significantly negative
    but the cost of not to take drugs will increase overtime and for each action of taking drugs
    to a point the drug abuser have no other choice
    
3.9
a.  state:  a five-tuple (M, C, M', C', B), where M, C, M' and C' are integers indicating missionaries and cannibals
            B are a binary value representing the position of the boat
    start state: (3, 3, 0, 0, 0)
    goal state: (0, 0, 3, 3, _)
    action: transfer at most two people from one side of the river to another while keeping M >= C and M' >= C'
            if B = 0, only transfer from M, C to M' and C' is valid and vice versa
            cost is uniform
b.  ./AI/searching
    this side: 3M + 3C, other side: 0M + 0C, boat on: This side
    this side: 2M + 2C, other side: 1M + 1C, boat on: Other side
    this side: 2M + 3C, other side: 1M + 0C, boat on: This side
    this side: 2M + 1C, other side: 1M + 2C, boat on: Other side
    this side: 2M + 2C, other side: 1M + 1C, boat on: This side
    this side: 1M + 1C, other side: 2M + 2C, boat on: Other side
    this side: 1M + 2C, other side: 2M + 1C, boat on: This side
    this side: 1M + 0C, other side: 2M + 3C, boat on: Other side
    this side: 1M + 1C, other side: 2M + 2C, boat on: This side
    this side: 0M + 0C, other side: 3M + 3C, boat on: Other side
    without checking repeated space, the searching will loop forever
c.  some move in an optimal solution is anti-intuitive

3.10
state: vertex, atomic description of the moving parts of the environment
state space: G.V, set of all possible states
search tree: tree derived from the state graph during searching
search node: a data structure keeping track of extra information tied to a state
goal: the destination in the state graph
action: transitions / edges in the state graph
transition model: adjacency list from a vertex in the state graph 
branching factor: out-degree of a vertex in the state graph

3.11
world state: the state in reality, without abstraction
state description: abstracted world state percepted and emulated by the agent
search node: data structure keeping track of state descriptions and else in search algorithm
none of any two of the above terms are equivalent:
several world state may correspond to the same state descriptions
there may be a lot search node for a single state description

3.12
for a problem with |V| states and branching factor b
making composite actions of n actions will increase total transitions from |V|b to |V|b^n
while only speed up the searching n times at best
the cost is not justified by the return

3.13
basic: 
    the frontier contains only the initial state s, path to any other state passes the initial state
induction:
    let u be the state expanded next, u is removed from the frontier after the iteration
    let s ~> v be the path from initial state to an unexplored state before expansion
    if u is not on s ~> v, as u is the only state removed from the frontier, s ~> v passes a state in the frontier
    if u is on s ~> v
        if v = u, v is nolonger unexplored
        if v != u, the path is s ~> u ~> v, where u ~> v must pass a successor of u
        by the algorithm, all successors of u are added to the frontier

3.14
a.  false, depth-first search may be quick to find suboptimal solutions
    if the initial state has an immediate goal state successor with high cost
    depth first search may accidentally explore it first and terminate
    A* search will only explore it after all node with lower-cost is explored
b.  true, h(n) = 0 is an admissible heuristic for any problem with non-negative costs
c.  false, not after proper abstraction
d.  true, breath-first search is ignorant to costs
e.  false, obvious

3.15
a.  digraph {
        0 [label="1"]
        1 [label="2"]
        2 [label="3"]
        3 [label="4"]
        4 [label="5"]
        5 [label="6"]
        6 [label="7"]
        7 [label="8"]
        8 [label="9"]
        9 [label="10"]
        10 [label="11"]
        11 [label="12"]
        12 [label="13"]
        13 [label="14"]
        14 [label="15"]
        0 -> 1
        0 -> 2
        1 -> 3
        1 -> 4
        2 -> 5
        2 -> 6
        3 -> 7
        3 -> 8
        4 -> 9
        4 -> 10
        5 -> 11
        5 -> 12
        6 -> 13
        6 -> 14
    }
b.  BFS:
        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11
    depth-limited (3):
        1, 2, 4, 8, 9, 5, 10, 11
    iterative deepening:
        1,
        1, 2, 3,
        1, 2, 4, 5, 3, 6, 7,
        1, 2, 4, 8, 9, 5, 10, 11
c.  reverse transition model has branching factor 1
    the search will be trivial in reverse order
d.  see part e
e.  ./AI/searching

3.16
a.  state: a railway and a set of unconnected pieces
    start state: an empty railway and 32 unconnected pieces
    goal state: a railway with no overlapping and loose ends with no unconnected pieces
    action: plug one of the unconnected piece to a hole on the railway
            for unsymmatric pieces there are two ways to plug it to a hole
    cost: insignificant
b.  as all goal states are in the same depth (depth 32)
    DFS will be most memory-efficient without incurring suboptimal time complexity
    also there's no significant cost, hence no reasonable heuristic to guide the searching
c.  for there being no loose ends, the number of plugs and holes must match
    removing any one of the fork pieces then the instance won't meet the prerequisite above
d.  the railway under construction has at most 3 holes
    there are 12 + (16 + 2 + 2) * 2 = 52 ways to plug a piece to a hole
    the maximum branching factor is 52 * 3 = 168, with maximum depth 32 the state space is at most 168^32

3.17
a.  given a finite program instance, let the set of action costs be C
    number of subsets of C is also finite
    hence the minimum positive difference between the sum of any two subsets is greater than a constant ε
    in graph search, the cost of a path is the sum of a subset of C
    therefore each iteration of iterative lengthening search will increase the cost limit by ε
    let the optimal cost be C*, when the cost limit is smaller than C*, there's no solution
    the algorithm will retry with a greater cost limit
    when the cost limit is greater or equal to C*, given non-negative costs
    uniform-cost search, as an optimal search algorithm, will find the optimal solution
    as C* / ε is finite, the algorithm will definitely terminate
b.  with unit step costs, ε >= 1
    d iterations at most
c.  it depends on the minimum positive difference ε defind above
d.  ./AI/searching/ils

3.18
a state space with branching factor f in which all state at depth d is a goal state
DFS will always terminate in O(d) time, while IDDFS algorithm have to analysis Ω(f^d) states at least

3.19
//  skipped
//  tracing every link on a page is extremely complicated nowadays with all the front end arcane magics
//  the state space is infinite (dynamic generated pages)
astar with heuristic based on common parts of the url
one (cheat) strategy may be heading a search engine from the first page then search the url of the second page
bidirectional search is infeasible as there's no proper way to generate complete of a predecessor
even with a search engine (single page applications, js user interfaces e.g. React, etc.)

3.20
a.  state space is tiny, any algorithm will suffice
    a possible admissible heuristic is f(n) = dirty squares * 2 - 1
    each square requires at least two actions (move to and suck) to clean
    except the square under the cleaner which can be cleaned by a single action
    non-negative cost actions only, graph search is preferable
b.  %%%
    ...
    ...
    Cleaner: (1, 1)
    %%%
    ...
    ...
    Cleaner: (1, 0)
    %.%
    ...
    ...
    Cleaner: (1, 0)
    %.%
    ...
    ...
    Cleaner: (0, 0)
    ..%
    ...
    ...
    Cleaner: (0, 0)
    ..%
    ...
    ...
    Cleaner: (1, 0)
    ..%
    ...
    ...
    Cleaner: (2, 0)
    ...
    ...
    ...
    Cleaner: (2, 0)
c.  search cost is negligible in such a tiny state space (a few milliseconds at most)
    avarage cost of 1000 samples is 4.493
d.  the astar search agent is optimal
e.  state space has size 2^(n^2) * n^2, the search cost will dominate move cost quickly
    reflex agent will outperform the search agent for any moderately large n

3.21
a.  when the transition model has uniform cost for all actions
    g(n) = depth of the node n, the shallowest node will be expanded next
    that's exactly the behavior of BFS
b.  assume the depth of a node is stored in the node itself
    let f(n) = -n.depth, the deepest node will always be expanded next
    which is exactly the behavior of DFS
c.  when h(n) = 0, f(n) = g(n) + f(n) = g(n), as well as uniform-cost search

3.22
//  this line of the pseudo-code
//      (result, best.f) ← RBFS(problem, best, min(f.limit, alternative))
//  contains a very sneaky assignment to best.f
//  without this assignment the implementation will blow the stack up
./AI/searching/src/rbfs.rs
much slower
partially due to naive unoptimized RBFS vs A* from an optimized extern crate
also RBFS is a tree search, exploring much more redundant states compared to A*

3.23
//  skipped

3.24
start state: 0
goal state: 3
costs:
    0 -> 1: 0
    1 -> 2: 0
    0 -> 2: 1
    2 -> 3: 2
heuristics:
    1: 2
    2: 0
    3: 0
frontier = [1, 2] after expanding start state
g(2) + h(2) = 1 < g(1) + h(1), 2 will be expanded next
the path 0 -> 1 -> 2 will never be expanded as 0 -> 2 adds 2 to explored set

3.25
when w = 0, f(n) = 2g(n), the algorithm is uniform cost search, not complete in tree search
when 0 < w < 2, f(n) = (2 - w)(g(n) + w * h(n) / (2 - w))
the algorithm is A* search with heuristic w * h(n) / (2 - w)
depending on h(n), this algorithm may or may not be complete
when w = 2, f(n) = 2h(n), the algorithm is greedy best first search, not complete in tree search
when h(n) is admissible, for 0 <= w / (2 - w) <= 1, w <= 1, (w / (2 - w))h(n) is also admissible

3.26
a.  4
b.  4k in graph search
    2(k + 1)(k + 2) in tree search
c.  Θ(4^(x + y))
d.  2(x + y + 1)(x + y + 2)
e.  true (manhattan distance)
f.  all nodes in the rectance((0, 0), (x, y)), (x + 1)(y + 1) nodes
g.  true, shortest path p0 ~> p1 may only increase when some link is removed
h.  no, if there is a link ((0, 0), (x, y)) when |x| + |y| > 1, the heuristic is no longer admissible

3.27
a.  A(n^2, n) = (n^2)! / n!
b.  <= 5^n
c.  manhattan distance
    it cannot move more than one grid without another vehicle
d.  (i) is not admissible:
        when n = 2, Σh = 4 but the optimal solution has cost 2
    (ii) is not admissible:
        given a state:
            . 2 3
            . . 1
            . . .
        the goal state is:
            3 2 1
            . . .
            . . .
        max{h} = 2, but the state can reach the goal state in a single action
    (iii) is admissible:
        let s be an arbitrary state, s' be one of its successor
        let h = (h1 .. hn) be heuristics in state s, h' = (h1' .. hn') be heuristics in s'
        let hi = min{h},
            if the ith vehicle stay put, hi' = hi, min{h'} >= min{h}
            if the ith vehicle moves normally, hi' >= hi - 1, min{h'} >= min{h} - 1
            if the ith vehicle hops over the jth car, hj' = hj >= min{h}, min{h'} >= min{h}
        thus an action can at most deduce min{h} by 1
        the goal state has min{h} = 0, if (s0 .. sk) is an optimal solution, h(s0) <= k

3.28
a totally random heuristic can easily lead A* to suboptimal solutions
./AI/searching
the optimal solution has f(n*) = g(n*) + h(n*) <= C* + c
any node explored before the optimal solution must have f(n) <= f(n*) = C* + c
assume h(n) is non-negative, f(n) = g(n) + h(n) >= g(n), g(n) <= C* + c

3.29
h(n) <= c(n, a, n') + h(n')
for an arbitrary state n0, let (n0 .. nk) be an optimal solution
optimal cost is sum of action costs Σc(ni, ai, ni+1) = C*
for each pair (ni, ni+1), h(ni) <= c(ni, ai, ni+1) + h(ni+1)
    Σh(ni) <= Σ(c(ni, ai, ni+1) + h(ni+1))
    // all but h(n0) and h(nk) cancels each other
    h(n0) <= Σ(c(ni, ai, ni+1)) + h(nk)
    h(n0) <= C* + 0 = C*
the heuristic in exercise 3.24

3.30
a.  a relaxed version of TSP where the tour does not have to be a hamiltonian cycle
b.  no description for straight line distance as a heuristic function provided
c.  cities: random points from [0, 1]^2
    start city: one of the cities
    a state keeps track of the current position of the salesman and the traveled city 
d.  refer CLRS

3.31
moving a square to the blank square can at most fix one misplaced square
for a solvable puzzle
    4 5 3
    1 2 6
    7 8 .
h1 = 4, h2 = 4, Gaschnig's heuristic = 5, optimal solution has cost 16
if a square i is misplaced, it occupies the tile of another square j, which occupies tile of square k, ..
these misplacements therefore forms cycles
each misplacement cycle of length k requires k actions to fix if the blank square is on the cycle
otherwise k + 1 actions

3.32
//  skipped

4.1
a.  simple hill-climbing search
b.  BFS if no information are shared between threads
c.  first-choice hill climbing
d.  random walk
e.  random walk (with mutations instead of successors from actions)

4.2
change from 3.16:
when connecting the next piece to the track, the piece can be rotated by an angle a in the range [-10, 10]
an angle a is accepted as the next state with probability e^(a/T), where T = schedule(t) is a function on time

4.3
a.  ./AI/searching
    average cost ratio over 1000 samples is 1.2425902960619555
b.  // oddly nowhere in the paper mentioned the fitness function they used
    // skipped

4.4
//  8-puzzle cannot be fomulated as a local search problem
//  the goal state is obvious for 8-puzzle, only the path matters
over 1000 samples of eight queens:
Steepest ascent cost: 31.666097ms
Steepest ascent success ratio: 0.16
First choice cost: 33.931055ms
First choice success ratio: 0.157
Simulated annealing cost: 172.206779ms
Simulated annealing success ratio: 0.14
Random restart cost: 220.052079ms

4.5
there must be a persistent map of type (state, SearchResult) as global variable or object field
the resulting plan may be a DAG instead of a tree
SearchResult is a tagged enum with three variants:
    Failed: there's no goal state reachable from this state
    Success(Plan): a goal state is reachable from this state
assume Plan type is already a shared pointer to the real plan structure
function OR-SEARCH(state, problem, path) returns a conditional plan, failure
    if Success(plan) = map[state] {
        return plan
    } else if Failed = map[state] {
        return failure
    } else if state is on path {
        map[state] <- Failed
    } else {
        // this is the first time state is explored
        plan <- new shared pointer to a plan structure
        for each action in problem.ACTIONS(state) do {
            plan <- AND-SEARCH(RESULTS(state, action), problem, map)
            if plan != failure {
                map[state] <- Success([action | plan])
                return [action | plan]
            }
        }
        map[state] <- Failed
        return failure
    }

4.6
//  incomplete
labeling:
    the search result is now Success(Plan), Loop(Plan) or Failure
    stored along each state on the path is the plan of that state
OR-SEARCH:
    if a loop is detected, return Loop(plan) with plan stored on the path
    in for each action loop, return Loop(plan) if there's no acyclic plan, return Failure if there's no plan at all
    the extended path [state | path] now must contain a placeholder plan LoopPlan, i.e. [(state, LoopPlan) | path]
    the value of the plan is replaced with the computed acyclic plan (if any) on return
AND-SEARCH:
    if resulting plan on all branches are cyclic, return Failure

4.7
max{h*(s) | s ∈ b}
let the original problem be P, the sensorless problem be P'
assume there is a solution (a0 .. ak) for believe state b, s ∈ b
applying (a0 .. ak) on s must bring s to a goal state of P, otherwise b is not a goal state of P'
therefore (a0 .. ak) is also a solution to s, c(a0 .. ak) >= h*(s)

4.8
a.  let b ⊆ b', RESULT(b, a) = {s' | s' = RESULTp(s, a) and s ∈ b}
                            ⊆ {s' | s' = RESULTp(s, a) and s ∈ b'}
                            = RESULT(b', a)
    let G be the set of goal states in the original problem
    for a sequence of actions a = (a0 .. ak), let APPLY(b, a) be the result of applying a to b in order
    clearly APPLY(b, a) ⊆ APPLY(b', a)
    if a is a solution to b', APPLY(b, a) ⊆ APPLY(b', a) ⊆ G, a is also a solution to b
    let c ⊆ b ∪ b', RESULT(c, a)
        = {s' | s' = RESULTp(s, a) and s ∈ c}
        ⊆ {s' | s' = RESULTp(s, a) and s ∈ b ∪ b'}
        = RESULT(b, a) ∪ RESULT(b', a)
    hence APPLY(c, a) ⊆ APPLY(b, a) ∪ APPLY(b', a) for any sequence a
    if a is a solution to both b and b', a is also a solution to c
b.  if b is in EXPLORED, for all b' ⊆ b, a solution to b is also a solution to b'
    therefore it's not necessary to add b to the frontier
c.  store solved states along with their plans
    on start of OR-SEARCH(b, ..), if a superset of b is solved, return the plan associated with it

4.9
let initial state be {s1, s2}, 
RESULT(s1, a) = RESULT(s1, b) = RESULT(s2, a) = RESULT(s2, b) = g
c(s1, a, g) = c(s2, b, g) = 3, c(s2, a, g) = c(s2, b, g) = 1
c({s1, s2}, a, {g}) = (1, 3), c({s1, s2}, b, {g}) = (3, 1), the two costs cannot be compared
the interval case and the set case cannot be solved by A*, as the heuristic is not well ordered
//  thanks solution manual
by principle of optimality:
    An optimal policy has the property that whatever the initial state and initial decision are, 
    the remaining decisions must constitute an optimal policy with regard to the state 
    resulting from the first decision.
 if the total cost of a path is defined as the minimal cost of any physical realization of the whole path
as stated in the solution manual, this definition violates principle of optimality
however if the cost of an action defined as the minimal (maximal) cost of any physical realization of an action
A* will return optimal solutions
when defined as interval or set, the cost is not well ordered

4.10
./AI/and_or_search
there's no path from initial state to any of the goal state
digraph {
    0 [label="[1, 2, 3, 4, 5, 6, 7, 8]"]
    1 [label="[1, 3, 5, 7]"]
    2 [label="[2, 4, 6, 8]"]
    0 -> 0 [label="Suck"]
    0 -> 1 [label="Left"]
    0 -> 2 [label="Right"]
    2 -> 2 [label="Suck"]
    2 -> 1 [label="Left"]
    2 -> 2 [label="Right"]
    1 -> 1 [label="Suck"]
    1 -> 1 [label="Left"]
    1 -> 2 [label="Right"]
}

4.11
//  skipped

4.12
a.  a physical state is consisted of a configuration of the maze and the position of the agent
    percepts and the position of the agent filters the set of possible configurations
    there are 12 possible internal walls, i.e. the inital believe state contains 2^12 configurations
    2^(2^12) * 3^2 believe states in total
b.  four: {Up, Right}, {Up}, {Right}, {}
c.  each percept of a preciously unknown wall shrinks the size of the believe state to 1/2
    each of the four possible percepts at the inital state shrinks the size of the believe state to 1/4
    any believe state with believe set of size 1 is a goal state
    if the maze is connected, 2 * MST = 18 is an upper bound of the length of a tour
    the size of the plan is roughly 4^18

4.13
//  skipped

4.14
by the line
    if s' is a new state (not in untried) then untried[s'] ← ACTIONS(s')
each step adds four actions to the untried list 
each step consumes only one action, the untried list is never empty
the algorithm will never backtrack
if ACTIONS(s') is pushed to untried[s'] with a fixed order, the same action will be repeated over and over
the agent can only reach (x, 0) or (0, y) for arbitrary x or y
a random walk agent will be complete: at each step the agent takes a random possible action or backtrack by chance
the path reaching (1, -1) can be any path possible

5.1
let p' be the single agent search problem derived from OM and the original two-player problem p
p'.RESULT(s, a) = OM(p.RESULT(s, a))
p'.GOAL-TEST(s) = state s which maximizes the utility function in p
then this problem can be solved by any search algorithm in chapter 3

5.2
a.  assume the two puzzles can be solved in arbitrary order (player can move pieces on either puzzle each move)
    a state is a tuple of two 8-puzzle states (s1, s2)
    the actions are sum of actions on the two puzzles 
    cost is uniform
b.  9!^2, or (9!/2)^2 counting only reachable states
c.  EXPECTMINIMAX from Figure 5.11
d.  it may not be the case
    let p(s) be the minimum number of moves required to solve a puzzle s
    terminal state is (s1, s2) where p(s1) = p(s2) = 0
    when p(s1) = 1 and p(s2) = 1
    if the player take an action which reduces p(s1) or p(s2), the other player will win with 50% chance next turn
    it seems better to move away from the terminal state by increasing p(s1) or p(s2)

5.3
a.  from left to right:
        dd(1): -4
        dd(2): -4
        cc: 2
b.  P is guranteed to win in at most 4 moves by the left half of the game tree
    each internal node in there has value >= its deepest descendants
    the right half is suboptimal moves for P and should not be taken
c.  from left to right:
        be, bf, be, bf, bc
d.  P can always drive E to node e or f and win the next turn as there's no way to escape
    the value of these nodes is bounded from below by
        - (the total time taken up to now + 2 * the length from P's position to node e or f)
    from left to right:
        be: 3 + 3 * 2 = 9
        bf: 3 + 3 * 2 = 9
        be: same to above
        bf: same to above
        bc: 3 + 3 * 2 = 9
e.  any node except the two paths from root to termination state dd(1) and dd(2)
f.  P always wins
    let the tree rooted from the node p occupied by P, let e be the node occupied by E
    if e = n, P wins immediately
    otherwise let c be the child of p which has e as its descendant, move E to c
    let S((p, e)) be the number of nodes in the subtree rooted at c, it's obvious that:
        each move of P reduces S((p, e))
        any move of E cannot increase S((p, e))
    when S((p, e)) = 1, P will win the next turn by moving to e, the only node in the subtree and a child of p
    so as long as S((p, e)) is finite at the beginning (i.e. the subtree is finite), P will always win

5.4
Monopoly (simplified)
State:
    position and assets of each player
Move: given state and player p,
    a chance node (2d6) decides the next stop of p
    mutate state according to the description of the stop (rants, taxes, may spawn more chance nodes by cards)
    if stopped at an unowned property, the player can choose whether to buy the property
Terminal test:
    all but one player is bankrupt
Utility function:
    the winning player has utility 1, others 0
Evaluation function:
    linear function of net asset and owned properties of players

5.5
Gomoku with time control
State:
    pieces on the board (next player can be computed from number of white / black pieces)
    remaining time of each player
Move:
    a piece of the player's color can be placed at any empty tile on the board
    a fixed time duration t + the remaining time of the player is allowed for the move
    if the time consumed exceeds t, the surplus is deducted from the remaining time of the player
Terminal test:
    5 or more pieces of the same color are placed on a row (vertial, horizontal or diagonal)
Utility function:
    the winning player has utility 1, other one 0
    or 1/2 and 1/2 if end in a draw
Evaluation function:
    a linear function of blocked / unblocked 3, 4, >= 5 pieces in a row for each color
    where the coefficient of >= 5 pieces in a row is big enough to dominate any other factors

5.6
even if the AI player can control exactly where to land its shots, in the absence of proper discretization
the branching factor is easily infinite, the AI does not stand a chance against a trained amateur
tennis players are constantly moving around regarding the position / velocity of the ball and the other player
it requires further discretization (to tiny time period?) to turn this constant move into turn-based nodes

5.7
induct on the height of the game tree from the shallowest MIN node
basis:
    all child nodes are terminal nodes from a MIN node
    if the decision of MIN at this node is not optimal, the value of the MIN node increases
    by definition of minimax algorithm, all MAX nodes above can only have value >= before
induction:
    the MIN nodes has children either terminal or is a MAX node
    by induction all the MAX children have value better or equal than before
    the optimal value of this MIN node is no better than that computed from minimax algorithm
    as the MIN player is suboptimal, the value of this MIN node is greater than minimax
    value of MAX nodes computed by minimax algorithm above it may never decrease
if MIN will always choose the same suboptimal move in a tree
RESULT(A, a1) = B
RESULT(A, a2) = C
RESULT(B, b1).UTILITY = 10
RESULT(B, b2).UTILITY = -10
RESULT(C, c).UTILITY = 5
if MIN is optimal, MAX should choose a2 in node A
if MIN is suboptimal and will always choose b1 at node B, MAX should chooce a1 at node A

5.8
a.  digraph G {
        node [shape=circle]
        1 [label="(1, 4) +1"]
        2 [label="(2, 4) +1"]
        3 [label="(2, 3) +1"]
        4 [label="(4, 3) +1", shape=box]
        5 [label="(1, 3) -1"]
        6 [label="(1, 4) ?", shape=box, peripheries=2]
        7 [label="(1, 2) -1"]
        8 [label="(3, 2) -1"]
        9 [label="(3, 1) -1", shape=box]
        10 [label="(3, 4) ?"]
        11 [label="(2, 4) ?", shape=box, peripheries=2]
        1 -> 2 -> 3 -> 4
        3 -> 5 -> 6
        5 -> 7 -> 8 -> 9
        8 -> 10 -> 11
    }
b.  assume both players prefer acyclic states over cyclic ones
c.  MINIMAX algorithm in the text is a DFS without redundancy check, it may loop forever with cyclic state graph
    record the path of states and assign special value Loop to repeated states, in the same way as and or graph search
    the value space is not a total order any more with the special value
    the assumption in part b above may not always be true
    if a player has to choose between certain lose and loop, they may prefer loop
d.  by graph generation, when n = 3 player B always wins, when n = 4 player A always wins
    for n = 2k, the start state is (1, 2k), the next state is (2, 2k)
    if A never goes back to the location 1, this is equivalent to a game where
        1. n = 2k - 1 
        2. the roles of two players are swapped
    by induction, when n = 2k - 1, player B will always win, i.e. reach the other end eariler than player A
    hence when n = 2k, player A can always win
    for n = 2k, the start state is (1, 2k + 1), the third state is (2, 2k)
    if A never goes back to location 1, this is equivalent to a game
        1. n = 2k - 1
    so player B will always win
    otherwise whenever A goes back to location 1, B moves moves left
    A's move is negated by B's move, reduces the state to a valid state in a game n = 2k - 1

5.9
a.  <= 3^9
    each tile may either be empty, X or O
    the next player can be decided by the number of Xs / Os on the board
b.  digraph G {
        node [shape=record]
        0 [label="...\n...\n...|1"]
        1 [label="x..\n...\n...|-1"]
        2 [label=".x.\n...\n...|-2"]
        3 [label="...\n.x.\n...|1"]
        4 [label="xo.\n...\n...|1"]
        5 [label="x.o\n...\n...|0"]
        6 [label="x..\n.o.\n...|-1"]
        7 [label="x..\n..o\n...|1"]
        8 [label="x..\n...\n..o|0"]
        9 [label="ox.\n...\n...|-1"]
        10 [label=".x.\no..\n...|0"]
        11 [label=".x.\n.o.\n...|-2"]
        12 [label=".x.\n...\no..|-1"]
        13 [label=".x.\n...\n.o.|0"]
        14 [label="o..\n.x.\n...|1"]
        15 [label=".o.\n.x.\n...|2"]
        0 -> 1, 2, 3
        1 -> 4, 5, 6, 7, 8
        2 -> 9, 10, 11, 12, 13
        3 -> 14, 15
    } 
c.  xo./.../...: 1
    x.o/.../...: 0
    x../.o./...: -1
    x../..o/...: 1
    x../.../..o: 0
    ox./.../...: -1
    .x./o../...: 0
    .x./.o./...: -2
    .x./.../o..: -1
    .x./.../.o.: 0
    o../.x./...: 1
    .o./.x./...: 2
d.  .../.../...: 1
    x../.../...: -1 
    .x./.../...: -2
    .../.x./...: 1
    best start move: .../.x./...
e.  if .../.x./... is explored first, all nodes but 
        o../.x./...
        .o./.x./...
        a random child of x../.../...
        a random child of .x./.../...
    will not be evaluated

5.10
a.  branching factor of a node at depth d is N - d
    depth of the game tree is at most N, number of nodes <= N!
b.  N! as stated above
c.  generalize the evaluation function defined in 5.9
    let Xi be the number of winning positions w ∈ W which is occupied by (|w| - i) X and no O
    the evaluation function is a linear combination of Xi and Oi, where
        if i < j, coefficient of Xi is greater than Xj, coefficient of Oi is smaller than Oj (both negative)
d.  with optimal exploration order, alpha-beta pruning only evaluates (N!)^(1/2) nodes
    100N * (N!)^(1/2) / (2 * 10^9) <= 1 => N <= 15
    100N * (N!)^(1/2) / (2 * 10^9) <= 60 => N <= 17
    100N * (N!)^(1/2) / (2 * 10^9) <= 3600 => N <= 20

5.11
a.  ./AI/minimax/src/othello.rs
b.  ./AI/minimax
c.  // skipped
b.  // skipped, papers paywalled

5.12
minimax works exactly as multiplayer game in section 5.2.2
each player maximizes their own utility and ignores the other's
the utility of one player will not affect the decision of the other
hence no node will be pruned by alpha-beta pruning
when the optimal terminal state for MAX is also the optimal terminal state for MIN
the game is just an optimal search problem which can be solved by any local search algorithm in chapter 4
otherwise it's still competitive

5.13
a.  n2 is a MAX node
    n2 = max{n | n is a child of n2}
    n1 = nj iff 
        n2 = min{n | n is a child of n1}
        n3 = max{n | n is a child of n2}
        ..
        nj = min{n | n is a child of nj-1}
    otherwise n1 is independent of nj
b.  n1  = min(l2, n2, r2)
        = min(l2, max(l3, n3, r3), r3)
        = ..
        = min(l2, max(l3, min(l4 .. min(lj, nj, rj) .. r4), r3), r2)
c.  if nj is greater than min(l2, r2, l5, r5, .., lj, rj), it will not affect n1
c.  if nj is smaller than max(l3, r3, l5, r5, .., lj, rj), it will not affect n1

5.14
assume 
    for a MIN node, the first child gives the minimum v
    for a MAX node, the first child gives the maximum v
if a MAX node is explored with (α, β) = (+∞, -∞)
    the first child (MIN) will be explored with (+∞, -∞), give the maximal v*
    α is updated to v*
    all the other children is explored with (v*, -∞)
if a MIN node is explored with (+∞, -∞)
    the first child (MAX) will be explored with (+∞, -∞), give the minimal v*
    β is updated to v*
    all the other children is explored with (+∞, v*)
if a MAX node is explored with (+∞, v*)
    if its parent is evaluated with (-∞, +∞)
        the first child (MIN) is explored with (+∞, v*), results in v >= v*
        otherwise the backed-up value of this node is greater than maximum
        no other child will be explored
    otherwise in the worst case it explores the same set of nodes as (-∞, +∞)
if a MIN node is explored with (v*, -∞)
    if its parent is evaluated with (-∞, +∞)
        the first child (MAX) is explored with (v*, -∞), results in v <= v*
        otherwise the backed-up value of this node is smaller than minimum
        no other child will be explored
    otherwise in the worst case it explores the same set of nodes as (-∞, +∞)
roughly half the levels are not expanded, O(b^(m/2)) nodes expanded in total

5.15
representation:
    there are 8 x 8 = 2^6 positions on the board, a position can be represented by 6 bits
    7 bits with the additional "not on board" position, or 1 byte with proper alignment
    there are 16 pieces for each side, 16 * 2 * 1 = 32 bytes
2^30 / 2^5 = 2^25 entries in 2GB table
10^7 * 3 * 60 = 1.8 * 10^9 > 2^25, the memory is not enough
without detail of the evaluation function / hash method / disk specification, no conclusion can be drawn

5.16
a.  digraph G {
        node [shape=none, label="", style=filled]
        1 [shape=triangle, label=1.5]
        2, 3 [shape=circle]
        2 [label=1.5]
        3 [label=-0.5]
        4, 5, 6, 7 [shape=invtriangle]
        4 [label=2]
        5 [label=1]
        6 [label=0]
        7 [label=-1]
        8, 9, 11, 13 [label="2"]
        10 [label="1"]
        12, 15 [label="0"]
        14 [label="-1"]
        1 -> 2, 3
        2 -> 4, 5 [label=0.5]
        3 -> 6, 7 [label=0.5]
        4 -> 8, 9
        5 -> 10, 11
        6 -> 12, 13
        7 -> 14, 15
    }
b.  let n3 be the right chance node
    if min{t7, t8} > 3, n3 > 1.5, the first action will be suboptimal
    if t7 = -1, n3 = -0.5, utility of t8 won't affect the optimal action
c.  0 <= n2 <= 2
d.  the last two terminal node

5.17
./AI/minimax

5.18
induct on the height of subtrees
basis (all children are terminal):
    MAX node:
        when a > 0, if x >= y, ax + b >= ay + b
        the optimal child will not change 
        backed-up value of this node is updated from x to ax + b
    MIN node:
        symmetric
    CHANCE node:
        Σ(aVi + b)Pi = ΣaViPi + ΣbPi = aΣViPi + b
        backed-up value of this node is updated from x to ax + b
induction:
    MAX node:
        all children have backed up value updated from x to ax + b
        the optimal child will not change
    MIN node:
        symmetric
    CHANCE node:
        similar to basis

5.19
it's a special case of Monte Carlo method which only expands chance nodes in random
the method describes a different game compared to the real scenario
(as players can forcast outcomes of all chance nodes)
in practice it may converge to expectminimax just like Monte Carlo to minimax

5.20
a.  no, adding a single terminal node may increase the optimal outcome
b.  no, adding a single terminal node may increase the value of its parent,
    either a MAX or CHANCE node, by an arbitrary value, which change may propagate all the way to the root
c.  no, as stated in b.
d.  no, as stated in b.
e.  yes, if any child evaluates to 1, other children can be pruned
f.  yes, alongside the situation of e., if
        a, left child of the root evaluates to 1
        b, right child of the root is a chance node with two children c, d with probability 0.5 each
        c = 0
    then b <= (0 + 1) / 2 = 0.5, d can be pruned
g.  highest probability first    
    more outcomes of children (probability-wise) are known about a node, more precise is the range of its own outcome 

5.21
a.  true
    as demonstrated by minimax algorithm, if both players are optimal, 
    the moves of one player is easily predictable by the other
b.  false, the assumption turns the game to a fully-observable game
c.  false, what will happen if two of such agents play against each other

5.22
a.  Monopoly, CHANCE nodes in other games have huge branching factor
b.  Scrabble, with fixed outcome of CHANCE nodes, poker and bridge are no longer partially observable,
    nature of the two games will change dramatically
c.  maintain believe states for all players assuming optimality
    likely to be extremely inpractical
    
6.1
./AI/csp
Solution with 2 colors: 0
Solution with 3 colors: 18
Solution with 4 colors: 768

6.2
a.  variables: positions of knights
b.  values: P ∈ Z8^2
c.  a single contraint between each pair of variables:
        for two knights at position (x0, y0) and (x1, y1), 
        (x1, y1) != (x0 + 2, y0 + 1) &&
        (x1, y1) != (x0 + 1, y0 + 2) &&
        (x1, y1) != (x0 - 2, y0 + 1) &&
        (x1, y1) != (x0 - 1, y0 + 2) &&
        (x1, y1) != (x0 + 2, y0 - 1) &&
        (x1, y1) != (x0 + 1, y0 - 2) &&
        (x1, y1) != (x0 - 2, y0 - 1) &&
        (x1, y1) != (x0 - 1, y0 - 2)
d.  ACTIONS: positions on the board not yet attacked by any knights existing
    RESULT: add a knight to the board with position given by ACTIONS
    objective function: number of knights on the board

6.3
a.  state: the board
    start state: no blank square is filled
    goal state: each blank square is filled
    actions: fill a word in the puzzle with a word in the dictionary, respecting crossword rules
    the path doesn't matter, local search is preferable
    if the blank squares are filled one character at a time, it will be harder to define legal actions
b.  variables: hole of words on the board
    values: words in the dictionary
    constraints: if one word spans another, the character on the intersection should be the same (2-constraints only)
    with characters as variables, constraints could involve more than 2 variables and would be harder to verify
    (each word on the board should be a word in the dictionary)

6.4
a.  variables: positions (coordinates of top left corner) of each rectangles
    values: coordinates in the larger rectangle
    constraints: no overlapping between any two rectangles (2-constraints only)
b.  variables: classes
    values: triple of time slot, classroom and professor
    1-constraints:
        professors are assigned to classes they can teach
    2-constraints:
        if two classes are allocated to the same time slot, 
        they should be assigned to different classroom and professor
c.  variables: cities
    values: order in the tour
    2-constraints:
        let (i, j) be the orders of two cities (a, b)
        if j = i + 1, there should be a road from a to b
    global constraints:
        ALLDIFF on orders

6.5
// thanks http://bach.istc.kobe-u.ac.jp/llp/crypt.html
938+938=1876
928+928=1856
867+867=1734
846+846=1692
836+836=1672
765+765=1530
734+734=1468

6.6
let domain of variable A be DA
consider any relation R ⊆ DA * DB * DC
define a new variable T with domain DA * DB
define 2-constraints:
    (A, T): T = (A, _)
    (B, T): T = (_, B)
    (C, T): T = (a, b), (a, b, c) ∈ R
for any R ⊆ Π(1 <= i <= n)Di
define T with domain Π(1 <= i <= n - 1)Di
the proof is similar to n = 3
if there are some unary constraints Ri ⊆ D for a variable V with domain D
let D' = ∩Ri
any value in D - ∩Ri will violate at least one unary constraint
all values in D - ∩Ri do not violate any of the unary constraints
change the domain to D - ∩Ri safely eliminates all the unary constraints on V

6.7
variables: five nationalities, five colors, five brands of candy, five drink and five pets, 25 in total
values: houses in domain {1, 2, 3, 4, 5}, ordered from left to right
most constraints will be unary or binary e.g.:
    The Norwegian lives in the first house on the left => Norwegian = 1
    The Englishman lives in the red house => Englishman = red
there are also 5 ALLDIFF constraints on five nationalities, colors, etc.

6.8
./AI/csp
[2, 0, 2, 0, 1, 2, 1, 0]

6.9
most constrained variable: smaller branching factor
least constrained variable: more likely to be consistent, closer to a goal state

6.10
./AI/csp
Sample size = 100
n = 10, k = 3, average time: 45.803µs
n = 10, k = 4, average time: 58.821µs
n = 20, k = 3, average time: 161.787µs
n = 20, k = 4, average time: 195.926µs
n = 30, k = 3, average time: 313.017µs
n = 30, k = 4, average time: 343.659µs
n = 40, k = 3, average time: 498.832µs
n = 40, k = 4, average time: 528.52µs
n = 50, k = 3, average time: 713.714µs
n = 50, k = 4, average time: 781.804µs
n = 60, k = 3, average time: 921.854µs
n = 60, k = 4, average time: 985.129µs
n = 70, k = 3, average time: 1.288054ms
n = 70, k = 4, average time: 1.376637ms
n = 80, k = 3, average time: 1.614063ms
n = 80, k = 4, average time: 1.688802ms
n = 90, k = 3, average time: 2.02847ms
n = 90, k = 4, average time: 2.081303ms
n = 100, k = 3, average time: 2.487953ms
n = 100, k = 4, average time: 2.664178ms
n = 110, k = 3, average time: 3.009056ms
n = 110, k = 4, average time: 3.096969ms
n = 120, k = 3, average time: 3.688433ms
n = 120, k = 4, average time: 3.92376ms
n = 130, k = 3, average time: 4.508409ms
n = 130, k = 4, average time: 4.536822ms
n = 140, k = 3, average time: 5.225739ms
n = 140, k = 4, average time: 5.445125ms
n = 150, k = 3, average time: 6.139016ms
n = 150, k = 4, average time: 6.203166ms
n = 160, k = 3, average time: 7.180469ms
n = 160, k = 4, average time: 7.404204ms
n = 170, k = 3, average time: 7.981097ms
n = 170, k = 4, average time: 8.107019ms
n = 180, k = 3, average time: 8.889312ms
n = 180, k = 4, average time: 9.092286ms
n = 190, k = 3, average time: 9.967555ms
n = 190, k = 4, average time: 10.107981ms
n = 200, k = 3, average time: 10.917996ms
n = 200, k = 4, average time: 11.265273ms
n = 210, k = 3, average time: 12.022255ms
n = 210, k = 4, average time: 12.487402ms
n = 220, k = 3, average time: 13.422819ms
n = 220, k = 4, average time: 13.653953ms
n = 230, k = 3, average time: 14.635311ms
n = 230, k = 4, average time: 14.935789ms
n = 240, k = 3, average time: 15.938145ms
n = 240, k = 4, average time: 16.251375ms
n = 250, k = 3, average time: 17.275465ms
n = 250, k = 4, average time: 17.656288ms
n = 260, k = 3, average time: 18.982531ms
n = 260, k = 4, average time: 19.508623ms
n = 270, k = 3, average time: 20.831927ms
n = 270, k = 4, average time: 21.118058ms
n = 280, k = 3, average time: 22.793173ms
n = 280, k = 4, average time: 23.28953ms
n = 290, k = 3, average time: 24.759013ms
n = 290, k = 4, average time: 24.864295ms
n = 300, k = 3, average time: 26.079515ms
n = 300, k = 4, average time: 26.54345ms
n = 310, k = 3, average time: 27.405902ms
n = 310, k = 4, average time: 28.365417ms
n = 320, k = 3, average time: 29.848001ms
n = 320, k = 4, average time: 30.388094ms
n = 330, k = 3, average time: 31.850545ms
n = 330, k = 4, average time: 32.298845ms
n = 340, k = 3, average time: 33.672704ms
n = 340, k = 4, average time: 34.208691ms
n = 350, k = 3, average time: 34.849628ms
n = 350, k = 4, average time: 35.996554ms
n = 360, k = 3, average time: 37.107424ms
n = 360, k = 4, average time: 37.22433ms
n = 370, k = 3, average time: 39.466757ms
n = 370, k = 4, average time: 40.312503ms
n = 380, k = 3, average time: 41.036165ms
n = 380, k = 4, average time: 41.037952ms
n = 390, k = 3, average time: 43.320708ms
n = 390, k = 4, average time: 43.584628ms
n = 400, k = 3, average time: 45.540979ms
n = 400, k = 4, average time: 45.755596ms
n = 410, k = 3, average time: 48.01493ms
n = 410, k = 4, average time: 48.26128ms
n = 420, k = 3, timeout after 5s
running time roughly quadratic in n

6.11
./AI/csp

6.12
n <- the number of variables, d <- the maximum size of domain
O(n) edges are checked, each takes O(d^2) time
O(nd^2) in total

6.13
only put an edge (Xq, Xk) back on the queue when:
    (Xk, Xi) is the current edge / constraint under examination
    (Xq, Xk) is an edge in the graph
    counter for some x ∈ Xk on (Xk, Xi) is reduced to 0
each edge has O(d) counters, which in turn takes value <= d
sum of all these counters at the beginning is O(cd^2)
counters are non-negative
// thanks solution manual
by algorithm AC-4 on the paper 
    Arc and path consistency revisited, Mohr and Henderson (1986) 
    http://cse.unl.edu/~choueiry/Documents/Mohr+Henderson-AIJ1986.pdf
the innermost loop decrements a counter, there are O(cd^2) = O(n^2d^2) such decrementations

6.14
the arc-consistent graph is later searched from the root
if arc-consistency is forced from parent to child, for (Xj, Xi = PARENT(Xj))
    for all xj ∈ Xj, there is xi ∈ Xi that (xi, xj) ∈ R
    if an arbitrary value xi in the domain is assigned to Xi
    {xj | xj ∈ Xj, (xi, xj) ∈ R} may be empty, backtracking may be necessary
if arc-consistency is forced from child to parent, for (Xi = PARENT(Xj), Xj)
    for all xi ∈ Xi, there is xj ∈ Xj that (xi, xj) ∈ R
    if an arbitrary value xi in the domain is assigned to Xi
    there is always an xj ∈ Xj that (xi, xj) ∈ R, no backtracking is required

6.15
for a given inital board:
START STATE: assign number in random order, only ensure each zone (3 x 3 square) is filled with number 1-9
GOAL STATE: by Sudoku rules 
ACTIONS: swap places of two numbers in a zone (3 x 3 square) that's not fixed by the inital board
size of the state space <= 9! * 9 <= 2^22, local search should be efficient in such a small state space

6.16
constraint:
    let (Di .. Dj) be a tuple of domains of variables, a constraint is a subset R ⊆ Di * .. * Dj
    or a structure C with an exposed method rel that C.rel(xi .. xj) = true iff (xi .. xj) ∈ R for some fixed R
backtracking search: 
    repeat the process:
    1.  assign a value to a variable with a domain of size >= 2
    2.  prune inconsistent values from other variables 
    3.  if any domain is reduced to empty, revert the changes made in 1. and 2. 
    4.  if any domain has size >= 2, go back to 1; otherwise return the assignment
arc consistency:
    csp has arc consistency <=> csp is consistent in respect to all the binary constraints
backjumping:
    in backtracking search, when the current assignment is proved to be inconsistent
    backtrack to the most recent variables that caused the inconsistency instead of the most recent variable
min-conflicts:
    upon choosing a new value for a variable
    choose a value that prunes as few as possible values from other variables
cycle cutset:
    let G be a CSP graph, S ⊆ G.V
    if by removing S and incdent edges from G, the resulting graph G' is a tree
    then S is a cycle cutset of G

6.17
enumerate all possible combination of k nodes in G, remove these nodes, check if the remaining graph is a tree
check can be performed in time O(V + E), O((V + E)n^k) in total
Approximation Algorithms for the Loop Cutset Problem, Becker, A. and Geige, D. (2013)
https://arxiv.org/ftp/arxiv/papers/1302/1302.6787.pdf
described an algorithm with time complexity O(E + VlgV)
for common CSP graphs, the optimal cycle cutset is already large
enumerating all the possible value assignments of S would be impractical

7.1
// ./AI/propositional
KB worlds:
  24: Pits: [(3, 1)], Wumpus: (1, 3)
a2 worlds:
  8: Pits: [(1, 3), (3, 1)], Wumpus: (1, 3)
  9: Pits: [(1, 3), (3, 1)], Wumpus: (2, 2)
  10: Pits: [(1, 3), (3, 1)], Wumpus: (3, 1)
  11: Pits: [(1, 3), (3, 1)], No wumpus
  12: Pits: [(1, 3)], Wumpus: (1, 3)
  13: Pits: [(1, 3)], Wumpus: (2, 2)
  14: Pits: [(1, 3)], Wumpus: (3, 1)
  15: Pits: [(1, 3)], No wumpus
  24: Pits: [(3, 1)], Wumpus: (1, 3)
  25: Pits: [(3, 1)], Wumpus: (2, 2)
  26: Pits: [(3, 1)], Wumpus: (3, 1)
  27: Pits: [(3, 1)], No wumpus
  28: Pits: [], Wumpus: (1, 3)
  29: Pits: [], Wumpus: (2, 2)
  30: Pits: [], Wumpus: (3, 1)
  31: Pits: [], No wumpus
a3 worlds:
  0: Pits: [(1, 3), (2, 2), (3, 1)], Wumpus: (1, 3)
  4: Pits: [(1, 3), (2, 2)], Wumpus: (1, 3)
  8: Pits: [(1, 3), (3, 1)], Wumpus: (1, 3)
  12: Pits: [(1, 3)], Wumpus: (1, 3)
  16: Pits: [(2, 2), (3, 1)], Wumpus: (1, 3)
  20: Pits: [(2, 2)], Wumpus: (1, 3)
  24: Pits: [(3, 1)], Wumpus: (1, 3)
  28: Pits: [], Wumpus: (1, 3).1

7.2
a.  mythical => immortal
b.  ~mythical => mortal ∧ mammal
c.  immortal ∨ mammal => horned
d.  horned => magical
from a and b:
    e.  immortal ∨ (mortal ∧ mammal)
from c and e:
    f.  horned
from d and f:
    g.  magical

7.3
a.  ./AI/propositional/src/lib.rs
b.  a => b with a = false
    a || b with a = true
    a && b with a = false
c.  TAOTOLOGY can be reduced to PL-TRUE? with empty model
    hence PL-TRUE? is NP-complete
d.  ./AI/propositional/src/lib.rs
    a => b with { a = true }
    a || b with { a = false }
    a && b with { a = true }
e.  if for a partial model P, PL-TRUE?(KB, P) is true or false
    all the full models M where P ⊆ M nolonger have to be checked

7.4
a.  true
b.  false (by definition of =>)
c.  true
d.  false {a = false, b = false}
e.  true 
f.  true
        !((A => C) ∨ (B => C))
    <=> (!A ∧ C) ∧ (!B ∧ C)
    <=> !(A ∨ B) ∧ C
    =>  !(A ∧ B) ∧ C
    =>  !(A ∧ B) ∨ C
    =>  !((A ∧ B) => C)
g.  true
        (C ∨ (!A ∧ !B))
    <=> (C ∨ !A) ∧ (C ∨ !B)
    <=> (A => C) ∧ (B => C)
h.  true (and-elimination)
i.  false {a = false, b = false, c = false, d = true, e = false}
j.  true {a = true, b = false}
k.  true {a = true, b = true}
l.  false (./AI/propositional)

7.5
let M(a) be the set of models in which a is true
a.      a is valid
    =>  a is true for every model
    =>  M(True) = M(a)
    =>  True |= a
        True |= a
    =>  M(True) ⊆ M(a), as M(True) is the universal set of models
    =>  a is valid
b.  M(False) = ∅ ⊆ M(a) for any a
c.      a |= b
    <=> M(a) ⊆ M(b)
    <=> for all model, if a is true, b must be true
    <=> !(a ∧ !b) is valid
    <=> (!a ∨ b) is valid
    <=> a <=> b is valid
d.  a |= b iff a => b is valid
    b |= a iff b => a is valid
    a ≡ b => (a => b && b => a), a <=> b is valid
    and vice versa
e.      a ∧ !b is unsatisfiable
    <=> !(a ∧ !b) is valid
    <=> !a ∨ b is valid
    <=> a => b is valid
    <=> a |= b

7.6
a.  a ∧ b => a is valid, so a ∧ b |= a and a ∧ b |= b
    if a |= γ, M(a ∧ b) ⊆ M(a) ⊆ M(γ), a ∧ b => γ
    if b |= γ, similar
b.      a |= (b ∧ γ)
    =>  a => b ∧ γ is valid
    =>  a => b ∧ a => γ is valid
    =>  a => b is valid and a => γ is valid
    =>  a |= b and a |= γ
c.  define
        a = True
        b = s
        γ = !s
    s ∨ !s is valid, True |= b ∨ γ
    but neither s nor !s is valid, both a |= b or a |= !s are false

7.7
a.  2^2
b.  2^4
c.  2^4

7.8
a.  XOR
b.  there are 2^4 = 16 functions in the domain N2 * N2 => N2
    each corresponds to a binary operation
c.  for some of them the result does not depend on all the operands

7.9
./AI/propositional

7.10
./AI/propositional
a.  valid
b.  neither
c.  neither
d.  valid
e.  valid
f.  valid
g.  valid

7.11
let S = {Si} be the set of symbols appeared in the sentence
let M = {Mi} be the set of models in which the sentence is not the case
let C(Mj) be a conjunction that:
    C(Mi) = L1 ∧ .. ∧ Ln where
    Lj  = Sj if Sj is true in Mi
        = !Sj if Sj is false in Mi
the sentence is equivalent to:
        !(∨{C(Mi) | Mi ∈ M})
    <=> ∧{!C(Mi) | Mi ∈ M}
    <=> ∧{!C'(Mi) | Mi ∈ M} where
    C'(Mi) = L'1 ∨ .. ∨ L'n
    L'j = !Sj if Sj is true in Mi
        = Sj if Sj is false in Mi
which is a CNF

7.12
~(~A & ~B) = A | B
A | B + ~B | A:
    A
~A | B | E + A:
    B | E
B | E + ~E | B:
    B
~B | F + B:
    F
~B | C + B:
    C
~C | ~F | ~B + B + F + C:
    {}
~A & ~B is proven

7.13
a.      (P1 & .. & Pm) => Q
    <=> ~(P1 & .. & Pm) | Q
    <=> (~P1 | .. | ~Pm) | Q
    <=> ~P1 | .. | ~Pm | Q
b.      ~P1 | .. | ~Pm | Q1 | .. | Qn
    <=> (~P1 | .. | ~Pm) | (Q1 | .. | Qn)
    <=> (P1 & .. & Pm) => (Q1 | .. | Qn)
c.  let two clauses be
        (P1 & .. & Pm) => (Q1 | .. | Qn) and
        (X1 & .. & Xs) => (Y1 | .. | Yt)
    where Pi = Yj, then
        (P1 & .. & Pi-1 & Pi+1 & Pm & X1 & .. & Xs) => (Q1 | .. | Qn | Y1 | .. | Yj-1 | Yj+1 | .. | Yt)

7.14
a.  (ii)
b.  (i):    ~R | ~E | C
    (ii):   (~R | ~E | C) & (~R | ~C | E)
    (iii):  ~R | ~C | E | ~E

7.15
a.  digraph G {
        X1 -> X2 -> X3 -> X4 -> X5 [label="Imp"]
    }
b.  ~A | B <=> A => B ("Imp")
    if Xi = true, all Xj with j > i must have Xj = true
    there are n solutions to this SAT problem
c.  with MAC:
    at first the solution X1 .. X5 = false is explored in O(n)
    assigning false to variables triggers no change to consistent domains, INFERENCE is O(1)
    then the algorithm backtracks to X1 .. Xi = false, starting from i = 4
    one run of MAC in time O(n - i) will reduce the domain of all variables Xi+1 .. Xn to a single value
    O(n^2) in total
d.  PL-FC-ENTAILS? only has running time linear in number of literals 
    if clauses can be indexed by literals in premise in constant time
    possible connection: for both problems edges (clauses in SAT, constraints in CSP) are only propagated once

7.16
let c = {l1 .. ln} be a clause
if l1 = A, assign true to A, c is satisfied
if l1 = ~A, assign false to A, c is satisfied
a 3-clause {A, B, C} is unsatisfied only when both A, B and C is false
when A, B and C mention distinct symbols, in 1 / 2^3 = 1/8 of all the models the clause is unsatisfiable
hence each clause rules out at most 1/8 of all the models, 5 / 8 < 1, 3-SAT of 5 clauses is always satisfiable
unsatisfiable 3-SAT problem contains at least 8 clauses

7.17
a.  ~C | G + ~G:
        ~C
    ~D | G + ~G:
        ~D
    ~B | D + ~D:
        ~B
    A | B + ~B:
        A
    ~A | C + A + ~C:
        {}
b.  there are C(n, 2) = n * (n - 1) / 2 pairs of distinct symbols, 4 semantically distinct clauses for each pair:
        {A, B}, {~A, B}, {A, ~B}, {~A, ~B}
    two semantically distinct clauses for each symbol alone:
        {A, A}, {~A, ~A}
    where {A, ~A} <=> True for any A
    4(n^2 - n)/2 + 2n + 1 = 2n^2 + 1 clauses in total
c.  resolving two 2-CNF clauses will result in another 2-CNF clause
    there are O(n^2) semantically distinct 2-CNF clauses
    constructing a new clause from the KB may take at most O(n^2) time by enumerating all pairs
    O(n^4) in total
d.  RC(3-CNF) is not closed, resolving two 3-CNF results in a 4-CNF clause

7.18
./AI/propositional
a.  satisfiable
b.  lhs: (~A | ~C | B)
    rhs: (~A | B) & (~C | B)
    all symbols appear in both CNF with the same sign
c.  original: (~A | ~B | B) & (~A | A | B) & (~A | B | C)
    negation: (~A | ~C | B) & (~B) & (A | C)
    none of them resolves to empty clause

7.19
a.  enumerate all the assignments in which the sentence is true
    construct a clause for each assignment that:
        li = si if s1 = true in the assignment
        li = ~si if s1 = false in the assignment
b.  first to NNF, then distribute each AND over ORs 
    ./AI/propositional/src/lib.rs#to_dnf_expr
c.  for each clause ci:
        for each literal li in ci:
            if symbol of li is already assigned with contradicting value, return UNSATISFIABLE
            otherwise assign li to make it satisfied
        assign all symbols not in ci true
        return assignment
d.  ((((~A & ~B) & ~C) | ((B & ~B) & ~C)) | (((~A & C) & ~C) | ((B & C) & ~C))) | ((((~A & ~B) & ~A) | ((B & ~B) & ~A)) | (((~A & C) & ~A) | ((B & C) & ~A)))
    A = B = C = false
e.  // thanks solutions manual
    the final step (distribute AND over ORs) frequently produces DNF of exponential size of the original sentence

7.20
// thanks http://formal.cs.utah.edu:8080/pbl/PBL.php
S1: (~A | B | E) & (~B | A) & (~E | A)
S2: ~E | D
S3: ~C | ~F | ~B
S4: ~E | B
S5: ~B | F
S6: ~B | C

7.21
more likely
a random 4-CNF clause is a random 3-CNF clause + a new literal, distinct or not 
which is more likely to be satisfiable

7.22
a.  assuming indices are one-based
    (X12 & X22 & ~X21) | (X12 & ~X22 & X21) | (~X12 & X22 & X21)
b.  for each of C(n, k) combinations of neighbors N
    construct a conjuction clause in which:
        li = Xi if Xi ∈ N
        li = ~Xi if Xi ∉ N
    return disjunction of these clauses
c.  KB contains assertions from all known numbers 
    resolve KB & Xi and KB & ~Xi
d.  there are N symbols in which exactly M of them must be true
    let DPLL take one more parameter: limit
    return false if the number of true symbols in model > limit
    return false if the model is complete but the number of true symbols < limit 
e.  no, if KB |= a, KB & b |= a
f.  when M = k, k-1 mines are already proved
    if there are two squares Xi and Xj that may contain a mine
    no matter how long the distance between Xi and Xj is, Xi <=> ~Xj

7.23
KB |= a is proved by unsatisfiability of KB & ~a
first all pure symbols will be assigned
a pure symbol can be extracted in time O(n) where n is the number of literals in the CNF
here a is not a pure symbol since both a and ~a ∈ (KB & ~a)
then all the unit clauses are assigned
again a unit clause can be find in time O(n) given a partial model 
once a is assigned, further recursive calls will immediately return false
let p be the number of pure symbols appear in the run, u be the number of unit clauses appear in the run
DPLL will take O(pn + un) time

7.24
// ./AI/propositional
(~A | ~B | L) & (~A | ~P | L) & (~B | ~L | M) & (~L | ~M | P) & (~P | Q) & (~Q) & (A) & (B)
{}
chosen Q = false by unique clause heuristic
{Q = false}
chosen P = false by unique clause heuristic
{P = false, Q = false}
chosen A = true by pure symbol heuristic
{A = true, P = false, Q = false}
chosen L = true by pure symbol heuristic
{A = true, L = true, P = false, Q = false}
chosen B = true by pure symbol heuristic
{A = true, B = true, L = true, P = false, Q = false}
chosen M = true by unique clause heuristic
{A = true, B = true, L = true, M = true, P = false, Q = false}
heuristics were unbelievably effective here

7.25
Locked(t+1) <=> (~Locked(t) & Lock(t)) | (Locked(t) & ~Unlock(t))

7.26
// only fluents mentioned in the text
FaceingEast(t+1) <=>    (FacingNorth(t) & TurnRight(t)) |
                        (FacingSouth(t) & TurnLeft(t)) |
                        (FacingEast(t) & ~TurnRight(t) & ~TurnLeft(t))
WumpusAlive(t+1) <=>    WumpusAlive(t) &
                        ~(FacingWumpus(t) & HaveArrow(t) & Shoot(t))
FacingWumpus is not a fluent, it only depends on background & fluents for the current time t

7.27
// thanks solutions manual
define symbol or(Pij, Pqr) for each pair of Pij and Pqr
then P31 | P22 will be included in the next believe state as or(P31, P22)
// code / performance test skipped

8.1
a.  road, city, coastline
b.  explicit: location of cities, start and end point of roads, scale of the map
    implicit: whether two cities are connected by roads, length of the roads, size of cities
c.  temporal states / fluents (e.g. traffic flow at the moment)
d.  exact shape of the coastline
    exact shape of a road
e.  statistical graphics
        pros: intuitive, expressive
        cons: lacks precision compared to raw data
    Venn diagram:
        pros: intuitive
        cons: causes certain kind of misunderstanding of set theories

8.2
true under closed-world assumption
in standard semantics there may be infinitely more unnamed objects x in the domain, for which ~P(x)

8.3
it is valid, without unique-name assumption x and y may be different names of the same object

8.4
∀x, y x = y

8.5
each constant symbol may point to one of D elements, c^D possible arrangements
a predicate of arity k is a subset of the powerset of domain D in which all sets are of size k
    there are C(D, k) different subsets of size k, 2^C(D, k) for each predicate
    2^C(D, k)pk for all predicates of arity k, Π(1 <= k <= A)2^C(D, k)pk = 2^Σ(1 <= k <= A)C(D, k)pk in total
size of the function domain D^k => D ∪ {empty} is (D + 1)^D^k
    ((D + 1)^D^k)^fk = (D + 1)^(D^k)fk for each k
    Π(1 <= k <= A)(D + 1)^(D^k)pk = (D + 1)^Σ(1 <= k <= A)(D^k)pk in total
c^D * 2^Σ(1 <= k <= A)C(D, k)pk * (D + 1)^Σ(1 <= k <= A)(D^k)pk

8.6
a.  valid
    ∃x x = x means the object domain is not empty
    hence for each object there is an object (itself) equals to it
b.  valid, only falsy in intuitionistic logic
c.  valid, x = x for all object by definition, x = x => Smart(x) | x = x 

8.7
1.  ∃x x = x
    true in any domain with at least one object, false in the empty domain
2.  ∃x P(x) | ~P(x)
    same to above
the empty domain leads to the strange consequence that ∀x α is no longer stronger than ∃x α
namely ∀x P(x) | ~P(x) is valid but not sentence 2. above

8.8
no, there's no axiom forcing monogamy in the system
∀x,y,z Spouse(y, x) & Spouse(z, x) => y = z
proof:
        Jim != George & Spouse(Jim, Laura)
    assume Spouse(Laura, George)
    =>  Jim = George & Jim != George // monogamy axiom
    contradiction
    =>  ~Spouse(Laura, George)
if Spouse if an unary function, monogamy is implicitly forced, but everything now in the domain must be married

8.9
a.  i.  invalid, Paris & Marseilles is not an object
    ii. correct
    iii.incorrect, disjunction instead of conjunction
b.  i.  correct
    ii. incorrect, true if any non-country object in the domain
    iii.invalid
    iv. invalid 
c.  i.  correct
    ii. correct, a & b => c <=> a => (b => c):
            a & b => c <=> ~(a & b) | c <=> ~a | ~b | c
            a => (b => c) <=> a => (~b | c) <=> ~a | ~b | c
    iii.incorrect, (a => b) => c <=> (~a | b) => c <=> ~(~a | b) | c <=> (a & ~b) | c
        additional restriction: for all object c, 
        if c is not in South America, c must not share border with Eucador & c must be a country
    iv. incorrect, all countries must be in South America & share border with Ecuador
d.  i.  correct
    ii. correct, a & b => ~c <=> ~(a & b) | ~c <=> ~a | ~b | ~c <=> ~(a & b & c)
    iii.incorrect: if there is a region in South America, there is a region in Europe sharing no border with it
    iv. invalid: ambiguous syntax
e.  i.  incorrect
    ii. correct
    iii.incorrect
    iv. invalid, x != y is not an object

8.10
a.  Occupation(Emily, Surgeon) | Occupation(Emily, Lawyer)
b.  Occupation(Joe, Actor) & ∃o Occupation(Joe, o) & o != Actor
c.  ∀p Occupation(p, Surgeon) => Occupation(p, Doctor)
d.  ~∃p Occupation(p, Lawyer) & Customer(Joe, p)
e.  ∃p Occupation(p, Lawyer) & Boss(p, Emily)
f.  ∃p1,p2 Occupation(p1, Lawyer) & (Customer(p2, p1) => Occupation(p2, Doctor))
g.  ∀p1∃p2 Occupation(p1, Doctor) => (Occupation(p2, Lawyer) & Customer(p1, p2))

8.11
a.  if two speaks the same language they may understand each other
b.  naturally Understands(x, y) is symmetric
c.  i.  ∀x,y Understands(x, y) => Friend(x, y)
    ii. ∀x,y,z Friend(x, y) & Friend(y, z) => Friend(x, z)

8.12
∀x NatNum(x) <=> x = 0 | ∃y x = S(y) & NatNum(y)

8.13
a.  ∀s Breezy(s) => ∃s' Adjacent(s, s') & Pit(s')
    ∀s ~Breezy(s) => ~∃s' Adjacent(s, s') & Pit(s')
        (a => b) & (~a => ~b)
    <=> (a => b) & (~~b | ~a)
    <=> (a => b) & (~a | b)
    <=> (a => b) & (a => b)
    <=> (a <=> b)
b.  ∀s,s' Pit(s) & Adjacent(s, s') => Breezy(s')
    this axiom won't factor out other causes of breeze
    no conclusions can be derived from Breezy(s)

8.14
∀p1,p2 GrandChild(p1, p2) <=> ∃p3 Parent(p3, p2) & Parent(p1, p3)
∀p1,p2 GrandParent(p1, p2) <=> GrandChild(p2, p1)
∀p1,p2 Grandgrandparent(p1, p2) <=> ∃p3 GrandParent(p3, p2) & Parent(p1, p3)
∀p1,p2 Ancestor(p1, p2, k) <=> Parent(p1, p2) & k = 1 | ∃p3 Ancestor(p3, p2, k-1) & Parent(p1, p3)
∀p1,p2 Sibling(p1, p2) <=> ∃p3 Parent(p3, p1) & Parent(p3, p2) & p1 != p2
∀p1,p2 Brother(p1, p2) <=> Sibling(p1, p2) & Male(p1)
∀p1,p2 Sister(p1, p2) <=> Sibling(p1, p2) & Female(p1)
∀p1,p2 Daughter(p1, p2) <=> Parent(p2, p1) & Female(p1)
∀p1,p2 Son(p1, p2) <=> Parent(p2, p1) & Male(p1)
∀p1,p2 FirstCousin(p1, p2) <=> ∃p3 GrandParent(p3, p1) & GrandParent(p3, p2) & ~Sibling(p1, p2) & p1 != p2
∀p1,p2 BrotherInLaw(p1, p2) <=> ∃p3 Spouse(p2, p3) & Brother(p1, p3)
∀p1,p2 SisterInLaw(p1, p2) <=> ∃p3 Spouse(p2, p3) & Sister(p1, p3)
∀p1,p2 Aunt(p1, p2) <=> ∃p3 Parent(p3, p2) & Sister(p1, p3)
∀p1,p2 Uncle(p1, p2) <=> ∃p3 Parent(p3, p2) & Brother(p1, p3)
https://en.wikipedia.org/wiki/Cousin
∀p1,p2,p3 MostRecentCommonAncestor(p3, p1, p2) <=>  ∃k1,k2 Ancestor(p3, p1, k1) & Ancestor(p3, p2, k2) &
                                                    ∀p4,k3,k4 Ancestor(p4, p1, k3) & Ancestor(p4, p2, k4) => (k1 <= k3 && k2 <= k4)
∀p1,p2,m,n Cousin(p1, p2, m, n) <=> ∃p3,k Ancestor(p3, p1, m + 1) & Ancestor(p3, p2, k) &
                                    MostRecentCommonAncestor(p3, p1, p2) & ~Sibling(p1, p2) & 
                                    k > 1 & |m + 1 - k| = n
engine used: pyDatalog https://sites.google.com/site/pydatalog/home
./AI/first_order/kinship.py
Elizabeth's grandchildren:
    Peter
    Harry
    Eugenie
    James
    William
    Beatrice
    Louise
    Zara
Diana's brothers-in-law:
    Andrew
    Edward
Zara's great-grandparents:
    Mum
    George
Eugenie's ancestors:
    George
    Elizabeth
    Philip
    Mum
    Andrew
    Sarah

8.15
there's no way to derive the absence of an element in a set

8.16
Nil = []
∀l List?(l) <=> l = [] | ∃h,t l = [h|t]
∀h,t First([h|t]) = h
First([]) = None
∀h,t Rest([h|t]) = t
Rest([]) = None
∀l Append([], l) = l
∀h,t,l Append([h|t], r) = [h|Append(t, r)]
∀x ~Find(x, [])
∀x,h,t Find(x, [h|t]) <=> h = x | Find(x, t)

8.17
the sentence may refer to squares out of bounds
the definition of adjacent is not symmetric
there's no way to prove non-adjacency

8.18
∀s,t At(Agent, s, t) & Stench(t) => Stenchy(s)
∀s Stenchy(s) <=> ∃s',t At(Wumpus, s', t) & Adjacent(s, s')
∀s,s' At(Wumpus, s, 0) & At(Wumpus, s', 0) => s = s'
∀s,t At(Wumpus, s, t) <=> At(Wumpus, s, 0)

8.19
∀p,p' Daughter(p, p') <=> Parent(p', p) & Female(p)
∀p1,p2,p3 ChildTogether(p1, p2, p3) <=> Parent(p2, p1) & Parent(p3, p1)
a.  ∃p Daughter(p, Joan)
b.  ∃1p Daughter(p, Joan)
c.  ∃1p Parent(Joan, p) & Female(p)
d.  ∃1p ChildTogether(p, Joan, Kevin)
e.  ∃p ChildTogether(p, Joan, Kevin) & ~(∃p,p' ChildTogether(p, Joan, p') & p' != Kevin)

8.20
a.  Even(0), ~Even(1)
    ∀x Even(x + 2) <=> Even(x)
b.  Prime(x) <=> x >= 2 & ∀y,z y * z = x => (y = 1 & z = x) | (y = x & z = 1)
c.  ∀x Even(x) => ∃y,z Prime(y) & Prime(z) & y + z = x

8.21
equality is transitive
if WA = red and any other region X also has X = red, WA = X entails

8.22
∀x,t∃t' Key(x) & Before(t', t) => Lost(x, t)
∀x,y,t∃t' Sock(x) & Sock(y) & Pair(x, y) & Before(t', t) => Lost(x, t) | Lost(y, t)

8.23
a.  x and y may be different names of the same object
    the sentence would be false whenever there is both a Person and a non-Person object
    ~∃x,y,n Person(x) & Person(y) & x != y & HasSS#(x, n) & HasSS#(y, n)
b.  correct
c.  sentence is stronger than in English: every Person has a SSN
    ∀x,n Person(x) & HasSS#(x, n) => Digits(n, 9) 
d.  ~∃x,y Person(x) & Person(y) => SS#(x) = SS#(y)
    SS#(John) = SS#(Mary)
    ∀x Person(x) => Digits(SS#(x), 9)

8.24
constants:  Franch, Greek, Spr2001
predicates: Take(s, l, t), Pass(s, l, t)
function:   Score(s, l, t)
a.  ∃s Take(s, Franch, Spr2001)
b.  ∀s,t Take(s, Franch, t) => Pass(s, Franch, t)
c.  ∃s Take(s, Greek, Spr2001) & (∀x,y Take(x, Greek, Spr2001) & Take(y, Greek, Spr2001) => x = y)
d.  ∀x,t1∃y,t2 Take(s, Franch, t1) => Score(y, Greek, t2) > Score(x, Franch, t1)
predicates: Smart(c), Expensive(p), Buy(c, p, a)
e.  ∀c (∃p,a Buy(c, p, a)) => Smart(c)
f.  ∀c,p Expensive(p) => ~Buy(c, p)
g.  ∃a∀c,p Buy(c, p, a) => ~Insured(c)
predicates: Shave(p1, p2), InTown(p)
h.  ∃p1∀p2 InTown(p2) & ~Shave(p2, p2) => Shave(p1, p2)
constants:  ByBirth, ByDescent
predicates: UKCitizen(p, c), UKResident(p), Parent(p1, p2)
functions:  BornPlace(p)
i.  ∀p1 BornPlace(p1) = UK & (∀p2 Parent(p2, p1) => (∃c UkCitizen(p2, c)) | UkResident(p2)) => UkCitizen(p1, ByBirth)
j.  ∀p1 BornPlace(p1) != UK & (∃p2 Parent(p2, p1) & UkCitizen(p2, ByBirth)) => UkCitizen(p1, ByDescent)
predicates: CanFool(p, f, t), People(f), Time(t)
k.  (∃p,f∀t Time(t) => CanFool(p, f, t)) & (∃p,t∀f People(f) => CanFool(p, f, t)) &
    ~(∃p∀f,t Time(t) & People(f) => CanFool(p, f, t))
predicates: Speak(x, l), Greek(x)
l.  ∀x,y Greek(x) & Greek(y) => ∃l Speak(x, l) & Speak(y, l)

8.25
constants:  Willington, Napoleon
predicates: Heard(p, o)
functions:  Death(p)
axiom:
    // if p1 heard about death of p2, p2 is dead upon p1's death, so p2 cannot hear about p1's death
    ∀p1,p2 Heard(p1, Death(p2)) => ~Heard(p2, Death(p1))
Heard(Willington, Death(Napoleon))
~Heard(Napoleon, Death(Willington))

8.26
define Ad(a, b, c) as a function N2 * N2 * N2 => N2^2:
    Ad1(a, b, c) = ((a + b + c) % 2, (a + b + c) / 2)
n-bit addition is a function N2^n * N2^n => (N2^n, N2) defined as:
    Adn([], []) = ([], 0)
    Adn([h1|t1], [h2|t2]) = 
        (s, c) <- Adn(t1, t2)
        (s', c') <- Ad1(h1, h2, c)
        ([s'|s], c')
define the function (syntax sugar over Signal) SignalAll:
    SignalAll(Out, c) = all output signals from a circult as a list
    SignalAll(In, c) = all input signals from a circult as a list
define a new circuit Add where
    ∀c Circuit(c) & Type(c) = Add => Arity(c, 3, 2)
    ∀c Circuit(c) & Type(c) = Add =>    
        (SignalAll(Out, c) = [0, 0] <=> SignalAll(In, c) = [0, 0, 0]) &
        (SignalAll(Out, c) = [1, 0] <=> SignalAll(In, c) = [1, 0, 0] | SignalAll(In, c) = [0, 1, 0] | SignalAll(In, c) = [0, 0, 1]) &
        (SignalAll(Out, c) = [0, 1] <=> SignalAll(In, c) = [1, 1, 0] | SignalAll(In, c) = [0, 1, 1] | SignalAll(In, c) = [1, 0, 1]) &
        (SignalAll(Out, c) = [1, 1] <=> SignalAll(In, c) = [1, 1, 1])
a 4-bit adder is defined as:
    Circuit(Ad4) & Arity(Ad4, 8, 5)
    Circult(Ad0) & Type(Ad1) = Add
    Circult(Ad1) & Type(Ad1) = Add
    Circult(Ad2) & Type(Ad1) = Add
    Circult(Ad3) & Type(Ad1) = Add
    Connected(Out(1, X0), In(1, Ad0))
    Connected(Out(1, Y0), In(2, Ad0))
    Connected(Out(1, X1), In(1, Ad1))
    Connected(Out(1, Y1), In(2, Ad1))
    Connected(Out(1, X2), In(1, Ad2))
    Connected(Out(1, Y2), In(2, Ad2))
    Connected(Out(1, X3), In(1, Ad3))
    Connected(Out(1, Y3), In(2, Ad3))
    Connected(Out(2, Ad0), In(3, Ad1))
    Connected(Out(2, Ad1), In(3, Ad2))
    Connected(Out(2, Ad2), In(3, Ad3))
    Connected(Out(1, Ad0), In(1, Z0))
    Connected(Out(1, Ad1), In(1, Z1))
    Connected(Out(1, Ad2), In(1, Z2))
    Connected(Out(1, Ad3), In(1, Z3))
    Connected(Out(2, Ad3), In(1, Z4))
for n = 4, there are (2^4)^2 = 256 different inputs, correctness of the function can be verified by enumeration

8.27
∀c CanApplyForPassport(c) <=> (∃p CanProveCitizenship(p, c)) & (∃p Photo(p) & Recognizable(p, c)) & CanPay(c, Fee)

8.28
a.  Wrote(Gershwin, TheManILove)
b.  ~Wrote(Gershwin, EleanorRigby)
c.  Wrote(Gershwin, TheManILove) | Wrote(McCartney, TheManILove)
d.  ∃s Wrote(Joe, s)
e.  ∃d Own(Joe, d) & CopyOf(d, Revolver)
f.  ∀s Sings(McCartney, s, Revolver) => Wrote(McCartney, s)
g.  ~∃s,p Sings(p, s, Revolver) & Wrote(Gershwin, s)
h.  ∀s Wrote(Gershwin, s) => ∃p,a Sings(p, s, a)
i.  ∃a∀s Wrote(Joe, s) => ∃p Sings(p, s, a)
j.  ∃d,a CopyOf(d, a) & Own(Joe, d) & Sings(BHoliday, TheManILove, a)
k.  ∀a∃d (∃s Sings(McCartney, s, a)) => Own(Joe, d) & CopyOf(d, a)
l.  ∃d,a Own(Joe, d) & CopyOf(d, a) & (∀s,p Sings(p, s, a) => p = BHoliday)

9.1
a.  assume ∀v a & ~SUBST({v/g}, a)
    as g is a term, ~SUBST({v/g}, a) <=> SUBST({v/g}, ~a) => ∃v ~a
    ~(∀v a & ~SUBST({v/g}, a)), therefore ∀v a |= SUBST({v/g}, a)
b.  let kb be the original knowledge base, let kb' be kb after existential instantiation
    =>: if kb & ~b is satisfiable, there is a model m in which ~b and kb are true
        for a existential subsentence ∃v a in kb which is true in m
        there must be at least one object C that SUBST({v/C}, a) is true
        for ∃v a is false in m, assume the domain is not empty, ~(∃v a) <=> ∀v ~a
        there must be an object C that SUBST({v/C}, ~a) <=> ~SUBST({v/C}, a) is true in m
        bind new constants in kb' to these objects, kb' would also be true in model m, kb' & ~b is true in m
    <=: if kb' & ~b is satisfiable, there is a model m in which ~b and kb' are true
        for a subsentence a in kb' is true in m
        if C is a constant in a, ∃v SUBST({C/v}, a) is true in m
        for a is false in m, ~a is true, ∃v SUBST({C/v}, ~a) <=> ~∃v SUBST({C/v}, a) is true in m
        existential quantifiers can be recovered without changing the satisfiability, kb & ~b => kb & ~b

9.2
a |= ∃v SUBST({k/v}, a) where k is a constant in a

9.3
a.  not legitimate, Everest is mentioned in the KB
b.  legitimate
c.  not legitimate but is entailed by KB, existential instantiation should only be applied once

9.4
./AI/first_order/inference
a.  Some({'y': B, 'x': A, 'z': B})
b.  None
c.  Some({'x': John, 'y': x})
d.  None

9.5
a.  Employs(x, y)
    Employs(Mother(a), y) | Employs(x, Father(b))
    Employs(Mother(John), y) | Employs(Mother(a), Father(b)) | Employs(x, Father(Richard))
    Employs(Mother(John), Father(b)) | Employs(Mother(a), father(Richard))
    Employs(Mother(John), Father(Richard))
b.  Employs(x, y)
    Employs(y, y)
    Employs(IBM, y)
c.  first find the sentences in KB which has the query in its subsumption lattice (by hashing)
    let the sentences be a set S
    FETCH(s) then returns unifiers from 2^S, the power set of S

9.6
a.  Horse(x) => Mammal(x)
    Cow(x) => Mammal(x)
    Pig(x) => Mammal(x)
b.  Offspring(x, y) & Horse(y) => Horse(x)
c.  True => Horse(Bluebeard)
d.  True => Parent(Bluebeard, Charlie)
e.  Offspring(x, y) <=> Parent(y, x)
f.  Mammal(x) => Parent(F(x), x)

9.7
a.  Parent, everyone have parents, no one is parent of themself
b.  ∀x∃y P(x, y) skolemizes to P(x, F(x))
    ~∃q P(q, q) <=> ∀q ~P(q, q) <=> ~P(q, q)
    without occur check, {q/x, x/F(x)} is a unifier, {} is entailed by KB
c.  ∀x∃y P(x, y) skolemizes to P(x, Sk)
    {x/Sk, q/Sk} is a unifier of P(x, Sk) and ~P(q, q), {} is entailed by KB
d.  if θ = {Sk1/A} is a unifier between P(Sk1) and ~P(A)
    SUBST(θ, P(Sk1)) and ~P(A) derives {}, P(Sk1) |= P(A) for arbitrary A

9.8
assume the first clause is (x1,1 | ~x1,2 | ~x1,3)
    (True(x1,1) | False(x1,2) | False(x1,3)) & .. => SAT
where x1,1 are variables to a domain of two objects: True and False
1.  True(True)
2.  False(False)
3.  True(x) XOR False(x)

9.9
// + here is a function i.e. can be unified with variables
a.  7 <= 3 + 9, unifies with only 8: 7 <= y & y <= 3 + 9
    sub y <= 3 + 9, unifies with 7: w + t <= 3 + 9
        y = w + t
        sub w <= 3, unifies with 1 
            w = 0
        sub t <= 9, unifies with 2
            t = 7
    sub 7 <= y
        7 <= 0 + 7, unifies with 8
        sub y <= 0 + 7, unifies with 6
            y = 7 + 0
        sub 7 <= y
            7 <= 7 + 0, true by 4
b.  from 1, 2 and 7:
    (1):    0 + 7 <= 3 + 9
    from 6:
    (2):    7 + 0 <= 0 + 7
    from (1), (2) and 8:
    (3):    7 + 0 <= 3 + 9
    from 4:
    (4):    7 <= 7 + 0
    from (4), (3) and 9:
            7 <= 3 + 9

9.10
sentences
    ~∃x Brother(x, I) | Sister(x, I)
    ∃x Son(x, Father(I)) & Father(Man) = x
after skolemization:
    1.  ∀x ~Brother(x, I) & ~Sister(x, I)
    2.  Son(C, Father(I))
    3.  Father(Man) = C
assume 
    4.  Father(Man) != I
from 3, 4 and transitivity of equation:
    (1):    C != I
from (1), 2 and family domain rules Father(x) = y => Parent(y, x):
    (2):    Parent(Father(I), C) & Parent(Father(I), I)
from (1), (2) and rule Sibling(x, y) <=> x != y & ∃p Parent(p, x) & Parent(p, y):
    (3):    Sibling(I, C)
from (3), 1 and the obvious Sibling(x, y) <=> Sister(x, y) | Brother(x, y):
    {}, contradiction
therefore Father(Man) = I, Son(Man, I)
the inference procedure have to understand transitivity of equations

9.11
S1: 
    Q1: O(1), assuming SSN is unique
    Q2: O(h), where h is the population of Houston
    Q3: O(p), where p is the total numbre of people in the database
    Q4: O(t) if ResidesIn(x, TinyTownUSA) is unified first, where t is the population of TinyTownUSA
S2:
    Q1: O(1)
    Q2: O(n)
    Q3: O(md) or O(n), where m is the number of mothers, d is the most children a mother may have in the database
        O(md) if queried by first argument of Mother, O(n) is queried by Mother predicate
    Q4: O(n)
S3:
    Q1: O(n)
    Q2: O(n) if everyone has an entry in ResidesIn table
    Q3: O(n) 
    Q4: O(n^2) if both table are unsorted
S4:
    same to S2 (how can the first argument be indexed without predicates?)
S5:
    Q1: O(1)
    Q2: O(h)
    Q3: O(n)
    Q4: O(t)

9.12
if a rule is applied multiple times during a proof and the variables are not standardized in place
the rule may fail to apply as the same variable cannot have two substitution at the same time 
e.g. in natural number domain
    Nat(n) => Nat(S(n))
to prove Nat(S(S(0))), the above rule must be applied twice
after the first application, n/S(0) ∈ θ
if sentences are standardized once and forall before inference, the second time will fail to unify

9.13
a.  ∃h Horse(h)
    and Offspring(h, y) // Offspring of a horse rule
        Parent(y, h) // Offspring definition
        or  Parent(Bluebeard, Charlie)
            y = Bluebeard, h = Charlie
        or  Mammal(h)
            Horse(h)
            // infinite loop from here
    and Horse(y) 
        // infinite loop from here
b.  the inference falls into an infinite loop from Horse(h) to Horse(h)
c.  Bluebeard and Charlie
d.  https://apps.dtic.mil/dtic/tr/fulltext/u2/a172502.pdf 4.1.4
    "In general, it is not decidable whether or not a given portion of a recursive
    search space is redundant. However, there are special cases where it is possible to
    prove redundancy without completely exploring the space. For repeating inference,
    a simple syntactic solution is possible. We can decide when to cut off inference by
    keeping track of the answers produced with each additional level of repetition. For
    divergent inference the problem is much harder. Here we must generate automatic
    proofs that no answers exist in a portion of the search space. These proofs are
    similar to proofs of program termination using well-founded sets."
    some kind of meta-analysis (auto-prove the redundency of part of the proof tree)

9.14
// skipped, refer figure 9.7

9.15
a.  P(A, [2, 1, 3])
    or  P(2, [2, 1, 3]), A = 2
    or  P((A, [1, 3]))
        or  P(1, [1, 3]), A = 1
        or  P(A, [3])
            or  P(3, [3]), A = 3
            or  P(A, []), stuck
    P(2, [1, A, 3])
    P(2, [A, 3])
    or  P(2, [2, 3]), A = 2
    or  P(2, [3])
        P(2, []), stuck
b.  Find / Member

9.16
// thanks https://swish.swi-prolog.org/
a.  sorted([]).
    sorted([_]).
    sorted([X, Y|Z]) :- sorted([Y|Z]), X =< Y.
b.  perm(X, Y) :- same_length(X, Y), contains(X, Y), contains(Y, X).
    contains([], _).
    contains([X|Z], Y) :- member(X, Y), contains(Z, Y).
c.  sort(L, M) :- perm(L, M), sorted(M).
d.  the algorithm filters all permutations of L by sorted, complexity is Ω(n! * n) 
e.  isort([], []).
    isort([X|Y], Z) :- isort(Y, I), insert(X, I, Z).
    insert(X, [], [X]).
    insert(X, [Y|Z], [X, Y|Z]) :- X =< Y.
    insert(X, [Y|Z], [Y|I]) :- X > Y, insert(X, Z, I).

9.17
a.  ∀X,Y simplify(X, Y) <=> (rewrite(X, Y) | ∃s rewrite(X, s) & simplify(s, Y)) & primitive(Y)
b.  x + y -> y + x
    x * 0 -> 0
    (x + y) + z -> x + (y + z)
    ..
c.  dx^n/dx -> nx^(n-1)
    dc/dx -> 0
    dy/dx -> 0
    d(-u)/dx -> -du/dx
    d(u + v)/dx -> du/dx + dv/dx
    duv/dx -> udv/dx + vdu/dx
    d(e^u)/dx -> e^u * du/dx
    ..
    // code skipped
    
9.18
solve(X, [X]) :- goal(X).
solve(X, [X|Y]) :- \+ goal(X), successor(X, Z), solve(Z, Y).
have no idea how to describe an efficient priority queue in prolog

9.19
a.  i.  {y/John} in 1st iteration
    ii. {y/John} in 2nd iteration
    iii.{} in 3rd iteration
    iv. never terminate
b.  no, the KB didn't state how to disprove Ancestor(x, y)
c.  no, exactly the same reason as above

9.20
a.  ∃p∀q S(p, q) <=> ~S(q, q)
b.  skolemization:
        ∃p S(p, q) <=> ~S(q, q)
        S(P, q) <=> ~S(q, q)
    1.  ~S(P, q) | ~S(q, q)
    2.  S(q, q) | S(P, q)
c.  SUBST({q/P}, 1):
    (1):    S(P, P)
    SUBST({q/P}, 2):
    (2):    ~S(P, P)
    from (1) and (2):
            {}

9.21
let True be the empty KB
True |= p <=> p is valid
if p is unsatisfiable, ~p is valid, True |= ~p

9.22
P(x), ~P(A) | ~P(B)
resolution gives ~P(A) or ~P(B)

9.23
a.  ∀x Horse(x) => Animal(x)
    ∀x,h HeadOf(h, x) & Horse(x) => Headof(h, x) & Animal(x)
b.  1.  ~Horse(x) | Animal(x) 
        ~(~(HeadOf(h, x) & Horse(x)) | (HeadOf(h, x) & Animal(x)))
        HeadOf(h, x) & Horse(x) & ~(HeadOf(h, x) & Animal(x))
        HeadOf(h, x) & Horse(x) & (~HeadOf(h, x) | ~Animal(x))
    2.  HeadOf(h, x)
    3.  Horse(x)
    4.  ~HeadOf(h, x) | ~Animal(h, x) 
    from 1 and 3:
    (1):    Animal(x)
    from (1) and 4:
    (2):    ~HeadOf(h, x)
    from (2) and 2:
            {}

9.24
a.  (A) for all natural numbers there is another natural number smaller than or equal to it
    (B) there is a natural number which is smaller than or equal to all natural numbers
b.  true, y = 0
c.  true, y = 0
d.  no, consider a domain with only ground facts n >= n for all natural number n
    A is true in this domain but B is not
e.  yes
f.  =>: ∃y∀x x >= y
        ~∀x∃y x >= y
            ∃x∀y ~(x >= y)
        skolemization:
        1.  x >= C0
        2.  ~(C1 >= y)
        θ = {x/C1, y/C0}, 1 and 2 derives {}
    <=: ∀x∃y x >= y
        ~∃y∀x x >= y
            ∀y∃x ~(x >= y)
        skolemization:
        1.  x >= F(x)
        2.  ~(x >= G(x))
        no unification
g.  how is that possible

9.25
when there are only definite clauses, both afterward and backward chaining return a (generator of) single unifier
in which no variable can be bind to two different ground terms

9.26
no, satisfication / validity of first order logic is undecidable
hence any algorithm deciding True |= s for some first order sentence s will either:
    be incorrect on some inputs, or
    not terminate on some inputs

10.1
the two concepts are equivalent
every planning problem can be described as a finite searching problem
each fninite searching problem can be described as a planning problem
(treat atomic states as constants, actions as schemas on constants)

10.2
Fly(P1, JFK, JFK)
Fly(P1, JFK, SFO)
Fly(P2, SFO, SFO)
Fly(P2, SFO, JFK)

10.3
a.  At(Monkey, A) & At(Bananas, B) & At(Box, C) & Height(Monkey, Low) & Height(Box, Low) & Height(Bananas, High) &
    Place(A) & Place(B) & Place(C) & Small(Bananas)
b.  Action(Go(from, to),
        PRECOND:    Place(from) & Place(to) & At(Monkey, from) & Height(Monkey, Low),
        EFFECT:     ~At(Monkey, from) & At(Monkey, to))
    Action(Push(o, from, to),
        PRECOND:    Place(from) & Place(to) & At(Monkey, from) & At(o, from) & Height(Monkey, Low) & Height(o, Low),
        EFFECT:     ~At(Monkey, from) & At(Monkey, to) & ~At(o, from) & At(o, to))
    Action(ClimbUp(p, o),
        PRECOND:    Place(p) & At(o, p) & At(Monkey, p) & Height(Monkey, Low) & Height(o, Low)
        EFFECT:     On(Monkey, o) & ~Height(Monkey, Low) & Height(Monkey, High))
    Action(ClimbDown(p, o),
        PRECOND:    On(Monkey, o),
        EFFECT:     ~On(Monkey, o) & ~Height(Monkey, High) & Height(Monkey, Low))
    Action(Grasp(o, p, h),
        PRECOND:    At(Monkey, p) & At(o, p) & Height(Monkey, h) & Height(o, h) & Small(o),
        /// an object hold by the monkey doesn't have At and Height properties
        EFFECT:     Holding(o) & ~At(o, p) & ~Height(o, h))
    Action(Ungrasp(o, p, h),
        PRECOND:    At(Monkey, p) & Height(Monkey, h) & Holding(o),
        /// ungrasping an object leaves it at where the monkey is
        EFFECT:     ~Holding(o) & At(o, p) & Height(o, h))
c.  Holding(Bananas, s) & ∃p At(Box, p, s0) & At(Box, p, s) & Following(s, s0)
    Following(s', s) <=> ∃a (Poss(a, s) & s' = Result(a, s)) | (∃s'' Poss(a, s'') & Following(s'', s) & s' = Result(a, s''))
d.  Action(Push(o, from, to),
        PRECOND:    Place(from) & Place(to) & At(Monkey, from) & At(o, from) & Height(Monkey, Low) & Height(o, Low) & ~Heavy(o)
        EFFECT:     ~At(Monkey, from) & At(Monkey, to) & ~At(o, from) & At(o, to))

10.4
a.  Action(Go(x, y, r),
        PRECOND:    In(x, r) & In(y, r) & At(Shakey, x),
        EFFECT:     ~At(Shakey, x) & At(Shakey, y))
    Action(Push(b, x, y, r),
        PRCOND:     Box(b) & In(x, r) & In(y, r) & In(b, r) & At(Shakey, x) & At(b, x),
        EFFECT:     ~At(b, x) & ~At(Shakey, x) & At(b, y) & At(Shakey, y))
    Action(ClimbUp(x, b),
        PRECOND:    Box(b) & At(b, x) & At(Shakey, x) & On(Shakey, Floor),
        EFFECT:     ~On(Shakey, Floor) & On(Shakey, b))
    Action(ClimbDown(b, x),
        PRECOND:    Box(b) & On(Shakey, b) & At(Shakey, x) & At(b, x),
        EFFECT:     ~On(Shakey, b) & On(Shakey, Floor))
    Action(TurnOn(x, s, b),
        PRECOND:    Box(b) & At(Shakey, x) & At(b, x) & At(s, x) & On(Shakey, b) & ~SwitchOn(s)
        EFFECT:     SwitchOn(s))
    Action(TurnOff(x, s, b),
        PRECOND:    Box(b) & At(Shakey, x) & At(b, x) & At(s, x) & On(Shakey, b) & SwitchOn(s)
        EFFECT:     ~SwitchOn(s))
    Init(
        In(S4, Room4) & In(Door4, Room4) & 
        In(S3, Room3) & In(Door3, Room3) & In(I0, Room3) &
        In(S2, Room2) & In(Door2, Room2) & 
        In(S1, Room1) & In(Door1, Room1) & In(B4, Room1) & In(B3, Room1) & In(B2, Room1) & In(B1, Room1) &
        In(Door4, Corridor) & In(Door3, Corridor) & In(Door2, Corridor) & In(Door1, Corridor) &
        At(Switch4, S4) & At(Switch3, S3) & At(Switch2, S2) & At(Switch1, S1) &
        At(Box4, B4) & At(Box3, B3) & At(Box2, B2) & At(Box1, B1) & At(Shakey, I0) &
        SwitchOn(Switch4) & ~SwitchOn(Switch3) & ~SwitchOn(Switch2) & SwitchOn(Switch1) &
        On(Shakey, Floor) & 
        In(R2, Room2)
    )
    [
        Go(I0, Door3, Room3),
        Go(Door3, Door1, Corridor),
        Go(Door1, B2, Room1),
        Push(Box2, B2, Door1, Room1),
        Push(Box2, Door1, Door2, Corridor),
    ]

10.5
State(s):
    current state of the 
Transition(s, v, s', v', d):
    Turing machine on state s and input value v transits to state s', write v' to the cell, move to direction d 
LeftOf(c, c'):
    cell c is at left of cell c'
ValueOf(c, v):
    cell c has value v
/// strictly speaking, each action should have 4 variants: with PRECOND v = v' or v != v', s = s' or s != s'
/// otherwise the EFFECT may contain contradictions
Action(Stay(c, s, v, s', v'),
    PRECOND:    Pos(c) & State(s) & ValueOf(c, v) & Transition(s, v, s', v', S),
    EFFECT:     ~State(s) & State(s') & ValueOf(c, v'))
Action(Left(c, s, v, c', s', v'),
    PRECOND:    Pos(c) & State(s) & ValueOf(c, v) & Transition(s, v, s', v', L) & LeftOf(c', c),
    EFFECT:     ~State(s) & State(s') & ~ Value(c, v) & ValueOf(c, v') & ~Pos(c) & Pos(c'))
Action(Right(c, s, v, c', s', v'),
    PRECOND:    Pos(c) & State(s) & ValueOf(c, v) & Transition(s, v, s', v', R) & LeftOf(c, c'),
    EFFECT:     ~State(s) & State(s') & ~ Value(c, v) & ValueOf(c, v') & ~Pos(c) & Pos(c'))
Init(
    /// All transitions
    /// All tape cell values
    /// All LeftOf relations between cells
    /// Initial state of the machine
)
Goal(State(Sgoal)) for some state Sgoal

10.6
PDDL uses database semantics, a negative literal ~A in the goal requires the absence of A in the state
let S' = RESULT(a, S) for some state S and action a
let a' be a without negative literals in effects
obviously the resulting state may only contain more literals, RESULT(a', S) ⊆ S'
this conclusion can be easily generalized to sequence of actions by induction
therefore any solution to the original problem is a solution to the relaxed problem
consider a problem where:
Init(A)
Action(Move(),
    PRECONDS:   A,
    EFFECTS:    ~A & B)
GOAL(A & B)
the original problem has no solution, while the relaxed problem has a solution [Move()]
by removing negative literals from effects, the relaxed problem has a proper superset of solutions

10.7
/// init state and actions described in Figure 10.3
Goal(On(B, C) & On(A, B))
/// solutions described in Figure 10.3
it's impossible for an noninterleaving planner if the planner only explores optimal solutions to subgoals
for subgoal On(B, C), the optimal solution is [Move(B, Table, C)]
but then the other subgoal cannot be achieved without undoing the first subgoal and vice versa

10.8
forward classic planning is complete:
    state space in classic planning is finite
    a forward planning algorithm will eventually try each possible action from each possible states 
    the state space is fully explored, a solution will be found if exists
backward classic planning is as powerful as forward planning:
    a state in backward planning is a believe state in forward planning
    if there's a solution to a problem in forward planning
    applying the actions in reverse order, along the path the believe state is never empty
    backward planning is able to find the same solution

10.9
/// skipped

10.10
a.  by definition of (parallel) planning graphs
    level k of planning graph contains all literals that can be derived in at most k actions from initial state
    basis, k = 0:
        S0 = initial state, all literals in initial state can be derived in 0 steps 
    induction:
        Sk contains all literals which can be derived in at most k actions from initial state
        by persistent actions, Sk ⊆ Sk+1 
        if L ∈ S for some state S, S is the result of k+1 actions
        S = RESULT(a, S') for some a and S', where S' is the result of k actions
        hence S' ⊆ Sk is mutex free, L ∈ S ⊆ Sk+1
    the let n be final level of planning graph, by definition Sn+i = Sn for all i >= 0
    if a literal L ∉ Sn, L cannot be derived from initial state by finite actions
b.  similar to a.
    if there's a solution with length k, those actions are mutex free 
    let goal state be Sg, Sg ⊆ Sk

10.11
it describes a relaxed problem where multiple non-mutually-exclusive actions can take place at the same time

10.12
a.  sounds promising, would be applicable if the branching factor in forward direction is small
b.  same as above
c.  maintain a DAG-like structure during the search, where nodes are states and edges are actions
    let S be a state in the current DAG, if S satisfies all preconditions of action a
    add the action and its result to the DAG, add constraint Before(a', a) for each action a' incoming S
    it's still a partial-ordered planning algorithm, searching in a DAG instead of a tree
    depends on implementation details, this algorithm may be able to update the middle of a sequence of actions
    while forward / backward searching may only update end of an action sequence 

10.13
forward / backward searching maintains a set of linear sequences of plan and update the end of them

10.14
a.  when HaveArrow(t) is False, HaveArrow(t+1) <=> HaveArrow(t) & ~Shoot(t) is always False
b.  yes, by part a, an illegal action will not affect the state
    the plan, axioms and initial state still entails the goal
c.  no, Poss(a, s) will not be satisfied

10.15
a.  Plane(p) & At(p, from, s) & Airport(from) & Airport(to) => Poss(Fly(p, from, to), s)
b.  Poss(a, s) => (At(p, y, Result(a, s)) <=> a = Fly(p, x, y) & Airport(x) & Airport(y) & Plane(p) & At(p, x, s))
c.  Plane(p) & At(p, from, s) & Airport(from) & Airport(to) & ~Warped(p, s) => Poss(Teleport(p, from, to), s)
    Poss(a, s) =>   (At(p, y, Result(a, s)) <=> (a = Fly(p, x, y) | a = Teleport(p, x, y))) & 
                    (Warped(p, Result(a, s)) <=>  Warped(p, s) | a = Teleport(p, x, y))
d.  for all clauses ci in precondition of an action a:
        c1' & .. & cn' => Poss(a, s)
    where for ci = P(x1, .., xn), ci' = P(x1, .., xn, s)
    Poss(a, s) => for all fluents X(..):
        X(.., RESULT(a, s)) <=>
            (X(.., s) & a = { actions which will leave X alone }) |
            (a = { actions which will introduce X into the state })
            
10.16
a.  if g(t) is satisfiable, g(0) | .. | g(Tmax) is satisfiable, so it will find a solution
b.  it may find a plan where g(t) | g(t') can be proven satisfiable but not g(t) or g(t') alone
c.  would be non-trivial as WALKSAT doesn't has the idea of length or cost built in

11.1
divide the planning problem to two subplans:
    achievement: a plan to the goal
    maintenance: a plan maintains both the goal state and the quantity of resources
the cost of the whole plan is some linear combination of cost of the two plans, determined by the nature of the problem 
the objective is to find a plan with accpectable cost
for the chandelier suspending problem, the planner may find a solution which:
    achievement: throw the chandelier in the air
    maintenance: catch the chandelier and throw it again
but the cost will be far higher than properly suspending the chandelier from the ceiling
as the maintenance plan keeps both the state and resources, it can be repeated indifinitely

11.2
Action(Navigate(t, from, to),
    PRECOND:    Truck(t) & Place(from) & Place(to) & At(t, from),
    EFFECT:     ~At(t, from) & At(t, to))
Action(Deliver(t, p, from, to),
    PRECOND:    Truck(t) & Package(p) & Place(from) & Place(to) & At(t, from) & At(p, from),
    EFFECT:     ~At(t, from) & ~At(p, from) & At(t, to) & At(p, to))
Navigate and Deliver refines to a sequence of Forward(t), TurnLeft(t), TurnRight(t), ..
Each truck may only deliver one package at a time

11.3
fn refine(actions: &[Action]) -> (State, State) {
    let mut preconds = State::new();
    let mut effects = State::new();
    for action in actions {
        let net_preconds = action.preconds() - effects;
        /// union operations here removes a literal when both A and ~A presents
        preconds = preconds ∪ net_preconds;
        effects = effects ∪ actions.effects();
    }
    (preconds, effects)
}

11.4
as stated in the text, no certain conclusion can be drawn from these two facts

11.5
/// assuming the sequence is legal, precondition check can be skipped
/// otherwise both optimistic and pessimistic descriptions are undefined 
fn angelic(descs: &[Description]) -> Description {
    let mut opt = Optimistic::new();
    let mut pes = Pessimistic::new();
    for desc in descs {
        for (flag, term) in desc.opt() {
            /// optimistic descriptions are updated by bit or
            /// each term is tagged by a bitflag describing whether the term may be added, removed or both
            /// possible definition of Flag:
            /// type Flag = u8;
            /// const ADD = 0b01;
            /// const REMOVE = 0b10;
            opt[term] = opt[term] | flag;
        }
        for (sign, term) in desc.pes() {
            /// pessimistic descriptions are updated by overwriting 
            pes[term] = sign;
        }
    }
    Description::new(opt, pes)
}

11.6
if the nondeterminism is managed by believe states or conditional effects, these fields must be embedded into effects
in certain scenarios e.g. risk of ruin these quantities can be treated as intervals instead of set of possible outcomes
in general, if the resources are only constrained by lower and upper bounds, they can be treated as intervals 
otherwise the precise value of them matters and the planner must keep track of set of possible outcomes

11.7
a.  Action(Assign(a, b, va, vb),
        PRECOND:    Variable(a) & Variable(b) & ValueOf(a, va), ValueOf(b, vb),
        EFFECT:     ~ValueOf(a, va) & ValueOf(a, vb))
b.  Action(Let(a, v),
        PRECOND:    Symbol(a) & ~Variable(a), Value(v),
        EFFECT:     Variable(a) & ValueOf(a, v))
    Init(Variable(A) & Variable(B) & Value(VA) & Value(VB) & ValueOf(A, VA) & ValueOf(B, VB) & Symbol(C))
    Goal(ValueOf(A, VB) & ValueOf(B, VA))
    [
        Let(C, VA),
        Assign(A, B),
        Assign(B, C),
    ]

11.8
Action(Flip(v),
    PRECOND:    Variable(v),
    EFFECT:     when True(v): ~True(v), when ~True(v): True(v))
if True(A) ∈ S, Result(Flip(A), S) = S - True(A)
if True(A) ∉ S, Result(Flip(A), S) = S ∪ {True(A)}
1-CNF is maintained

11.9
Action(Move(b, x, y),
    PRECOND:    On(b, x) & Clear(b) & Clear(y),
    EFFECT:     when y = Table: On(b, Table) & Clear(x) & ~On(b, x)
                when y != Table: On(b, y) & Clear(x) & ~On(b, x) & ~Clear(y))

11.10
Action(Move(p),
    PRECOND:    Place(p),
    EFFECT:     At(p))
Action(Suck(p),
    PRECOND:    At(p),
    EFFECT:     Clean(p))
Init(At(Left | Right), Place(Left), Place(Right), Clean(Left)?, Clean(Right)?)
Goal(Clean(Left), Clean(Right))

11.11
Suck a column, look for dirty spots on the column, suck these spots until they are clean, move to the next column
a contigent plan with action monitoring

11.12
what medication problem?

12.1
constants:
    Marks: X, O
    Players: Po, Px
    Squares: Q11 .. Q33
    Initial state: S0
functions:
    Opponent(Px) = Po
    Opponent(Po) = Px
    MarkOf(Px) = X
    MarkOf(Po) = O
    TurnOf(S0) = Px
    TurnOf(Result(a, s)) = Opponent(TurnOf(s))
predicates:
    Win(p, s) <=> ∃q1,q2,q3 MarkAt(MarkOf(p), q1, s) & MarkAt(MarkOf(p), q2, s) & MarkAt(MarkOf(p), q3, s) &
        (
            (q1 = Q11 & q2 = Q12 & q3 = Q13) |
            ... /// winning positions
        )
    Empty(q, s) <=> ∀m ~MarkAt(m, q, s)
    Reachable(r, s) <=> r = s | r = Result(a, t) & Poss(a, t) & Reachable(t, s)
    Endgame(e, s) <=> Reachable(e, s) & ~∃a Poss(a, e)
    ForcedWin(p, s) <=> ∀e Endgame(e, s) => Win(p, e)
actions:
    PlaceMark(p, q)
situation calculus:
    ∀m,q ~MarkAt(m, q, S0)
    Square(q) & Player(p) & Empty(q, s) & ∀p ~Win(p, s) => Poss(PlaceMark(p, q), s)
    Poss(a, s) => MarkAt(m, q, Result(a, s)) <=> 
        (a = PlaceMark(p, q) & TurnOf(s) = p & MarkOf(p) = m) |
        MarkAt(m, q, s)
    
12.2
/// skipped

12.3
predicates:
    Above(t, b, s)
    Active(w, s)
    Legal(t, l, b, r) <=> t < b & l < r
    Focused(w, a) <=> a = Focus(w) | a = Create(w, ..) | a = Move(w, ..)
functions:
    /// order of N ∪ { None }: None is the new minimum
    Top(w, S0) = None
    Top(w, Result(Move(w, t, l, b, r), s)) = t
    Top(w, Result(Destory(w), s)) = None
    Top(w, Result(Create(w, t, l, b, r), s)) = t
    Top(w, Result(a, s)) = Top(w, s)
    Left(w, s) = .. // similar to Top
    Bottom(w, s) = .. 
    Right(w, s) = .. 
    State(w, S0) = Nonexist
    State(w, Result(Create(w, t, l, b, r), s)) = Displayed
    State(w, Result(Minimize(w), s)) = Minimized
    State(w, Result(Display(w), s)) = Displayed
    State(w, Result(a, s)) = State(s)
axioms:
    ∀x,y,z Above(x, y, s) & Above(y, z, s) => Above(x, z, s)
actions:
    State(w, s) = Nonexist & Legal(t, l, b, r) => Poss(Create(w, t, l, b, r))
    Exist(w, s) => Poss(Destory(w))
    State(w, s) = Displayed & Legal(t, l, b, r) => Poss(Move(w, t, l, b, r))
    State(w, s) = Displayed => Poss(Minimize(w))
    State(w, s) = Minimized => Poss(Display(w))
    Exist(w, s) => Poss(Bring(w))
    Exist(w, s) => Focus(w)
situation calculus:
    Poss(a, s) =>
        (Above(w, m, Result(a, s)) <=>
            /// new window is on top
            (Focused(w, a) & m != w & Exist(m, Result(a, s))) |
            /// inherit from the last situation
            (w != m & ~Focused(w, a) & ~Focused(m, a) & Exist(w, Result(a, s)) & Exist(m, Result(a, s)) & Above(w, m, s)) |
            /// transitivity
            (∃p Above(w, p, Result(a, s)) & Above(p, m, Result(a, s)))) & 
        (Active(w, Result(a, s)) <=> 
            Focused(w, a) |
            ((∀w ~Focused(w, a)) & Active(w, s)))

12.4
a.  Above(W2, W1, S0) & Left(W1, S0) < Left(W2, S0) & Right(W1, S0) > Right(W2, S0)
b.  State(w, s) = Displayed => Top(w, s) < Bottom(w, s)
c.  State(w, Result(Create(w, ..), s)) = Displayed
d.  State(w, s) = Displayed => Poss(Minimize(w))

12.5 - 12.6
/// skipped, the result would be either overly complicated or overly far-fetched

12.7
a.  ∀w w ∈ Water => FreezingPoint(w, KPa(100)) = Celsius(0) & BoilingPoint(w, KPa(100)) = Celsius(100)
    Liquid(w, t, p) <=> FreezingPoint(w, p) < t < BoilingPoint(w, p)
b.  ∀w w ∈ Water => BoilingPoint(w, KPa(100)) = Celsius(100)
c.  ∀w w ∈ Water & In(w, JohnsWaterBottle) => Frozen(w)
d.  Perrier ⊆ Water
e.  ∃w w ∈ Perrier & In(w, JohnsWaterBottle)
f.  ∀w (∃t,p Liquid(w, t, p)) => FreezingPoint(w, p) != None
g.  ∀w,a w ∈ Water & a ∈ Alcohol & Volumn(w) = Liter(1) & Volumn(a) = Liter(1) => Weight(w) > Weight(a)

12.8
a.  ExhaustivePartDecomposition(s, c) <=> (∀i PartOf(i, c) <=> ∃p PartOf(i, p) & p ∈ s)
b.  PartPartition(s, c) <=> ExhaustivePartDecomposition(s, c) & PairwiseDisjoint(s)
c.  PairwiseDisjoint(s) <=> (∀c1,c2,p c1 != c2 => (PartOf(p, c1) => ~PartOf(p, c2)))

12.9
1.  it's not type safe
    all unit functions map to real numbers, so one can compare Pound(Weight(A)) and Kilogram(Weight(A))
    or even worse, Pound(Weight(A)) and Inch(Length(A))
2.  there's no way to easily name an abstract quantity with unit as $(50)
    only as $(v) = 50 for some value v
3.  conversion between units looks like:
        2.54 * Centimeter(l) = Inch(l)

12.10
/// plural and singular forms of a name describe the same category
∀n,c Name(n, c) => Name(Plural(n), c) & Name(Singular(n), c)
/// a name followed by its generalization forms a name of the same category
/// yet it's hard to specify the definition of proper generalization
/// "Laptop PC" is a proper name for Laptop, but "Laptop Thing" is not
∀n,g,c,p Name(n, c) & Name(g, p) & ProperGeneralization(c, p) => Name(n + g, c)
/// and rule, would be much more complicated as "Laptop and Laptop" is not a proper category name
/// in reality this rule cannot be chained to arbitrary length
/// a fifty-word string consisted of synonyms of laptop is not a proper name
∀n1,n2,c Name(n1, c) & Name(n2, c) => Name(n1 + " and " + n2, c)

12.11
/// thanks solutions manual
predicates:
    Inbound(p) // position p is in the bound
    InFrontOf(p, d, t) // t is in front of p by direction d
functions:
    Left(North) = West // and other four directions
    Right(North) = East // and other four directions
    Front(p, d) = .. // position in front of direction d from position p, may be out of bound
Initiates(e, HaveArrow(a), t) <=> e = Start
Terminates(e, HaveArrow(a), t) <=> e = Shoot(a)
Initiates(Start, Facing(a, North), 0)
/// the agent may only facing one direction at a time
T(Facing(a, d), t) => ∀d' T(Facing(a, d'), t) => d = d'
T(TurnLeft(a), i) <=> 
    ∃h,d Meets(h, i) & T(Facing(a, d), h) => Clipped(Facing(a, d), i) & Restored(Facing(a, Left(d)), i)
T(TurnRight(a), i) <=> 
    ∃h,d Meets(h, i) & T(Facing(a, d), h) => Clipped(Facing(a, d), i) & Restored(Facing(a, Right(d)), i)
T(Forward(a), i) <=>
    ∃h,p Meets(h, i) & T(At(a, p), h) & T(Facing(a, d), h) & Inbound(Front(p, d)) => 
    Clipped(At(a, p), i) & Restored(At(a, Front(p, d)), i)
T(Grab(a), i) <=>
    ∃h,p Meets(h, i) & T(At(a, p), h) & T(Gold(p), h) => 
    Clipped(Gold(p), i) & Restored(HaveGold(a), i)
T(Shoot(a), i) <=>
    ∃h,p,t Meets(h, i) & T(At(a, p), h) & T(HaveArrow(a), t) & InFrontOf(p, d, t) & Wumpus(t) =>
    Clipped(WumpusAlive, i) & Restored(WumpusDead, i)
T(Climb(a), i) <=>
    ∃h Meets(h, i) & T(At(a, S11), h) => Clipped(At(a, S11), i) & Restored(OutOfCave(a), i)

12.12
During(IK, LK)
During(PK, LK)
During(LK, LJ)
Meet(LK, PJ)
Overlap(LK, LO)
Before(IK, PK)
During(IK, LJ)
Before(IK, PJ)
Before(IK, LO)
During(PK, LJ)
Meet(PK, PJ)
Overlap(PK, LO)
During(PJ, LJ)
Overlap(LJ, LO)
During(PJ, LO)

12.13
Allen, J. and Ferguson, G. (1994): Actions and Events in Interval Temporal Logic
http://www.cs.rochester.edu/u/james/Papers/AllenFerguson-events-actions.pdf
Pelavin, R. (1992): Planning with simultaneous actions and external events, in Reasoning About Plans
/// available on libgen
an example of combinatoric axiom explosion:
    say a set of actions mutually interfere
    if encoded naively, precondition of each action in the set must mension all the other actions
    resulting in axioms of length Ω(n^2)
a solution is described in Pelavin (1992)

12.14
$(n, t), maps integer n and a time instance t to the domain of abstract value 

12.15
T(Fixed(Location(x)), (t1, t2)) <=> ∃p∀t t1 <= t <= t2 => Location(x) = p

12.16
T(Trade(a, b, p, q), i) <=> 
    ∃h Meet(h, i) & T(Own(a, p), h) & T(Own(b, q), h) =>
    Clipped(Own(a, p), i) & Clipped(Own(b, q), h) & Restore(Own(a, q), i) & Restore(Own(b, p), i)
Buy(a, b, p, m) = Trade(a, b, p, $(m))

12.17
/// skipped

12.18
functions:
    King(p): number of king in a pair p
    Ace(p): number of ace in a pair p
/// one may only have A-A, A-K or K-K pairs 
1.  ∀a Ka(Has(a, A-A) | Has(a, A-K) | Has(a, K-K))
/// 4 kings and 4 aces in total
2.  ∀a,p1,p2,p3 Ka(Has(A, p1) & Has(B, p2) & Has(C, p3) =>
    King(p1) + King(p2) + King(p3) = 4 & Ace(p1) + Ace(p2) + Ace(p3) = 4)
/// Alice knows the pair of Bob and Carlos
3.  ∀p1,p2 Has(B, p1) & Has(C, p2) => Ka(Has(B, p1) & Has(C, p2))
/// similarly Bob and Carlos knows the pairs of others
a.  A-K
b.  ~∃p Ka(Has(A, p)) & Has(B, K-K)
    if Has(C, K-K)
    =>  Ka(Has(C, K-K) & Has(B, K-K)) // rule 3
        Ka(Has(A, A-A)) // rule 2 
    hence ~Has(C, K-K)
    similarly, ~∃p Ka(Has(B, p)) => ~Has(C, A-A)
    therefore Has(C, A-K) // rule 1
c.  ~Has(C, K-K) // rule 2
    if Has(C, A-A)
        Kb(~∃p Ka(Has(a, p)) & Ka(Has(C, A-A)))
        Kb(~Has(B, A-A))
        Kb(~∃p Kc(Has(a, p)) & Ka(Has(A, K-K)))
        Kb(~Has(B, K-K))
        Kb(Has(B, A-K))
    therefore Has(C, A-K)
d.  if Has(C, A-A), this game is the same to Game 2, Alice should be able to answer at the beginning of the second round
    if Has(C, K-K), this game is symmetric to Game 2, Alice should be able to answer
    therefore Has(C, A-K)
e.  lemma: if a game has a winner in some order (a permutation of [A, B, C]), it has a winner in any order
        let the sequence of answers be a = [A, B, C, A, B, C, ..]
        it's always possible to insert answers to the sequence, making it into another order
        e.g. [C, B, A] can be achieved by insert C between (A, B), A between (B, C), ..
        the extended sequence should still have a winner since:
            1.  inserting answers only add knowledge to each player, never removes them
            2.  at the end of the sequence, the player in turn has enough knowledge to answer the question
    part b proved the existence of a winner of {A-A, A-A, K-K} and {A-A, K-K, A-K}
    part c proved the existence of a winner of {A-K, A-K, K-K}
    part c proved the existence of a winner of {A-K, A-K, A-K}
    along with symmetric worlds (swap roles of A and K) and lemma above, these are all the possible worlds

12.19
a.  quite reasonable whenever with limited number of states (e.g. 52 cards in bridge)
b.  just as the text described, if logical omniscience is reasonable,
    these moves (complicating the situation) would never be advantageous
c.  quite reasonable: knowledge base is limited by cost & moderate in size and complexity
d.  if an agent can derive discrete logarithm from axioms immediately
    DH key exchange protocol or RSA makes no sense

12.20
∀m 
    (∃s1,s2,s3 s1 != s2 & s1 != s3 & s2 != s3 & Son(s1, m) & Son(s2, m) & Son(s3, m)) &
    (∃d1,d2∀d3 Daughter(d3, m) => d3 = d1 | d3 = d2) &
    (∀s Sone(s, m) => Unemployed(s) & Married(s) & ∀p Spouse(s, p) => Doctor(p)) &
    (∀d Daughter(d, m) => Professor(d) & (Department(d, Physics) | Department(Math, d)))
it's rather inconvenient to express AtLeast in first order logic

12.21
a.  ∀x Brand(x, Dodge) & Year(x, 1973) & Model(x, Vans) => Worth(x, 575) 
    ∀x Brand(x, Ford) & Year(x, 2010) & Model(x, F150) => Worth(x, 16500)
    ∀x Brand(x, Honda) & Year(x, 2017) & Model(x, Civic) => Worth(x, 13999)
    in worst case all such rules will be unified before the result is emitted
b.  backward chaining will have linear complexity
    while complexity of inheritance depends on the depth of inheritance heirarchy and usually is O(1)
c.  if predicates are properly indexed by all combination of predicates and arguments
    given the three facts about the car (Brand, Year and Model), the price can be queried in nearly O(1)
d.  if price of the car depends on non-uniform aspects of the car
    (e.g. not only Brand, Year and Model but also tens of exceptional conditions)
    there may not be an efficient indexing scheme 
e.  assuming the 3-tuple Year, Brand and Model completely defines Worth
    a hash table keyed by all the three predicates can give Worth of a car in average O(1) time

12.22
Subset(A, B) does not mean ∀x,y x ∈ A & y ∈ B => Subset(x, y)
x and y may not even be sets

12.23 - 12.25
/// skipped


13.1
P(a | b && a)    = P(a && b && a) / P(b && a)
                = P(a && b) / P(a && b)
                = 1

13.2
given by equation 13.1
all possible values of a random variable forms the sample space of possible worlds

13.3
a.  P(a | b && c) = P(b | a && c)
    =>  P(a && b && c) / P(b && c) = P(a && b && c) / P(a && c)
    =>  P(b && c) = P(a && c)
    =>  P(b && c) / P(c) = P(a && c) / P(c)
    =>  P(b | c) = P(a | c)
b.  a = "It's rainy in Bucharest today"
    b = "Jack has toothache"
    c = "Jack has cavity"
c.  let the domain be t ∈ N * N
    a = "t = (x, y) && x + y is even"
    b = "t = (0, _)"
    c = "t = (_, 0)"
    P(a | b) = 1/2
    P(a | c) = 1/2
    P(a | b && c) = 1

13.4
P(a || b) = P(a) + P(b) - P(a && b)
0.5 = 0.7 - P(a && b), P(a && b) = 0.2
if P(a || b) = 0.7, P(a && b) = 0
how rational it is depends on the believe of the agent
both 0.5 and 0.7 may be justified by the knowledge base

13.5
a.  if s1 and s2 are two distinct atomic events 
    there exists at least one boolean variable xi that xi ∈ s1 and ~xi ∈ s2
    s1 || s2 by resolution derives {}, hence s1 || s2 is unsatisfiable
b.  for an arbitrary assignment, there is a corresponding proposition in the disjunction
    therefore the disjunction is true for any possible model / assignment
    the disjunction is true in all models => true |= disjunction
    as p |= true for all proposition p, true <=> disjunction
c.  same to b but in a subset of all possible worlds in which the proposition is true

13.6
P(a && b) + P(a && ~b) + P(~a && b) + P(~a && ~b) = 1 // 13.1
P(a || b)   = 1 - P(~(a || b)) // 13.2
            = 1 - P(~a && ~b)
            = P(a && b) + P(a && ~b) + P(~a && b)

13.7
a.  C(52, 5) = 2_598_960
b.  1 / 2_598_960
c.  royal stright flush: 
        4 atomic events
        4 / C(52, 5)
    four of a kind:
        13 * (52 - 4) = 624 atomic events
        624 / C(52, 5)

13.8
a.  0.108 + 0.012 + 0.016 + 0.064 = 0.2
b.  0.108 + 0.012 + 0.072 + 0.008 = 0.2
c.  0.108 + 0.012 = 0.12
d.  0.108 + 0.012 + 0.072 = 0.192

13.9
winning chance of player E:
    E ~ NB(7 - 2, 0.5), E >= 3
    P(E >= 3) = 1 - Σ(k ∈ {0,1,2})P(E = k) = 0.7734375
winning chance of player O:
    1 - P(E <= 7) = 0.2265625
the pot of money should be divided by their winning chance

13.10
a.  0.25^3 * (20 + 15 + 5 + 3) + // three in a row
    0.25^2 * (1 - 0.25) * 2 + // two cherries
    0.25 * ((1 - 0.25)^2 + (1 - 0.25) * 0.25)  // one cherry
    = 0.953125
b.  3 * 0.25^3 + 0.25^2 * (1 - 0.25)
    = 0.109375
c.  ./AI/uncertainty
    median <= 21
    mean = 216

13.11
./AI/uncertainty
(1 - ε)^(n + 1) + ε(n+1)(1 - ε)^n >= 1 - δ
n = 147

13.12
P(a | b) = P(a)
=>  P(a & b) / P(b) = P(a)
=>  P(a & b) / P(a) = P(b)
=>  P(b | a) = P(b)
P(b | a) = P(b)
=>  P(a & b) / P(a) = P(b)
    P(a & b) = P(a)P(b)
P(a & b) = P(a)P(b)
if P(b) != 0
=>  P(a & b) / P(b) = P(a)
=>  P(a | b) = P(a)
if P(b) = 0
P(a & b) = P(a)P(b) = 0
P(a | b) is not well defined

13.13
P(a)    = P(a & (b || ~b))
        = P((a & b) || (a & ~b))
        = P(a & b) + P(a & ~b) // exclusive
        = P(a | b)P(b) + P(a | ~b)P(b)
P(a_pos)    = P(a_pos | virus)P(virus) + P(a_pos | ~virus)P(~virus)
            = 0.95 * 0.01 + 0.1 * 0.99
            = 0.1085
P(virus | a_pos)    = P(a_pos | virus)P(virus) / P(a_pos)
                    = 0.95 * 0.01 / 0.1085
                    = 0.0875576
P(b_pos)    = P(b_pos | virus)P(virus) + P(a_pos | ~virus)P(~virus)
            = 0.9 * 0.01 + 0.05 * 0.99
            = 0.0585
P(virus | b_pos)    = P(b_pos | virus)P(virus) / P(b_pos)
                    = 0.1538462
B is more indicative

13.14
with x fixed, P(Toss1 = Head & Toss2 = Head | x) = x^2 = P(Toss1 = Head | x)Poss(Toss2 = Head | x)
when x is unknown, the posterior distribution of Toss is dependent on obvervations so far
the next toss is dependent on all tosses before

13.15
P(disease | pos)    = P(pos | disease)P(disease) / P(pos)
                    = P(pos | disease)P(disease) / (P(pos | disease)P(disease) + P(pos | ~disease)P(~disease))
                    = αP(disease) / (αP(disease) + (1 - α)(1 - P(disease)))
                    = α / (α + (1 - α)(1 / P(disease) - 1))
hence P(disease | pos) is monotonically decreasing function in P(disease)

13.16
/// it's unclear how to translate rules from and to bold P notations
/// the operations on bold P notations (divide, multiply, etc.) are not defined yet
/// P(X) op P(Y) applies op to the cartisan product of two vector, indexed by particular values of X and Y
/// e.g. P(X)P(Y), P(X)/P(Y), P(X) + P(Y)
/// fixing one variable slices a hyperplane from the value space
/// most arithmetic rules should hold in this domin, to name a few:
///     P(X)P(Y) / P(Y) = P(X)
///     (P(X)P(Y))P(Z) = P(X)(P(Y)P(Z))
let E be some random variable, e ∈ E
a.  P(X, Y | E) = P(X, Y, E) / P(E)
                = (P(X, Y, E) / P(Y, E)) * (P(Y, E) / P(E))
                = P(X | Y, E)P(Y | E)
    fixing E = e, P(X, Y | e) = P(X | Y, e)P(Y | e)
b.  P(Y | X, e) = P(X, Y | e) / P(X | e)
                = P(X | Y, e)P(Y | e) / P(X | e)

13.17
errata:
    P(B | X, Z) = P(Y | Z) => P(Y | X, Z) = P(Y | Z)
P(X, Y | Z) = P(X | Z)P(Y | Z)
=>  P(X | Y, Z)P(Y | Z) = P(X | Z)P(Y | Z)
=>  P(X | Y, Z) = P(X | Z)
P(X | Y, Z) = P(X | Z)
=>  P(X, Y | Z) / P(Y | Z) = P(X | Z)
=>  P(X, Y | Z) / P(X | Z) = P(Y | Z)
=>  P(Y | X, Z) = P(Y | Z)
P(Y | X, Z) = P(Y | Z)
=>  P(X, Y | Z) = P(X | Z)P(Y | Z)

13.18
a.  P(head) = P(head | fake)P(fake) + P(head | ~fake)P(~fake)
            = 1/n + 1/2 * (n-1)/n
            = (n+1)/2n
    P(fake | head)  = P(head | fake)P(fake) / P(head)
                    = 1/n / ((n+1)/2n)
                    = 2 / (n + 1)
b.  P(k_head)   = P(k_head | fake)P(fake) + P(k_head | ~fake)P(~fake)
                = 1/n + (1/2)^k(n-1)/n
                = ((n-1)(1/2)^k + 1) / n
    P(fake | k_head)    = P(k_head | fake)P(fake) / P(k_head)
                        = 1 / ((n-1)(1/2)^k + 1)
c.  error = (~k_head & fake) || (k_head & ~fake)
    P(error)    = P((~k_head & fake) || (k_head & ~fake))
                = P(~k_head & fake) + P(k_head & ~fake) // exclusive
                = P(~k_head | fake)P(fake) + P(k_head | ~fake)P(~fake)
                = 0 + (1/2)^k * (n-1)/n
                = 1/2^k * (n-1) / n

13.19
P(s | ~m)P(~m) + P(s | m)P(m) = P(s)
P(s | ~m)   = (0.01 - 0.7 * 0.00002) / 0.99998
            ~= 0.001
P(m | s) + P(~m | s) = 1
P(~m | s)   = P(s | ~m)P(~m) / P(s)
            = p * (49999 / 50000) / P(s)
            // α = 1/P(s)
            = α(0.99998 * 0.001)
            ~= α(0.0001)
P(m | s)    = P(s | m)P(m) / P(s)
            = α(0.000014)
0.000114α = 1
α ~= 8771.9298
P(~m | s) = 0.8772
P(m | s) = 0.1228

13.20
name the probability of atomic events by binary numbers 
i.e. 010 = P(~x & y & ~z)
    a = 000
    b = 001
    c = 010
    d = 011
    e = 100
    f = 101
    g = 110
    h = 111
P(X, Y | Z) = P(X | Z)P(Y | Z)
P(X, Y, Z) = P(X, Z)P(Y, Z) / P(Z)
a = (a + c)(a + e) / (a + c + e + g)
    a^2 + ac + ae + ag = a^2 + ae + ac + ce
    ag = ce
b = (b + d)(b + f) / (b + d + f + h)
    b^2 + bd + bf + bh = b^2 + bf + bd + df
    bh = df
all other equations are redundant by symmetry

13.21
a.  P(blue | looks_blue)    = P(looks_blue | blue)P(blue) / P(looks_blue)
                            = 0.75P(blue) / P(looks_blue)
                            = 0.75P(blue) / (P(looks_blue | blue)P(blue) + P(looks_blue | ~blue)P(blue))
                            = 0.75P(blue) / (0.75P(blue) + 0.25(1 - P(blue)))
                            = 0.75 / (0.75 + 0.25 / P(blue) - 0.25)
    the result is dependent on P(blue)
b.  P(blue) = 0.1
    P(blue | looks_blue) = 0.25
    P(~blue | looks_blue) = 0.75
    the taxi is most likely to be green

13.22
a.  from the trainning set, the prior probabilities
        P(Category)
        P(Wi | Category) and P(Wi) // Wi is a binary random variable for each word wi in documentations
        P(W1, .., Wn | Category) // from P(Wi | Category) since Wi are mutually independent
        P(W1, .., Wn, Category) // from above and P(Category)
    can be computed
b.  if W1 and W2 are independent
    P(Category | W1, .., Wn) can be computed from 
        P(Category, W1, .., Wn) / P(W1, .., Wn)
        = P(Category, W1, .., Wn) / ΠP(Wi)
c.  in most case the assumption is not reasonable
    e.g. there are clear dependency among the words Bayes and models, rules or probability

13.23
Pij and Pkl are no longer independent, e.g. if N = 1, P(Pij | Pkl = true) = 0
P(P11, .., P44) now have C(16, 3) atomic events with probability 1/C(N, N/5) each
other 2^16 - C(16, 3) atomic events have probability 0
there are five cases of frontier consistent with known & b:
    case a: all P13, P22, P31 = true, 1 atomic event
    case b: P13 = P31 = true, P22 = false, C(16 - 6, 1) = 10 atomic events
    case c: P22 = true, P13 = P31 = false, C(16 - 6, 2) = 45 atomic events
    case d: P13 = P22 = true, P31 = false, C(16 - 6, 1) = 10 atomic events
    case e: P31 = P22 = true, P13 = false, C(16 - 6, 1) = 10 atomic events
76 atomic events in total
P(P13 = true) = P(a | kb) + P(b | kb) + P(d | kb) = (1 + 10 + 10) / 56 = 21/76
P(P22 = true) = 1 - P(b | kb) = 66 / 76

/// to avoid ambiguity, from now on P(A) indicates bold P notation, p(a) indicates probability of event a
13.24
P(P13 | known, b) = αP(P13)Σp(b | known, P13, frontier)p(frontier)
for P13 = true, there are three cases of frontier with p(b | known, frontier, P13) = 1:
    P22 = P31 = true, with p = 0.00001
    P22 = true, P31 = false, with p = 0.0099
    P22 = false, P31 = true, with p = 0.0099
    0.0199 in total
for P13 = false, there are two cases of frontier with p(b | known, frontier, P13) = 1:
    P22 = true, P31 = false, p = 0.0099
    P22 = P31 = true, p = 0.0001
    0.01 in total
P(P13) = α<0.01 * 0.0199, 0.99 * 0.01> = α<0.000199, 0.0099> = <0.0197, 0.9803>
for P22 = true, any frontier is consistent with known & b, Σp(b | known, P13, frontier)p(frontier) = 1
for P22 = false, it must have P13 = P31 = true, p = 0.01^2
P(P22) = α<0.01, 0.99 * 0.01^2> = α<0.01, 0.000099> = <0.99, 0.01>
P22 is true with almost certainty, while a pure logical agent will try P13, P22 or P31 for random

13.25
/// skipped

14.1
a.  ./AI/bayes
b.  [0.10526315789473688, 0.4736842105263158, 0.42105263157894735]

14.2
a.  P(z | y)    = P(y, z) / P(y)
                = Σx P(x, y, z) / Σx,z P(x, y, z)
b.  Σx P(x, y, z) = Σx θ(z | y)θ(y | x)θ(x)
    Σx,z P(x, y, z) = Σx,z θ(z | y)θ(y | x)θ(x)
                    = Σx(θ(y | x)θ(x)Σz(θ(z | y)))
                    = Σx θ(y | x)θ(x)
c.  (θ(z | y)θ(y | x)θ(x) + θ(z | y)θ(y | ~x)θ(~x)) / 
    (θ(y | x)θ(x) + θ(y | ~x)θ(~x))
    = θ(z | y)
d.  lemma: 
        let A be the set of ancestors of Xi, including Xi
        p(A = a) 
        = Σ(U - A) Πθ(x | Parents(X))
        let B = U - A, as long as B != ∅,
        there must be an Xi ∈ B where Xi is not a parent of any other variables in B (otherwise the network is cyclic)
        = Σ(B - {Xi}) Π(U - {Xi})(x | Parents(X)) Σxi θ(xi | Parents(Xi))
        = Σ(B - {Xi}) Π(U - {Xi})(x | Parents(X)) // sum to 1
        repeat until B = ∅, U - B = A
        p(A = a) = Π(x ∈ A)θ(x | Parents(X))
    for all Xi = xi, Parents(Xi) = parents,
    let A be the set of ancestors of Xi
    let other be A - {xi} - parents
    p(xi | parents) = p(xi, parents) / p(parents)
    = p(xi, parents | a)p(a) / p(parents | a)p(a)
    = Σother p(xi, parents, other) / Σxi,other p(xi, parents, other)
    = Σother Π(x ∈ a)θ(x | Parents(X)) / Σxi,other Π(x ∈ A)θ(x | Parents(X))
    by definition of A, there's no Y ∈ A where X ∈ Parents(Y)
    Σxi,other Π(x ∈ a)θ(x | Parents(X))
    = Σother Π(x ∈ a - {xi})θ(x | Parents(X)) Σxi θ(xi | Parents(Xi))
    = Σother Π(x ∈ a - {xi})θ(x | Parents(X))
    Σother Π(x ∈ a)θ(x | Parents(X)) / Σxi,other Π(x ∈ a)θ(x | Parents(X))
    = θ(xi | parents)Σother Π(x ∈ a - {xi})θ(x | Parents(X)) / Σother Π(x ∈ a - {xi})θ(x | Parents(X))
    = θ(xi | parents)
    since above holds for arbitrary value of Xi and Parents(Xi),
    P(Xi | Parents(Xi)) = θ(Xi | Parents(Xi))

14.3
a.  let PX = Parents(X), PY = Parents(Y) before arc reversal
    after arc reversal:
        Parents(X) = PX ∪ (PY - {X}) ∪ {Y}
        Parents(Y) = PX ∪ (PY - {X})
        size(X) = k^(|Parents(X)| + 1)
        size(Y) = k^(|Parents(Y)| + 1)
        if PY - {X} ⊂ PX:
            |Parents(X)| = |PX ∪ {Y}| > |PX|
            |Parents(Y)| = |PX| >= |PY|
        if PX ⊂ PY - {X}:
            |Parents(X)| = |PY| 
            |Parents(Y)| = |PY| - 1 > |PX|
        if PX = PY - {X}:
            |Parents(X)| = |PX| + 1 = |PY|
            |Parents(Y)| = |PY| - 1 = |PX|
            size swapped
        otherwise, PX ^ (PY - {X}) != ∅:
            |Parents(X)| > |PX|
            |Parents(Y)| >= |PY|
    total size of CPTs will not decrease
b.  by part a, only when Parents(X) = Parents(Y) - {X}
c.  Pnew(Y | U, V, W)Pnew(X | U, V, W, Y)
    = (Σx P(Y | V, W, x)P(x | U, V)) P(Y | X, V, W)P(X | U, V) / P(Y | U, V, W)
    = (Σx P(Y | V, W, x)P(x | U, V) / P(Y | U, V, W)) P(Y | X, V, W)P(X | U, V)
    // thanks solutions manual
    // given Parents(Y) = V ∪ W ∪ {X}, Y is conditionally independent of non-descendents U
    = (Σx P(Y | U, V, W, x)P(x | U, V) / P(Y | U, V, W)) P(Y | X, V, W)P(X | U, V)
    = (Σx P(Y, U, V, W, x)P(U, V, x)P(U, V, W) / P(Y, U, V, W)P(U, V, W, x)P(U, V)) P(Y | X, V, W)P(X | U, V)
    = (Σx P(x | Y, U, V, W)P(x | U, V) / P(x | U, V, W)) P(Y | X, V, W)P(X | U, V)
    // given Parents(X) = U ∪ V, X is conditionally independent of non-descendents W
    = (Σx P(x | Y, U, V, W)) P(Y | X, V, W)P(X | U, V)
    = P(Y | Parents(Y))P(X | Parents(X))
    as Parents(Xi) for Xi != X or Y will not change by arc reversal of (X, Y)
    P(Xi .. Xn) = ΠP(Xi | Parents(Xi)) is the same before and after 

14.4
a.  by lemma of 14.2.d, P(Burglary) and P(Earthquake) are a sequence of constant, 
    independent to all other random variables
b.  no
    ./AI/bayes
    // probability is listed in order [False, True] 
    P(Earthquake | alarm, burglary) = [0.9979787664092253, 0.0020212335907746643]
    P(Earthquake | alarm, ~burglary) = [0.632446134347275, 0.36755386565272496]
    P(Burglary | alarm, earthquake) = [0.996731576412303, 0.0032684235876969656]
    P(Burglary | alarm, ~earthquake) = [0.515214027849407, 0.48478597215059305]

14.5
a.  let 
        Other = U - MB(Y) - {Y}
        C = children of Y
        PP = Parents(C) - {Y}
    by conditional independency between P(Other | MB(Y)) and P(Y | MB(Y)),
        P(Other | Y, MB(Y)) = P(Other | MB(Y))
    where for any specific MB(Y) = mb(Y)
        P(Other | mb(Y))
        = P(Other, mb(Y)) / P(mb(Y))
        = αP(Other, mb(Y))
        = αΣy P(Other, mb(Y), y)
        = αΣy Π(X ∈ Other)θ(X | Parents(X)) Π(x ∈ mb(Y) ∪ y)θ(x | Parents(X))
        = αΠ(X ∈ Other ∪ PP)θ(X | Parents(X)) Σy Π(x ∈ mb(Y) ∪ {Y} - PP)θ(x | Parents(X))
        // the summation results in a constant
        = α'Π(X ∈ Other ∪ PP)θ(X | Parents(X))
    independent of the CPT of Y
b.  rejection sampling:
        works fine as P(Other, Y | mb(Y)) = P(Other | mb(Y))P(Y | mb(Y)) = αP(Other | mb(Y))
        sampling Other, Y or Other alone won't change the normalized distribution of Other
    likelihood weighting:
        again P(Other, Y | mb(Y)) = αP(Other | mb(Y))
        the additional weight factor P(Y | mb(Y)) is will be normalized out anyway

14.6
a.  figure (c)
b.  figure (a), handedness is independent to everything except gene of the same person
c.  figure (a)
d.  when Gmother = Gfather = l, Gchild may only be r through a mutation with p = m
    when Gmother = l, Gfather = r, Gchild = l either:
        inherit from mother with p = 1/2, avoid a mutation with p = (1 - m), p = 1/2(1 - m)
        inherit from father with p = 1/2, mutation with p = m, 1/2m
        1/2(1 - m + m) = 1/2 in total
    other cases are symmetric
    P(Gchild | Gmother = l, Gfather = l) = <1 - m, m>
    P(Gchild | Gmother = l, Gfather = r) = <1/2, 1/2>
    P(Gchild | Gmother = r, Gfather = l) = <1/2, 1/2>
    P(Gchild | Gmother = r, Gfather = r) = <m, 1 - m>
e.  P(Gchild)   = ΣGmother,Gfather P(Gchild | Gmother, Gfather)P(Gmother)P(Gfather)
                = q^2<1 - m, m> + 
                    q(1-q)<1/2, 1/2> +
                    q(1-q)<1/2, 1/2> +
                    (1-q)^2<m, 1 - m>
                = <(1-m)q^2 + q(1-q) + m(1-q)^2, mq^2 + q(1-q) + (1-m)(1-q)^2>
                = <q + m - 2mq, 1 - q - m + 2mq>
f.  when m is negligible, q = q + m - 2mq, m = 2mq, q = 1/2
    only 10% of global population is left-handed, the hypothesis must be wrong

14.7
let 
    Other = U - MB(Y) - {Y}
    C = children of Y
    PP = Parents(C) - {Y}
P(Other | xi, mb(Xi))
= P(Other, xi, mb(Xi)) / P(xi, mb(Xi))
= αΠP(X | Parents(X))
= αΠ(X ∈ Other ∪ PP)P(X | Parents(X)) Π(X ∈ {Y} ∪ C)P(x | parents(X))
= α'Π(X ∈ Other ∪ PP)P(X | Parents(X))
P(Other | mb(Xi))
= P(Other, mb(Xi)) / P(mb(Xi))
= βΣxi ΠP(X | Parents(Xi))
= βΠ(X ∈ Other ∪ PP)P(X | Parents(X)) Σxi ΠP(X ∈ {Y} ∪ C)(x | parents(Xi))
= β'Π(X ∈ Other ∪ PP)P(X | Parents(X))
the two expressions are equivalent after normalization
hence P(Other | mb(Xi)) = P(Other | xi, mb(Xi)), Other and Xi are conditionally independent

14.8
a.  IcyWeather: a parent of Start with no parents
    StarterMotor: a parent of Start, with a sole parent Battery
b.  no idea
c.  2^8 - 1 = 255
d.  a boolean variable with k parents contains 2^k independent values
    3(2^0) + 4(2^1) + 2^4 = 27
e.  for a noisy-AND node, P(xi | Parents(Xi)) will be almost zero unless Xj = true for all Xj ∈ Parents(Xj)
    each parent has an associated probability qj
        qj = p(xi | ~xj, parents(Xi) - {xj})
    which indicates the probability that Xi holds eventhough Xj failed, then
        P(xi | Parents(Xi)) = Π(xj = F)qi
    by swapping the role of T and F, the same model can be expressed with a noisy-Or node

14.9
let 1/(2π)^(1/2) = α
a.  p(x1) = α/σ1 e^(-(x1 - μ1)^2 / 2σ1^2)
    p(x2 | x1) = α/σ2 e^(-(x2 - (a + bx1))^2 / 2σ2^2)
    p(x1, x2) 
    = p(x2 | x1)p(x1)
    = α^2 / σ1σ2 e^(-(x1 - μ1)^2 / 2σ1^2 - (x2 - (ax1 + b))^2 / 2σ2^2))
    = α^2 / σ1σ2 e^(-(x1 - μ1)^2 / 2σ1^2 - (x2 - (a + bx1)^2 / 2σ2^2))
    = α^2 / σ1σ2 e^(-(σ2^2(x1 - μ1)^2 + σ1^2(x2 - (ax1 + b))^2) / 2σ1^2σ2^2)
    /// skipped, derive Σ from the expression above
b.  p(x1, .., xn+1) = p(xn+1 | x1, .., xn)p(x1, .., xn)
    the induction holds if product of a linear Gaussian and a multivariate Gaussian is again a multivariate Gaussian 
    /// details skipped

14.10
a.  by a linear combination of these threshold functions
b.  sorted:
        from the lowest possible value x0
        define a threshold function to assign probability for X = x0 and X > x0
        when X > x0 use a second threshold function to assign probability for X = x1 and X > x1
        repeat until the greatest value
    unsorted:
        no obvious way to assign probabilities to each possible value 

14.11
a.  T -> Fg
    Fg, T -> G
    Fa, G -> A
b.  not a polytree, two different paths T -> G
c.  order of probability: <low, high>
    P(G | T = high, Fg = F) = <1 - x, x>
    P(G | T = normal, Fg = F) = <x, 1 - x>
    P(G | T = high, Fg = T) = <1 - y, y>
    P(G | T = normal, Fg = T) = <y, 1 - y>
d.  order of probability: <slient, sound>
    P(A | G = normal, Fa = F) = <1, 0>
    P(A | G = high, Fa = F) = <0, 1>
    P(A | G = normal, Fa = T) = <1, 0>
    P(A | G = high, Fa = T) = <1, 0>
e.  P(T | A = sound, Fa = F, Fg = F)
    = αP(T, A = sound, Fa = F, Fg = F)
    = αΣg P(T, G = g, A = sound, Fa = F, Fg = F)
    = αΣt,g P(T = t, G = g, A = sound, Fa = F, Fg = F)
    = α(P(T = low)P(Fg = F | T = low)(1-x)P(Fa = F) + P(T = high)P(Fg = F | T = high)xP(Fa = F))

14.12
a.  (ii) and (iii), N should not be independent to F given M
b.  (ii), fewer dependencies
c.  P(M1 | N = 1) = (1-e)(1-f)<0, 1, 0, 0, 0>
        + e(1-f)<1/2, 0, 1/2, 0, 0>
        + f<1, 0, 0, 0, 0>
    = <e + 3f/2 - 3ef/2, 1 - e - f + ef, e - ef, 0, 0>
    P(M1 | N = 2) = (1-e)(1-f)<0, 0, 1, 0, 0>
        + e(1-f)<0, 1/2, 0, 1/2, 0>
        + f<1, 0, 0, 0, 0>
    = <f, (e - ef)/2, 1 - e - f + ef, (e - ef)/2, 0>
    P(M1 | N = 3) = (1-e)(1-f)<0, 0, 0, 1, 0>
        + e(1-f)<0, 0, 1/2, 0, 1/2>
        + (1-e)f<1, 0, 0, 0, 0>
        + ef<1/2, 1/2, 0, 0, 0>
    = <f - ef/2, ef/2, (e - ef)/2, 1 - e - f + ef, (e - ef)/2>
d.  N ∈ {M - 1 .. M + 4}
    2 <= N <= 5
e.  prior P(N) and the precise number of e and f, then query P(N | M1 = 1, M3 = 3)

14.13
P(N | M1 = 2, M2 = 2)   = Σf1,f2,n θP(X | Parents(X))
                        // the only non-zero case is F1 = F2 = false
                        = αP(N)P(M1 = 2 | N, F1 = false, F2 = false)P(M)>P(M2 = 2) | N, F1 = false, F2 = false
                        = αP(N)(P(M1 = 2 | N, F1 = false, F2 = false)^2)
                        = αP(N)<e/2, 1-e, e/2>

14.14
a.  (ii): Parents(J) = G, J is conditionally independent to I given G
    (iii): MB(M) = {G, B, I}, M and J are conditionally independent given MB(M)
b.  p(b, i, ~m, g, j) 
    = Πp(x | parents(x))
    = 0.9 * 0.5 * (1 - 0.1) * 0.8 * 0.9
    = 0.2916
c.  ./AI/bayes
    P(J | b, i, m) = [0.19, 0.81]
d.  between G and M given I = false
e.  from wikipedia:
        A pardon is an executive order granting clemency for a conviction. It may be granted 
        "at any point after the ... commission" of the crime.[6] As per Justice Department regulations, 
        convicted persons may only apply five or more years eir sentence has been completed.[7] 
        However, the President's power to pardon is not restricted 
        by any temporal constraints except that the crime must have been committed. Its practical effect is 
        the restoration of civil rights and statutory disabilities (i.e., firearm rights, occupational licensing) 
        associated with a past criminal conviction.[7] In rarer cases, such as the pardon of Richard Nixon, 
        a pardon can also halt criminal proceedings and prevent an indictment.
    PresidentialPardon should be a parent of I, G and J

14.15
a.  /// thanks https://www.wolframalpha.com
    P(B | j, m) 
    = αP(B) Σe p(e) Σa P(a | B, e)p(j | a)p(m | a)
    = αP(B) Σa,e p(e)P(a | B, e)p(j | a)p(m | a)
    = α[0.001, 0.999](
        0.002 * [0.95, 0.29] * 0.9 * 0.7 + 
        0.998 * [0.94, 0.001] * 0.9 * 0.7 +
        0.002 * [0.05, 0.71] * 0.05 * 0.01 + 
        0.998 * [0.06, 0.999] * 0.05 * 0.01
    )
    = α[0.001, 0.999][0.592243, 0.00149335]
    = α[0.000592, 0.00149]
    = [0.284, 0.716]
b.  if f1(A) x f2(B) = f3(C), C is a vector of k variables
    the computation requires 2^k productions
    summing over a variable from f(A) performs 2^|A| / 2 summations
    f6(B, E)    = Σa f3(A, B, E) x f4(A) x f5(A)
                = Σa f3(A, B, E) x f4'(A) // 2 multiplications
                = Σa f3'(A, B, E) // 8 multiplications
                = f6(B, E) // 4 summations, 14 in total
    f7(B)   = Σe f2(E) x f6(B, E)
            = Σe f6'(B, E) // 4 multiplications
            = f7(B) // 2 summations, 6 in total
    P(B | j, m) = αf1(B) x f7(B) // 2 multiplications
    22 operations in total
    assume the enumeration algorithm is performed in the order of figure 14.8
    16 operations in total
c.  enumeration: 
        basis: Xn node, 1 multiplication, 1 = 2^1 - 1
        induction: Xi node, computes two Xi+1 node, then one summation
            2(2^(n-1) - 1) = 2^n - 2 + 1 = 2^n - 1
        two values have to be computed, 2^(n+1) - 2
    variable elimination:
        each step is 
        1.  a pointwise production f1(A, B) x f2(B), 4 production
        2.  summation over a, 2 summations, 6 operations in total
        6(n-1) in total
d.  // thanks https://ermongroup.github.io/cs228-notes/inference/ve/
    induct on the number of nodes
    basis: trivial
    induction: eliminate a leaf of the polytree
        since polytree is an undirected tree, the polytree must have a leaf
        let the leaf be Xi, the only node connected to it is Xj
        if the edge is of direction Xi -> Xj:
            Parents(Xj) = {Xi}, Xi and Xj are the only two nodes involving Xi
            factor of Xi is fi(Xi), factor of Xj is fj(Xj, Parents(Xj))
            the pointwise production has entries Xj, Parents(Xj), can be computed in time linear to size of CPT of Xj
            similarly summing out Xi takes time linear in size of CPT of Xj
        if the edge is of direction Xj -> Xi:
            symmetric
        the resulting sub network is again a polytree
    the running time of variable elimination will be linear in n * max{size of CPT}


14.16
a.  for each symbol xi in the 3-CNF, define a random variable Xi that:
        P(Xi) = <0.5, 0.5>
    for each clause ci in the 3-CNF with symbols X1, X2, X3, define a random variable Ci that:
        P(Ci | X1, X2, X3)  = <1, 0> if disjunction of literals is true
                            = <0, 1> if disjunction of literals is false
    define a binary node Con that:
        P(Con | C1 .. Ck)   = <1, 0> if conjunction of clauses is true
                            = <0, 1> if conjunction of clauses is false
    query P(Con), the 3-CNF if satisfiable iff P(Con) > 0
    Con may require special structure (Noisy-AND), otherwise its CPT will have exponential size
b.  define node Con with k values, where k is the number of clauses
        P(Con = i | C1 .. Ck)   = 1 if i in k clauses is true
                                = 0 otherwise
    again query P(Con), the largest i that P(Con = i) > 0 is the maximum satisfiable clauses

14.17
a.  by scanning over P(X), add last entry to the current one on the way
    sampling can be done in O(lgk) by binary search
b.  /// no idea, solution on the solutions manual doesn't sound quite right
c.  let f(x) be the pdf, F(x) be the cdf
    F^-1(x) then is a transformation from uniform [0, 1] to that distribution
d.  the value P(Xi = xi | Parents(Xi)) will be 0
    instead of a particular value of Xi, a small range may have to be sampled instead
    there's no obvious way to do this, generating a distribution of range from range of parameters is non-trivial
    
14.18
a.  |Cloudy| * |Rain| = 4
b.  ./AI/bayes
    value order: [F, T]
    P(Cloudy | rain, sprinker, wet_grass) = [0.5555555555555555, 0.4444444444444444]
    P(Cloudy | ~rain, sprinker, wet_grass) = [0.9523809523809523, 0.04761904761904761]
    P(Rain | cloudy, sprinker, wet_grass) = [0.18518518518518515, 0.8148148148148148]
    P(Rain | ~cloudy, sprinker, wet_grass) = [0.7843137254901961, 0.2156862745098039]
c.  Q^2 is the transition matrix of 2 transitions in a row
d.  if lim(n -> ∞)Q^n is defined, it's the equilibrium matrix
    a stationary distribution can be derived from the equilibrium matrix and detailed balance
e.  as stationary distribution is unique in ergodic transition matrix, P(x | e) can be computed from Q^n
    but even when Q^n can be computed efficiently, the martix is of size (Π|Xk|)^2, nearly always impractical

14.19
a.  let q = [α, q1; 1-α, q2]
    π(x)q(x -> x')  = π(x)(αq1(x -> x') + (1 - α)q2(x -> x'))
                    = απ(x)q1(x -> x') + (1 - α)π(x)q2(x -> x')
                    = απ(x')q1(x' -> x) + (1 - α)π(x')q2(x' -> x)
                    = π(x')q(x' -> x)
b.  π(x)q(x -> x')  = Σy π(x)q1(x -> y)q2(y -> x')
                    = Σy π(y)q1(y -> x)q2(y -> x')
                    = Σy π(x')q2(x' -> y)q1(y -> x)
                    = π(x') Σy q2(x' -> y)q1(y -> x)
                    = π(x')q(x' -> x)

14.20
a.  α(x' | x)   = min(1, π(x')q(x | x') / π(x)q(x' | x))
                = min(1, π(x')q(x' -> x) / π(x)q(x -> x'))
                = min(1, 1)
                = 1
b.  when x != x' and α(x' | x) < 1,
        π(x)q(x -> x')  = π(x)q(x' | x)α(x' | x)
                        = π(x')q(x | x')
                        // if min(1, p) < 1, p < 1, min(1, 1/p) = 1
                        = π(x')q(x' -> x)
    when x != x' and α(x' | x) >= 1,
        symmetric
    when x = x',
        π(x)q(x -> x') = π(x) = π(x') = π(x')q(x' -> x)

14.21
a.  A = B = C = <1/4, 1/4, 1/4, 1/4>
    given A = a, B = b, let d = a - b,
    p(draw) = 1 - (|d| + 3 / 7)
    p(win) = (1 - p(draw))(4 + d) / 8
    p(lose) = (1 - p(draw))(4 - d) / 8
    MAB = <p(win), p(draw), p(lose)>
b.  ./AI/bayes
c.  P(MBC | MAB = win, MAC = draw) = [0.25931259600614437, 0.38556067588325654, 0.35512672811059903]
d.  if evaluated in topological order, all team qualities must be summed over 
    time complexity will be exponential by enumeration
    as the network is a polytree, time complexity will be linear in nodes by variable elimination
e.  it depends on the number of nodes the values assigned to CPTs of team qualities and match results
    with too biased values the MCMC algorithm may stuck in a state for very long, emit a sequence of biased samples
    in this case the markov branket of a team quality is the entire network
    the sampling result usually converges with ε = 0.01 in a single digit of samples
    the Gibbs sampling algorithm may occationally break down if 
    the event is initiated to an impossible case (i.e. contradicting conditions)
    either the CPTs should not contain 0 entries or the algorithm has to be adjusted
    
15.1
define another (set of) variable X', where X'i+1 = Xi
Xi fully determines X'i+1, given Xi, X'i+1 is conditionally independent to all other variables
P(Xi+2, X'i+2 | Xi, X'i, Xi+1, X'i+1)
= P(Xi+2, X'i+2 | X'i+1, X'i, Xi+1)
// given X'i+1 = Xi, Xi+1, Xi+2 and X'i+2 are conditionally independent to X'i
= P(Xi+2, X'i+2 | X'i+1, Xi+1)
transition model of X' requires no CPTs
transition model of X has the same CPT model but different parents
the no more parameters are required

15.2
value order: <false, true>
a.  P(Xt+1 | e1:t+1)
    = αP(et+1 | Xt+1)Σxt P(Xt+1 | xt)p(xt | e1:t)
    let P(Xt+1 | e1:t+1) = <1-a, a>, P(Xt | e1:t) = <1-b, b>
    <1-a, a>
    = α<0.2, 0.9> ((1-b)<0.7, 0.3> + b<0.3, 0.7>)
    = α<0.2, 0.9> <0.7 - 0.4b, 0.3 + 0.4b>
    = α<0.14 - 0.08b, 0.27 + 0.36b>
    a = (0.27 + 0.36b) / (0.41 + 0.28b) is monotonically increasing when b <= 0.897
    b starts with p(rain) = 1/2
    the limit is a = b = 0.897
b.  P(Xt+k+1 | e1:t) = Σxt+k P(Xt+k+1 | xt+k) P(xt+k | e1:t)
    let P(Xt+k+1 | e1:t) = <1-a, a>, P(Xt+k | e1:t) = <1-b, b>
    <1-a, a>
    = (1-b)<0.7, 0.3> + b<0.3, 0.7>
    = <0.7 - 0.4b, 0.3 + 0.4b>
    a = 0.3 + 0.4b, b starts as p(r2 | u1, u2) = 0.883
    when b >= 0.5, a <= b
    the limit is a = b = 0.5

15.3
a.  P(Xk | e1:t) = αf1:k x bk+1:t
    where 
        f1:k can be computed from αFORWARD(f1:k-1, ek), ultimately f1:0 and e1:h
        bk+1:t can be computed from BACKWARD(bk+2:t, ek+1), ultimately bh+1:t and e1:h
b.  similar to part a
    given f1:h, bt+1:t and e1:t, P(Xk | e1:t) can be computed for h+1 <= k <= t
c.  FORWARD-BACKWARD(s, k, f1:s, bk+1:t)
        // mutable capture: p
        if s = k:
            p[s] <- αf1:s x bk+1:t
        else:
            h <- (t - s + 1) / 2
            compute f1:h, f1:h+1, bh:t, bh+1:t
            p[h] <- αf1:h x bh+1:t
            FORWARD-BACKWARD(s, h-1, f1:s, bh:t);
            FORWARD-BACKWARD(h+1, k, f1:h+1, bk+1:t);
d.  time complexity:
    f(n)    = 2f(n/2) + O(n)
            = O(nlgn)
    space complexity:
    f(n)    = f(n/2) + O(1)
            = f(lgn)

15.4
Xk is assigned the value xk given the highst p(Xk = xk | e1:t)
if for some Xk, xk, Xk+1 and xk+1, P(Xk+1 = xk+1 | Xk = xk) = 0
but max{p(Xk+1 = xk+1 | e1:t)} = xk+1
p(x1:t | e1:t)
= αp(et | xt)p(xt | xt-1)p(x1:t-1 | e1:t-1)
= αΠp(ek | xk)p(xk | xk-1)
= 0

15.5
P(et+1 | Xt+1)Σxt P(Xt+1 | xt)p(xt, e1:t)
// first order Markov assumption
= P(et+1 | Xt+1)Σxt P(Xt+1 | xt, e1:t)p(xt, e1:t)
= P(et+1 | Xt+1)Σxt P(Xt+1, xt, e1:t)
= P(et+1 | Xt+1)P(Xt+1, e1:t)
// sensor Markov assumption
= P(et+1 | Xt+1, e1:t)P(Xt+1, e1:t)
= P(Xt+1, e1:t+1)
where l1:0 = P(X0, e1:0) = P(X0)

15.6
// thanks solutions manual
if |NEIGHBORS(s)| = k for all s, probability of reaching two locations at time t could differ by a factor of Ω(k^t):
    assume except a single path, all other paths of length t leads to a location m
    P(Xt != m) = (1/k)^t
    P(Xt = m) = 1 - (1/k)^t
    P(Xt = m) / P(Xt != m) = k^t - 1 = Ω(k^t)
for any given ε and l, there may be a map and location m where ε >> 1 / k^t hence P(Xt = m | e1:t) > P(Xt = l | e1:t)
the map may be nonintuitive, containing one way passes and self loops

15.7
./AI/markov
localization error at ε = 0.01: 0.015237257310423948
localization error at ε = 0.05: 0.005521138181048611
localization error at ε = 0.1: 0.02440621890172354
localization error at ε = 0.2: 0.49024219817702674
at t = 100, most of the time the localization error (in term of weighted manhattan distance) is small

15.8
/// code skipped
the error may be greater as the consequence of a sensor failure is now more severe
vitabi algorithm can only derive valid paths, if the room is empty enough,
a sensor failure may drive the estimation far away from the real path

15.9
/// code skipped
if the room is huge and the robot starts somewhere in the middle
the filtered distribution will dissolve in a huge area over time (just like repeatedly applying gaussian blur)
weighted error may be huge
if the robot starts on the side or in a corner, the error may be constrained

15.10
a.  P(X1)   = Σs0 int(x0) P(X1 | x0, s0)p(x0)p(s0)
            = Σs0p(s0) int(x0) p(x0)P(X1 | x0, s0)
    by 15.4.1, int(x0) p(x0)P(X1 | x0, s0) is multivariate Gaussian
    int(x1)p(x1)    = int(x1) Σs0p(s0) int(x0)p(x0) p(x1 | x0, s0)
                    = Σs0p(s0) int(x0)p(x0) int(x1) p(x1 | x0, s0)
                    = Σs0p(s0) int(x0)p(x0)
                    = Σs0p(s0) = 1
    the sum of weights = Σs0p(s0) = 1
b.  P(Xt+1, St+1 | e1:t+1)
    = αP(et+1 | Xt+1, St+1)P(Xt+1, St+1 | e1:t)
    // sensor Markov assumption
    = αP(et+1 | Xt+1)Σst int(xt) P(Xt+1, St+1 | xt, st)p(xt, st | e1:t)
    = αP(et+1 | Xt+1)Σst int(xt) P(Xt+1, St+1 | xt, st)p(xt, st | e1:t)
    // conditional independency from Bayesian network
    = αP(et+1 | Xt+1)Σst P(St+1 | st) int(xt) P(Xt+1 | xt, st)p(xt, st | e1:t)
    // xt and st are d-connected by e1:t
    // but the solutions manual claims p(xt, st | e1:t) = p(xt | e1:t)p(st| e1:t)
    // will follow solutions manual here
    = αP(et+1 | Xt+1)Σst P(St+1 | st)p(st | e1:t) int(xt) p(xt | e1:t)P(Xt+1 | xt, st)
    as p(xt | e1:t) is m mixture Gaussian, P(Xt+1 | xt, st) linear Gaussian
    int(xt) p(xt | e1:t)P(Xt+1 | xt, st) is still a m mixture Gaussian
    multiply p(st | e1:t) for each st and sum over them, the result is a km mixtrue Gaussian
    αP(et+1 | Xt+1) rebalances the factors to weight 1
c.  each weighted Gaussian distribution in the mixture corresponds to a possible sequence of s1:t
    the factor is the posibility of that sequence

15.11
(z1 - x1)^2 / σz^2 + (x1 - μ0)^2 / (σ0^2 + σx^2)
= ((σ0^2 + σx^2)(x1^2 - 2x1z1 + z1^2) + σz^2(x1^2 - 2x1μ0 + μ0^2)) / σz^2(σ0^2 + σx^2)
= ((σ0^2 + σx^2 + σz^2)x1^2 + (-2(σ0^2 + σx^2)z1 -2σz^2μ0)x1 + (σ0^2 + σx^2)z1^2 + σz^2μ0^2) / σz^2(σ0^2 + σx^2)
where a = σ0^2 + σx^2 + σz^2, b = -2(σ0^2 + σx^2)z1 - 2σz^2μ0, c = (σ0^2 + σx^2)z1^2 + σz^2μ0^2 
-b / 2a = ((σ0^2 + σx^2)z1 + σz^2μ0) / (σ0^2 + σx^2 + σz^2)
= (σ0^2 + σx^2 + σz^2)(x1 - ((σ0^2 + σx^2)z1 + σz^2μ0) / (σ0^2 + σx^2 + σz^2))^2 / σz^2(σ0^2 + σx^2) + α
= (x1 - ((σ0^2 + σx^2)z1 + σz^2μ0) / (σ0^2 + σx^2 + σz^2))^2 / (σz^2(σ0^2 + σx^2) / (σ0^2 + σx^2 + σz^2)) + α
where e^α is a costant

15.12
./AI/markov
a.  // given factors in Figure 15.10
    σ0^2 = 1
    σ1^2 = 0.8333333333333334
    σ2^2 = 0.8285714285714285
    σ3^2 = 0.8284313725490196
    σ4^2 = 0.8284272497897393
    σ5^2 = 0.8284271284271284
    σ6^2 = 0.8284271248545468
    σ7^2 = 0.8284271247493799
    σ8^2 = 0.828427124746284
    σ9^2 = 0.8284271247461928
    σ10^2 = 0.8284271247461902
b.  let σt^2 = st, σx^2 = sx, σz^2 = sz
    st+1 = (s + sx)sz / (s + sx + sz) = st
    szs + sxsz = s^2 + (sx + sz)s
    s^2 + sxs - sxsz = 0
    s = (-sx + (sx^2 + 4sxsz)^(1/2)) / 2
    when st < s, st < st+1 < s
c.  σx^2 -> 0, st+1 = stsz / (st + sz)
    s = (-sx + (sx^2 + 4sxsz)^(1/2)) / 2 -> 0
    when the transition model is precise, the filter result will eventually be precise
    σz^2 -> 0, st+1 = 0 
    when the sensor model is precise, the filter result will be precise immediately after a single observation

15.13
dynamic BN:
    S = enough sleep
    Z = sleep on class
    R = red eyes
    P(S0) = <0.3, 0.7>
    P(St+1 | st) = <0.2, 0.8>
    P(St+1 | ~st) = <0.7, 0.3>
    P(Rt | st) = <0.8, 0.2>
    P(Rt | ~st) = <0.3, 0.7>
    P(Zt | st) = <0.9, 0.1>
    P(Zt | ~st) = <0.7, 0.3>
HMM:
    T = [
        0.7, 0.3,
        0.2, 0.8,
    ]
    O = [
        // ~rt, ~zt
        0.3 * 0.7, 0.8 * 0.9,
        // rt, ~zt
        0.7 * 0.7, 0.2 * 0.9,
        // ~rt, zt
        0.3 * 0.3, 0.8 * 0.1,
        // rt, zt
        0.7 * 0.3, 0.2 * 0.1,
    ]

15.14
./AI/markov
a.  P(EnoughSleep1 | e1:1) = [0.13573407202216067, 0.8642659279778394]
    P(EnoughSleep2 | e1:2) = [0.4989942816222079, 0.5010057183777922]
    P(EnoughSleep3 | e1:3) = [0.8955446712328069, 0.10445532876719311]
b.  P(EnoughSleep1 | e1:3) = [0.2722518408882662, 0.7277481591117337]
    P(EnoughSleep2 | e1:3) = [0.7243159107237754, 0.2756840892762246]
    P(EnoughSleep3 | e1:3) = [0.8955446712328069, 0.10445532876719311]
c.  evidence from day 3 increased the probability of ~EnoughSleep1 and 2

15.15
α<0.21, 0.02> Σst P(St+1 | st) <p, 1-p> = <p, 1-p>
α<0.21, 0.02> (<0.7, 0.3>p + <0.2, 0.8>(1-p)) = <p, 1-p>
α<0.21, 0.02> <0.2 + 0.5p, 0.8 - 0.5p> = <p, 1-p>
α<0.042 + 0.105p, 0.016 - 0.01p> = <p, 1-p>
p = 0.956802

15.16
a.  P(BatteryBrokent) -> <0, 1>
    and the observations have less and less impact on the model
    E(Batteryt) will eventually reach 0
b.  BMBrokent has an extra continuous parent Temperaturet
    p(bmbrokent+1 | ~bmbrokent, temperaturet) now depends on temperature
c.  it may infer the current temperature by filtering 
    by infering continuous value from (indirect) binary discrete observation 
    precision of the inference would be inacceptable

15.17
each evidence variable has fixed value
reduced forward:
    Ri is a 1-d factor with no parent
    Ri+1 is a 2-d factor with a sole parent Ri
    pointwise production results in a 2-d factor, then Ri is summed out
reduced backward:
    Ri is a 2-d factor with a sole parent Ri-1
    Ri+1 is a 2-d factor with a sole parent Ri
    by picking corresponding rows from both factors, Ri can be eliminated in O(n^2) time

16.1
/// skipped

16.2
Pat is more likely to have a better car
Pat is more likely to be disappointed
the calculation involves some M = max{X1 .. Xk}, Xi ~ N(0, 1)
the value E[M] is hard to bound
by a simple simulation, mean of max{X1 .. X10} is nearly 150% of max{X1 .. X4}

16.3
let X be the number of tosses until the first head
a.  E[2^X]  = Σx 2^xP(X = x)
            = Σx 2^x(1/2)^x
            = Σx 2^x(1/2)^x
            = Σx 1 = ∞
b.  if a single game charges more than 2^k, with p > 1 - (1/2)^(k-1) the player will lose
    a risk-averse person will not pay more than 2^10 
c.  let the player's initial wealth be $k, the cost of a single play be $c
    E(k - c + 2^X)  = Σx U(k - c + 2^x)P(X = x)
                    = Σx (alog2(k - c + 2^x) + b)(1/2)^x
d.  the maximum amount to pay is given by
        U(k) = U(k - c + 2^x)
        alog2(k) + b = Σx (alog2(k - c + 2^x) + b)(1/2)^x

16.4
/// simple binary search, code skipped

16.5
./AI/bayes
a.  (ii) and (iii) can correctly represent the joint probability
    (i) implies that Wrapper is independent to Shape but 
        P(Wrapper | round)  = Σflavor P(Wrapper, flavor | round)
                            = Σflavor p(flavor | round)p(Wrapper | flavor, round)
                            = αΣflavor p(round | flavor)p(flavor)P(Wrapper | flavor, round)
                            = α(0.8 * 0.7 * <0.8, 0.2> + 0.1 * 0.3 * <0.1, 0.9>)
                            = α<0.451, 0.139>
        P(Wrapper | square) = αΣflavor p(square | flavor)p(flavor)P(Wrapper | flavor, square)
                            = α(0.2 * 0.7 * [0.8, 0.2] + 0.9 * 0.3 * [0.1, 0.9])
                            = α[0.139, 0.271]
b.  (iii), least size of CPTs, prior of Wrapper and Shape is not given directly
c.  yes and it's incorrect
d.  p(red)  = Σflavor p(red | flavor)p(flavor)
            = 0.8 * 0.7 + 0.1 * 0.3 = 0.59 
e.  p(strawberry | round, red) = 0.9933481152993348
f.  0.7s + 0.3a
g.  the value should not change
    P(Flavor) is still <0.7, 0.3> given an unopened box

16.6
B > A:
    U([1.0, 3000; 0.0, 0]) > U([0.8, 4000; 0.2, 0])
C > D:
    U([0.2, 4000; 0.8, 0]) > U([0.25, 3000; 0.75, 0]) 
    U([0.25, A; 0.75, 0]) > U([0.25, B;  0.75, 0])
    but by axiom if substitution,
    U([0.25, A; 0.75, 0]) < U([0.25, B; 0.75, 0])

16.7
a problem for the agent
as descripted in the text, an irrational agent violates some axiom of utility
which in turn makes it exploitable for some adversary in certain situations

16.8
E[L]    = 1/50 * 10 + 1/2_000_000 * 1_000_000 - 1
        = 1/5 + 1/2 - 1
        = -0.3
given the utility of money in Figure 16.2.b
it's rational to buy only when the player is in deep bet

16.9
/// skipped

16.10
how much would one pay for a car equipment that eliminates all possibility of death from car accidents in a year
how much would one request for working in area farther from hospital

16.11
let M = max{X1 .. Xk}
F'(m)   = P(M <= m)
        = ΠP(Xi <= m)
        = F(m)^k
f'(m)   = dF'(m) / dm
        = dF(m)^k / dm
        = kF(m)^k-1 dF(m)/dm
        = kf(m)F(m)^k-1

16.12
errata:
    U(x) = -e^(-x/R)
a.  assume Mary has no initial fund
    U(500) = -e^-1 = -0.368
    U([0.6, 5000; 0.4, 0]) = -0.6e^-10 - 0.4 = -0.4
    Mary would choose $500 for certain
b.  // thanks https://www.wolframalpha.com
    e^(-100 / R) = 0.5 * e^(-500 / R) + 0.5
    R = 152.38

16.13
/// skipped

16.14
/// skipped

16.15
./AI/bayes
a.  Parents(M) = {B}
    Parents(P) = {B, M}
    Parents(U) = {B, P}
b.  EU(~b) = 1300
    EU(b) = 1620
c.  he should by the book

16.16
a.  /// skipped, no idea for any value in that graph
b.  /// a possible way to solve decision problems by bayesian network libraries:
    /// define decision nodes D as parent-free nodes with arbitrary prior distribution
    /// define utility node with n parents {P1 .. Pn} as as a node with Π|Pk| value
    /// each row of its CPT (corresponds to a unique combination of its parents) has 1 in one entry and 0 in others
    /// query the utility node with d ∈ D (thus prior of decision nodes doesn't matter)
    /// multiply the result with utility function
c.  the node Noise now has a different CPT with regard to its parent AirTraffic
d.  the factor of noise now has a greater weight in the utility function
e.  /// VPIe(Ej) can be calculated by query with evidence {d, ej}

16.17
./AI/bayes
a.  Parents(T) = Q
    Parents(U) = Q
b.  0.7 * 2000 + 0.3 * 1300 - 1500 = 290
c.  /// value order: [fail, pass]
    P(Test) = [0.335, 0.665]
    /// value order: [bad, good]
    P(Quality | pass) = [0.15789473684210528, 0.8421052631578948]
    P(Quality | fail) = [0.5820895522388061, 0.41791044776119396]
d.  EU(c | pass) = 389.47368421052636
    EU(c | fail) = 92.53731343283576
    either case the optimal decision is to buy
e.  VPI(Test) = 0
    the test is meaningless

16.18
a.  VPIe(Ej) = (Σk p(Ej = ejk | e)EU(αejk | e, Ej = ejk)) - EU(α | e)
    where 
        EU(aejk | e, Ej = ejk) = max(a){U(a | e, Ej = ejk)}
        EU(a | e) = max(a){U(a | e)}
    by common sense, EU(aejk | e, Ej = ejk) >= EU(a | e) for all ejk
    therefore VPIe(Ej) >= 0
    /// the definition of VPIe(Ej, Ek) = VPIe(Ej) + VPIe,ej(Ek)
    /// is questionable as the value of ej is unspecified in the context
    /// a possibly more accurate definition is used here
    VPIe(Ej, Ek)    = (Σej p(ej | e)(EU(α | e, ej) + VPIe,ej(Ek) - EU(α | e, ej))) - EU(α | e)
                    = (Σej p(ej | e)VPIe,ej(Ek)) - EU(α | e)
                    = (Σej p(ej | e)Σek p(ek | ej, e)EU(α | e, ej, ek)) - EU(α | e)
                    = (Σej,ek p(ej | e)p(ek | ej, e)EU(α | e, ej, ek)) - EU(α | e)
                    = (Σej,ek p(ek, ej | e)EU(α | e, ej, ek)) - EU(α | e)
                    = (Σek p(ek | e)(EU(α | e, ek) + VPIe,ek(Ej) - EU(α | e, ek))) - EU(α | e)
b.  they don't want to ruin the event of birth
    by their utility function, the combined value of 
        knowing the sex of their child + attending childbirth few months later
    is smaller than 
        the two events at once
c.  assume in a decision network, knowing only Ej will not change the optimal action
    but knowing both Ej and Ek will improve the optimal action
    let A = {}, B = {ej}, x = ek, then
        f(A ∪ {x}) - f(A) = 0
        f(B ∪ {x}) - f(B) > 0

17.1
./AI/mdp
P(X = (0, 0)) = 0.02462000000000001
P(X = (1, 0)) = 0.02824000000000001
P(X = (2, 0)) = 0.026270000000000016
P(X = (3, 0)) = 0.08688000000000001
P(X = (0, 1)) = 0.18054000000000006
P(X = (1, 1)) = 0
P(X = (2, 1)) = 0.04443000000000001
P(X = (3, 1)) = 0.013680000000000005
P(X = (0, 2)) = 0.02524000000000001
P(X = (1, 2)) = 0.06224000000000002
P(X = (2, 2)) = 0.17994000000000007
P(X = (3, 2)) = 0.3279200000000001
the calculation is similar to prediction in a sensorless model

17.2
./AI/mdp
/// by simulation, go Left whenever possible
P(X = (0, 0)) = 0.3333333333333347
P(X = (1, 0)) = 0
P(X = (2, 0)) = 0
P(X = (3, 0)) = 0
P(X = (0, 1)) = 0.3333333333333346
P(X = (1, 1)) = 0
P(X = (2, 1)) = 0
P(X = (3, 1)) = 0
P(X = (0, 2)) = 0.3333333333333342
P(X = (1, 2)) = 0
P(X = (2, 2)) = 0
P(X = (3, 2)) = 0

17.3
the optimal action now also depends on the history of states
if the state with maximum reward is reached, all actions later are indifferent to the agent
on infinite horizon it may still be possible to define MEU utilities
which should lead the agent to the reachable state with maximum reward
but there's no longer intuitive algorithms to calculate such an utility

17.4
a.  U(s) = max(a) {R(s, a) + γΣs' P(s' | s, a)U(s')}
    U(s) = max(a) {Σs' P(s' | s, a)(γU(s') + R(s, a, s'))}
b.  /// thanks solutions manual
    define a new MDP where:
        γ' = γ^(1/2)
        define a new state r(s, a, s') for all triple (s, a, s') where P(s' | s, a) > 0
        define a new action a(s, a) for all r(s, a, s') that
        ACTION(r(s, a, s')) = { a(s, a) }
        P'(r(s, a, s') | s, a) = P(s' | s, a)
        P(s' | r(s, a, s'), a(s, a)) = 1.0
        R'(r(s, a, s'), a(s, a)) = R(s, a, s') / γ'
        R'(s, a) = 0
    let U' be an optimal utility of the new MDP,
        U'(r(s, a, s')) = max(a) {R'(s, a) + γΣs' P'(s' | r(s, a, s'), a)U'(s')}
                        = R(s, a, s') / γ' + γ'U'(s')
        U'(s)           = max(a) {R'(s, a) + γ'Σs' P'(s' | s, a)U'(s')}
                        = max(a) {γ'Σs' P'(r(s, a, s') | s, a)U'(r(s, a, s'))}
                        = max(a) {γ'Σs' P(s' | s, a)(R(s, a, s') / γ' + γ'U'(s'))
                        = max(a) {Σs' P(s' | s, a)(γU'(s) + R(s, a, s'))}
    therefore the proper subset of U'(s) is also an optimal utility of the original MDP
c.  define a new MDP where:
        γ' = γ^(1/2)
        define a new state r(s, a) for all a ∈ ACTION(s)
        define a new action a(s, a) for all r(s, a)
        ACTION(r(s, a)) = { a(s, a) }
        P'(r(s, a) | s, a) = 1
        P'(s' | r(s, a), a(s, a)) = P(s' | s, a)
        R'(r(s, a)) = R(s, a) / γ'
        R'(s) = 0
    let U' be an optimal utility of the new MDP,
        U'(r(s, a)) = R'(r(s, a)) + γ'max(a) {Σs' P'(s' | r(s, a), a)U'(s')}
                    = R(s, a) + γ'Σs' P(s' | s, a)U'(s')
        U'(s)       = R'(s) + γ'max(a) {Σs' P'(s' | s, a)U'(s)}
                    = γ'max(a) U'(r(s, a))
                    = γ'max(a) { R(s, a) / γ' + γ'Σs' P(s' | s, a)U'(s') }
                    = max(a) {R(s, a) + γΣs' P(s' | s, a)U'(s')}
    therefore the proper subset of U'(s) is also an optimal utility of the original MDP

17.5
./AI/mdp
[-2, -1.6500000000000385]
[-1.6490000000000387, -1.565000000000048]
[-1.564000000000048, -0.7320000000001099]
[-0.7310000000001099, -0.45300000000010965]
[-0.45200000000010965, -0.08500000000010932]
[-0.08400000000010932, -0.045000000000109286]
[-0.044000000000109285, -0.02800000000010927]
[-0.02700000000010927, -0.023000000000109266]
[-0.022000000000109265, -0.00000000000010925288451701931]

17.6
a.  let argmax(a){f(a)} = a*, argmax(a){g(a)} = b*
    assume max(a){f(a)} - max(a){g(a)} >= 0 (otherwise symmetric)
    |max(a){f(a)} - max(a){g(a)}|
    = f(a*) - g(b*)
    <= f(a*) - g(a*)
    <= max(a){ f(a) - g(a) }
    = max(a){ |f(a) - g(a)| }
b.  |(BUi - BUi')(s)|
    = |γmax(a)Σs'{P(s' | s, a)Ui(s')} - γmax(a)Σs'{P(s' | s, a)Ui'(s')}
    <= max(a){|γΣs'P(s' | s, a)Ui(s') - γΣs'P(s' | s, a)Ui'(s')|}
    = γmax(a){|Σs'P(s' | s, a)(Ui(s') - Ui'(s'))|}
    <= γmax(a,s'){|Ui(s') - Ui'(s')|}
    = γmax(s'){|Ui(s') - Ui'(s')|}
    = γmax(s){|Ui(s) - Ui'(s)|}
    = γ||Ui - Ui'||
    hence ||BUi - BUi'|| = max(s)|(BUi - BUi')(s)| <= γ||Ui - Ui'||

17.7
a.  UA(s) = R(s) + max(a){P(s' | a, s)UB(s')}
    UB(s) = R(s) + min(a){P(s' | a, s)UA(s')}
b.  similar to one player value iteration, but UAi+1 is calculated from UBi and vice versa
c.  ./AI/mdp/src/worlds#simple_game
d.  ./AI/mdp
    Utility of Player A:
    UA(0, 1) = -1
    UA(0, 2) = -1
    UA(0, 3) = 1
    UA(1, 0) = -1
    UA(1, 2) = 1
    UA(1, 3) = 1
    UA(2, 0) = -1
    UA(2, 1) = 1
    UA(2, 3) = 1
    UA(3, 1) = 1
    UA(3, 2) = 1
    Utility of Player B:
    UB(0, 1) = -1
    UB(0, 2) = -1
    UB(0, 3) = -1
    UB(1, 0) = -1
    UB(1, 2) = -1
    UB(1, 3) = 1
    UB(2, 0) = -1
    UB(2, 1) = -1
    UB(2, 3) = 1
    UB(3, 1) = 1
    UB(3, 2) = 1

17.8
./AI/mdp
a.  r = 100
    [Some(W), Some(W), None]
    [Some(N), Some(W), Some(S)]
    [Some(N), Some(W), Some(W)]
    (0, 2) is not terminal and has positive reward
    agent will try reach (0, 2) and stay there
b.  r = -3
    [Some(E), Some(E), None]
    [Some(E), Some(E), Some(N)]
    [Some(E), Some(E), Some(N)]
    (0, 2) has lower than average negative reward
    agent will head to (2, 2) ASAP and avoid (0, 2)
c.  r = 0
    [Some(E), Some(E), None]
    [Some(N), Some(N), Some(N)]
    [Some(N), Some(N), Some(N)]
    agent will head to (2, 2) ASAP, but will not avoid (0, 2)
d.  r = 3
    [Some(W), Some(W), None]
    [Some(N), Some(W), Some(S)]
    [Some(N), Some(W), Some(W)]
    same to a.

17.9
U(Up) = γ(50 - Σ(1 <= k <= 100) γ^k)
U(Down) = γ(-50 + Σ(1 <= k <= 100) γ^k)
approximately when γ > 0.9844, U(Down) > U(Up)

17.10
./AI/mdp
a.  state 1 and 2 have negative rewards
    agent in state 1 and 2 will head to state 3 ASAP
b.  Init = [B, B, _]: [Some(B), Some(A), None]
c.  Init = [A, A, _], discount = 1.0: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.9: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.8: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.7: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.6: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.5: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.4: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.3: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.2: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.1: [Some(B), Some(A), None]
    Init = [A, A, _], discount = 0.0: [Some(B), Some(A), None]
    discount will not affect the result of policy iteration

17.11
a.  ./AI/mdp/src/worlds#two_terminals
b.  // skipped
    // would be easy with ./AI/mdp
c.  // skipped

17.12
errata:
    value determination -> policy evaluation
run the policy evaluation with estimated model P on the real utility

17.13
./AI/mdp
believe state at t = 1:
P(X = (0, 0)) = 0.06569
P(X = (1, 0)) = 0.03650
P(X = (2, 0)) = 0.32847
P(X = (3, 0)) = 0.00365
P(X = (0, 1)) = 0.03650
P(X = (1, 1)) = 0.00000
P(X = (2, 1)) = 0.32847
P(X = (3, 1)) = 0.03285
P(X = (0, 2)) = 0.06569
P(X = (1, 2)) = 0.03650
P(X = (2, 2)) = 0.06569
P(X = (3, 2)) = 0.00000
believe state at t = 2:
P(X = (0, 0)) = 0.02022
P(X = (1, 0)) = 0.05939
P(X = (2, 0)) = 0.13579
P(X = (3, 0)) = 0.00008
P(X = (0, 1)) = 0.00931
P(X = (1, 1)) = 0.00000
P(X = (2, 1)) = 0.59807
P(X = (3, 1)) = 0.06573
P(X = (0, 2)) = 0.02022
P(X = (1, 2)) = 0.01316
P(X = (2, 2)) = 0.07801
P(X = (3, 2)) = 0.00000

17.14
each action generates a new believe state
at depth d there are at most |A|^d different states
each utility can be computed in |A||S|
time complexity is O(|S||A|^(d+1))

17.15
for all believe state, 
p(S = 0 | E = 0)
= p(E = 0 | S = 0)p(S = 0) / p(E = 0)
= 0.9p(S = 0) / p(E = 0)
= 0.9(1 - p(S = 1)) / p(E = 0)
= (0.9 - 0.9p(S = 1)) / p(E = 0)
p(S = 1 | E = 0)
= p(E = 0 | S = 1)p(S = 1) / p(E = 0)
= 0.5p(S = 1) / p(E = 0)
when E = 0 and
    0.9 - 0.9p(S = 1) > 0.5p(S = 1)
    0.9 > 1.4p(S = 1)
    p(S = 1) < 9/14
the agent is more likely in state 0 and should Go
similarly
p(S = 0 | E = 1)
= p(E = 1 | S = 0)p(S = 0) / p(E = 1)
= 0.1(1 - p(S = 1)) / p(E = 1)
p(S = 1 | E = 1)
= p(E = 1 | S = 1)p(S = 1) / p(E = 1)
= 0.5p(S = 1) / p(E = 1)
when E = 1 and
    0.1 - 0.1p(S = 1) > 0.5p(S = 1)
    p(S = 1) < 1/6
the agent is more likely in state 0 and should Go

17.16
dominant strategy is better to any other strategy whether the other players switch strategies or not
Nash equilibrium only requires the strategy to be optimal assuming other players have fixed strategies
dominant strategy equilibrium is a stronger condition than Nash equilibrium

17.17
            rock    paper   scissors    fire    water
rock        0       -1      1           -1      1
paper       1       0       -1          -1      1
scissors    -1      1       0           -1      1
fire        1       1       1           0       -1
water       -1      -1      -1          0       1
the game is symmetric, the best expected outcome is 0
the mixed strategy P = [1/9, rock; 1/9, paper; 1/9, scissors; 1/3, fire; 1/3, water] is optimal because:
    for the five pure strategies,
    E[O | rock, P] = -1/9 + 1/9 - 1/3 + 1/3 = 0
    E[O | paper, P] = -1/9 + 1/9 - 1/3 + 1/3 = 0
    E[O | scissors, P] = -1/9 + 1/9 - 1/3 + 1/3 = 0
    E[O | fire, P] = 3/9 + -1/3 = 0
    E[O | water, P] = -3/9 + 1/3 = 0
any mixed strategy of O will have expected outcome 0

17.18
Fed: do nothing dominates Fed: expand
Pol: do nothing dominates Pol: contract
among the remaining pure strategies, (Fed: contract, Pol: expand) is a nash equilibrium
it is not pareto-optimal: (Fed: do nothing, Pol: do nothing) has better outcomes for both parties

17.19
/// thanks solutions manual
let bi(v) be the bidding function on the private value of player i
let Wi(b) be the winning chance of player i bidding b, Wi is non-decreasing
given bidding functions and prior private value distributions of other players
the outcome of player i is vi - b
and the expected outcome is int(0, vi, Qi(b)), where Qi(vi, b) = Wi(b)(vi - b)
the condition of Nash equilibrium is:
    ∀i,b Qi(vi, bi(vi)) >= Qi(vi, b)
let v and v' be two different private values, b = bi(v), b' = bi(v')
    Qi(v, b) >= Qi(v, b')
        Wi(b)(v - b) >= Wi(b')(v - b')
        v(Wi(b) - Wi(b')) >= bWi(b) - b'Wi(b')
    Qi(v', b') >= Qi(v', b)
        v'(Wi(b') - Wi(b)) >= b'Wi(b') - bWi(b)
adding two equations
    (v' - v)(Wi(b') - Wi(b)) >= 0
if v' > v, Wi(b') >= Wi(b)
    if W(b') > W(b), b' > b
    if W(b') = W(b) = p, b' and b has the same winning probability
    a rational bidding function will have b' = b = min(b) {Wi(b) = p}
therefore bi(v) is non-decreasing
if all the players have the identical bidding function, the auction is effective

17.20
bidding b in this auction is equivalent to bidding b/2 in a normal auction
the normal auction ends in a bidding b where b - d <= vi < b, b <= vi + d
where vi is the second highest private value
this auction ends in a bidding (b' - d) / 2 <= vi < b'/2, b' <= 2vi + d, b'/2 = vi + d/2
the revenue in the worst case is d/2 lower

17.21
a.  it's not a zero-sum game before or after
b.  home:
        2p + 0.22(2q + (0.9 - q) + 0.1)
        = 2p + 0.22q + 0.22
    visiting:
        2(1-p) + 0.22(q + 2(0.9-q) + 0.1)
        = 2 - 2p + 0.22(1.9 - q)
        = -2p - 0.22q + 2.418
c.  home with pact:
        2q + (0.9 - q) + 0.1
        = 1 + q
    visiting with pact:
        q + 2(0.9 - q) + 0.1
        = 1.9 - q
    when 1 + q > 2p + 0.22q + 0.22
        0.78q > (2p - 0.78)
    and 1.9 - q > -2p - 0.22q + 2.418
        0.78q < 2p - 0.518
    pact is a dominate strategy for both parties
d.  the new rule encouraged some teams into a pact 

18.1
typical reinforced learning
improvement:
    the ability to recognize and speak words in a language
    (the whold idea of "learning a language" is too complicated)
prior knowledge:
    none
data representation:
    visual and sound input
    e.g. the spoken word "firehydron" and a real firehydron in front of it
output and feedback:
    a certain viberation pattern of vocal folds
    mental/physical rewards and punishment from trainers

18.2
reinforced learning when learning tennis from immediate matches
supervised learning when learning tennis from a coach 
improvement:
    tennis skill (mainly performance in a match)
prior knowledge:
    none
data representation:
    in matches: visual, sense of movement
    in coaching: the same
output and feedbacks:
    in matches: match outcome
    in coaching: reward/punishment from coach

18.3
since the training data is generated from the decision tree
it's consistent with the decision tree and error-free
when the training-set size goes to infinity, each possible combination of input attributes are generated 
the trained tree will be consistent with all the input data, hence equivalent to the original decision tree

18.4
a.  at a leaf node it has to return Positive or Negative
    the error rate is (1 - C) / (p + n), where C = p or n
    the minimum error rate is min(p / (p + n), n / (p + n)) = min(p, n) / (p + n) = 1 - max(p, n) / (p + n)
    by choosing to return the majority, C = max(p, n), error rate = (1 - max(p, n)) / (p + n) is minimized
b.  let a be the fixed output
    L2  = Σ(yk - a)^2
        = (p(1 - a)^2 + na^2) / (p + n)
    dL2/da  = (-2p(1 - a) + 2na) / (p + n)
            = (2pa - 2p + 2na) / (p + n)
            = 0
    dL2/da is non-decreasing => L2 has a global minimum
    (2p + 2n)a = 2p
    a = p / (p + n) minimizes L2

18.5
/// thanks solutions manual
/// the calculation on solutions manual is somehow incorrect 
dB(p)/dp = lg(p/(1-p)) / ln2
Gain    = B(p/(p + n)) - Σ(pk + nk)B(pk / (pk + nk)) / (p + n)
subject to Σpk = p, Σnk = n, the Lagrange function is 
L = B(p/(p + n)) - Σ(pk + nk)B(pk / (pk + nk))/(p + n) - λ1(Σpk - p) - λ2(Σnk - n)
dL/dpk  = -(1/(p + n))d(pk + nk)B(pk / (pk + nk))/dpk - λ1 = 0
-B(pk / (pk + nk)) - (pk + nk)lg(pk/nk)/ln2 * d(pk/(pk + nk))/dpk = λ1(p + n)
1.  -B(pk / (pk + nk)) - (pk + nk)lg(pk/nk)/ln2 * nk/(pk + nk)^2 = λ1(p + n)
dL/dpk  = -(1/(p + n))d(pk + nk)B(pk / (pk + nk))/dnk - λ2 = 0
-B(pk / (pk + nk)) - (pk + nk)lg(pk/nk)/ln2 * d(pk / (pk + nk))/dnk = λ2(p + n)
2.  -B(pk / (pk + nk)) - (pk + nk)lg(pk/nk)/ln2 * (-pk/(nk + pk)^2) = λ2(p + n)
substract 1. and 2.
-(pk + nk)lg(pk/nk)/ln2 * (pk + nk)/(pk + nk)^2 = (λ1 - λ2)(p + n)
-lg(pk/nk) = ln2(λ1 - λ2)(p + n)
the right side of the equation are independent to k
therefore on stationary points pi/ni = pj/nj for all i,j
for fixed sequence of sk = pk + nk, pk/nk = p/n for all k is the only stationary point of Lagrange function
on which Σ(pk+nk)/(p+n)B(pk / (pk + nk)) = B(p/(p+n)), Gain = 0
assume p > 0 && n > 0, all subsets have uniform classification, Gain = B(p(p + n)) - 0 > 0
therefore pk/nk = p/n is a global minimum, all other points have positive Gain

18.6
./AI/classification
In examples ["x2", "x3", "x5", "x1", "x4"]
Attribute A2 has minimum remaining entropy 0.5509775004326938
In examples ["x3", "x5", "x4"]
Attribute A1 has minimum remaining entropy 0

18.7
a.  an n-bit XOR decision tree must have 2^n nodes:
        XOR cannot be decided without evaluating all bits, each path from root to leaf must has length >= n
        every bit may affect the final XOR, each node must have two children
b.  an n-bit XOR decision graph may have only n+1 nodes, where the last node has all other nodes as parents
        XOR cannot be decided without evaluating all bits, each path from root to leaf must has length >= n
        a node with all other nodes as parents can compute XOR of the string
    
18.8
a.  truth table of 2-bit XOR
    top-down X^2 pruning will return a constant tree with error rate 50%
    bottom-up X^2 pruning will not remove any node
b.  ./AI/classification/src/decision_tree.rs

18.9
a.  // code skipped
    // each node now keeps a dictionary of weights of attributes
    // calculated from examples filtered along the path
b.  simply calculate the Gain(A) after filtering out all missing values 

18.10
information content: lg(|A|)
GainRatio(A) = Gain(A) / lg(|A|)
./AI/classification/src/decision_tree.rs

18.11
in leave-one-out cross-validation, if the chosen test example is positive:
    the training set contains 99 positive example and 100 negative example
    the majority classifier will return Negative, fail on the only test example
if the chosen test example is negative:
    symmetric

18.12
A1 = 1 => Output = 1
A3 = 1 & A4 = 0 => Output = 0
A2 = 0 => Output = 1
Output = ~A4

18.13
each path from root to a leave represents a sequence of (attribute, value) pairs
create a node in decision list for each leaf in the tree with the corresponding sequence of pairs
return the classification of the leaf or move to the next node
for an arbitrary input I, the decision tree returns output O
there is a unique path of (attribute, value) pairs the input traversed in the tree
any other paths to another leaf will have different pairs
so only the test in the corresponding node in the decision list will match I, returning the same output O
example: 2-bit XOR
    a minimal decision tree has 4 leaves
    a minimal decision list has 2 test nodes (A1 = A2 = 0 and A1 = A2 = 1)

18.14
a.  enumerate each row of the truth table
    2^n test nodes
b.  the same to 18.13
    length of the sequence of pairs is limited by the depth k

18.15
L1:
    7, sort than take the 4th value
    if there are even number of values, anything between n/2th and n/2+1th value is correct
    median
L2:
    dΣ(yi - m)^2/dm = -Σ2(yi - m) = 2nm - 2Σyi = 0
    m = Σyi / n is the only stationary point
    as 2nm - 2Σyi is non-decreasing, m = Σyi / n is a global minimum
    mean

18.16
a.  (x1 - a)^2 + (x2 - b)^2 - r^2 = 0
    x1^2 - 2ax1 + a^2 + x2^2 - 2bx2 + b^2 - r^2 = 0
    -2ax1 - 2bx2 + x1^2 + x2^2 = r^2 - a^2 - b^2
    hence for -2ax1 - 2bx2 + x1^2 + x2^2 <= r^2 - a^2 - b^2, point (x, y) are in the circle
    with w = [-2a, -2b, 1, 1], input is mapped to a 1-d space with a clear boundary r^2 - a^2 - b^2
b.  c(x1 - a)^2 + d(x2 - b)^2 - 1 = 0
    cx1^2 - 2cax1 + ca^2 + dx2^2 - 2dbx2 + db^2 - 1 = 0
    -2cax1 - 2dbx2 + cx1^2 + dx2^2 = 1 - ca^2 - db^2
    w = [-2ca, -2db, c, d, 0]
    4-demension will suffice

18.17
[1, 1]      => [1, 1], 0
[1, -1]     => [1, -1], 1
[-1, 1]     => [-1, -1], 1
[-1, -1]    => [-1, 1], 0
maximal margin seperator: x1x2 = 0
margin is 2
in the original input space, the seperator is the x and y axes

18.18
more than K/2 + 1 hypothesis must be wrong
ε' = ε^(K/2 + 1)
/// calculation omitted
if the ensemble hypothesis has error rate ε' 
the minimum average error rate of K hypotheses is ε'(K/2 + 1) / K < ε'
one possible way to achieve this lower bound is a group of hypothesis where 
among a total K inputs, either all hypotheses return the correct output
or exactly K/2 + 1 hypotheses return the wrong output, and the error is evenly distributed among hypotheses

18.19
a single NN node can do linear seperable classification
both AND and OR are linear seperable
given input x1 and x2,
    define a node n1 computing x1 & x2
    define a node n2 computing x1 | x2
    define a node n3 computing ~n1 & n2
output of n3 should be the XOR function

18.20
/// refer Rojas, R. (1996). Neural Networks: A Systematic Introduction.
2Σi C(2^n - 1, i)

18.21
p = 1 / (1 + e^-Σwx)
dlogp/dwi   = -dlog(1/p)/dwi
            = -p d(1 + e^-Σwx)/dwi
            = p * e^-Σwx * xi
            = xi * e^-Σwx / (1 + e^-Σwx)
            = xi(1 - p)
dlog(1-p)/dwi   = dlog(e^-Σwx / (1 + e^-Σwx))/dwi
                = (1 + 1 / e^-Σwx)d(1-p)/dwi
                = -(1 + 1 / e^-Σwx)(1 / (1 + e^-Σwx)^2)d(e^-Σwx)/dwi
                = (1 + 1 / e^-Σwx)(1 / (1 + e^-Σwx)^2)e^-Σwx * xi
                = xi(1 + e^-Σwx)(1 / (1 + e^-Σwx)^2)
                = xi(1 / (1 + e^-Σwx))
                = xip
/// thanks solutions manual
when y = 1, the positive likelihood is included, p^y = p
when y = 0, p^y = 1, the positive likelihood is ignored
similarly for negative likelihood
L   = log(p^y * (1-p)^(1-y))
    = -ylogp - (1-y)log(1-p)
dL/dwi  = -yxi(1-p) + (1-y)xip
        = xi(-y + yp + p - yp)
        = xi(p - y)
        = -xi(y - p)
which is similar to linear gradient decendent

18.22
a.  assume nodes in the hidden layer have weights wi = [wk,i] and constant ci
    for each output node j, let ai be the input from ith node in the hidden layer
    let wi,j be the weight of ai
    output of the node j is
        gj(Σi wi,jai)
        = cj(Σi wi,jai)
        = cjΣi wi,jgi(wix)
        = cjΣi wi,j * ciwix
        = cjΣi wi,j * ciΣk wk,i * xk
        = cjΣi,k ci * wi,j * wk,i * xk
        = cjΣk (Σi ci * wi,j * wk,i) * xk
    the output node is equivalent to a node that
        takes input directly from input nodes
        has weights wk,j = Σi ci * wi,j * wk,i
b.  the same to part a
    inductively, each transformation will eliminate the first hidden layer
c.  before: 
        h hidden nodes each has n weights
        n output nodes each has h weights
        2nh in total
    after:
        n output nodes each has n weights
        n^2 in total
    when h << n, the number of weights is almost quadratic after the transformation

18.23
assume the only output node has weights wj,k and inputs aj
assume L2 loss function
Errk    = 0.8(1 - hw)^2 + 0.2hw^2
        = 0.8hw^2 - 1.6hw + 0.8 + 0.2hw^2
        = hw^2 - 1.6hw + 0.8
Errk is convex
at global optimum, hw must minimize the Errk on the same input
dErrk/dhw = 2hw - 1.6 = 0
hw = 0.8

18.24
/// skipped

18.25
a.  if the three points are not colinear, they form a triangle on the plane
    if one is positive and the other two are negative (or vice versa):
        connecting mid points of two sides of the triangle
        there is a line seperating any one of them from the other two
    if all three of them are positive / negative:
        there is a line having all three of the points on one side
b.  XOR function
c.  if the four points are not coplanar, they form a tetrahedron
    any one or two vertices can be seperated from other three by a plane
d.  a tetrahedron with positive vertices, and a negative point inside it
    any planar seperating the point inside and a vertex 
    must have some other vertices on the same side with the negative point
e.  /// skipped

19.1
Nationality(x, n) & Nationality(y, n) & Language(x, l) => Language(y, l)
~Nationality(x, n) | ~Nationality(y, n) | ~Language(x, l) | Language(y, l) {x/Fernando, n/Brazil, l/Portuguese}
Nationality(Fernando, Brazil) & Language(Fernando, Portuguese)
=>  ~Nationality(y, Brazil) | Language(y, Portuguese)
=>  Nationality(y, Brazil) => Language(y, Portuguese)
=>  Nationality(x, Brazil) => Language(x, Portuguese)

19.2
a.  false: the coin may be a fake with the same design and denomination but different material / weight
b.  false: the program may be random or has side effects
c.  false: human body is still a black box to modern medical science
d.  false: no evidence

19.3
Isn't that just Bayesian network

19.4
a.  C1 = ~P(x, y) | Q(x, y)
    exist no C1, C2 => C
b.  C1 = P(A, B) | ~Q(A, B), C2 = Q(A, B)
c.  C = ~P(x, y) | P(x, f(y))
    C1 = ~P(x, y) | P(x, f(y)) | Q(x, y)
    C2 = ~P(x, y) | P(x, f(y)) | ~Q(x, y)

19.5
/// thanks solutions manual
/// this problem is asking the behavior of a prolog program
/// skipped

19.6
a.  with previously mentioned variables only:
        5^2 = 25
    with one new variable (the other must be one of the 5 variables):
        5 + 5 = 10
    35 * 2 = 70 in total with negation
b.  at most r - 1 new variables may be introduced by the literal
    each parameter of the predicate may be one of the (n + r - 1) variables
        (n + r - 1)^r
    exclude the cases where the new literal contains no old variables 
        (r - 1)^r
    2((n + r - 1)^r - (r - 1)^r) in total
c.  if the literal has anything to do with the definition of the new predicate
    there must be some combination of legal additions that connects it to the variables mentioned in the new predicate
    otherwise it's irrelavent and should not be considered

19.7
/// skipped, FOIL is abandonware now

20.1
./AI/stat_learn
Hypothesis: p(cherry) = 1.00, p(lime) = 0.00
P(hi | d1, .., dN)
⡁       ⡠⠔⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 1.0
⠄     ⡔⠉
⠂    ⡜
⡁   ⡔⠁
⠄  ⢸
⠂  ⡎
⡁ ⢠⠃
⠄ ⢸
⠂ ⡇
⣹⣀⡇
⡼⡼⠘⡄
⣇⠷⡀⢱
⣿ ⡇ ⠑⡄
⣿ ⠸⡀ ⠘⢄
⡟⠤⡀⠱⣀⡀ ⠉⠢⢄⣀⣀
⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 0.0
0.0                                                      100.0
P(DN+1 = lime | d1, .., dN)
⡇                                                             0.5
⡇
⡇
⡇
⢧
⢺
⣹
⠄⡇
⠂⢸
⡁ ⡇
⠄ ⢣
⠂ ⠘⡄
⡁  ⢣
⠄   ⠑⡄
⠂    ⠈⠢⠤⣀⣀
⠁         ⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 0.0
0.0                                                      100.0
Hypothesis: p(cherry) = 0.75, p(lime) = 0.25
P(hi | d1, .., dN)
⣱⡇      ⢀⠤⠒⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 1.0
⢼⡇     ⡠⠃
⢺⡇   ⡀⡸
⣹⡇  ⡜⣇⠇
⢼⡇ ⢰⠁⢿
⡞⡇ ⡜
⡇⣇⡄⡇
⡇⢻⢹
⡇⣸⡸⡄
⣿⣿⡇⢇
⡽⢿⠃⢸ ⣴
⡇⢸  ⢇⡟⡄
⡇⢸  ⠈⠃⢣
⣧⢸⡄   ⠈⠢⡀
⡟⢾⠱⡀    ⠑⠤⣀⣀⣀
⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 0.0
0.0                                                      100.0
P(DN+1 = lime | d1, .., dN)
⡇                                                             0.5
⡇
⡇
⡇⢠⡆
⡇⢸⡇
⡇⢸⢇
⡇⢸⢸
⡇⢸⢸
⡇⡸ ⡇
⣹⡇ ⢇
⢼⡇ ⢸ ⣴
⢺⡇  ⢣⡟⡄
⣹⡇   ⠁⢣
⠼⡇    ⠈⠢⡀
⠂       ⠑⠤⣀⣀⣀
⠁            ⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 0.2
0.0                                                      100.0
Hypothesis: p(cherry) = 0.50, p(lime) = 0.50
P(hi | d1, .., dN)
⣿ ⣾         ⢀⠤⠤⠤⣀ ⡠⣀⡠⣀⠤⣀⣄⠤⡔⠔⠒⠒⠊⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠒⠒⠊⠉⠒⠔⠒⢤⠢⠒⠒⠒⠉⠉⠁ 1.0
⣿ ⣿     ⢀⡔⠢⡎⠃    ⠻  ⠁⠈ ⠙⠈
⣿ ⣿   ⢀⢠⠃
⣿ ⣿ ⢀⢤⡟⠇
⣿⢰⢹⡠⠎⠈⠃
⣿⢸⢸⡇
⣿⡧⣼⡇
⣿⠇⠙⡇
⢻  ⡇
⣹  ⡇
⢼  ⢸ ⡀
⢺⡀ ⢸⡸⣧⡆   ⢀⡄     ⣴
⣿⢇ ⢸⢧⡟⢷⢇⢰⡇⡎⣧⡆   ⢀⢿  ⡀⣠ ⣴
⣿⠘⡄⡇⠈⢻⣴⠈⡎⠻ ⠙⢣  ⢠⠃ ⣇⠎⢧⠻⡠⠛⢾⢀⡄⡀                         ⣠⢀
⣿ ⠙   ⠉⠊⠘⠴⣀⣀⣈⣢⣤⠃  ⠈      ⠁⠙⠣⣀⠴⣀⣀⣀⣀         ⣀⣀⣀⣀⠴⣀⣀⠔⠣⠒⠉⠛⠤⡠⢆⣀⢀⡀
⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 0.0
0.0                                                      100.0
P(DN+1 = lime | d1, .., dN)
⡁    ⢰⡇⡀                                                      0.6
⠄  ⢀⡄⢸⣿⢇
⠂  ⢸⢇⡎⠙⠸⡀
⡁  ⢸⢸⡇  ⡇     ⡀
⣷⡇ ⡎ ⠁  ⢸⣾ ⢀⡰⠉⠘⢄  ⣠ ⢀ ⣀ ⡀⣀⡠⡠⠒⠢⠒⠒⠒⠒⠒⠒⠒⠉⠉⠊⠉⠒⠒⠒⠒⠒⠒⠤⠒⠒⠢⠔⠤⣀⢄⠤⠒⠔⠒⠒⠂
⣿⡇ ⡇    ⠈⠃⠣⡏⠃   ⠑⣤⠃⠉⠃⠻ ⠻⠙ ⠁                          ⠈
⣿⢸ ⡇             ⠈
⣿⢸⢀⠇
⣿ ⣿
⣿ ⣿
⣿ ⣿
⣿ ⠙
⣿
⣿
⣿
⠉                                                             0.3
0.0                                                      100.0
Hypothesis: p(cherry) = 0.25, p(lime) = 0.75
P(hi | d1, .., dN)
⡇       ⣾        ⡠⠒⠒⠢⠔⠤⢆⠔⠒⠒⠒⠉⠊⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 1.0
⡇       ⣿   ⣴ ⡀⡰⠉
⡇     ⣠ ⡏⡆ ⢰⠙⠎⣧⠃
⡇    ⢀⢿ ⡇⡇ ⡇  ⠈
⡇  ⣰⡇⢸⠘⣴⠁⢇⡜
⣷  ⣿⢱⡇ ⢿ ⢸⠁
⣿ ⢸⢸⢸⡇ ⡾⡀⣸
⣿⡄⡎⢸⡇⢱ ⡇⡇⡇⡇
⣿⢇⡇⢸⡇⢸⢰⠁⣷⠁⢸
⣿⣸⡀⢸ ⠈⣾ ⠻  ⡇
⣿⠇⡟⣼  ⢿    ⢇  ⣾
⣿ ⢇⢹       ⢸⢰⢇⡟⡄
⡏⡇⢸⢸        ⢿⠈⠃⢣
⡇⢸ ⣿           ⠈⠢⡀
⡇ ⢇⢸⣀⢄  ⣀⡀       ⠑⢄⣀⠎⠢⠒⠎⠢⣀⡠⣀⣀⣀
⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 0.0
0.0                                                      100.0
P(DN+1 = lime | d1, .., dN)
⡁  ⣾                                                          0.9
⠄ ⢰⢹
⠂ ⡸⢸
⡁ ⡇⢸
⠄⢠⠃⢸
⠂⡸ ⢸
⡁⡇ ⢸        ⢀  ⢀⠤⠔⠊⠉⠒⠊⠒⠢⠒⠉⠑⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁
⢼  ⢸  ⣠    ⡰⠙⠔⢧⠊
⢺  ⠈⡆⢠⠻⡀  ⡠⠃
⡹   ⠸⡎ ⢣ ⢰⠁
⡇       ⡇⡇
⡇       ⢿
⡇
⡇
⡇
⠁                                                             0.5
0.0                                                      100.0
Hypothesis: p(cherry) = 0.00, p(lime) = 1.00
P(hi | d1, .., dN)
⣿       ⡠⠔⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 1.0
⣿ ⡀  ⢀⠔⠉
⣿⠉⢣  ⡎
⣿⡄⢸⢀⠎
⡇⡇ ⡿
⡇⢱ ⣧
⡇⢸⢸⢸
⣧ ⡏⠈⡆
⣿ ⡇ ⠸⡀
⣿⡸⡇  ⡇
⣿⠇⢸  ⢱
⣿ ⢸  ⠈⡆
⡏⡇ ⡇  ⠸⡀
⡇⢸ ⢱   ⠈⢆
⡇ ⢇ ⠑⢄⡀  ⠑⠤⢄⣀⣀
⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 0.0
0.0                                                      100.0
P(DN+1 = lime | d1, .., dN)
⡁     ⡠⠤⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁ 1.0
⠄   ⢀⠎
⠂  ⡰⠁
⡁ ⢀⠇
⠄ ⡸
⠂ ⡇
⡁⢰⠁
⠄⡎
⢲⠁
⣹
⡼
⡇
⡇
⡇
⡇
⠁                                                             0.5
0.0                                                      100.0

20.2
/// thanks solutions manual
assume there are N candies in the bag & Bob is aware of the content of all the unwrapped candies just like Ann
a state (C, L) is determined by the numbers of unwrapped candies with cherry flavor (C) and lime flavor (L)
H, posterior distribution of hypotheses is determined by (C, L)
UB, value of the remaining bag of candy to Bob is determined by H and N - (C + L)
by conditional independence UB can be computed from C, L, UB = UB(C, L)
UA involves one more parameter: the decision A made at state (C, L)
let QA(C, L, D) be the expected utility of A with decision D
    QA(C, L, unwrap) = (caP(cherry | H)P(H | C, L) + UA(C+1, L)) + (la(lime | H)P(H | C, L) + UA(C, L+1))
    QA(C, L, sell) = UB(C, L)
and UA = max(D){ QA(C, L, D) }
it's a dynamic programming problem with boundary conditions QA(C, L) = 0 where C + L = N

20.3
before research:
    the first statistician will take both drugs
    the second statistician will take only the drug for disease B
after research:
    the first statistician will take both drugs
    the second statistician will take only the drug for disease A

20.4
when computing the posterior distribution of C,
    P(C | x) = αP(C)Πxi P(xi | C)
by associate P(xi | C) with a weight wi, iteration step of boosting method can be performed on naive Bayes learning
with K posterior distributions of P(C | x), the ensemble can be computed as a normalized weighted sum of them

20.5
L = Σi -log(2π)/2 - logσ - (yi - (θ1xi + θ2))^2 / 2σ^2)
dL/dθ1 = d-(yi - (θ1xi + θ2))^2/dθ1 = 0
dL/dθ2 = d-(yi - (θ1xi + θ2))^2/dθ2 = 0
maximizing L on θ1, θ2 is just linear regression on (yi, xi)
    θ1 = (NΣxiyi - ΣxiΣyi) / NΣxi^2 - (Σxi)^2
    θ2 = (Σyi - θ1Σxi) / N
dL/dσ is the same to univariate Gaussian distribution
    σ = (Σ(xi - Σxi / N)^2 / N)^(1/2)

20.6
let yi and xij take values {0, 1}
p(fever | xi) = 1 - Πj qj^xij
P(y | x)    = Πp(fever | xi)^yip(~fever | xi)^(1 - yi)
            = Π(1 - Πj qj^xij)^yi(Πj qj^xij)^(1 - yi)
L   = log(P(y | x))
    = Σ(yilog(1 - Πj qj^xij) + (1 - yi)Σxijlogqj)
dL/dqj  = Σ(yi * xij/(1 - Πj qj^xij) * qj(xij - 1) + (1 - yi)xij/qj) = 0 for all j

20.7
/// thanks wolframAlpha
a.  beta[a, b](θ) = αθ^(a - 1)(1 - θ)^(b - 1)
    int(0, 1, beta[a, b](θ)) = Γ(a)Γ(b) / Γ(a + b)
    α = 1 / int(0, 1, beta[a, b](θ)) = Γ(a + b) / Γ(a)Γ(b)
b.  E[beta[a, b](θ)]
    = int(0, 1, θbeta[a, b](θ))
    = αΓ(a + 1)Γ(b) / Γ(a + b + 1)
    = αaΓ(a)Γ(b) / (a + b)Γ(a + b)
    = a / (a + b)
c.  d(beta[a, b](θ))/dθ
    = α((a-1) * θ^(a-2) * (1-θ)^(b-1) - (b-1) * θ^(a-1) * (1-θ)^(b-2))
    = αθ^(a-2)(1-θ)^(b-2)((a-1) * (1-θ) - (b-1) * θ)
    = 0
    a - θa - 1 + θ - θb + θ = 0
    a - 1 = θ(a + b - 2)
    θ = (a - 1) / (a + b - 2)
d.  beta[ε, ε](θ)   = αθ^(ε - 1)(1 - θ)^(ε - 1)
                    ≒ α/θ(1-θ)
    ./AI/stat_learn
    ⣹                                                          ⡎  120.4
    ⢼                                                          ⡇
    ⢺                                                          ⡇
    ⣹                                                          ⡇
    ⢼                                                          ⡇
    ⢺                                                          ⡇
    ⣹                                                          ⡇
    ⠼⡀                                                         ⡇
    ⠂⡇                                                        ⢸
    ⡁⡇                                                        ⢸
    ⠄⢱                                                        ⡜
    ⠂⠘⡄                                                      ⢀⠇
    ⡁ ⠱⡀                                                     ⡜
    ⠄  ⠑⢄⡀                                                 ⣀⠜
    ⠂    ⠈⠑⠒⠤⠤⠤⣀⣀⣀⣀⣀⣀                          ⢀⣀⣀⣀⣀⣀⡠⠤⠤⠔⠒⠉
    ⠁                ⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁                  4.0
    0.0                                                      1.0
    the distribution is almost concentrated around 0.0 and 1.0
    after an update, the distribution collapses to one side
    ⡁                                                          ⡎  119.4
    ⠄                                                          ⡇
    ⠂                                                          ⡇
    ⡁                                                          ⡇
    ⠄                                                          ⡇
    ⠂                                                          ⡇
    ⡁                                                          ⡇
    ⠄                                                         ⢀⠇
    ⠂                                                         ⢸
    ⡁                                                         ⢸
    ⠄                                                         ⡎
    ⠂                                                        ⢠⠃
    ⡁                                                       ⢀⠎
    ⠄                                                     ⣀⠤⠊
    ⠂                             ⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠤⠤⠒⠒⠊⠉
    ⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉                                1.0
    0.0                                                      1.0

20.8
by adding a link between node X and its new parent Yi
the CPT of X now contains a new entry 
by setting P(X | Y, Yi = yi) = P(X | Y) for all yi, the new network is equivalent to the old network
the maximum likelihood of the old network is a feasible likelihood of the new network
the maximum likelihood of the new network therefore is equal or greater than the maximum likelihood of the old network

20.9
a.  D = π^p * (1-π)^n
b.  L = plogπ + nlog(1-π)
    dL/dπ = p/π - n/(1-π) = 0
    p(1-π) - nπ = 0
    π(p + n) = p
    π = p / (p + n)
c.  the naive Bayes model: attributes are leaves and the class is the only root, reverse to the causal relation
d.  P(Xk, Yk)   = P(Xk | Yk)P(Yk)
                = ΠP(Xki | Yk)P(Yk)
    if Yk = 0,
                = (1-π)ΠP(Xki | Yk = 0)
                = (1-π)Π(Xki = 0)(1 - αi)Π(Xki = 1)αi
    if Yk = 1,
                = πΠP(Xki | Yk = 1)
                = πΠ(Xki = 0)(1 - βi)Π(Xki = 1)βi
    D   = ΠP(Xk, Yk)
        = π^p * (1-π)^n * Π(αi^pi+ * (1-αi)^ni+ * βi^pi- * (1-βi)^ni-
e.  L   = plogπ + nlog(1-π) + Σ(pi+logαi + ni+log(1-αi) + pi-logβi + ni-log(1-βi))
    dL/dπ didn't change, L is still maximized at π = p / (p + n)
    dL/dαi  = pi+/αi - ni+/(1-αi) = 0
    αi = pi+ / (pi+ + ni+)
    dL/dβi  = pi+/βi - ni+/(1-βi) = 0
    βi = pi+ / (pi+ + ni+)
f.  (p, n) = (2, 2)
    (p1+, n1+, p1-, n1-) = (1, 1, 1, 1)
    (p2+, n2+, p2-, n2-) = (1, 1, 1, 1)
    π = 1/2
    (α1, β1) = (1/2, 1/2)
    (α2, β2) = (1/2, 1/2)
g.  P(Y = true| x1, x2) = 1/2 for each example

20.10
a.  /// thanks solutions manual
    with two observable attributes, the data has 4 categories and 3 degree of freedom
    there are 5 free parameters in the model, which cannot be recovered from data with 3 degree of freedom
b.  ./AI/bayes
    θ(1) = 0.6124306106264775
    θF1(1) = 0.6684082742546456
    θF2(1) = 0.38869507391681696
    θW1(1) = 0.6483118060276541
    θW2(1) = 0.38174842702950806
    θH1(1) = 0.6558479816127738
    θH2(1) = 0.3827408051562639
c.  ./AI/bayes
    parameters are fixed starting from the first iteration
d.  /// skipped

21.1
./AI/mdp/src/learn.rs

21.2
a simple example world: 
    two states {SA, SB}, B is terminal
    a single action A on SA
    P(SB | SA, A) = ε, P(SA | SA, A) = 1 - ε
when ε is small enough, with sufficently few number of trials the transition (SA, A) -> SB may not be observed
a policy [Some(A), None] is proper in the real model but not in the learned transition model
when policy is proper & evaluated only at the end of a trial, 
there is a sequence of actions leading every state in the trial to a terminal state with positive probability
states not visited by any trial will be terminal states themselves in learned model

21.3
/// skipped
/// unfortunately the current abstraction doesn't support efficient enumeration of predecessors

21.4
θ0 = θ0 + α(u(S(x, y)) - U(x, y))
θ1 = θ1 + α(u(S(x, y)) - U(x, y))x
θ2 = θ2 + α(u(S(x, y)) - U(x, y))y
θ3 = θ3 + α(u(S(x, y)) - U(x, y))((x - xg)^2 + (y - yg)^2)^(1/2)

21.5
./AI/mdp
a.  exact utils:
    0.812   0.868   0.918   1.000
    0.762   0.000   0.660   -1.000
    0.705   0.655   0.611   0.388
    tp utils:
    0.801   0.850   0.893   1.000
    0.753   0.000   0.538   -1.000
    0.705   0.649   0.593   0.426
    linear tp utils:
    0.919   0.947   0.976   1.005
    0.775   0.803   0.832   0.861
    0.631   0.659   0.688   0.717
b.  exact utils:
    0.509   0.563   0.616   0.670   0.724   0.778   0.833   0.888   0.944   1.000
    0.482   0.534   0.586   0.638   0.689   0.741   0.792   0.843   0.894   0.944
    0.440   0.491   0.541   0.592   0.642   0.693   0.743   0.793   0.843   0.888
    0.396   0.443   0.493   0.543   0.593   0.643   0.693   0.743   0.792   0.833
    0.351   0.393   0.443   0.493   0.543   0.593   0.643   0.693   0.741   0.778
    0.306   0.345   0.393   0.443   0.493   0.543   0.593   0.642   0.689   0.724
    0.261   0.296   0.343   0.393   0.443   0.493   0.543   0.592   0.638   0.670
    0.215   0.248   0.294   0.343   0.393   0.443   0.493   0.541   0.586   0.616
    0.169   0.200   0.248   0.296   0.345   0.393   0.443   0.491   0.534   0.563
    0.124   0.169   0.215   0.261   0.306   0.351   0.396   0.440   0.482   0.509
    tp utils:
    0.450   0.541   0.617   0.675   0.720   0.783   0.836   0.908   0.942   1.000
    0.365   0.498   0.564   0.633   0.675   0.748   0.802   0.845   0.899   0.953
    0.393   0.481   0.532   0.590   0.640   0.691   0.742   0.789   0.846   0.901
    0.301   0.417   0.498   0.545   0.597   0.640   0.694   0.739   0.798   0.843
    0.191   0.276   0.379   0.471   0.530   0.582   0.648   0.700   0.753   0.797
    0.061   0.148   0.272   0.401   0.473   0.538   0.605   0.654   0.707   0.727
    -0.080  -0.008  0.124   0.046   0.366   0.476   0.541   0.606   0.651   0.657
    -0.240  -0.198  -0.068  0.144   0.274   0.405   0.504   0.549   0.577   0.615
    -0.355  -0.169  -0.046  0.097   0.216   0.322   0.428   0.500   0.487   0.537
    -0.233  -0.170  -0.085  0.041   0.174   0.271   0.355   0.428   0.351   0.239
    linear tp utils:
    // didn't converge, there must must be some design mistake in my implementation
c.  exact utils:
    0.554   0.592   0.636   0.680   0.705   0.680   0.636   0.592   0.547   0.502
    0.599   0.638   0.686   0.732   0.761   0.732   0.686   0.636   0.587   0.547
    0.644   0.687   0.736   0.784   0.819   0.784   0.736   0.686   0.636   0.592
    0.689   0.736   0.786   0.836   0.877   0.836   0.786   0.736   0.686   0.636
    0.733   0.784   0.836   0.887   0.937   0.887   0.836   0.784   0.732   0.680
    0.761   0.819   0.877   0.937   1.000   0.937   0.877   0.819   0.761   0.705
    0.733   0.784   0.836   0.887   0.937   0.887   0.836   0.784   0.732   0.680
    0.689   0.736   0.786   0.836   0.877   0.836   0.786   0.736   0.686   0.636
    0.644   0.687   0.736   0.784   0.819   0.784   0.736   0.687   0.638   0.592
    0.599   0.644   0.689   0.733   0.761   0.733   0.689   0.644   0.599   0.554
    tp utils:
    0.461   0.489   0.590   0.661   0.696   0.672   0.628   0.568   0.487   0.371
    0.561   0.583   0.665   0.718   0.773   0.745   0.687   0.627   0.561   0.516
    0.620   0.667   0.737   0.775   0.828   0.795   0.722   0.671   0.636   0.586
    0.678   0.741   0.788   0.838   0.885   0.830   0.781   0.734   0.688   0.645
    0.740   0.794   0.844   0.884   0.941   0.885   0.838   0.781   0.721   0.681
    0.752   0.802   0.863   0.935   1.000   0.912   0.863   0.831   0.780   0.700
    0.725   0.750   0.805   0.879   0.927   0.880   0.831   0.793   0.746   0.677
    0.661   0.736   0.778   0.817   0.874   0.832   0.781   0.736   0.686   0.609
    0.575   0.653   0.716   0.769   0.794   0.779   0.737   0.677   0.605   0.507
    0.544   0.612   0.671   0.726   0.755   0.729   0.679   0.623   0.549   0.424
    linear tp utils:
    // didn't converge, as supposed 

21.6
1.  length of shortest path to nearest negative and positive terminals
2.  average branching factor on the paths above

21.7
/// skipped

21.8
Environment 1
0.509   0.563   0.616   0.670   0.724   0.778   0.833   0.888   0.944   1.000
0.482   0.534   0.586   0.638   0.689   0.741   0.792   0.843   0.894   0.944
0.440   0.491   0.541   0.592   0.642   0.693   0.743   0.793   0.843   0.888
0.396   0.443   0.493   0.543   0.593   0.643   0.693   0.743   0.792   0.833
0.351   0.393   0.443   0.493   0.543   0.593   0.643   0.693   0.741   0.778
0.306   0.345   0.393   0.443   0.493   0.543   0.593   0.642   0.689   0.724
0.261   0.296   0.343   0.393   0.443   0.493   0.543   0.592   0.638   0.670
0.215   0.248   0.294   0.343   0.393   0.443   0.493   0.541   0.586   0.616
0.169   0.200   0.248   0.296   0.345   0.393   0.443   0.491   0.534   0.563
0.124   0.169   0.215   0.261   0.306   0.351   0.396   0.440   0.482   0.509
U(x, y) = 0.0176 + 0.0582x + 0.0582y
Environment 2
0.509   0.563   0.616   0.670   0.724   0.778   0.833   0.888   0.944   1.000
0.482   0.534   0.586   0.638   0.689   0.741   0.792   0.843   0.894   0.944
0.440   0.491   0.541   0.592   0.642   0.693   0.743   0.793   0.843   0.888
0.396   0.443   0.493   0.543   0.593   0.643   0.693   0.743   0.792   0.833
0.351   0.393   0.443   0.493   0.543   0.593   0.643   0.693   0.741   0.778
0.306   0.345   0.393   0.443   0.493   0.543   0.593   0.642   0.689   0.724
0.261   0.296   0.343   0.393   0.443   0.493   0.543   0.592   0.638   0.670
0.215   0.248   0.294   0.343   0.393   0.443   0.493   0.541   0.586   0.616
0.169   0.200   0.246   0.295   0.344   0.393   0.442   0.481   0.442   -1.000
0.124   0.165   0.211   0.256   0.301   0.346   0.390   0.422   0.380   0.182
U(x, y) = 0.0151 + 0.0456x + 0.0665y
Environment 3
0.387   0.000   0.000   0.585   0.633   0.679   0.724   0.000   0.944   1.000
0.437   0.496   0.550   0.604   0.661   0.719   0.780   0.844   0.894   0.944
0.414   0.466   0.518   0.570   0.622   0.673   0.723   0.000   0.844   0.000
0.371   0.421   0.472   0.522   0.572   0.622   0.668   0.000   0.787   0.731
0.327   0.373   0.423   0.472   0.522   0.572   0.619   0.669   0.725   0.686
0.282   0.327   0.378   0.423   0.472   0.522   0.569   0.618   0.666   0.639
0.237   0.273   0.000   0.378   0.423   0.471   0.519   0.567   0.608   0.591
0.191   0.225   0.273   0.329   0.373   0.421   0.469   0.516   0.554   0.000
0.145   0.176   0.217   0.000   0.329   0.372   0.418   0.456   0.416   -1.000
0.100   0.138   0.183   0.229   0.279   0.322   0.366   0.397   0.355   0.160
U(x, y) = 0.0200 + 0.0388x + 0.0538y
Environment 4
0.504   0.559   0.615   0.671   0.728   0.778   0.833   0.888   0.944   1.000
0.464   0.514   0.565   0.615   0.000   0.741   0.792   0.843   0.894   0.944
0.420   0.465   0.514   0.559   0.000   0.697   0.743   0.793   0.843   0.888
0.375   0.416   0.464   0.504   0.000   0.652   0.694   0.743   0.792   0.833
0.329   0.367   0.412   0.450   0.000   0.607   0.645   0.693   0.741   0.778
0.284   0.318   0.361   0.395   0.000   0.561   0.597   0.643   0.689   0.724
0.238   0.269   0.310   0.342   0.000   0.515   0.548   0.593   0.638   0.670
0.191   0.220   0.259   0.288   0.000   0.469   0.500   0.543   0.586   0.616
0.145   0.172   0.208   0.235   0.000   0.423   0.450   0.484   0.444   -1.000
0.119   0.166   0.215   0.266   0.320   0.370   0.400   0.425   0.383   0.185
U(x, y) = 0.0044 + 0.0458x + 0.0579y
Environment 5
0.554   0.592   0.636   0.680   0.705   0.680   0.636   0.592   0.547   0.502
0.599   0.638   0.686   0.732   0.761   0.732   0.686   0.636   0.587   0.547
0.644   0.687   0.736   0.784   0.819   0.784   0.736   0.686   0.636   0.592
0.689   0.736   0.786   0.836   0.877   0.836   0.786   0.736   0.686   0.636
0.733   0.784   0.836   0.887   0.937   0.887   0.836   0.784   0.732   0.680
0.761   0.819   0.877   0.937   1.000   0.937   0.877   0.819   0.761   0.705
0.733   0.784   0.836   0.887   0.937   0.887   0.836   0.784   0.732   0.680
0.689   0.736   0.786   0.836   0.877   0.836   0.786   0.736   0.686   0.636
0.644   0.687   0.736   0.784   0.819   0.784   0.736   0.687   0.638   0.592
0.599   0.644   0.689   0.733   0.761   0.733   0.689   0.644   0.599   0.554
U(x, y) = 0.1360 + 0.0665x + 0.0665y

21.9
// skipped

21.10
It's not a proper model for evolution as species has no control over parameters of themselves
the reward signal itself is generated randomly in the process

