1.1
a.  ability to act rationally based on knowledge
b.  intelligence originates not in biomass but in artifacts
c.  actor
d.  make near optimal decisions under certain criteria
e.  inference correct conclusions given a set of facts

1.2
https://www.csee.umbc.edu/courses/471/papers/turing.pdf
The Theological Objection:
    have no familiarity in christianity
    by my buddhist grandma, even the most insignificant kind of bugs have souls equal to human's
    so why not intelligent machines
The "Heads in the Sand" Objection
    this idea is even stronger today fueled by 70 years of sci-fi movies and literatures since then
    threat of a possible "AI domination" scenario is unlikely to stop the creation of AI (if ever possible)
    similar to nuclear weapons, AI technology is too much an advantage to any major power
The Mathematical Objection
    machines cannot solve the halting problem for certain, yet there's no proof whether human can do it better
    modern compilers can sometimes detect infinite loops which are unobvious to human eyes
The Argument from Consciousness
    seems to be the most relevant objection today and Turing's refutations on this topic was quite weak
    the idea was later expanded by the Chinese room argument
    recent breakthrough in ML and AI research made machines more rational, but not necessarily more humanly
Arguments from Various Disabilities
    either solved by now or not falsifiable at first place
Lady Lovelace's Objection
    if creativity on music composing or literature writing can be specified in terms of scores
    ML algorithms can train a machine to do it
    this objection lands more on ambiguity in human feelings / languages than ability of machines
Argument from Continuity in the Nervous System 
    no idea 
The Argument from Informality of Behaviour 
    AI today is already more error-prone than human while e.g. driving a car
The Argument from Extrasensory Perception
    can't believe they took ESP into consideration seriously
    was that common back in 1950s?
it turns out unskilled interrogator is too easy to fool:
    "Artificial Stupidity", The Economist, 324 (7770): 14, 1 August 1992
at this rate no one knows what will possibly happen in 50 years 

1.3
it's rational in the sense that it's based on practical reasons instead of emotions
it's not intelligent as it's not a decision by reasoning

1.4
IQ test is designed for normal human beings, not machines on a special task
the microworld of IQ test is overly restrictive

1.5
10^4 / 10^-3 = 10^7
human power: 10^14

1.6
1.  people lies for various reasons
2.  people who lacks certain knowledges cannot express their thought and emotion correctly
3.  people with satisfactory knowledge of thought and emotion is a biased sample

1.7
a.  pattern recognition for non-natural patterns (artifical barcodes)
b.  pattern recognition (NLP) & decision making (order of entries by preference / collected browse histories)
c.  pattern recognition for natural patterns (human voice)
d.  decision making

1.8
these computational models immitates result of human evolution instead of human knowledge
people don't have to understand the ATP cycle to lift and get fit

1.9
self-preservation as a population
by natural selection, entities without the urge of self-preservation are more likely to be eliminated by the environment
and in most case self-preservation is considered rational

1.10
both the theory and the realization of the theory should be considered AI
recent boom in ML is mostly boosted by powerful video cards 

1.11
the latter is true in the same way to 
"human thoughts are just a bunch of electrical and chemical signals being tossed around inside the brain"
thus the first sentence cannot be derived from the latter

1.12
the latter is not true, human thought and behavior is not that deterministic by gene

1.13
same to 1.11

1.14
List of entries not possible yet:
    h:  NLP or the ability of catching fuzzy ideas as "funny" is still lagging behind other aspects
    k:  not heard, but AI can help surgeons perform better
        by https://publishing.rcseng.ac.uk/doi/pdf/10.1308/rcsbull.2017.87, better computer vision is the missing piece

1.15
//  skipped

2.1
consider a vaccum cleaner with limited battery, in an environment where dust is generated randomly over time
performance is again measured by how clean the floor is on average 
if T is short compared to the battery capacity
the cleaner may choose to search very actively, consume more energy than optimal in the first T time steps
and do no work at all afterwards 

2.2
a.  there are 4 cases for the initial environment, 2 cases of initial place of the vaccum cleaner
    4 * 2 = 8 inital cases in total, where half the cases are symmetric
    assume the cleanliness is asserted at the start of each time step 
    A: Clean, B: Clean, Start: A,
        performance measure is 2000 for any agents
    A: Dirt, B: Clean, Start: A,
        the agent cleans dirt in square A in time step 1, move back and forth afterwards
        performance measure is 1999
        1999 is the optimal performance as the dirty square cannot be cleaned before time step 1
    A: Dirt, B: Clean, Start: B,
        the agent moves to square A at the end of time step 1, clean square A in time step 2
        performance measure is 1998
        1998 is optimal as any agent needs at least 2 time steps to move and clean the dirt in square A
    A: Dirt, B: Dirt, Start: A,
        1: clean A, 2: move to B, 3: clean B
        performance measure is 1996
        1996 is optimal as any agent needs at least 3 time steps to clean the two squares
b.  same to figure 2.3 but stop moving when no square is dirty
    if the agent can observe the cleanliness of both squares, no internal state required
    otherwise the agent has to remember the cleanliness of another square
c.  it may be beneficial to learn the dirt distribution and the geography of the room
    dirts will be cleaned earlier if the agent returns to the square with a highest posibility to generate dirt

2.3
a.  false, the vaccum cleaner agent in figure 2.2 is perfectly rational despite sensing only partial information
b.  true, an environment in which a button should be pushed when two parts are defected in a row
c.  true, a vaccum cleaner world as figure 2.3 in which no dirt are ever generated
d.  false, agent function always takes sequence of percepts, while agent program may be pure reflex
e.  false, it may be phisically impossible due to limitation on storage or computation power
f.  true, where the performance measure is independent to the actions of agents
g.  true, the vaccum cleaner agent in 2.2.b is also rational in the world defined in figure 2.3
h.  false, an agent that never clean squares cannot be rational in the vaccum cleaner world, with sensor or not
i.  false, it's logically impossible: what will happen when two of such agents play against each other?

2.4
a.  playing soccer (as a single player):
        P:  (depends on position) goals, assists, pass success rate, ...
        E:  position, momentum, ... of other players, ball, judges, ...
        A:  all possible moves of a human being
        S:  visual, sound and physical contact
b.  subsurface ocean of Titan (as a rover / submarine):
        P:  completeness and accuracy of aquired data, project cost, (if manned) casualty
        E:  Titan (atmosphere, climate, marine condition, ...)
        A:  sampling devices, thrust, ...
        S:  visual inputs
// skipped

2.5
agent function:         formal definition of the performance logic of an agent
agent program:          implementation of an agent function
autonomy:               ability to complete tasks / make decisions under no supervision
reflex agent:           agent whose action is independent to history of percepts
model-based agent:      agent which simulates unobservable part of environment as interal states from percepts
                        and make decision based on those simulations
goal-based agent:       agent which asserts and changes its actions by a binary performance measure (goal / defeat)
utility-based agent:    agent which asserts and changes its actions by a performance measure function
learning agent: agent   agent which asserts and improves its actions from experiences

2.6
a.  let F be an reflex agent function, P be an agent program implementing F
    let P' be a program that on a percept:
        runs P twice, return the result of the second run
    by definition P' is also an implementation of F
b.  none, ignoring storage and computation limitations, the table-driven method is always possible
c.  yes or no according to a more precise definition of randomized agent function / program
d.  infinite, there are infinite possible implementations of a trivial agent which yields a single action forever 
e.  no, by structure of the program it will only emit an action on percept inputs
    speed up the machine only boost the reaction speed of the agent
    if the agent is a valid implementation of the agent function at the first space it have to wait more

2.7
a.  function GOAL-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    DESIRABLE, boolean function with built-in goal
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if DESIRABLE(state)
                return action'
b.  function UTILITY-BASED-AGENT(percept) returns an action
        persistent: state, the agent's current simulation of the world
                    model, the simulation model used to update state
                    action, the most recent action taken
                    UTILITY, function from state to a performance measure
        state <- UPDATE-STATE(state, action, percept, model)
        for action' in all feasible actions
            // estimate the impact of a chosen action
            state <- UPDATE-STATE(state, action', NULL, model)
            if UTILITY(state) is maximized
                return action'

2.8
./AI/vaccum_cleaner

2.9
./AI/vaccum_cleaner
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 2000
World: TwoSquare { left: Clean, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 2000
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1998
World: TwoSquare { left: Clean, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Left }, Score: 1999
World: TwoSquare { left: Dirt, right: Clean }, agent: ReflexCleaner { pos: Right }, Score: 1998
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Left }, Score: 1996
World: TwoSquare { left: Dirt, right: Dirt }, agent: ReflexCleaner { pos: Right }, Score: 1996

2.10
a.  it cannot
    when there's initially no dirt in the environment, the simple reflex agent will move mindlessly
    an agent that does nothing at all will achieve score 2000 instead of 1000 by the simple reflex agent
b.  refer 2.2
c.  refer 2.2

2.11
./AI/vaccum_cleaner
a.  it cannot, a simple deterministic agent will only clean dirt on a fixed path
    if the initial dirt distribution generates no dirt along that path, no dirt will be cleaned by the reflex agent
    another agent may outperform it by cleaning even one square of dirt (by chance or better perception)
b.  it's possible as stated in part a
    results are concluded in part d
c.  a spiral environment
    @: start position of agent
    ##########
    #%%%%%%%%@
    #%########
    #%#%%%%%%#
    #%#%####%#
    #%#%#%%#%#
    #%#%%%%#%#
    #%######%#
    #%%%%%%%%#
    ##########
    after cleaning the square, the random cleaner will bump into walls frequently
    dispite there are only one correct direction to progress
d.  a stateful agent that records the visited part of the environment, decide the next moving direction by
        1. prioritize unvisited squares
        2. choose a non-obstacle square if surrounded by visited squares
    % ##%%# %%
    # # % % %#
    %%  %  %
    %   #  %
    %  #    %
    # %%#  %
    %###%#%###
    %% %% % #
    %#%#%  % #
    % #   #
    Bot position: (1, 0)
    RandomCleaner Score: 68262
    Stateful BumpCleaner Score: 73193
    #  %#% %
    % %# %
    %     %%
    # % %#% %%
    %% % #% %
    ## #%   %%
    % #% #%% %
    # % %%%
    % % %  %
    %#%% % # %
    Bot position: (4, 1)
    RandomCleaner Score: 70134
    Stateful BumpCleaner Score: 78198
    %  #  # %
    % # %  %
    ####  % %%
    %
    %%   %%%
    % #%#%% %
    #  %% %
    %%   % %
    %     %#%#
    ##%% % % %
    Bot position: (4, 8)
    RandomCleaner Score: 74664
    Stateful BumpCleaner Score: 80026
    %# %%  %#
    # #%%%#
    %  %#%%%#
    #   %  %
    %%# %
    %% #%%%%%%
    %%#%  %%#
    %%%%   %%%
    %%   %% %
    %% %% #%
    Bot position: (4, 3)
    RandomCleaner Score: 73039
    Stateful BumpCleaner Score: 82044
    #    %
    # %#% #%%#
    %%% % %%
    % % % % %
        # #  %
    %%% #%
    % % #  % #
    %%# %%
    %#%  %%%
    #  %   %
    Bot position: (6, 6)
    RandomCleaner Score: 74993
    Stateful BumpCleaner Score: 80545
    a cleaner cannot be perfectly rational without full percept of the environment

2.12
the agent in 2.11.d is already based on bump detection
if the bump detector fails, the cleaner should fall back to randomized reflex mode

2.13
a.  repeat the Clean action until the the dirt sensor gives "clean" twice in a row
b.  again a cleaner cannot be perfectly rational without full percept of the environment
