// solutions to practice exercises:
// http://os-book.com/OS9/practice-exer-dir/
// whenever possible C programming assignment will be done in Rust

1.1
as stated in the text,
    "In general, we have no completely adequate definition of an operating system."
more or less an operating system has to
    1.  provide an interface of hardwares to softwares (may not be the case for embedded systems)
    2.  run the user applications in a safe manner (again may not be the case for embedded systems)
    3.  manage hardware resources

1.2
1.  when strictly uniform performance is of importance
    e.g. the operating system has to perform exactly the same on different hardwares, maybe for real-time purpose
2.  for security reasons
    e.g. the recent Intel CPU exploits which operating systems sacrifices ~15% performance to patch
3.  to provide a backward-compatible user interface to softwares
    the operating system have to support legacy softwares and run in a mode designed for hardwares decades ago 
all these "waste" are necessary to fulfill their design requirements

1.3
the running time of all the instructions must be deterministic, which means:
    GC is almost impossible: the operating system cannot randomly pause for a milliseconds and collect garbages
    the language in use must have a very strict semantic model, mapping program to instructions in a well-defined manner
    some probabilistic algorithms and data structures cannot be utilized, e.g. hashtable

1.4
should:
    web browser is an absolutely necessary utility which should be provided by an operating system out-of-box
    compiling an entire web browser is both painful and difficult, web browsers are usually distributed in binary
shouldn't:
    it isn't really part of an operating system, web browser definitely doesn't run in kernel mode
    for linux distros most of their users should know how to fetch a web browser with wget or curl

1.5
user applications cannot run kernel mode privileged instructions directly which may potentially interfere the operating 
system and other user applications
they may only delegate access to these instructions to the operating system by means of well-defined syscalls
so these syscalls can be analyzed and verified by the operating system in the context of processes

1.6
a.  privileged, otherwise a user application can hold the CPU forever
b.  not privileged
c.  privileged, otherwise user applications can interfere with each other unchecked
d.  not privileged, by design
e.  privileged, otherwise a user application can deny syscalls from other applications and hold the CPU forever
f.  privileged, hardware is managed by the operating system, not directly by the user applications
g.  privileged, otherwise renders the whole dual-mode system useless
h.  privileged, IO devices are managed by the operating system, user applications should use syscalls

1.7
1.  the operating system, loaded into memory by the firmware on startup from secondary storage, cannot be modified by 
    the operating system itself, i.e. no update
2.  the protection is not complete: an operating system is not entirely static, it has to hold dynamic memories 
    for e.g. file system

1.8
ring -3: Intel ME, a separated chip beside Intel CPU which control and monitor the entire computer from beneath
ring -2: processor microcode, under machine code instructions, the real instructions running on the chip
ring -1: hypervisor of the virtual machine environment, manage multiple virtualized operating systems   
ring 0: kernel, operating system

1.9
starting from a time set by the user or synced from a remote server:
    the process asks the kernel to wake it a fixed time later by a syscall and go to sleep
    the operating system schedules the process to run after the fixed time
    the process upon wake up and increment the time by the fixed time

1.10
cache is useful due to the fact that:
    they are faster in term of access time compared to the storage on levels below
    by caching multiple small read / write can be performed in batch, possibly extending the lifetime of the storage 
    if it can only survive a finite time of read / write operations
cons: caching complicates access to the storage, also cache invalidation is one of the only two hard things in compsci
even if the cache is as big as the storage device, it may not be persistent, the content still have to be written to the 
storage device before power off

1.11
in client-server distributed systems hosts either generate or satisfy requests, in peer-to-peer distributed systems
every host do both of them

1.12
a.  1.  without proper isolation one user may have access to all the sensitive contents belongs to another user
    2.  critical failure of one user process may affect or even terminate other processes 
b.  virtualization may be able to provide the security, but in practice there's always another exploit yet to be found

1.13
a.  CPU, memory, storage devices
b.  CPU, memory, IO devices, network links
c.  (in addition to workstations) battery 

1.14
1.  when the user only use no more than a fraction of the hardware capacity, part of a time-sharing system can be 
    rented instead of an entire PC or workstation
2.  when the user works in a group and have to share read / write access to some files within the group, it's cheaper to 
    work on a time-sharing system then doing so via network links

1.15
symmetric: 
    all CPU cores are equal in functionality, with the same amount of SRAM cache, computing power and access to the 
    main memory, tasks are distributed in software level as well as communication between cores
    common design of desktop CPUs
asymmetric: 
    CPUs have different functionality: a main CPU controls how work is distributed among worker CPUs
    software software communicates solely to the main CPU
    common design of mobile CPUs
pros (compared to multiple single-processor PC):
    cheaper, no duplicated IO device and IO bus
    communication / memory sharing among processors are faster by magnitudes 
    easier to program: just spawn threads and the operating system can distribute them evenly among processors
cons:
    it is a single PC with a single set of IO device at the end, cannot be used by two people at the same time

1.16
instead of multiple processors managed by a single operating system, clustered systems "are composed of two or more 
individual systems—or nodes—joined together"
hardware: a network, wired or wireless
software: a distributed operating system, running on hosts and monitors each other

1.17
1.  duplicate each write to the two nodes, distribute reads evenly, higher availability, half efficiency
2.  distribute write evenly between two nodes, properly guide reads to the correct node (e.g. by DHT)
    full efficiency, a hardware failure permanently wipes data

1.18
a network computer (thin client) passes most of its tasks, which would be computed locally by a traditional PC, to a 
remote server assigned to it
by handing over its tasks, network computer consumes less energy thus has longer battery lifetime
if the data is stored in the server as well, users may access their data from different terminals seamlessly

1.19
to inform the CPU about a software / hardware event so it can handle it
traps are software generated interrupts, indicating either an software error (e.g. divide by zero) or a request of 
operating system service (syscalls)

1.20
a.  CPU sets up buffers, pointers and counters for the DMA controller, which copies a whole block of data into the
memory accordingly, meanwhile the CPU can do something else
b.  by a hardware interrupt
c.  thanks https://en.wikipedia.org/wiki/Direct_memory_access#Cache_coherency
    the memory transfer is invisible to the CPU before completion, DMA may cause cache incoherency between CPU cache and 
    the main memory 

1.21
the published instruction sets may be an abstraction over the real instructions running on the hardware (microcode), 
hence a dual-mode instruction set can be build on a mode-less hardware
such an architecure may be exploitable to adversaries with physical access to the computer

1.22
to better handle the typical computing tasks: in practice user applications works on both shared and isolated memories

1.23
depends on the memory ordering of read / write accesses, a write operation performed by one processors may or may not 
be visible to other processors immediately
refer https://doc.rust-lang.org/stable/std/sync/atomic/enum.Ordering.html

1.24
a.  DMA may perform memory write temporarily invisible to CPU and cache
b.  different cores may perform independent read / write thus have different cache before synchronization
c.  same to multiprocessor

1.25
dual-mode instructions
memory access is privileged instruction, user applications can only perform memory access through syscalls, operating 
system maintains memory region of each process, upon receiving a syscall examines if the process is accessing its own 
memory or not, if not return the the infamous segmentation fault error to the user application

1.26
a.  LAN, all devices are physically reachable by a network administrator
b.  WAN
c.  WAN, a neighborhood in general is not an organization

1.27
limited power supply
limited computing power
different IO devices (touch screen instead of keyboard & mouse)
higher requirement of stability (phones are rarely turned off)

1.28
higher availability: client-server architecure has a single point of failure which is the server
better scalability: peer-to-peer architecure in nature is more scalable compared to extending the capacity of a server 

1.29
file distribution (e.g. bittorrent)
video streaming 
network traffic tunneling (e.g. TOR)

1.30
pros:
    more participants, earlier bug detection, more agile response to needs (for developers)
    lower cost (for consumers)
    provides learning material to students
cons:
    bigger attack surface
    copies cannot be sold

2.1
provide an compatible and easy-to-use interface to user softwares and higher layer system components
hide the real implementation of system functionalities which is subject to change

2.2
// section 1.6
create and deletion
suspension and resumption
synchronization
communication
deadlock handling

2.3
// section 1.7
keeping track of which parts of memory are currently being used and who is using them
deciding which processes (or parts of processes) and data to move into and out of memory
allocating and deallocating memory space as needed


2.4
// section 1.8
free space management 
storage allocation
disk scheduling

2.5
to provide a programmable, concise and unambiguous user interface to run system and user applications
it's not implemented as a module of the kernel because it doesn't have to, a shell can be built on top of syscalls

2.6
fork() to create a process inheriting the stack of the current one

2.7
enhance modularity of the operating system, part of the operating system can be built upon the kernel instead of in it

2.8
modularity, the operating system can be implemented, reasoned and debugged one part of a time
but layered approach increases the overhead of syscalls

2.9
GUI
    human friendly access to user and system programs
    a user program in Linux land but a system program in Windows and OSX
    in a closed-source OS, the GUI usually is hard-coded into the system
program execution
    OS essential
    user program cannot access the layer under syscall
IO operation
    OS essential
    user program cannot access the layer under syscall
file system manipulation
    organize file in a human readable way
    it must be part of the OS, user program cannot define their own flavor of file system on the one provided by the OS
resource allocation
    OS essential
    kind of doable in user land if the OS exposes lower level syscalls to memory management functions
    earlier version of Rust used a custom memory allocator instead of the OS default

2.10
in that way the OS doesn't have to be loaded into the main memory
those hardwares simply have no enough memory to load the system

2.11
operating systems write their location on the disk to a special boot block on the disk during installation
on startup the boot loader reads these locations then let the user choose one from them

2.12
1.  UI and exposed API: load, IO, file system, communication, error detection
2.  internal system functions: resource allocation, resource accounting, memory protection
functions in the first category is visible to the user in form of exposed API and syscalls
functions in the second category is not visible but a promise the OS makes to the user

2.13
1.  put in a predefined sequence of registers, e.g. first few parameters in Linux ABI
2.  pushed to stack, e.g. after a few parameters in Linux ABI
3.  stored in a table in the memory

2.14
build the program with debug symbols, let the operating system interrupt the program frequently with a timer, record
where the execution is each time the program is interrupted
without time profiling optimization of a program of decent complexity is nearly impossible

2.15
// section 1.8.1
creating and deleting files
creating and deleting directories
file and directory manipulating primitives
mapping files onto secondary storage (mmap?)
backup to stable storage media

2.16
pros:
    these two concepts share a lot of resemblance
    a common API to both of them simplifies the system design and its interface
cons:
    not all file operations are possible to all devices
    e.g. seek a tty device results in an error
    certain devices are too danger to be manipulated as a file, pipe something to /dev/hda will wipe the boot block

2.17
possible if the default one is implemented in user land, i.e. all it does is call system programs

2.18
message passing: safer with bigger overhead and requires extra memory
shared memory: faster but must be done right carefully or will cause data race / deadlock

2.19
like any other program, the resulting operating system will be more configurable

2.20
device driver and memory management
memory management must be built upon drivers, but drivers may also need access to the memory

2.21
all but the bare minimum functions of the operating system is implemented as separate programs
user program requests services from the kernel
which in turn passes these requests to the corresponding module program
kernel components in microkernel architecure communicates by inter-process communication, much slower than function
calls in the same program

2.22
so these functions can be implemented and compiled separately and dynamically
installation of a new device can be done without recompiling or even restarting the system

2.23
they are both mobile operating system, serving roughly the same set of hardware and market 
iOS is closed-source, Android is partially open-source

2.24
https://web.archive.org/web/20080604070631/http://blogs.sun.com/jrose/entry/with_android_and_dalvik_at
main reason listed: GPL of openJDK, power consumption of JVM, efficiency of assembly v.s. Java bytecode

2.25
https://www.semanticscholar.org/paper/An-Overview-of-the-Synthesis-Operating-System-Pu-Massalin/c8f49d5d94e1f9adf608b7c99e1e731c2381d82a
Factoring Invariants method
    if a system service is called frequently with the same parameters, the execution path of the call is cached and
    carried out next time the same service is called
Collapsing Layers method
    lower layer of services can be invoked directly in execution
Executable Data Structures method 
    traversal order is stored alongside the data in data structures
faster syscalls in exchange of less flexible operating system architecure

2.26
./OS/posix-file-copy
sudo dtrace -n 'syscall:::entry /execname == "posix-file-copy"/ { @[probefunc] = count(); }'
dtrace: description 'syscall:::entry ' matched 535 probes
^C
  __mac_syscall                                                     1
  access                                                            1
  bsdthread_register                                                1
  exit                                                              1
  fcntl                                                             1
  getrlimit                                                         1
  issetugid                                                         1
  shared_region_check_np                                            1
  sysctlbyname                                                      1
  thread_selfid                                                     1
  close                                                             2
  csops_audittoken                                                  2
  fcntl_nocancel                                                    2
  fsgetpath                                                         2
  fstat64                                                           2
  getentropy                                                        2
  getpid                                                            2
  proc_info                                                         2
  read_nocancel                                                     2
  write                                                             2
  ioctl                                                             3
  read                                                              3
  csrctl                                                            4
  open                                                              4
  mprotect                                                         10
  stat64                                                           42

3.1
PARENT: value = 5
child process inherits a copy of stack of the parent process, modification in the child process will not reflect to the 
parent process

3.2
8

3.3
context switching 
short-term process scheduling  
accounting of process memory and inter-process communication

3.4
a special syscall switches the CPU to the other register set
some register set has to be swapped out to the memory to make room for the new context

3.5
only c. shared memory segments, others are copied

3.6
similar to the design of TCP
=>  request
<=  process call
    ACK (lost)
=>  request (resend)
<=  duplicate sequence number, ignored
    ACK
=>  ACK received

3.7
retransmission

3.8
short-term scheduling:
    controls which process in the ready queue could run and allocates CPU time to it
    executes once every ~100ms
medium-term scheduling:
    controls which process should be kept in memory 
    swaps a process out from memory is it's waiting on an IO event or others
    executes on every major IO event
long-term scheduling:
    controls in which order pooled processes could run
    executes every time a process terminated

3.9
saves the context of the current process to its PCB structure
loads the context of another process from its PCB structure

3.10
// from a fresh WSL debian
init(1) -> init(2)
init(2) -> bash
bash -> ps

3.11
when a process is terminated without waiting on its child processes, its child processes become orphans
the OS assigns init as the new parent of these orphan processes
init calls wait periodically so these orphan processes can be terminated on completion

3.12
16

3.13
in the child process if the call to fork succeeded

3.14
A: 0
B: 2603
C: 2603
D: 2600

3.15
ordinary pipe:
    message passing between parent and child processes
named pipe:
    abstract over the functionality of some program to provide an standard file interface to other programs
    e.g. a pipe that compresses the data passed to it and store them in a file
        // https://en.wikipedia.org/wiki/Named_pipe
        mkfifo my_pipe
        gzip -9 -c < my_pipe > out.gz &

3.16
at most once:
    an RPC service on an embedded system controlling a light bulb
    the bulb blinks instead of being switched on / off
exactly once:
    a distributed file system
    a write operation may be lost due to connection failure
any use case of UDP is also a use case of unreliable RPC

3.17
CHILD: 
    0, -1, -4, -9, -16
PARENT:
    0, 1, 2, 3, 4

3.18
a.  synchronous communication:
        if blocks on both ends the messages are passed from producer to consumer directly, no buffer required
        same as other blocking operations wastes cycles
    asynchronous communication:
        easier to use for programmers 
        the messages have to be buffered somewhere
b.  either the programmer or the kernel has to carry the burden of buffering
c.  extra memory copy & occupation v.s. danger of dangling pointers & data race
    OS must be able to mark memory as shared in the latter case
d.  fixed-sized messages: easier to implement but harder to use
    variable-sized messages: the opposite

3.19
./OS/process/zombie-child
% ps -l
  UID   PID  PPID        F CPU PRI NI       SZ    RSS WCHAN     S             ADDR TTY           TIME CMD
  501  2037  1916     4006   0  31  0  4297288   2136 -      Ss                  0 ttys001    0:00.03 /bin/zsh -l
  501  2279  2037     4006   0  31  5  4268544    740 -      SN                  0 ttys001    0:00.05 target/debug/zombie-child
  501  2289  2279     2006   0   0  5        0      0 -      ZN                  0 ttys001    0:00.00 (zombie-child)

3.20
// skipped, would be a simple wrapper around a hash set in this stage

3.21
./OS/process/collatz-child

3.22
./OS/process/collatz-child-shm
apparently there's no way to list and remove shared memories in OSX which one forget to unlink

3.23 - 3.25
// skipped, entry level application layer programming

3.26
./OS/process/case-reverse-pipe

3.27
./OS/process/file-copy-pipe

Project 1 — UNIX Shell and History Feature
./OS/process/unix-shell

4.1
1.  a web server, single-threaded server can only handle one client at a time even though each client only need a tiny 
    amount of computation power and is waiting for IO most of the time
2.  video compression on a multi-processor computer, a single-threaded program can only ultilize one of many cores, 
    even middle-range CPUs today has dozens of cores

4.2
user threads are
    1.  directly created by user program or a thread library
    2.  managed solely by user space code
where kernel threads are
    1.  created by the operating system in a pre-defined model (M:M, M:N or M:1) to meet the request of user threads
    2.  managed by kernel space code

4.3
store and load stack, register and thread local storage
in contrast to process context switch, file system, VM space, file descriptors and signal handlers are not switched

4.4
same as above, in general fewer memory is allocated for threads than for processes 

4.5
it must, otherwise one thread blocking on IO will block the other threads on the same LWP as well
this kind of blocking and pause is unpredictable and unacceptable in real-time system 

4.6
1.  when there's only one processor, any CPU bound program will not have better performance with multithreading
2.  when the program is IO bound but they all access the same IO device and the IO device can only serve one thread at
    a time in a very slow pace, multithreading will not provide any significantly better performance

4.7
when they are IO bound accessing different IO devices, instead of the sum of the IO response time, only the maximum of
the IO response time would be necessary

4.8
b and c

4.9
when they are CPU bound, on a multi-processor system all cores can be utilized

4.10
if the program is carefully designed so that the error in client script won't corrupt memory shared among threads
the isolation may still be achieved if the signals can be handled locally to the thread

4.11
as stated in the text: multithreaded program in a single processor system

4.12
a.  10 / 7
b.  20 / 11

4.13
a.  only task, the same data set is shared
b.  both task and data
c.  both task and data
d.  task but not data, the same data (web pages) are accessed by each thread

4.14
a.  one, this task won't benefit from multithreading
b.  depends, if the tasks are uniform 4 is enough, otherwise finer granularity is necessary or some CPU would be idle

4.15
a.  6
b.  2, or 4 if threads are duplicated by fork()

4.16
as stated in the text, the former manages data associated to a task (process or thread) separately , storing only 
pointers to the data in the PCB, while the former has different data structure for processes and threads

4.17
CHILD: value = 5, global variables is shared (unsafely) among threads
PARENT: value = 0, each process has its own copy of global variables

4.18
a.  the full computing power of the system will not be made use of
b and c may not be distinguishable from the view of the program

4.19
1.  resource release e.g. closing a file descriptor
2.  write to a log file

// about the difference between Pthread API and Rust thread API:
// they are semantically equivalent but:
// 1. threads in Pthread take input by a void pointer on thread spawning, while Rust threads are a closures
// 2. Rust thread can return a Sized value, which can be emulated with Pthread API by allocate a dedicated chunk of 
//    memory for the thread to store the return value and later accessed by the main thread
4.20
./OS/process/pid-manager
a lock (mutex) is necessary to prevent data race
not sure if such a lock or variable length vector is available in kernel space

4.21
./OS/process/multi-thread-statistic

4.22
./OS/process/monte-carlo-pi

4.23
// skipped

4.24
./OS/process/prime-numbers

4.25
// skipped, similar exercise in CSAPP

4.26
./OS/process/fibonacci

4.27
// skipped, similar exercise in CSAPP

Project 1

Project 2
// skipped, similar exercise in CLRS

5.1
system clock is implemented by periodical clock interrupts 
if interruption is disabled system-wise, the clock interrupts may be delayed, the assumption that these interruption 
occurs once a fixed time period no longer holds
either interruptions of the clock thread should not be disabled for too long, or the system time should be synchronized 
to a remote server from time to time

5.2
these synchronization primitives have different semantics and diffferent costs
spin lock:
    busily wait by testing and modifying a shared variable atomically 
    used when the critical section is short, i.e. syscalls and interrupts are significant overheads in comparison
mutex lock:
    semantically equivalence of spin lock provided by the kernel
    instead of atomic instructions in a busy loop, the threads waiting for the lock is put to sleep and awaken later
    more efficient than spin lock if the critical section is long
semaphore:
    similar to mutex lock, but instead of a binary lock (one thread working, all other threads waiting), (a fixed
    number of) multiple threads can enter the critical section at the same time
    used when a fixed number of resources have to be managed across threads
adaptive mutex lock:
    a hybrid implementation of lock as both a spin lock and a mutex lock
    when the lock is held by a working thread, threads blocking on waiting spin
    when the lock is held by a sleeping thread, threads blocking on waiting is put to sleep too
condition variable:
    a synchronization mechanism defined by two methods: .wait() and .signal()
    x.wait() blocks the current thread until another thread calls x.signal()
    x.signal() awakes a thread or all threads blocking on x.wait()

5.3
waiting by busily trying to acquire a lock in a loop
alternatively a thread can be put to sleep (by a syscall or a kernel mutex lock) and be awaken later by the OS when the
resources become available
it's not always efficient to avoid busy waiting, these syscalls and interrupts have their own costs

5.4
usually the kernel cannot tell the difference between a thread doing meaningful job and a spinning thread
on single-processor system the spinning thread when active will consume 100% of the computing power of the CPU
also for a resource to be released, the spinning thread must be switched out for another thread to make progress
on multi-processor system the spinning thread will only consume 1/N of the computing power where N is the number of 
processors in the system
the resource may be released by another thread running on another processor, switching out the spinning thread may be
much more expansive compared to spinning a few hundred instructions 

5.5
without atomic operations the memory order may be arbitrary, decrementation of the counter in one thread may not be 
visible from another thread in time, two or more threads may acquire the resource guarded by a binary semaphore

5.6
a binary semaphore is semantically equivalent to a mutex lock

5.7
assume the instructions are interleaved in order:
LD R0 BAL   LD R1 ACC
ADD R0 A0   
            SUB R1 A1
ST R0 BAL   
            ST R0 BAL
the BAL memory location storing the account balance may have value BAL - A1 or BAL + A0 while it should have value
BAL + A0 - A1
the operations to the same account must be guarded by a mutex lock

5.8
assume turn is initialized to either i or j (otherwise the program causes UB)
assume all operations on variable turn and flag[i] are atomic
mutual exclusion:
    Pi will only enter its critical section when flag[i] && !flag[j] and vice versa
    the two conditions cannot be satisfied at the same time
progress:
    if Pj is absent when Pi is trying to enter the critical section, flag[i] && !flag[j]
    otherwise flag[i] && flag[j]
    if turn == i Pi spins, otherwise Pi set flag[i] = false hence Pj can make progress and set turn = i on exit
bounded waiting:
    when two threads compete for the resource, only the thread pointed by variable turn can make progress
    on exit turn is set to another thread, so a blocking thread at most have to wait for another thread to exit

5.9
// thanks https://personal.cis.strath.ac.uk/sotirios.terzis/classes/CS.304/Remaining%20Contemplation%20Questions.pdf
mutual exclusion:
    the first thread Pi entering critical section has checked that 
        1.  itself has state in_cs
        2.  all other threads has state != in_cs
    any thread entering state in_cs later cannot exit the first while loop
progress:
    if multiple threads are in state want_in, all but the one immediately after turn is stuck in a spin lock
bounded waiting:
    on exit, threads increments turn by at least 1 (skips idle threads)
    a waiting thread cannot be skipped, after at most n - 1 turns this thread will be served

5.10
in single-processor system it's important that a single thread should not hold the CPU too long, which is enforced by 
a timer, otherwise a dead lock will freeze the entire system
disabling all interrupts will disable the timer interrupt at well, let a single thread hold CPU indefinitely

5.11
disabling / enabling interrupts to all threads on demand of a single thread is way too expensive

5.12
// thanks https://stackoverflow.com/questions/4752031/
trying to acquire a semaphore may put the thread to sleep, which defeats the purpose of spin lock: spin locks should 
only be used if the critical section is short, sleeping with a spin lock will block all other threads trying to acquire
the spin lock for a long time

5.13
linked list:
    if a thread is deleting a node when another thread is trying to insert a node before it, the pointer to the previous
    mode may be dangling, the inserting thread would be dereferencing NULL pointers and cause segfault
bitmap:
    setting the same field to both 0 and 1 in different threads may have varying result depends on the order of their 
    instructions

5.14
assign id 1 to n to n threads 
each thread acquire the lock by
    while (compare_and_swap(&lock, i, 0))
and set the lock to the next non-idle thread based on a flag[n] array

5.15
void acquire(lock *mutex) {
    compare_and_swap(&lock->available, 0, 1);
}
void release(lock *mutex) {
    test_and_set(&lock->available, 0);
}

5.16
instead of while loop, acquire invokes a syscall with the lock (implemented in the kernel), the kernel then put the 
thread to sleep and insert it into a waiting list associated to that lock

5.17
short duration: spin lock
long duration: mutex or spin lock, depends on the number of cores available and the time 
sleep while lock: mutex by 5.12

5.18
2T

5.19
the second one is much more efficient, the variable hit is never read, its value is of no importance to the service 
threads, a mutex lock may put threads to sleep while atomic operations will be compiled to a hardware instruction 

5.20
a.  all operations on number_of_processes may cause data race, the real number of processes exceed MAX_PROCESSES
b.  before each read and write to number_of_process
c.  without a mutex lock two threads may call allocate_process() at the same time when 
        number_of_processes == MAX_PROCESSES - 1
    the maximum number of processes will be exceeded

5.21
initialize a semaphore to the maximum number of sockets
a thread opening a connection has to acquire the semaphore at first, block if the semaphore is 0

5.22
such a lock may be useful when concurrent quick read is frequent, write is sparse
if write is as frequent as read or the read operating is long, the threads trying to write may be starving

5.23
assume read operation to semaphore is atomic
typedef struct {
    boolean block;
    int count;
} semaphore;
void wait(semaphore *lock) {
    while(true) {
        while (test_and_set(&lock->block))
            ;
        // acquired lock
        if (lock->count > 0) {
            lock->count -= 1;    
            break;
        }
    }
}
void signal(semaphore *lock) {
    while (test_and_set(&lock->block))
        ;
    lock->count += 1;
}

5.24
the child thread must inform the parent thread about a ready number in some means (e.g. condvar), or while very 
inefficient the parent thread can check an atomic count of ready numbers in a loop
./OS/process/fibonacci-iterative

5.25
mutual exclusion of a monitor can be emulated by a semaphore guarding all methods of that monitor

5.26
// skipped, straight forward

5.27
// thanks solutions manual
a.  if the content is lengthy, copying it to / from the buffer may take considerable time, another thread trying to 
    read or write the buffer must wait in between 
b.  instead of produced buffer elements, the buffer may store pointers to these elements, copying these pointers would
    be far more efficient
    threads must be careful that these memory region should not be accessed once its put into the shared buffer 

5.28
in a common use case of read-write lock, there will be much more read threads than write threads
if the lock does not favor write threads, a write attempt will block until there's no thread reading the resource
such an event may simply never happen if the resource is read frequently
a possible solution is, if there are write threads waiting on the lock, all incoming read threads are blocked
in this case write threads are will starve but read thread may

5.29
calling signal() on a conditional variable awakes one thread blocking on the corresponding wait() call, it there's no 
threads blocking on the conditional variable the call to signal() has no effect
in contrast calling signal() on a semaphore will always release one allowance to the poll

5.30
threads calling signal() no longer has to wait the awakened thread to exit
all operations on the next semaphore and next_count variable can be eliminated

5.31
along with the resources and conditional variable, the monitor maintains a waiting priority queue 
when no printer is available, incoming threads enter the waiting queue and wait on the conditional variable
whenever a printer is released, all threads waiting on the conditional variable are awaken (pthread_cond_broadcast)
they in turn check if they are the top of the priority queue and there's printer available, if so they acquire a 
printer, otherwise they call wait() again

5.32
with Rust Condvar and Mutex:
struct FileSharing {
    sum_lock: Mutex<u32>,
    cv: Condvar,
    limit: u32,
}
impl FileSharing {
    fn new(limit: u32) -> Arc<Self> {
        Arc::new(Self {
            sum_lock: Mutex::new(0),
            cv: Condvar::new(),
            limit,
        })
    }
    fn acquire(&self, num: u32) {
        let mut sum = self.sum_lock.lock().unwrap();
        while *sum + num >= self.limit {
            sum = self.cv.wait(sum).unwrap();
        }
        *sum += num;
        // acquired file, enter critical section somewhere else
    }
    fn release(&self, num: u32) {
        let mut sum = self.sum_lock.lock().unwrap();
        *sum -= num;
        drop(sum);
        self.cv.notify_all();
    }
}

5.33
upon release the calling thread is no longer holding shared resources, so it makes more sense to continue execution in 
this case, if the control is transferred to the notified thread the calling thread may have to call release() in 
async manner, e.g. spawn another thread to call it

5.34
a.  monitor rw_lock {
        int read_count;
        int write_count;
        initialize() {
            read_count = 0;
            write_count = 0;
        }
        read() {
            await(write_count == 0);
            read_count += 1;
            // critical section
            read_count -= 1;
        }
        write() {
            await(read_count == 0 && write_count == 0);
            write_count += 1;
            // critical section
            write_count -= 1;
        }
    }
b.  there's no mechanism there to notify the threads and the await() function that the value of the expression has
    changed, the expression has to be evaluated from time to time even if the expression is expensive
    also the system instead of the user program has to decide which thread to awake first when the condition is 
    satisfied, without specific knowledge of the user program the decision made by the system may not be efficient
c.  the paper in mention is paywalled 
    maybe the expression has to be simple boolean expressions of local variables so its value can be tracked

5.35
with Rust Condvar and Mutex:
struct AlarmClock {
    clock_lock: Mutex<u32>,
    cv: Condvar,
}
impl AlarmClock {
    fn new() -> Arc<Self> {
        Arc::new(Self {
            clock_lock: Mutex::new(0),
            cv: Condvar::new(),
        })
    }
    fn wait(&self, ticks: u32) {
        let mut clock = self.clock_lock.lock().unwrap();
        let now = *clock;
        while clock.wrapping_sub(now) < ticks {
            clock = self.cv.wait(clock).unwrap();
        }
    }
    fn tick(&self) {
        let mut clock = self.clock_lock.lock().unwrap();
        *clock = clock.wrapping_add(1);
        self.cv.notify_all();
    }
}

5.36
// already done in 4.20
./OS/process/pid-manager

5.37
a.  available_resources
b.  two call to either decrease_count() or increase_count() may cause inconsistent write to the variable
c.  ./OS/process/resource-counter

5.38
./OS/process/resource-counter

5.39
./OS/process/monte-carlo-multi

5.40
// skipped, no OpenMP in Rust

5.41
./OS/process/barrier-point

Programming Project 1
./OS/process/sleeping-ta

Programming Project 2
./OS/process/dinning-philosophers

Programming Project 3
// skipped, simply a VecDeque in a Mutex

6.1
n! if the system is non-preemptive
(Σti / q)! if the the system is preemptive, length of ith process is ti, the time quantum is q

6.2
non-preemptive scheduling will not pause a process and switch context unless it exits / waits on resources by a syscall
preemptive scheduling may pause a process midway to ensure fair share or to start a process of higher priority

6.3
a.  P1: 0 -> 8
    P2: 8 -> 12
    P3: 12 -> 13
    avg turnaround: 10.533
b.  P1: 0 -> 8
    P3: 8 -> 9
    P2: 9 -> 13
    avg turnaround: 9.533
c.  P3: 1 -> 2
    P2: 2 -> 6
    P1: 6 -> 14
    avg turnaround: 6.866

6.4
processes with high priority are usually interactive, a shorter time quantum will let it be executed more frequently
hence reduce the response latency
processes with low priority are usually CPU bound daemons doing heavy computing job, a longer time quantum will reduce
the time wasted on context switching

6.5
a.  SJF is a special form priority scheduling, priority is decided solely by burst time
b.  FCFS is a special feedback queue with a single queue running FCFS algorithm 
c.  FCFS is a special priority scheduling where all processes are assigned equal priority and ties are broken by FCFS
d.  different, RR is preemptive and SJF is non-preemptive

6.6
a process with low priority (e.g. a CPU-bound processes) will eventually be assigned higher priority since it's never
executed in the recent past due to its low priority

6.7
on systems with M:1 or M:N thread model, the thread library uses the PCS scheme to map user threads to LWP where threads
in a process compete for LWP, then the OS uses SCS scheme to map LWP to a physical CPU where LWPs system-wide compete
for the hardware
switch to SCS scheme on an M:1 or M:N system effectively turns its thread model to 1:1

6.8
same to 4.5

6.9
P1: 80
P2: 69
P3: 65
lower

6.10
the same to 6.4: IO-bound programs should have lower response time and CPU-bound programs should be preempted less 

6.11
a.  lower response time can be enforced by shorter time quantum and preemptive system, but these changes cause more
    context switch and lower CPU utilization
b.  as illustrated in 6.3, to minimize average turnaround time the OS should wait for a short period when there's only
    a few ready processes, which of course increases the average waiting time
c.  better IO utilization means more syscalls, more interruption and more context switch

6.12
more than one lottery tickets can be assigned to a process, increasing its winning probability accordingly

6.13
1.  pros:
        no shared memory, queues can be safely manipulated by the core at any time
    cons:
        without some other inter-core scheduling mechanism, the processes have to wait in the queue of a busy core even
        if another core is idle, result in worse load balance
2.  pros:
        load balance is naturally assured, all idle cores will get processes from the global queue
    cons:
        the queue must be protected by synchronization tools, access to the queue may be unsafe or at least take 
        longer time

6.14
a.  the prediction is always 100ms
b.  the CPU burst history has almost no effect on the prediction

6.15
CPU bound, a process which do not invoke a syscall for IO in its time quantum will have a higher priority and longer 
time quantum next time it's executed

6.16
a.  FCFS:
        0 -> 2: P1
        2 -> 3: P2
        3 -> 11: P3
        11 -> 15: P4
        15 -> 20: P5
    SJF:
        0 -> 1: P2
        1 -> 3: P1
        3 -> 7: P4
        7 -> 12: P5
        12 -> 20: P3
    priority:
        0 -> 8: P3
        8 -> 13: P5
        13 -> 15: P1
        15 -> 19: P4
        19 -> 20: P2
    RR (time quantum in order):
        P1, P2 (t = 1), P3, P4, P5, P3, P4, P5, P3, P5 (t = 1), P3
b.  see part a
c.  turnaround time - burst time 
    result skipped

6.17
// skipped, unclear how the priority should affect the RR schedule

6.18
if all the user threads are assigned a 0 nice value, decreasing the nice value of a process without proper permission 
may be considered as privilege escalation 

6.19
b and d, a if the system is non-preemptive and the first coming process does not halt

6.20
a.  the process is executed twice as frequent as other processes
b.  alongside the pointers store a time quantum specific to that process

6.21
a.  one round consists of: 11 CPU burst of length 1 and 11 context switch
    11 + 0.1 * 11 = 12.1 ms in total, CPU utilization is 11 / 12.1 = 0.909
b.  one round consists of: 10 CPU burst of length 1, 1 CPU burst of length 10, 11 context switch
    10 + 10 + 0.1 * 11 = 21.1 ms in total, CPU utilization is 20 / 21.1 = 0.948

6.22
assign higher on priority to their process on creation, invoke dummy syscalls to limit the length of CPU burst so the
processes remains in the higher priority queue

6.23
a.  the running process will have highest priority than all waiting processes, the earlier processes in the ready queue
    have higher priority than later processes, effectively FCFS
b.  the running process will be preempted by a new coming process, without new processes the running process cannot be
    preempted by waiting processes, new processes have the highest priority, a preemptive LIFO queue

6.24
a.  no discrimination at all
b.  no discrimination at all
c.  short processes will be assigned higher priority by their short CPU burst

6.25
// skipped, refer table 6.22

6.26
HIGH_PRIORITY_CLASS and HIGHEST

6.27
// skipped, refer table 6.23

6.28
// thanks https://en.wikipedia.org/wiki/Completely_Fair_Scheduler#Algorithm
both CPU bound:
    vruntime of A will decay faster than B
A IO, B CPU:
    A will have a slower increasing value of vruntime which also decays faster
    whenever A is ready B will be preempted after its time quantum
A CPU, B IO:
    A will have a faster increasing value of vruntime which decays faster
    potentially both A and B will have vruntime float around a limit, which depend on their burst time and nice value
    which ever have a lower stable vruntime value has higher priority

6.29
as stated in the text, when a higher priority process requires use of a shared resource currently locked by a lower
priority process, the lower priority process temporarily inherits the higher priority to prevent being preempted by
another process
a possible solution is to transfer the shares of the blocking high priority process to the locking lower priority
process so the lock can be released eariler 

6.30
priority of a process in rate-monotonic scheduling is fixed, when there are a huge number of processes CPU utilization
of rate-monotonic scheduling is limited to ~70% but EDF may be able to fully utilize the CPU

6.31
a.  P1 has higher priority
    0 -> 25: P1
    25 -> 50: P2 (5 remaining)
    50 -> 75: P1
    P2 miss deadline, cannot be scheduled by rate-monotonic scheduling
b.  0 -> 25: P1 (d = 50)
    25 -> 55: P2 (d = 75)
    55 -> 80: P1 (d = 100)
    80 -> 110: P2 (d = 150)
    110 -> 135: P1 (d = 150)

6.32
they may affect both response and turnaround time

7.1
1.  a traffic jam in which cars from both sides waiting for the other to yield
2.  a quarrel between two children, both waiting for the other to apologize
3.  a draw by threefold repetition in chess, both players waiting for the other to make a mistake

7.2
when processes do not require and release all the needed resources at once, i.e. the resources locked by a process can
be partially released so another process can make progress

7.3
./OS/process/banker
a.  0   0   0   0
    0   7   5   0
    1   0   0   2
    0   0   2   0
    0   6   4   2
b.  is safe
c.  can be granted immediately

7.4
there will be no hold and wait situation in this system: any process trying to acquire any resource must first acquire
F, but F is mutual exclusive thus only one process can acquire F, a process either hold F or no resource at all, but the
resource utilization is much lower than circular-wait scheme in which resources can be acquired by multiple processes

7.5
each iteration of step 2 and 3 takes O(mn) time, each iteration marks one finish[i] as true, O(n) iterations in total
O(mn^2) operations in total

7.6
a.  if the deadlocks can be completely avoided, the system will save 20 dollar per month
b.  the turnaround time is increased by 20%, if the jobs are time critical that would be unacceptable

7.7
if the synchronization tools used by the process is managed by the system (mutex, semaphore, about anything except a
spin lock), the system can scan periodically over the waiting list of a mutex and detect that a process is never run
for a long time

7.8
a.  no, the processes now can be preempted 
b.  a process may be constantly preempted by incoming new processes, never gain access to needed resources

7.9
when the state is unsafe, no process can make progress without another thread releasing its resources first, by 
definition the state is a deadlock

7.10
if the mutex lock is not recursive, the thread can try to lock the mutex twice and deadlock by itself

7.11
mutual exclusive: each intersection can only be occupied by traffics from one direction at a time
hold and wait: all the four line traffics are occupying an intersection and waiting another to be empty
no preemption: traffics cannot be removed from the intersection without passing it
circular wait: illustrated by the figure

7.12
possible, write attempts are still mutual exclusive

7.13
if only the process running do_work_one is preempted by process running do_work_two after
    pthread_mutex_lock(&first_mutex);
for e.g. fairness then the deadlock will occur

7.14
assign a unique total ordered id to the accounts, always lock the account with a smaller id first

7.15
a.  circular-wait scheme incurs less overhead than banker's algorithm, no quadratic or higher complexity procedure is 
    running periodically, the requests can be fulfilled or rejected in O(1) time
b.  throughput of circular-wait scheme is considerably lower than banker's algorithm as some rejections in circular-wait
    scheme are totally artificial, indicating no potential unsafe state or deadlock

7.16
a.  safe, ignore these new resources and the sequence is still safe
b.  unsafe, after a process releasing its resources the next process in the sequence may not be able to acquire needed
    resources 
c.  unsafe, same to b
d.  safe, the same set of resources can be allocated to the process anyway
e.  safe as long as they request resources through the deadlock avoidance system
f.  safe

7.17
by pigeon hole principle there will be at least one process being allocated two resources and can accomplish its job

7.18
for a deadlock each process must be holding less than necessary resources and there should be no resources available
the n processes can only be holding less than m + n - n = m resources, there should be resources available

7.19
use banker's algorithm and set max resource need to 2 for each philosopher 
or simply if it's the first request by a philosopher, there should be 2 chopsticks available

7.20
assume the philosopher holding most chopsticks is holding n, there should be 3 - n available in the system

7.21
let available = [1, 1], max = [[1, 1], [1, 1]], no request would be rejected but if
    P0 hold A and request B
    P1 hold B and request A
there may be a deadlock

7.22
./OS/process/banker
a.  unsafe, no safe sequence
b.  safe

7.23
./OS/process/banker
a.  (P0, P3, P4, P1, P2)
b.  can be granted
c.  cannot be granted, unsafe state and possible deadlock

7.24
processes that acquire resources will eventually releases these resources
when the programmer is careless a resource may not be released, the system may not be able to recover these resources
automatically on process exit

7.25
a state in mutex and a condvar
enum Bridge {
    South(u32), // south villager increment counter, north villager block on condvar
    Empty,      // both pass and change to state above or below
    North(u32), // south villager block on condvar, north villager increment counter
}
when a passenger reached the other end the counter is decremented by 1

7.26
the state additionally contains a traffic light which switches between two colors periodically (modified by a separate
thread or implicitly inferred from the system time by villager threads)
when the light is GREEN only south villagers can pass
when the light is RED only north villagers can pass

7.27
./OS/process/vermont-bridge

Programming Project
./OS/process/banker

